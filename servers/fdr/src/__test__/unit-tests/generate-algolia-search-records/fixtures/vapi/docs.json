{
  "config": {
    "navigation": {
      "tabs": [
        {
          "type": "group",
          "title": "Documentation",
          "icon": "book",
          "items": [
            {
              "type": "page",
              "id": "introduction.mdx",
              "title": "Introduction",
              "urlSlug": "introduction",
              "fullSlug": ["introduction"],
              "hidden": false
            },
            {
              "type": "section",
              "title": "General",
              "urlSlug": "general",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "section",
                  "title": "How Vapi Works",
                  "urlSlug": "how-vapi-works",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "quickstart.mdx",
                      "title": "Core Models",
                      "urlSlug": "core-models",
                      "fullSlug": ["quickstart"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "how-vapi-works.mdx",
                      "title": "Orchestration Models",
                      "urlSlug": "orchestration-models",
                      "fullSlug": ["how-vapi-works"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "knowledgebase.mdx",
                      "title": "Knowledge Base",
                      "urlSlug": "knowledge-base",
                      "fullSlug": ["knowledgebase"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Pricing",
                  "urlSlug": "pricing",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "pricing.mdx",
                      "title": "Overview",
                      "urlSlug": "overview",
                      "fullSlug": ["pricing"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "billing/cost-routing.mdx",
                      "title": "Cost Routing",
                      "urlSlug": "cost-routing",
                      "fullSlug": ["billing", "cost-routing"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "billing/billing-limits.mdx",
                      "title": "Billing Limits",
                      "urlSlug": "billing-limits",
                      "fullSlug": ["billing", "billing-limits"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "billing/estimating-costs.mdx",
                      "title": "Estimating Costs",
                      "urlSlug": "estimating-costs",
                      "fullSlug": ["billing", "estimating-costs"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "billing/examples.mdx",
                      "title": "Billing Examples",
                      "urlSlug": "billing-examples",
                      "fullSlug": ["billing", "examples"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Enterprise",
                  "urlSlug": "enterprise",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "enterprise/plans.mdx",
                      "title": "Vapi Enterprise",
                      "urlSlug": "vapi-enterprise",
                      "fullSlug": ["enterprise", "plans"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "enterprise/onprem.mdx",
                      "title": "On-Prem Deployments",
                      "urlSlug": "on-prem-deployments",
                      "fullSlug": ["enterprise", "onprem"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "page",
                  "id": "changelog.mdx",
                  "title": "Changelog",
                  "urlSlug": "changelog",
                  "fullSlug": ["changelog"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "support.mdx",
                  "title": "Support",
                  "urlSlug": "support",
                  "fullSlug": ["support"],
                  "hidden": false
                },
                { "type": "link", "title": "Status", "url": "https://status.vapi.ai/" }
              ],
              "skipUrlSlug": false
            },
            {
              "type": "section",
              "title": "Quickstart",
              "urlSlug": "quickstart",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "page",
                  "id": "quickstart/dashboard.mdx",
                  "title": "Dashboard",
                  "urlSlug": "dashboard",
                  "fullSlug": ["quickstart", "dashboard"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "quickstart/inbound.mdx",
                  "title": "Inbound Calling",
                  "urlSlug": "inbound-calling",
                  "fullSlug": ["quickstart", "phone", "inbound"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "quickstart/outbound.mdx",
                  "title": "Outbound Calling",
                  "urlSlug": "outbound-calling",
                  "fullSlug": ["quickstart", "phone", "outbound"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "quickstart/web.mdx",
                  "title": "Web Calling",
                  "urlSlug": "web-calling",
                  "fullSlug": ["quickstart", "web"],
                  "hidden": false
                },
                {
                  "type": "section",
                  "title": "Client SDKs",
                  "urlSlug": "client-sd-ks",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "sdks.mdx",
                      "title": "Overview",
                      "urlSlug": "overview",
                      "fullSlug": ["sdks"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "sdk/web.mdx",
                      "title": "Web SDK",
                      "urlSlug": "web-sdk",
                      "fullSlug": ["sdk", "web"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "examples/voice-widget.mdx",
                      "title": "Web Snippet",
                      "urlSlug": "web-snippet",
                      "fullSlug": ["examples", "voice-widget"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Examples",
                  "urlSlug": "examples",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "examples/outbound-sales.mdx",
                      "title": "Outbound Sales",
                      "urlSlug": "outbound-sales",
                      "fullSlug": ["examples", "outbound-sales"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "examples/inbound-support.mdx",
                      "title": "Inbound Support",
                      "urlSlug": "inbound-support",
                      "fullSlug": ["examples", "inbound-support"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "examples/pizza-website.mdx",
                      "title": "Pizza Website",
                      "urlSlug": "pizza-website",
                      "fullSlug": ["examples", "pizza-website"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "examples/outbound-call-python.mdx",
                      "title": "Python Outbound Snippet",
                      "urlSlug": "python-outbound-snippet",
                      "fullSlug": ["examples", "outbound-call-python"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "page",
                  "id": "resources.mdx",
                  "title": "Code Resources",
                  "urlSlug": "code-resources",
                  "fullSlug": ["resources"],
                  "hidden": false
                }
              ],
              "skipUrlSlug": false
            },
            {
              "type": "section",
              "title": "Customization",
              "urlSlug": "customization",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "page",
                  "id": "customization/provider-keys.mdx",
                  "title": "Provider Keys",
                  "urlSlug": "provider-keys",
                  "fullSlug": ["customization", "provider-keys"],
                  "hidden": false
                },
                {
                  "type": "section",
                  "title": "Custom LLM",
                  "urlSlug": "custom-llm",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "customization/custom-llm/fine-tuned-openai-models.mdx",
                      "title": "Fine-tuned OpenAI models",
                      "urlSlug": "fine-tuned-open-ai-models",
                      "fullSlug": ["customization", "custom-llm", "fine-tuned-openai-models"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "customization/custom-llm/using-your-server.mdx",
                      "title": "Custom LLM",
                      "urlSlug": "custom-llm",
                      "fullSlug": ["customization", "custom-llm", "using-your-server"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Custom Voices",
                  "urlSlug": "custom-voices",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "customization/custom-voices/custom-voice.mdx",
                      "title": "Introduction",
                      "urlSlug": "introduction",
                      "fullSlug": ["customization", "custom-voices", "custom-voice"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "customization/custom-voices/elevenlabs.mdx",
                      "title": "Elevenlabs",
                      "urlSlug": "elevenlabs",
                      "fullSlug": ["customization", "custom-voices", "elevenlabs"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "customization/custom-voices/playht.mdx",
                      "title": "PlayHT",
                      "urlSlug": "play-ht",
                      "fullSlug": ["customization", "custom-voices", "playht"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "page",
                  "id": "customization/custom-keywords.mdx",
                  "title": "Custom Keywords",
                  "urlSlug": "custom-keywords",
                  "fullSlug": ["customization", "custom-keywords"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "customization/knowledgebase.mdx",
                  "title": "Knowledge Base",
                  "urlSlug": "knowledge-base",
                  "fullSlug": ["customization", "knowledgebase"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "customization/multilingual.mdx",
                  "title": "Multilingual",
                  "urlSlug": "multilingual",
                  "fullSlug": ["customization", "multilingual"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "customization/jwt-authentication.mdx",
                  "title": "JWT Authentication",
                  "urlSlug": "jwt-authentication",
                  "fullSlug": ["customization", "jwt-authentication"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "customization/speech-configuration.mdx",
                  "title": "Speech Configuration",
                  "urlSlug": "speech-configuration",
                  "fullSlug": ["customization", "speech-configuration"],
                  "hidden": false
                }
              ],
              "skipUrlSlug": false
            },
            {
              "type": "section",
              "title": "Core Concepts",
              "urlSlug": "core-concepts",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "section",
                  "title": "Assistants",
                  "urlSlug": "assistants",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "assistants.mdx",
                      "title": "Introduction",
                      "urlSlug": "introduction",
                      "fullSlug": ["assistants"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "assistants/function-calling.mdx",
                      "title": "Function Calling",
                      "urlSlug": "function-calling",
                      "fullSlug": ["assistants", "function-calling"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "assistants/persistent-assistants.mdx",
                      "title": "Persistent Assistants",
                      "urlSlug": "persistent-assistants",
                      "fullSlug": ["assistants", "persistent-assistants"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "assistants/dynamic-variables.mdx",
                      "title": "Dynamic Variables",
                      "urlSlug": "dynamic-variables",
                      "fullSlug": ["assistants", "dynamic-variables"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "assistants/call-analysis.mdx",
                      "title": "Call Analysis",
                      "urlSlug": "call-analysis",
                      "fullSlug": ["assistants", "call-analysis"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "assistants/background-messages.mdx",
                      "title": "Background Messages",
                      "urlSlug": "background-messages",
                      "fullSlug": ["assistants", "background-messages"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Blocks",
                  "urlSlug": "blocks",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "blocks.mdx",
                      "title": "Introduction",
                      "urlSlug": "introduction",
                      "fullSlug": ["blocks"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "blocks/steps.mdx",
                      "title": "Steps",
                      "urlSlug": "steps",
                      "fullSlug": ["blocks", "steps"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "blocks/block-types.mdx",
                      "title": "Block Types",
                      "urlSlug": "block-types",
                      "fullSlug": ["blocks", "block-types"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Server URL",
                  "urlSlug": "server-url",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "server-url.mdx",
                      "title": "Introduction",
                      "urlSlug": "introduction",
                      "fullSlug": ["server-url"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "server-url/setting-server-urls.mdx",
                      "title": "Setting Server URLs",
                      "urlSlug": "setting-server-ur-ls",
                      "fullSlug": ["server-url", "setting-server-urls"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "server-url/events.mdx",
                      "title": "Server Events",
                      "urlSlug": "server-events",
                      "fullSlug": ["server-url", "events"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "server-url/developing-locally.mdx",
                      "title": "Developing Locally",
                      "urlSlug": "developing-locally",
                      "fullSlug": ["server-url", "developing-locally"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Phone Calling",
                  "urlSlug": "phone-calling",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "phone-calling.mdx",
                      "title": "Introduction",
                      "urlSlug": "introduction",
                      "fullSlug": ["phone-calling"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Squads",
                  "urlSlug": "squads",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "squads.mdx",
                      "title": "Introduction",
                      "urlSlug": "introduction",
                      "fullSlug": ["squads"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "squads-example.mdx",
                      "title": "Example",
                      "urlSlug": "example",
                      "fullSlug": ["squads-example"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                }
              ],
              "skipUrlSlug": false
            },
            {
              "type": "section",
              "title": "Advanced Concepts",
              "urlSlug": "advanced-concepts",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "section",
                  "title": "Calls",
                  "urlSlug": "calls",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "call-forwarding.mdx",
                      "title": "Call Forwarding",
                      "urlSlug": "call-forwarding",
                      "fullSlug": ["call-forwarding"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "calls/call-ended-reason.mdx",
                      "title": "Ended Reason",
                      "urlSlug": "ended-reason",
                      "fullSlug": ["calls", "call-ended-reason"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "advanced/calls/sip.mdx",
                      "title": "SIP",
                      "urlSlug": "sip",
                      "fullSlug": ["advanced", "calls", "sip"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "calls/call-features.mdx",
                      "title": "Live Call Control",
                      "urlSlug": "live-call-control",
                      "fullSlug": ["calls", "call-features"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "page",
                  "id": "GHL.mdx",
                  "title": "Make & GHL Integration",
                  "urlSlug": "make-ghl-integration",
                  "fullSlug": ["GHL"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "tools-calling.mdx",
                  "title": "Tools Calling",
                  "urlSlug": "tools-calling",
                  "fullSlug": ["tools-calling"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "prompting-guide.mdx",
                  "title": "Prompting Guide",
                  "urlSlug": "prompting-guide",
                  "fullSlug": ["prompting-guide"],
                  "hidden": false
                }
              ],
              "skipUrlSlug": false
            },
            {
              "type": "section",
              "title": "Glossary",
              "urlSlug": "glossary",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "page",
                  "id": "glossary.mdx",
                  "title": "Definitions",
                  "urlSlug": "definitions",
                  "fullSlug": ["glossary"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "faq.mdx",
                  "title": "FAQ",
                  "urlSlug": "faq",
                  "fullSlug": ["faq"],
                  "hidden": false
                }
              ],
              "skipUrlSlug": false
            },
            {
              "type": "section",
              "title": "Community",
              "urlSlug": "community",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "section",
                  "title": "Videos",
                  "urlSlug": "videos",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "community/appointment-scheduling.mdx",
                      "title": "Appointment Scheduling",
                      "urlSlug": "appointment-scheduling",
                      "fullSlug": ["community", "appointment-scheduling"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/comparisons.mdx",
                      "title": "Comparisons",
                      "urlSlug": "comparisons",
                      "fullSlug": ["community", "comparisons"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/conferences.mdx",
                      "title": "Conferences",
                      "urlSlug": "conferences",
                      "fullSlug": ["community", "conferences"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/demos.mdx",
                      "title": "Demos",
                      "urlSlug": "demos",
                      "fullSlug": ["community", "demos"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/ghl.mdx",
                      "title": "GoHighLevel",
                      "urlSlug": "go-high-level",
                      "fullSlug": ["community", "ghl"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/guide.mdx",
                      "title": "Guide",
                      "urlSlug": "guide",
                      "fullSlug": ["community", "guide"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/inbound.mdx",
                      "title": "Inbound",
                      "urlSlug": "inbound",
                      "fullSlug": ["community", "inbound"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/knowledgebase.mdx",
                      "title": "Knowledgebase",
                      "urlSlug": "knowledgebase",
                      "fullSlug": ["community", "knowledgebase"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/outbound.mdx",
                      "title": "Outbound",
                      "urlSlug": "outbound",
                      "fullSlug": ["community", "outbound"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/podcast.mdx",
                      "title": "Podcast",
                      "urlSlug": "podcast",
                      "fullSlug": ["community", "podcast"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/snippets-sdks-tutorials.mdx",
                      "title": "Snippets & SDKs Tutorials",
                      "urlSlug": "snippets-sd-ks-tutorials",
                      "fullSlug": ["community", "snippets-sdks-tutorials"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/special-mentions.mdx",
                      "title": "Special Mentions",
                      "urlSlug": "special-mentions",
                      "fullSlug": ["community", "special-mentions"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/squads.mdx",
                      "title": "Squads",
                      "urlSlug": "squads",
                      "fullSlug": ["community", "squads"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/television.mdx",
                      "title": "Television",
                      "urlSlug": "television",
                      "fullSlug": ["community", "television"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "community/usecase.mdx",
                      "title": "Usecase",
                      "urlSlug": "usecase",
                      "fullSlug": ["community", "usecase"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "page",
                  "id": "community/myvapi.mdx",
                  "title": "My Vapi",
                  "urlSlug": "my-vapi",
                  "fullSlug": ["community", "myvapi"],
                  "hidden": false
                },
                {
                  "type": "page",
                  "id": "community/expert-directory.mdx",
                  "title": "Expert Directory",
                  "urlSlug": "expert-directory",
                  "fullSlug": ["community", "expert-directory"],
                  "hidden": false
                }
              ],
              "skipUrlSlug": false
            },
            {
              "type": "section",
              "title": "Providers",
              "urlSlug": "providers",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "section",
                  "title": "Voice",
                  "urlSlug": "voice",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "providers/voice/elevenlabs.mdx",
                      "title": "ElevenLabs",
                      "urlSlug": "eleven-labs",
                      "fullSlug": ["providers", "voice", "elevenlabs"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/voice/playht.mdx",
                      "title": "PlayHT",
                      "urlSlug": "play-ht",
                      "fullSlug": ["providers", "voice", "playht"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/voice/azure.mdx",
                      "title": "Azure",
                      "urlSlug": "azure",
                      "fullSlug": ["providers", "voice", "azure"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/voice/openai.mdx",
                      "title": "OpenAI",
                      "urlSlug": "open-ai",
                      "fullSlug": ["providers", "voice", "openai"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/voice/neets.mdx",
                      "title": "Neets",
                      "urlSlug": "neets",
                      "fullSlug": ["providers", "voice", "neets"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/voice/cartesia.mdx",
                      "title": "Cartesia",
                      "urlSlug": "cartesia",
                      "fullSlug": ["providers", "voice", "cartesia"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/voice/imnt.mdx",
                      "title": "LMNT",
                      "urlSlug": "lmnt",
                      "fullSlug": ["providers", "voice", "imnt"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/voice/rimeai.mdx",
                      "title": "RimeAI",
                      "urlSlug": "rime-ai",
                      "fullSlug": ["providers", "voice", "rimeai"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/voice/deepgram.mdx",
                      "title": "Deepgram",
                      "urlSlug": "deepgram",
                      "fullSlug": ["providers", "voice", "deepgram"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Models",
                  "urlSlug": "models",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "providers/model/openai.mdx",
                      "title": "OpenAI",
                      "urlSlug": "open-ai",
                      "fullSlug": ["providers", "model", "openai"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/model/groq.mdx",
                      "title": "Groq",
                      "urlSlug": "groq",
                      "fullSlug": ["providers", "model", "groq"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/model/deepinfra.mdx",
                      "title": "DeepInfra",
                      "urlSlug": "deep-infra",
                      "fullSlug": ["providers", "model", "deepinfra"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/model/perplexity.mdx",
                      "title": "Perplexity",
                      "urlSlug": "perplexity",
                      "fullSlug": ["providers", "model", "perplexity"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/model/togetherai.mdx",
                      "title": "TogetherAI",
                      "urlSlug": "together-ai",
                      "fullSlug": ["providers", "model", "togetherai"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/model/openrouter.mdx",
                      "title": "OpenRouter",
                      "urlSlug": "open-router",
                      "fullSlug": ["providers", "model", "openrouter"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "section",
                  "title": "Transcription",
                  "urlSlug": "transcription",
                  "collapsed": false,
                  "hidden": false,
                  "items": [
                    {
                      "type": "page",
                      "id": "providers/transcriber/deepgram.mdx",
                      "title": "Deepgram",
                      "urlSlug": "deepgram",
                      "fullSlug": ["providers", "transcriber", "deepgram"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/transcriber/gladia.mdx",
                      "title": "Gladia",
                      "urlSlug": "gladia",
                      "fullSlug": ["providers", "transcriber", "gladia"],
                      "hidden": false
                    },
                    {
                      "type": "page",
                      "id": "providers/transcriber/talkscriber.mdx",
                      "title": "Talkscriber",
                      "urlSlug": "talkscriber",
                      "fullSlug": ["providers", "transcriber", "talkscriber"],
                      "hidden": false
                    }
                  ],
                  "skipUrlSlug": false
                },
                {
                  "type": "page",
                  "id": "providers/voiceflow.mdx",
                  "title": "Voiceflow",
                  "urlSlug": "voiceflow",
                  "fullSlug": ["providers", "voiceflow"],
                  "hidden": false
                }
              ],
              "skipUrlSlug": false
            },
            {
              "type": "section",
              "title": "Security & Privacy",
              "urlSlug": "security-privacy",
              "collapsed": false,
              "hidden": false,
              "items": [
                {
                  "type": "page",
                  "id": "security-and-privacy/hipaa.mdx",
                  "title": "HIPAA Compliance",
                  "urlSlug": "hipaa-compliance",
                  "fullSlug": ["security-and-privacy", "hipaa"],
                  "hidden": false
                },
                { "type": "link", "title": "SOC-2 Compliance", "url": "https://security.vapi.ai/" },
                { "type": "link", "title": "Privacy Policy", "url": "https://vapi.ai/privacy" },
                { "type": "link", "title": "Terms of Service", "url": "https://vapi.ai/terms-of-service" }
              ],
              "skipUrlSlug": false
            }
          ],
          "urlSlugOverride": "documentation",
          "urlSlug": "documentation",
          "skipUrlSlug": false
        },
        {
          "type": "group",
          "title": "API Reference",
          "icon": "terminal",
          "items": [
            {
              "type": "apiV2",
              "node": {
                "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                "type": "apiReference",
                "title": "API Reference",
                "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                "paginated": true,
                "slug": "api-reference/api-reference",
                "hideTitle": true,
                "showErrors": false,
                "children": [
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_calls",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_calls.list",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_calls.list",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "List Calls",
                        "slug": "api-reference/api-reference/calls/list"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_calls.create",
                        "type": "endpoint",
                        "method": "POST",
                        "endpointId": "endpoint_calls.create",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Create Call",
                        "slug": "api-reference/api-reference/calls/create"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_calls.get",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_calls.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Get Call",
                        "slug": "api-reference/api-reference/calls/get"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_calls.delete",
                        "type": "endpoint",
                        "method": "DELETE",
                        "endpointId": "endpoint_calls.delete",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Delete Call Data",
                        "slug": "api-reference/api-reference/calls/delete"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_calls.update",
                        "type": "endpoint",
                        "method": "PATCH",
                        "endpointId": "endpoint_calls.update",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Update Call",
                        "slug": "api-reference/api-reference/calls/update"
                      }
                    ],
                    "title": "Calls",
                    "slug": "api-reference/api-reference/calls",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/calls/list"
                  },
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_assistants",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_assistants.list",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_assistants.list",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "List Assistants",
                        "slug": "api-reference/api-reference/assistants/list"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_assistants.create",
                        "type": "endpoint",
                        "method": "POST",
                        "endpointId": "endpoint_assistants.create",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Create Assistant",
                        "slug": "api-reference/api-reference/assistants/create"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_assistants.get",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_assistants.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Get Assistant",
                        "slug": "api-reference/api-reference/assistants/get"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_assistants.delete",
                        "type": "endpoint",
                        "method": "DELETE",
                        "endpointId": "endpoint_assistants.delete",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Delete Assistant",
                        "slug": "api-reference/api-reference/assistants/delete"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_assistants.update",
                        "type": "endpoint",
                        "method": "PATCH",
                        "endpointId": "endpoint_assistants.update",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Update Assistant",
                        "slug": "api-reference/api-reference/assistants/update"
                      }
                    ],
                    "title": "Assistants",
                    "slug": "api-reference/api-reference/assistants",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/assistants/list"
                  },
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_phoneNumbers",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_phoneNumbers.list",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_phoneNumbers.list",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "List Phone Numbers",
                        "slug": "api-reference/api-reference/phone-numbers/list"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_phoneNumbers.create",
                        "type": "endpoint",
                        "method": "POST",
                        "endpointId": "endpoint_phoneNumbers.create",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Create Phone Number",
                        "slug": "api-reference/api-reference/phone-numbers/create"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_phoneNumbers.get",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_phoneNumbers.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Get Phone Number",
                        "slug": "api-reference/api-reference/phone-numbers/get"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_phoneNumbers.delete",
                        "type": "endpoint",
                        "method": "DELETE",
                        "endpointId": "endpoint_phoneNumbers.delete",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Delete Phone Number",
                        "slug": "api-reference/api-reference/phone-numbers/delete"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_phoneNumbers.update",
                        "type": "endpoint",
                        "method": "PATCH",
                        "endpointId": "endpoint_phoneNumbers.update",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Update Phone Number",
                        "slug": "api-reference/api-reference/phone-numbers/update"
                      }
                    ],
                    "title": "Phone Numbers",
                    "slug": "api-reference/api-reference/phone-numbers",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/phone-numbers/list"
                  },
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_squads",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_squads.list",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_squads.list",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "List Squads",
                        "slug": "api-reference/api-reference/squads/list"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_squads.create",
                        "type": "endpoint",
                        "method": "POST",
                        "endpointId": "endpoint_squads.create",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Create Squad",
                        "slug": "api-reference/api-reference/squads/create"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_squads.get",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_squads.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Get Squad",
                        "slug": "api-reference/api-reference/squads/get"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_squads.delete",
                        "type": "endpoint",
                        "method": "DELETE",
                        "endpointId": "endpoint_squads.delete",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Delete Squad",
                        "slug": "api-reference/api-reference/squads/delete"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_squads.update",
                        "type": "endpoint",
                        "method": "PATCH",
                        "endpointId": "endpoint_squads.update",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Update Squad",
                        "slug": "api-reference/api-reference/squads/update"
                      }
                    ],
                    "title": "Squads",
                    "slug": "api-reference/api-reference/squads",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/squads/list"
                  },
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_blocks",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_blocks.list",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_blocks.list",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "List Blocks",
                        "slug": "api-reference/api-reference/blocks/list"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_blocks.create",
                        "type": "endpoint",
                        "method": "POST",
                        "endpointId": "endpoint_blocks.create",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Create Block",
                        "slug": "api-reference/api-reference/blocks/create"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_blocks.get",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_blocks.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Get Block",
                        "slug": "api-reference/api-reference/blocks/get"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_blocks.delete",
                        "type": "endpoint",
                        "method": "DELETE",
                        "endpointId": "endpoint_blocks.delete",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Delete Block",
                        "slug": "api-reference/api-reference/blocks/delete"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_blocks.update",
                        "type": "endpoint",
                        "method": "PATCH",
                        "endpointId": "endpoint_blocks.update",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Update Block",
                        "slug": "api-reference/api-reference/blocks/update"
                      }
                    ],
                    "title": "Blocks",
                    "slug": "api-reference/api-reference/blocks",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/blocks/list"
                  },
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_tools",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_tools.list",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_tools.list",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "List Tools",
                        "slug": "api-reference/api-reference/tools/list"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_tools.create",
                        "type": "endpoint",
                        "method": "POST",
                        "endpointId": "endpoint_tools.create",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Create Tool",
                        "slug": "api-reference/api-reference/tools/create"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_tools.get",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_tools.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Get Tool",
                        "slug": "api-reference/api-reference/tools/get"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_tools.delete",
                        "type": "endpoint",
                        "method": "DELETE",
                        "endpointId": "endpoint_tools.delete",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Delete Tool",
                        "slug": "api-reference/api-reference/tools/delete"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_tools.update",
                        "type": "endpoint",
                        "method": "PATCH",
                        "endpointId": "endpoint_tools.update",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Update Tool",
                        "slug": "api-reference/api-reference/tools/update"
                      }
                    ],
                    "title": "Tools",
                    "slug": "api-reference/api-reference/tools",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/tools/list"
                  },
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_files",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_files.list",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_files.list",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "List Files",
                        "slug": "api-reference/api-reference/files/list"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_files.create",
                        "type": "endpoint",
                        "method": "POST",
                        "endpointId": "endpoint_files.create",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Upload File",
                        "slug": "api-reference/api-reference/files/create"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_files.get",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_files.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Get File",
                        "slug": "api-reference/api-reference/files/get"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_files.delete",
                        "type": "endpoint",
                        "method": "DELETE",
                        "endpointId": "endpoint_files.delete",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Delete File",
                        "slug": "api-reference/api-reference/files/delete"
                      },
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_files.update",
                        "type": "endpoint",
                        "method": "PATCH",
                        "endpointId": "endpoint_files.update",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Update File",
                        "slug": "api-reference/api-reference/files/update"
                      }
                    ],
                    "title": "Files",
                    "slug": "api-reference/api-reference/files",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/files/list"
                  },
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_analytics",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_analytics.get",
                        "type": "endpoint",
                        "method": "POST",
                        "endpointId": "endpoint_analytics.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "Create Analytics Queries",
                        "slug": "api-reference/api-reference/analytics/get"
                      }
                    ],
                    "title": "Analytics",
                    "slug": "api-reference/api-reference/analytics",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/analytics/get"
                  },
                  {
                    "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:subpackage_logs",
                    "type": "apiPackage",
                    "children": [
                      {
                        "id": "031f13c0-3070-48ca-bfcf-90aa72f935d8:endpoint_logs.get",
                        "type": "endpoint",
                        "method": "GET",
                        "endpointId": "endpoint_logs.get",
                        "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                        "isResponseStream": false,
                        "title": "List logs",
                        "slug": "api-reference/api-reference/logs/get"
                      }
                    ],
                    "title": "Logs",
                    "slug": "api-reference/api-reference/logs",
                    "apiDefinitionId": "031f13c0-3070-48ca-bfcf-90aa72f935d8",
                    "pointsTo": "api-reference/api-reference/logs/get"
                  }
                ],
                "pointsTo": "api-reference/api-reference/calls/list"
              }
            },
            {
              "type": "apiV2",
              "node": {
                "id": "7505d70c-f6cd-463d-899f-289ec6f7b2fa",
                "type": "apiReference",
                "title": "Webhooks",
                "apiDefinitionId": "7505d70c-f6cd-463d-899f-289ec6f7b2fa",
                "paginated": true,
                "slug": "api-reference/webhooks",
                "hideTitle": false,
                "showErrors": false,
                "children": [
                  {
                    "id": "7505d70c-f6cd-463d-899f-289ec6f7b2fa:endpoint_.serverMessage",
                    "type": "endpoint",
                    "method": "POST",
                    "endpointId": "endpoint_.serverMessage",
                    "apiDefinitionId": "7505d70c-f6cd-463d-899f-289ec6f7b2fa",
                    "isResponseStream": false,
                    "title": "Server Message",
                    "slug": "api-reference/webhooks/server-message"
                  },
                  {
                    "id": "7505d70c-f6cd-463d-899f-289ec6f7b2fa:endpoint_.clientMessage",
                    "type": "endpoint",
                    "method": "POST",
                    "endpointId": "endpoint_.clientMessage",
                    "apiDefinitionId": "7505d70c-f6cd-463d-899f-289ec6f7b2fa",
                    "isResponseStream": false,
                    "title": "Client Message",
                    "slug": "api-reference/webhooks/client-message"
                  }
                ],
                "pointsTo": "api-reference/webhooks/server-message"
              }
            },
            {
              "type": "section",
              "title": "",
              "urlSlug": "",
              "collapsed": false,
              "hidden": false,
              "items": [
                { "type": "link", "title": "Swagger", "url": "https://api.vapi.ai/api" },
                { "type": "link", "title": "OpenAPI", "url": "https://api.vapi.ai/api-json" }
              ],
              "skipUrlSlug": false
            }
          ],
          "urlSlugOverride": "api-reference",
          "urlSlug": "api-reference",
          "skipUrlSlug": false
        }
      ],
      "landingPage": { "type": "page", "id": "welcome.mdx", "title": "Welcome", "urlSlug": "", "hidden": false }
    },
    "logoHeight": 28,
    "logoHref": "/",
    "colorsV3": {
      "type": "darkAndLight",
      "light": {
        "accentPrimary": { "r": 55, "g": 170, "b": 157, "a": 1 },
        "logo": "dbe2763d-cbce-4f9c-bd30-b2e8e82390f1",
        "background": { "type": "solid", "r": 255, "g": 255, "b": 255, "a": 1 },
        "headerBackground": { "r": 255, "g": 255, "b": 255, "a": 1 }
      },
      "dark": {
        "accentPrimary": { "r": 148, "g": 255, "b": 210, "a": 1 },
        "logo": "f3ed2cdd-1506-48ff-ad1b-f95c6f55acbf",
        "background": { "type": "solid", "r": 0, "g": 0, "b": 0, "a": 1 },
        "headerBackground": { "r": 0, "g": 0, "b": 0, "a": 1 }
      }
    },
    "navbarLinks": [
      { "type": "minimal", "text": "Home", "url": "https://vapi.ai/" },
      { "type": "minimal", "text": "Pricing", "url": "/pricing" },
      { "type": "minimal", "text": "Status", "url": "https://status.vapi.ai/" },
      { "type": "minimal", "text": "Changelog", "url": "/changelog" },
      { "type": "minimal", "text": "Support", "url": "/support" },
      {
        "type": "filled",
        "text": "Dashboard",
        "url": "https://example.com/login",
        "rightIcon": "fa-solid fa-chevron-right",
        "rounded": true
      }
    ],
    "footerLinks": [
      { "type": "github", "value": "https://github.com/vapiai" },
      { "type": "twitter", "value": "https://twitter.com/vapi_ai" },
      { "type": "discord", "value": "https://discord.gg/pUFNcf2WmH" },
      { "type": "website", "value": "https://vapi.ai/" },
      { "type": "linkedin", "value": "https://www.linkedin.com/company/vapi-ai" }
    ],
    "title": "Vapi",
    "favicon": "744a024e-6e39-44e9-91f3-8315809a4c6c",
    "layout": {
      "headerHeight": { "type": "px", "value": 80 },
      "searchbarPlacement": "HEADER",
      "tabsPlacement": "HEADER",
      "contentAlignment": "CENTER",
      "headerPosition": "FIXED",
      "disableHeader": false
    },
    "css": {
      "inline": [
        ".fern-header .fern-button.filled .fa-icon {\n  height: 10px !important;\n  width: 10px !important;\n}\n\n.fern-header * {\n  font-weight: 500;\n}\n\n/* for a grid of videos */\n\n.video-grid {\n  display: flex;\n  flex-wrap: wrap;\n  gap: 20px; /* Spacing between videos */\n}\n\n.video-grid iframe,\n.video-grid a {\n  flex: 0 0 calc(50% - 20px); /* Flex grow is 0, basis is 50% minus the gap */\n  aspect-ratio: 560 / 315; /* Maintain the aspect ratio of 16:9 */\n  max-width: calc(50% - 20px); /* Max width is also set to 50% minus the gap */\n  height: auto; /* Allow height to auto adjust based on aspect ratio */\n}\n\n.video-grid a {\n  aspect-ratio: 1;\n}\n\n@media (max-width: 600px) {\n  .video-grid iframe {\n    flex: 0 0 100%; /* Flex grow is 0, basis is 100% */\n    max-width: 100%; /* Allow max-width to be full width on mobile */\n  }\n}\n\n.card-img {\n  height: 200px;\n  object-fit: contain;\n  margin: auto;\n  background: white; /*TODO: change color as per theme*/\n}\n\n.card-content {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  margin-top: auto;\n  text-align: center;\n}\n\n.card-content > h3 {\n  margin: 16px 0 8px 0;\n  font-size: 1.5em;\n  text-align: center;\n}\n\n.card-content > p {\n  font-size: 1em;\n  text-align: center;\n}\n\n.video-embed-wrapper {\n  position: relative;\n  width: 100%;\n  padding-top: 56.25%; /* 16:9 Aspect Ratio (divide 9 by 16 = 0.5625) */\n}\n\n.video-embed-wrapper iframe {\n  position: absolute;\n  top: 0;\n  left: 0;\n  width: 100%;\n  height: 100%;\n}\n\n#\\/api-reference\\/webhooks\\/server-message\\#request h3 + div:first-of-type > div:first-of-type  {\n  display: none;\n}\n\n#\\/api-reference\\/webhooks\\/server-message\\#request h3 + div:first-of-type:before  {\n  content: \"Vapi will make a request to your server with the following object:\";\n  font-size: .875rem;\n}\n\n:is(.light) #\\/api-reference\\/webhooks\\/server-message\\#request h3 + div:first-of-type:before {\n  color: #0008059f;\n}\n\n:is(.dark) #\\/api-reference\\/webhooks\\/server-message\\#request h3 + div:first-of-type:before {\n  color: #f6f5ffb6;\n}\n\n#\\/api-reference\\/webhooks\\/client-message\\#request h3 + div:first-of-type > div:first-of-type  {\n  display: none;\n}\n\n#\\/api-reference\\/webhooks\\/client-message\\#request h3 + div:first-of-type:before  {\n  content: \"Vapi will make a request to your server with the following object:\";\n  font-size: .875rem;\n}\n\n:is(.light) #\\/api-reference\\/webhooks\\/client-message\\#request h3 + div:first-of-type:before {\n  color: #0008059f;\n}\n\n:is(.dark) #\\/api-reference\\/webhooks\\/client-message\\#request h3 + div:first-of-type:before {\n  color: #f6f5ffb6;\n}"
      ]
    },
    "js": { "files": [] },
    "integrations": {},
    "analyticsConfig": {}
  },
  "files": {
    "f3ed2cdd-1506-48ff-ad1b-f95c6f55acbf": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/logo/logo-dark.png",
    "dbe2763d-cbce-4f9c-bd30-b2e8e82390f1": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/logo/logo-light.png",
    "744a024e-6e39-44e9-91f3-8315809a4c6c": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/favicon.png",
    "ca3908d2-ecee-458f-8c63-226234e871c2": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/intro/custom-vs-vapi.png",
    "6331f69d-6898-4521-90ff-6123bdcb7597": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/quickstart-banner.png",
    "d9d9a2ae-deb5-496e-8474-7665b81a92e8": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/platform/vapi-orchestration.png",
    "adde559a-870e-4373-9ef3-09f8bb07d6ee": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/pricing/voice-pipeline-cost-breakdown.png",
    "e516a5ae-85e2-4aa5-9ec5-de353c98f659": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/cost-routing.png",
    "fbf84b40-499d-4f63-8dee-c7dcdce341fe": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/billing-limits.png",
    "1b83d54f-319e-4b5f-8193-002566616528": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/billing-limits-exceeded.png",
    "d50a0410-d5a5-4f26-9c29-bd22d716933a": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/cost-estimate.gif",
    "404dd489-8a21-4fe1-b1c6-4e0e25b612bf": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/call-pricing-breakdown.png",
    "ab4e9cb7-5df5-40f1-b64b-2509072a0ef9": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/billing-example-template.png",
    "3b2ecede-ee4f-41cd-b5f3-d3bf7fff4480": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/custom-model-inbound-phone-example.png",
    "dd0b4692-a514-4974-b234-a2f89fa8dec8": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/outbound-phone-example.png",
    "26e63dd7-80a1-460b-a858-1fb4d48abf07": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/learn/billing/web-interviews-example.png",
    "88a31f93-503c-4aa0-9d30-8d50060af0f3": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/vapis-pizzeria.png",
    "2a36b470-9983-4270-aa74-bb2fccc56af4": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/auth-ui.png",
    "20f9dc5a-cf0e-4750-ad63-82f8a16c8a0d": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/vapi-dashboard-post-signup.png",
    "0eb3cc5f-5aa4-435f-8a56-fd05ae3ff398": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/create-new-assistant-button.png",
    "80f913e8-fd91-46e6-9cff-f574bd100f10": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/choose-blank-template.png",
    "a4fc51f3-6f6d-43c8-b577-da34b255198a": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/name-your-assistant.png",
    "300206c9-ee39-484c-910e-e45e3a28567d": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/assistant-created.png",
    "d9e19883-176c-4849-ab6f-91eebf3615e1": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/model-provider-keys.png",
    "8f55b735-a327-428a-af55-e0918f9e7b46": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/assistant-model-set-up.png",
    "38df25eb-0169-45e1-82ba-1580d3707aed": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/transcriber-providers-keys.png",
    "f3d0e9b2-5fd5-41bb-9132-39eb0a091218": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/assistant-transcriber-config.png",
    "f063fc59-93ea-4a16-bb0a-f973f54c51f3": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/voice-provider-keys.png",
    "cbcb3b79-84bf-4502-8489-a8e5c9b91d47": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/assistant-voice-config.png",
    "e2e34f52-7467-4407-8d7d-2ba8575b3418": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/call-assistant-web-dashboard.png",
    "5aacd386-b019-4352-8f21-e232e1e883c0": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/buy-a-phone-number.png",
    "ff861b6e-dcd7-439c-9463-2f0b1a6f42b9": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/buy-phone-number-modal.png",
    "4fd9c589-942f-4474-8df7-562255d342d1": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/phone-number-config.png",
    "f5182f5d-8fea-4e6a-8e0e-98ac685885f0": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/dashboard/inbound-assistant-set.png",
    "2c08fcf6-7b50-4b04-a0f6-3645a686037b": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/phone/buy-phone-number-twilio.png",
    "63cef21d-bdb4-4f87-b361-c488563018ac": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/phone/twilio-api-key-nav.png",
    "b015946b-4e29-47eb-b82f-f47e6d2dd139": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/phone/twilio-credentials.png",
    "6f315749-3171-4de8-af7a-52819b782f50": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/phone/dashboard-import-phone-number.png",
    "c1b7453e-aff6-4fe0-a8eb-6d4b5fe4da49": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/phone/import-twilio-number-dashboard.png",
    "03d9b5e3-7716-42eb-ae73-5b3a1249ce15": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/phone/phone-number-import-complete.png",
    "2019a4c3-9877-4419-b1d0-23d9b3f37fec": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/phone/outbound/assistant-model-setup.png",
    "92bd7abc-7dee-40b6-8ed9-5a7dee6f7a87": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/phone/outbound/dial-outbound-call-dashboard.png",
    "b52d2012-5fdb-43ea-bf04-aafe59cbf3e6": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/quickstart/assistant-id-dashboard.png",
    "d8566a10-6eb8-4c35-9675-656908e4f669": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/blocks/food-order-steps.png",
    "5f424870-816e-45c3-9621-9e72f193c40c": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/server-url/overview-graphic.png",
    "9af8c527-f77a-4fa0-aca4-3007ec661c11": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/server-url/settings-server-urls/server-url-priority.png",
    "cb0c584b-e78b-4086-8c49-65727b892ae2": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/server-url/settings-server-urls/org-settings-server-urls.png",
    "3843780d-2a27-4252-9fe1-e9537b096dbc": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/server-url/settings-server-urls/assistant-server-url-dashboard.png",
    "83b701b7-11e8-48cf-82b2-c8170f23a7da": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/server-url/settings-server-urls/function-call-server-url-dashboard.png",
    "332e79a9-0603-4fdb-9e3b-6801d15c2e33": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/server-url/developing-locally/reverse-proxy-developing-locally.png",
    "45d632a4-b960-41d3-9caa-a74f7e187858": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/server-url/developing-locally/ngrok-cli-ui.png",
    "308e0dea-1089-4add-ac6a-eb9e4812ca2c": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23%3A23%3A55.437Z/static/images/server-url/developing-locally/logging-events-locally.png"
  },
  "jsFiles": {
    "snippets/faq-snippet.mdx": "<AccordionGroup>\n  <Accordion title=\"Is Vapi right for my usecase?\" icon=\"hammer\" iconType=\"regular\" defaultOpen={true}>\n\nIf you are **a developer building a voice AI application simulating human conversation** (w/ LLMs — to whatever degree of application complexity) — Vapi is built for you.\n\nWhether you are building for a completely \"turn-based\" use case (like appointment setting), all the way to robust agentic voice applications (like virtual assistants), Vapi is tooled to solve for your voice AI workflow.\n\nVapi runs on any platform: the web, mobile, or even embedded systems (given network access).\n\n  </Accordion>\n  <Accordion title=\"Sounds good, but I’m building a custom X for Y...\" icon=\"face-monocle\" iconType=\"solid\" defaultOpen={false}>\n\nNot a problem, we can likely already support it. Vapi is designed to be modular at every level of the voice pipeline: Text-to-speech, LLM, Speech-to-text.\n\nYou can bring your own custom models for any part of the pipeline.\n\n- **If they’re hosted with one of our providers:** you just need to add your [provider keys](customization/provider-keys), then specify the custom model in your API requests.\n- **If they are hosted elsewhere:** you can use the `Custom LLM` provider and specify the [URL to your model](customization/custom-llm/fine-tuned-openai-models) in your API request.\n\nEverything is interchangeable, mix & match to suit your usecase.\n\n  </Accordion>\n  <Accordion title=\"Couldn’t I build this myself and save money?\" icon=\"piggy-bank\" iconType=\"solid\" defaultOpen={false}>\n\nYou could (and the person writing this right now did, from scratch) — but there are good reasons for not doing so.\n\nWriting a great realtime voice AI application from scratch is a fairly challenging task (more on those challenges [here](/challenges-of-realtime-conversation)). Most of these challenges are not apparent until you face them, then you realize you are 3 weeks into a rabbit hole that may take months to properly solve out of.\n\nThink of Vapi as hiring a software engineering team for this hard problem, while you focus on what uniquely generates value for your voice AI application.\n\n---\n\nBut to address cost, the vast majority of cost in running your application will come from provider cost (Speect-to-text, LLM, Text-to-speech) direct with vendors (Deepgram, OpenAI, ElevenLabs, etc) — where we add no fee (vendor cost passes-through). These would have to be incurred anyway.\n\nVapi only charges its small fee on top of these for the continuous maintenance & improvement of these hardest components of your system (which would have costed you time to write/maintain).\n\nNo matter what, some cost is inescapable (in money, time, etc) to solve this challenging technical problem.\n\nOur focus is solely on foundational Voice AI orchestration, & it’s what we put our full time and resources into.\n\nTo learn more about Vapi’s pricing, you can visit our [pricing page](/pricing).\n\n  </Accordion>\n  <Accordion title=\"Is it going to be hard to set up?\" icon=\"gear\" iconType=\"solid\" defaultOpen={false}>\n\n    No — in fact, the setup could not be easier:\n    - **Web Dashboard:** It can take minutes to get up & running with our [dashboard](https://dashboard.vapi.ai/).\n    - **Client SDKs:** You can start calls with 1 line of code with any of our [client SDKs](/sdks).\n\n    For more advanced features like function calling, you will have to set up a [Server URL](/server-url) to receive and respond to messages.\n\n  </Accordion>\n  <Accordion title=\"How is Vapi different from other Voice AI services?\" icon=\"bowling-pins\" iconType=\"solid\" defaultOpen={false}>\n\n    Vapi focuses on developers. Giving developers modular, simple, & robust tooling to build any voice AI application imaginable.\n\n    Vapi also has some of the lowest latency & (equally important) highest reliability amongst any other voice AI platform built for developers.\n\n  </Accordion>\n</AccordionGroup>\n",
    "snippets/quickstart/dashboard/assistant-setup-inbound.mdx": "<AccordionGroup>\n  <Accordion title=\"Sign-up or Log-in to Vapi\" icon=\"user-plus\" iconType=\"solid\">\n    If you haven't already signed-up, you're going to need an account before you can use the web dashboard. When you visit [dashboard.vapi.ai](https://dashboard.vapi.ai) you may see something like this:\n\n    <Frame>\n      <img src=\"../static/images/quickstart/dashboard/auth-ui.png\" />\n    </Frame>\n\n    Sign-up for an account (or log-in to your existing account) — you will then find yourself inside the web dashboard. It will look something like this:\n\n    <Frame caption=\"Your dashboard may look a bit different if you already have an account with assistants in it. The main idea is that we’re in the dashboard now.\">\n      <img src=\"../static/images/quickstart/dashboard/vapi-dashboard-post-signup.png\" />\n    </Frame>\n\n  </Accordion>\n  <Accordion title=\"Create an Assistant\" icon=\"layer-plus\" iconType=\"solid\">\n    Now that you're in your dashboard, we're going to create an [assistant](/assistants).\n\n    Assistants are at the heart of how Vapi models AI voice agents — we will be setting certain properties on a new assistant to model an order-taking agent.\n\n    Once in the \"Assistants\" dashboard tab (you should be in it by-default after log-in), you will see a button to create a new assistant.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"../static/images/quickstart/dashboard/create-new-assistant-button.png\" />\n    </Frame>\n\n    After clicking the create new assistant button, you will see a pop-up modal that asks you to pick a starter template. For our example we will start from a blank slate so choose the `Blank Template` option.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"../static/images/quickstart/dashboard/choose-blank-template.png\" />\n    </Frame>\n\n    You will then be able to name your assistant — you can name it whatever you'd like (`Vapi’s Pizza Front Desk`, for example):\n\n    <Info>\n      This name is only for internal labeling use. It is not an identifier, nor will the assistant be\n      aware of this name.\n    </Info>\n\n    <Frame caption=\"Name your assistant.\">\n      <img src=\"../static/images/quickstart/dashboard/name-your-assistant.png\" />\n    </Frame>\n\n    Once you have named your assistant, you can hit \"Create\" to create it. You will then see something like this:\n\n    <Frame caption=\"The assistant overview. You can edit your assistant’s transcriber, model, & voice — and edit other advanced configuration.\">\n      <img src=\"../static/images/quickstart/dashboard/assistant-created.png\" />\n    </Frame>\n\n    This is the assistant overview view — it gives you the ability to edit different attributes about your assistant, as well as see **cost** & **latency** projection information for each portion of it’s voice pipeline (this is very important data to have handy when building out your assistants).\n\n  </Accordion>\n  <Accordion title=\"Model Setup\" icon=\"microchip\" iconType=\"solid\">\n    Now we’re going to set the \"brains\" of the assistant, the large language model. We're going to be using `GPT-4` (from [OpenAI](https://openai.com/)) for this demo (though you're free to use `GPT-3.5`, or any one of your favorite LLMs).\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your OpenAI Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        Before we proceed, we can set our [provider key](customization/provider-keys) for OpenAI (this is just your OpenAI secret key).\n\n        <Note>\n          You can see all of your provider keys in the \"Provider Keys\" dashboard tab. You can also go\n          directly to [dashboard.vapi.ai/keys](https://dashboard.vapi.ai/keys).\n        </Note>\n\n        Vapi uses [provider keys](customization/provider-keys) you provide to communicate with LLM, TTS, & STT vendors on your behalf. It is most ideal that we set keys for the vendors we intend to use ahead of time.\n\n        <Frame caption=\"We set our provider key for OpenAI so Vapi can make requests to their API.\">\n          <img src=\"../static/images/quickstart/dashboard/model-provider-keys.png\" />\n        </Frame>\n\n        While we're here it'd be ideal for you to go & set up provider keys for other providers you're familiar with & intend to use later.\n      </Accordion>\n      <Accordion title=\"Set a First Message\" icon=\"message\" iconType=\"light\">\n        Assistants can **optionally** be configured with a `First Message`. This first message will be spoken by your assistant when either:\n\n        - **A Web Call Connects:** when a web call is started with your assistant\n        - **An Inbound Call is Picked-up:** an [inbound call](/glossary#inbound-call) is picked-up & answered by your assistant\n        - **An Outbound Call is Dialed & Picked-up:** an [outbound call](/glossary#outbound-call) is dialed by your assistant & a person picks up\n\n        <Warning>\n          Note that this first message cannot be interrupted & is guaranteed to be spoken. Certain use cases\n          need a first message, while others do not.\n        </Warning>\n\n        For our use case, we will want a first message. It would be ideal for us to have a first message like this:\n\n        ```text\n        Vappy’s Pizzeria speaking, how can I help you?\n        ```\n\n        <Info>\n          Some text-to-speech voices may struggle to pronounce 'Vapi' correctly, compartmentalizing it to be\n          spoken letter by letter \"V. A. P. I.\"\n\n        Some aspects of configuring your voice pipeline will require tweaks like this to get the target\n        behaviour you want.\n\n        </Info>\n\n        This will be spoken by the assistant when a web or inbound phone call is received.\n      </Accordion>\n      <Accordion title=\"Set the System Prompt\" icon=\"message\" iconType=\"solid\">\n        We will now set the `System Prompt` for our assistant. If you're familiar with OpenAI's API, this is the first prompt in the message list that we feed our LLM (learn more about prompt engineering on the [OpenAI docs](https://platform.openai.com/docs/guides/prompt-engineering)).\n\n        The system prompt can be used to configure the context, role, personality, instructions and so on for the assistant. In our case, a system prompt like this will give us the behaviour we want:\n\n        ```text\n        You are a voice assistant for Vappy’s Pizzeria,\n        a pizza shop located on the Internet.\n\n        Your job is to take the order of customers calling in. The menu has only 3 types\n        of items: pizza, sides, and drinks. There are no other types of items on the menu.\n\n        1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza\n        (often called \"veggie\" pizza).\n        2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.\n        3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a\n        brand name like \"coca cola\", just let them know that we only offer \"soda\")\n\n        Customers can only order 1 of each item. If a customer tries to order more\n        than 1 item within each category, politely inform them that only 1 item per\n        category may be ordered.\n\n        Customers must order 1 item from at least 1 category to have a complete order.\n        They can order just a pizza, or just a side, or just a drink.\n\n        Be sure to introduce the menu items, don't assume that the caller knows what\n        is on the menu (most appropriate at the start of the conversation).\n\n        If the customer goes off-topic or off-track and talks about anything but the\n        process of ordering, politely steer the conversation back to collecting their order.\n\n        Once you have all the information you need pertaining to their order, you can\n        end the conversation. You can say something like \"Awesome, we'll have that ready\n        for you in 10-20 minutes.\" to naturally let the customer know the order has been\n        fully communicated.\n\n        It is important that you collect the order in an efficient manner (succinct replies\n        & direct questions). You only have 1 task here, and it is to collect the customers\n        order, then end the conversation.\n\n        - Be sure to be kind of funny and witty!\n        - Keep all your responses short and simple. Use casual language, phrases like \"Umm...\", \"Well...\", and \"I mean\" are preferred.\n        - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.\n        ```\n\n        You can copy & paste the above prompt into the `System Prompt` field. Now the model configuration for your assistant should look something like this:\n\n        <Frame caption=\"Note how our model provider is set to OpenAI & the model is set to GPT-4.\">\n          <img src=\"../static/images/quickstart/dashboard/assistant-model-set-up.png\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Transcriber Setup\" icon=\"microphone\" iconType=\"solid\">\n    The transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your Deepgram Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        We will be using [Deepgram](https://deepgram.com) (which provides blazing-fast & accurate Speech-to-Text) as our STT provider.\n\n        We will set our provider key for them in \"Provider Keys\":\n\n        <Frame>\n          <img src=\"../static/images/quickstart/dashboard/transcriber-providers-keys.png\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Transcriber\" icon=\"language\" iconType=\"solid\">\n        We will set the model to `Nova 2` & the language to `en` for English. Now your assistant's transcriber configuration should look something like this:\n\n        <Frame caption=\"Note how our transcriber is set to 'deepgram', the model is set to 'Nova 2', & the language is set to English.\">\n          <img src=\"../static/images/quickstart/dashboard/assistant-transcriber-config.png\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Voice Setup\" icon=\"head-side-cough\" iconType=\"solid\">\n    The final portion of the voice pipeline is turning LLM output-text into speech. This process is called \"Text-to-speech\" (or TTS for short).\n\n    We will be using a voice provider called [PlayHT](https://play.ht) (they have very conversational voices), & a voice provided by them labeled `Jennifer` (`female`, `en-US`).\n\n    You are free to use your favorite TTS voice platform here. [ElevenLabs](https://elevenlabs.io/) is\n    another alternative — by now you should get the flow of plugging in vendors into Vapi (add\n    provider key + pick provider in assistant config).\n\n    You can skip the next step(s) if you don't intend to use PlayHT.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your PlayHT Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        If you haven't already, sign up for an account with PlayHT at [play.ht](https://play.ht). Since their flows are liable to change — you can just grab your `API Key` & `User ID` from them.\n\n        <Frame>\n          <img src=\"../static/images/quickstart/dashboard/voice-provider-keys.png\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Voice\" icon=\"person\" iconType=\"solid\">\n        You will want to select `playht` in the \"provider\" field, & `Jennifer` in the \"voice\" field. We will leave all of the other settings untouched.\n\n        <Frame caption=\"Each voice provider offers a host of settings you can modulate to customize voices. Here we will leave all the defaults alone.\">\n          <img src=\"../static/images/quickstart/dashboard/assistant-voice-config.png\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n",
    "snippets/quickstart/dashboard/provision-phone-number-with-vapi.mdx": "The quickest way to secure a phone number for your assistant is to purchase a phone number directly through Vapi.\n\n<Info>\n  Ensure you have a card on file that Vapi can bill before proceeding, you can add your billing\n  information in your dashboard at [dashboard.vapi.ai/billing](https://dashboard.vapi.ai/billing)\n</Info>\n\nNavigate to the \"Phone Numbers\" section & click the \"Buy number\" button:\n\n<Frame caption=\"Make sure you are in the 'Phone Numbers' dashboard tab.\">\n  <img src=\"/static/images/quickstart/dashboard/buy-a-phone-number.png\" />\n</Frame>\n\nWe will use the area code `415` for our phone number (these are area codes domestic to the US & Canada).\n\n<Frame caption=\"Choose an area code for your phone number.\">\n  <img src=\"/static/images/quickstart/dashboard/buy-phone-number-modal.png\" />\n</Frame>\n\n<Info>\n  Currently, only US & Canada phone numbers can be directly purchased through Vapi. Phone numbers in\n  other regions must be imported, see our [phone calling](/phone-calling) guide.\n</Info>\n\nClick \"Buy\", after purchasing a phone number you should see something like this:\n\n<Frame caption=\"Here we can attach an assistant to the number for inbound calls (or perform an outbound call, with a select assistant).\">\n  <img src=\"/static/images/quickstart/dashboard/phone-number-config.png\" />\n</Frame>\n\nThe phone number is now ready to be used (either for inbound or outbound calling).\n",
    "snippets/quickstart/phone/get-a-phone-number.mdx": "There are **2 ways** we can get a phone number into our Vapi account:\n\n1. **Purchase a Number Through Vapi:** we can directly purchase phone numbers through Vapi.\n\n   - Vapi will provision the phone number for us via Twilio\n   - This can be done in the dashboard, or via the API (we will use the dashboard)\n\n2. **Import from Twilio or Vonage:** if we already have a phone number with an external telephony provider (like Twilio or Vonage), we can import them into our Vapi account.\n\n<AccordionGroup>\n  <Accordion title=\"Provision via Vapi (faster)\" icon=\"v\" iconType=\"solid\">\n    The quickest way to secure a phone number for your assistant is to purchase a phone number directly through Vapi.\n\n    <Info>\n      Ensure you have a card on file that Vapi can bill before proceeding, you can add your billing\n      information in your dashboard at [dashboard.vapi.ai/billing](https://dashboard.vapi.ai/billing)\n    </Info>\n\n    Navigate to the \"Phone Numbers\" section & click the \"Buy number\" button:\n\n    <Frame caption=\"Make sure you are in the 'Phone Numbers' dashboard tab.\">\n      <img src=\"/static/images/quickstart/dashboard/buy-a-phone-number.png\" />\n    </Frame>\n\n    We will use the area code `415` for our phone number (these are area codes domestic to the US & Canada).\n\n    <Frame caption=\"Choose an area code for your phone number.\">\n      <img src=\"/static/images/quickstart/dashboard/buy-phone-number-modal.png\" />\n    </Frame>\n\n    <Info>\n      Currently, only US & Canada phone numbers can be directly purchased through Vapi. Phone numbers in\n      other regions must be imported, see our [phone calling](/phone-calling) guide.\n    </Info>\n\n    Click \"Buy\", after purchasing a phone number you should see something like this:\n\n    <Frame caption=\"Here we can attach an assistant to the number for inbound calls (or perform an outbound call, with a select assistant).\">\n      <img src=\"/static/images/quickstart/dashboard/phone-number-config.png\" />\n    </Frame>\n\n    The phone number is now ready to be used (either for inbound or outbound calling).\n  </Accordion>\n  <Accordion title=\"Import from Twilio or Vonage\" icon=\"hashtag\" iconType=\"regular\">\n    We can also import an existing phone number we already own with either Twilio or Vonage.\n\n    For example's sake, we will proceed with [**Twilio**](https://twilio.com) (though the steps are the same for Vonage as\n    well).\n\n    <AccordionGroup>\n      <Accordion title=\"Buy a Phone Number via Twilio (optional)\" icon=\"hashtag\" iconType=\"solid\">\n        If you don't already have a number in Twilio, you can purchase one by going to your Twilio console's \"Buy a number\" section:\n\n        <Frame caption=\"The Twilio 'Buy a Number' page in the Twilio console.\">\n          <img src=\"../static/images/quickstart/phone/buy-phone-number-twilio.png\" />\n        </Frame>\n\n        Once you've purchased a number, it will immediately be ready for import into Vapi.\n      </Accordion>\n      <Accordion title=\"Locate Twilio Account SID & Auth Token\" icon=\"key\" iconType=\"solid\">\n        To complete the import on Vapi's side, we will need to grab our Twilio **\"Account SID\"** & **\"Auth Token\"**.\n\n        You should see a section for \"API keys & tokens\", the credentials we will need for the import will live here.\n\n        <Frame caption=\"We will want to navigate to the credentials section of our account.\">\n          <img src=\"../static/images/quickstart/phone/twilio-api-key-nav.png\" />\n        </Frame>\n\n        Once we are in our \"API keys & tokens\" section, we will grab the Account SID & Auth Token:\n\n        <Frame>\n          <img src=\"../static/images/quickstart/phone/twilio-credentials.png\" />\n        </Frame>\n\n        We will use both of these credentials in the next step of importing via the Dashboard.\n      </Accordion>\n      <Accordion title=\"Import via Dashboard\" icon=\"v\" iconType=\"solid\">\n        Navigate to the “Phone Numbers” section & click the “Import” button:\n\n        <Frame caption=\"Click 'Import' in the 'Phone Numbers' tab of your dashboard.\">\n          <img src=\"../static/images/quickstart/phone/dashboard-import-phone-number.png\" />\n        </Frame>\n\n        There you will input your phone number, as well as the credentials you retrieved in the previous step:\n\n        <Frame >\n          <img src=\"../static/images/quickstart/phone/import-twilio-number-dashboard.png\" />\n        </Frame>\n\n        Hit \"Import\" & you will come to the phone number detail page:\n\n        <Frame caption=\"The phone number detail page, we can configure our phone number here.\">\n          <img src=\"../static/images/quickstart/phone/phone-number-import-complete.png\" />\n        </Frame>\n\n        Your number is now ready to be attached to an assistant for inbound or outbound phone calling.\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n",
    "snippets/quickstart/platform-specific/no-code-prerequisites.mdx": "<Accordion title=\"Helpful Prerequisites (No Code)\" icon=\"readme\">\n  The following quickstart guides **require no code** & will give you a good framework for understanding\n  how Vapi works.\n\nThey may be helpful to go through before following this guide:\n\n  <CardGroup cols={2}>\n    <Card title=\"Dashboard Quickstart\" icon=\"browser\" iconType=\"solid\" href=\"/quickstart/dashboard\">\n      The easiest way to start with Vapi. Run a voice agent in minutes.\n    </Card>\n    <Card\n      title=\"Inbound Calling\"\n      icon=\"phone-arrow-down-left\"\n      iconType=\"solid\"\n      href=\"/quickstart/phone/inbound\"\n    >\n      Quickly get started handling inbound phone calls.\n    </Card>\n    <Card\n      title=\"Outbound Calling\"\n      icon=\"phone-arrow-up-right\"\n      iconType=\"solid\"\n      href=\"/quickstart/phone/outbound\"\n    >\n      Quickly get started sending outbound phone calls.\n    </Card>\n  </CardGroup>\n</Accordion>\n",
    "snippets/quickstart/web/links.tsx": "export const quickstartDemoLink = \"https://stackblitz.com/~/github.com/VapiAI/quickstart-react\";\n",
    "snippets/sdk.mdx": "export const SdkCards = ({ iconColor }) => (\n  <CardGroup cols={3}>\n    <Card title=\"Vapi Web\" icon=\"window\" iconType=\"duotone\" color={iconColor} href=\"/sdk/web\">\n      Add a Vapi assistant to your web application.\n    </Card>\n    <Card\n      title=\"Vapi iOS\"\n      icon=\"mobile-notch\"\n            color={iconColor}\n      href=\"https://github.com/VapiAI/ios\"\n    >\n      Add a Vapi assistant to your iOS app.\n    </Card>\n    <Card\n      title=\"Vapi Flutter\"\n      icon=\"mobile-notch\"\n            color={iconColor}\n      href=\"https://github.com/VapiAI/flutter\"\n    >\n      Add a Vapi assistant to your Flutter app.\n    </Card>\n    <Card\n      title=\"Vapi React Native\"\n      icon=\"mobile-notch\"\n            color={iconColor}\n      href=\"https://github.com/VapiAI/react-native-sdk\"\n    >\n      Add a Vapi assistant to your React Native app.\n    </Card>\n    <Card\n      title=\"Vapi Python\"\n      icon=\"fa-brands fa-python\"\n            color={iconColor}\n      href=\"https://github.com/VapiAI/python\"\n    >\n      Multi-platform. Mac, Windows, and Linux.\n    </Card>\n  </CardGroup>\n);\n",
    "snippets/sdks/web/import-web-sdk.mdx": "Import the package:\n\n```javascript\nimport Vapi from \"@vapi-ai/web\";\n```\n\nThen, create a new instance of the Vapi class, passing your **Public Key** as a parameter to the constructor:\n\n```javascript\nconst vapi = new Vapi(\"your-public-key\");\n```\n\nYou can find your public key in the [Vapi Dashboard](https://dashboard.vapi.ai/account).\n",
    "snippets/sdks/web/install-web-sdk.mdx": "Install the package:\n\n```bash\nyarn add @vapi-ai/web\n```\n\nor w/ npm:\n\n```bash\nnpm install @vapi-ai/web\n```\n",
    "snippets/video/videos.tsx": "export const YouTubeEmbed = ({ videoUrl, altTitle }) => {\n\nreturn <Frame>\n\n  <div class=\"video-embed-wrapper\">\n    <iframe\n      src={videoUrl}\n      title={`An embedded YouTube video titled \\\"${altTitle}\\\"`}\n      frameborder=\"0\"\n      allow=\"fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n      allowfullscreen\n      referrerpolicy=\"strict-origin-when-cross-origin\"\n    />\n  </div>\n</Frame>\n};\n"
  },
  "filesV2": {
    "f3ed2cdd-1506-48ff-ad1b-f95c6f55acbf": {
      "type": "image",
      "width": 1483,
      "height": 421,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/logo/logo-dark.png"
    },
    "dbe2763d-cbce-4f9c-bd30-b2e8e82390f1": {
      "type": "image",
      "width": 1483,
      "height": 421,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/logo/logo-light.png"
    },
    "744a024e-6e39-44e9-91f3-8315809a4c6c": {
      "type": "image",
      "width": 48,
      "height": 48,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/favicon.png"
    },
    "ca3908d2-ecee-458f-8c63-226234e871c2": {
      "type": "image",
      "width": 2966,
      "height": 884,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/intro/custom-vs-vapi.png"
    },
    "6331f69d-6898-4521-90ff-6123bdcb7597": {
      "type": "image",
      "width": 2800,
      "height": 1200,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/quickstart-banner.png"
    },
    "d9d9a2ae-deb5-496e-8474-7665b81a92e8": {
      "type": "image",
      "width": 2442,
      "height": 1068,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/platform/vapi-orchestration.png"
    },
    "adde559a-870e-4373-9ef3-09f8bb07d6ee": {
      "type": "image",
      "width": 1400,
      "height": 752,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/pricing/voice-pipeline-cost-breakdown.png"
    },
    "e516a5ae-85e2-4aa5-9ec5-de353c98f659": {
      "type": "image",
      "width": 1400,
      "height": 752,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/cost-routing.png"
    },
    "fbf84b40-499d-4f63-8dee-c7dcdce341fe": {
      "type": "image",
      "width": 1538,
      "height": 422,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/billing-limits.png"
    },
    "1b83d54f-319e-4b5f-8193-002566616528": {
      "type": "image",
      "width": 810,
      "height": 210,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/billing-limits-exceeded.png"
    },
    "d50a0410-d5a5-4f26-9c29-bd22d716933a": {
      "type": "image",
      "width": 2430,
      "height": 396,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/cost-estimate.gif"
    },
    "404dd489-8a21-4fe1-b1c6-4e0e25b612bf": {
      "type": "image",
      "width": 810,
      "height": 438,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/call-pricing-breakdown.png"
    },
    "ab4e9cb7-5df5-40f1-b64b-2509072a0ef9": {
      "type": "image",
      "width": 1400,
      "height": 752,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/billing-example-template.png"
    },
    "3b2ecede-ee4f-41cd-b5f3-d3bf7fff4480": {
      "type": "image",
      "width": 1400,
      "height": 752,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/custom-model-inbound-phone-example.png"
    },
    "dd0b4692-a514-4974-b234-a2f89fa8dec8": {
      "type": "image",
      "width": 1400,
      "height": 752,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/outbound-phone-example.png"
    },
    "26e63dd7-80a1-460b-a858-1fb4d48abf07": {
      "type": "image",
      "width": 1400,
      "height": 752,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/learn/billing/web-interviews-example.png"
    },
    "88a31f93-503c-4aa0-9d30-8d50060af0f3": {
      "type": "image",
      "width": 2800,
      "height": 1200,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/vapis-pizzeria.png"
    },
    "2a36b470-9983-4270-aa74-bb2fccc56af4": {
      "type": "image",
      "width": 1722,
      "height": 1170,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/auth-ui.png"
    },
    "20f9dc5a-cf0e-4750-ad63-82f8a16c8a0d": {
      "type": "image",
      "width": 3576,
      "height": 1892,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/vapi-dashboard-post-signup.png"
    },
    "0eb3cc5f-5aa4-435f-8a56-fd05ae3ff398": {
      "type": "image",
      "width": 598,
      "height": 294,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/create-new-assistant-button.png"
    },
    "80f913e8-fd91-46e6-9cff-f574bd100f10": {
      "type": "image",
      "width": 1170,
      "height": 831,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/choose-blank-template.png"
    },
    "a4fc51f3-6f6d-43c8-b577-da34b255198a": {
      "type": "image",
      "width": 1444,
      "height": 885,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/name-your-assistant.png"
    },
    "300206c9-ee39-484c-910e-e45e3a28567d": {
      "type": "image",
      "width": 3086,
      "height": 1772,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/assistant-created.png"
    },
    "d9e19883-176c-4849-ab6f-91eebf3615e1": {
      "type": "image",
      "width": 2602,
      "height": 1612,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/model-provider-keys.png"
    },
    "8f55b735-a327-428a-af55-e0918f9e7b46": {
      "type": "image",
      "width": 2428,
      "height": 1372,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/assistant-model-set-up.png"
    },
    "38df25eb-0169-45e1-82ba-1580d3707aed": {
      "type": "image",
      "width": 1438,
      "height": 650,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/transcriber-providers-keys.png"
    },
    "f3d0e9b2-5fd5-41bb-9132-39eb0a091218": {
      "type": "image",
      "width": 2104,
      "height": 752,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/assistant-transcriber-config.png"
    },
    "f063fc59-93ea-4a16-bb0a-f973f54c51f3": {
      "type": "image",
      "width": 2592,
      "height": 1272,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/voice-provider-keys.png"
    },
    "cbcb3b79-84bf-4502-8489-a8e5c9b91d47": {
      "type": "image",
      "width": 1730,
      "height": 1352,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/assistant-voice-config.png"
    },
    "e2e34f52-7467-4407-8d7d-2ba8575b3418": {
      "type": "image",
      "width": 1456,
      "height": 408,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/call-assistant-web-dashboard.png"
    },
    "5aacd386-b019-4352-8f21-e232e1e883c0": {
      "type": "image",
      "width": 1200,
      "height": 620,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/buy-a-phone-number.png"
    },
    "ff861b6e-dcd7-439c-9463-2f0b1a6f42b9": {
      "type": "image",
      "width": 1796,
      "height": 992,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/buy-phone-number-modal.png"
    },
    "4fd9c589-942f-4474-8df7-562255d342d1": {
      "type": "image",
      "width": 3080,
      "height": 1366,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/phone-number-config.png"
    },
    "f5182f5d-8fea-4e6a-8e0e-98ac685885f0": {
      "type": "image",
      "width": 1866,
      "height": 436,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/dashboard/inbound-assistant-set.png"
    },
    "2c08fcf6-7b50-4b04-a0f6-3645a686037b": {
      "type": "image",
      "width": 3570,
      "height": 1724,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/phone/buy-phone-number-twilio.png"
    },
    "63cef21d-bdb4-4f87-b361-c488563018ac": {
      "type": "image",
      "width": 716,
      "height": 1120,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/phone/twilio-api-key-nav.png"
    },
    "b015946b-4e29-47eb-b82f-f47e6d2dd139": {
      "type": "image",
      "width": 1568,
      "height": 662,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/phone/twilio-credentials.png"
    },
    "6f315749-3171-4de8-af7a-52819b782f50": {
      "type": "image",
      "width": 1148,
      "height": 450,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/phone/dashboard-import-phone-number.png"
    },
    "c1b7453e-aff6-4fe0-a8eb-6d4b5fe4da49": {
      "type": "image",
      "width": 1022,
      "height": 1210,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/phone/import-twilio-number-dashboard.png"
    },
    "03d9b5e3-7716-42eb-ae73-5b3a1249ce15": {
      "type": "image",
      "width": 3088,
      "height": 1334,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/phone/phone-number-import-complete.png"
    },
    "2019a4c3-9877-4419-b1d0-23d9b3f37fec": {
      "type": "image",
      "width": 2430,
      "height": 1240,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/phone/outbound/assistant-model-setup.png"
    },
    "92bd7abc-7dee-40b6-8ed9-5a7dee6f7a87": {
      "type": "image",
      "width": 1128,
      "height": 650,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/phone/outbound/dial-outbound-call-dashboard.png"
    },
    "b52d2012-5fdb-43ea-bf04-aafe59cbf3e6": {
      "type": "image",
      "width": 888,
      "height": 396,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/quickstart/assistant-id-dashboard.png"
    },
    "d8566a10-6eb8-4c35-9675-656908e4f669": {
      "type": "image",
      "width": 337,
      "height": 755,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/blocks/food-order-steps.png"
    },
    "5f424870-816e-45c3-9621-9e72f193c40c": {
      "type": "image",
      "width": 1399,
      "height": 594,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/server-url/overview-graphic.png"
    },
    "9af8c527-f77a-4fa0-aca4-3007ec661c11": {
      "type": "image",
      "width": 1401,
      "height": 683,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/server-url/settings-server-urls/server-url-priority.png"
    },
    "cb0c584b-e78b-4086-8c49-65727b892ae2": {
      "type": "image",
      "width": 2374,
      "height": 2060,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/server-url/settings-server-urls/org-settings-server-urls.png"
    },
    "3843780d-2a27-4252-9fe1-e9537b096dbc": {
      "type": "image",
      "width": 1130,
      "height": 500,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/server-url/settings-server-urls/assistant-server-url-dashboard.png"
    },
    "83b701b7-11e8-48cf-82b2-c8170f23a7da": {
      "type": "image",
      "width": 872,
      "height": 754,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/server-url/settings-server-urls/function-call-server-url-dashboard.png"
    },
    "332e79a9-0603-4fdb-9e3b-6801d15c2e33": {
      "type": "image",
      "width": 1400,
      "height": 598,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/server-url/developing-locally/reverse-proxy-developing-locally.png"
    },
    "45d632a4-b960-41d3-9caa-a74f7e187858": {
      "type": "image",
      "width": 2354,
      "height": 760,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/server-url/developing-locally/ngrok-cli-ui.png"
    },
    "308e0dea-1089-4add-ac6a-eb9e4812ca2c": {
      "type": "image",
      "width": 819,
      "height": 414,
      "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/vapi.docs.buildwithfern.com/2024-10-23T23:23:55.437Z/static/images/server-url/developing-locally/logging-events-locally.png"
    }
  },
  "pages": {
    "welcome.mdx": {
      "markdown": "---\ntitle: Welcome to Vapi's Developer Documentation\nlayout: overview\n---\n\nEverything you need to build, test, & deploy voice AI agents in minutes rather than months\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Quickstart\"\n    icon=\"clock\"\n    href=\"/quickstart/dashboard\"\n  >\n    Get started now with the Vapi web dashboard. \n  </Card>\n  <Card title=\"Documentation\" icon=\"book\" href=\"introduction\">\n    Reference our documentation for how to use Vapi's Voice AI platform. \n  </Card>\n  <Card title=\"Community\" icon=\"fa-brands fa-discord\" href=\"https://discord.gg/pUFNcf2WmH\">\n    Connect with our team & other developers using Vapi.\n  </Card>\n  <Card title=\"GitHub\" icon=\"fa-brands fa-github\" href=\"https://github.com/VapiAI\">\n    Check out our GitHub to see what the Vapi team has been up to. \n  </Card>\n</CardGroup>\n"
    },
    "introduction.mdx": {
      "markdown": "---\ntitle: Introduction\nsubtitle: Vapi is the Voice AI platform for developers.\nslug: introduction\n---\n\n<Frame>\n  <img src=\"file:ca3908d2-ecee-458f-8c63-226234e871c2\" />\n</Frame>\n\nVapi lets developers build, test, & deploy voice AI agents in minutes rather than months — solving for the foundational challenges voice AI applications face:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Simulating the Flow of Natural Human Conversation\"\n    icon=\"people-arrows\"\n        color=\"#e6aa20\"\n  >\n    Turn-taking, interruption handling, backchanneling, and more.\n  </Card>\n  <Card title=\"Realtime/Low Latency Demands\" icon=\"timer\" iconType=\"regular\" color=\"#f25130\">\n    Responsive conversation demands low latency. Internationally. (\\<500-800ms voice-to-voice).\n  </Card>\n  <Card title=\"Taking Actions (Function Calling)\" icon=\"function\" iconType=\"regular\">\n    Taking actions during conversation, getting data to your services for custom actions.\n  </Card>\n  <Card title=\"Extracting Conversation Data\" icon=\"waves-sine\" iconType=\"regular\" color=\"#CC42F9\">\n    Review conversation audio, transcripts, & metadata.\n  </Card>\n</CardGroup>\n\n<Info>\n  Implemented from scratch, this functionality can take months to build, and\n  large, continuous, resources to maintain & improve.\n</Info>\n\nVapi abstracts away these complexities, allowing developers to focus on the core of their voice AI application's business logic. **Shipping in days, not months.**\n\n## Quickstart Guides\n\nGet up & running in minutes with one of our [quickstart](/quickstart) guides:\n\n#### No Code\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Dashboard Quickstart\"\n    icon=\"browser\"\n    iconType=\"solid\"\n    color=\"#54a7ff\"\n    href=\"/quickstart/dashboard\"\n  >\n    The easiest way to start with Vapi. Run a voice agent in minutes.\n  </Card>\n  <Card\n    title=\"Inbound Calling\"\n    icon=\"phone-arrow-down-left\"\n    iconType=\"solid\"\n    color=\"#54a7ff\"\n    href=\"/quickstart/phone/inbound\"\n  >\n    Quickly get started handling inbound phone calls.\n  </Card>\n  <Card\n    title=\"Outbound Calling\"\n    icon=\"phone-arrow-up-right\"\n    iconType=\"solid\"\n    color=\"#54a7ff\"\n    href=\"/quickstart/phone/outbound\"\n  >\n    Quickly get started sending outbound phone calls.\n  </Card>\n</CardGroup>\n\n#### Platform-Specific\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Web Quickstart\"\n    icon=\"window\"\n    iconType=\"duotone\"\n    color=\"#54a7ff\"\n    href=\"/quickstart/web\"\n  >\n    Quickly get started making web calls. Web developers, this is for you.\n  </Card>\n</CardGroup>\n\n## Examples\n\nExplore end-to-end examples for some common voice workflows:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Outbound Sales\"\n    icon=\"phone-office\"\n    iconType=\"solid\"\n    color=\"#f5ac4c\"\n    href=\"/examples/outbound-sales\"\n  >\n    We’ll build an outbound sales agent that can schedule appointments.\n  </Card>\n  <Card\n    title=\"Inbound Support\"\n    icon=\"phone\"\n    iconType=\"solid\"\n    color=\"#f5ac4c\"\n    href=\"/examples/inbound-support\"\n  >\n    We’ll build an technical support assistant that remembers where we left off.\n  </Card>\n  <Card\n    title=\"Pizza Website\"\n    icon=\"browser\"\n    iconType=\"duotone\"\n    color=\"#f5ac4c\"\n    href=\"/examples/pizza-website\"\n  >\n    We'll build an order taking agent for our pizza website.\n  </Card>\n</CardGroup>\n\n## Key Concepts\n\nGain a deep understanding of key concepts in Vapi, as well as how Vapi works:\n\n#### Core Concepts\n\n<CardGroup cols={2}>\n  <Card title=\"Assistants\" icon=\"robot\" iconType=\"duotone\" href=\"/assistants\">\n    Assistants set the foundation for applications built on Vapi.\n  </Card>\n  <Card\n    title=\"Server URLs\"\n    icon=\"webhook\"\n    iconType=\"duotone\"\n    href=\"/server-url\"\n  >\n    Server URLs allow Vapi to deliver your application data in realtime.\n  </Card>\n  <Card\n    title=\"Phone Calling\"\n    icon=\"phone-volume\"\n    iconType=\"solid\"\n    href=\"/phone-calling\"\n  >\n    Learn the ins-and-outs of telephony & conducting phone calls on Vapi.\n  </Card>\n  <Card\n    title=\"Privacy\"\n    icon=\"signature-lock\"\n    iconType=\"duotone\"\n    href=\"security-and-privacy/hipaa\"\n  >\n    Learn about privacy concepts like HIPAA & data privacy on Vapi.\n  </Card>\n</CardGroup>\n\n#### Platform\n\n<CardGroup cols={2}>\n  <Card\n    title=\"How Vapi Works\"\n    icon=\"network-wired\"\n    iconType=\"solid\"\n    href=\"/quickstart\"\n  >\n    Learn what goes on behind-the-scenes to make Vapi work.\n  </Card>\n</CardGroup>\n\n## Explore Our SDKs\n\nOur SDKs are open source, and available on [our GitHub](https://github.com/VapiAI):\n\n<CardGroup cols={3}>\n    <Card title=\"Vapi Web\" icon=\"window\" iconType=\"duotone\" color=\"#f4ff54\" href=\"/sdk/web\">\n      Add a Vapi assistant to your web application.\n    </Card>\n    <Card\n      title=\"Vapi iOS\"\n      icon=\"mobile-notch\"\n      color=\"#f4ff54\"\n      href=\"https://github.com/VapiAI/ios\"\n    >\n      Add a Vapi assistant to your iOS app.\n    </Card>\n    <Card\n      title=\"Vapi Flutter\"\n      icon=\"mobile-notch\"\n      color=\"#f4ff54\"\n      href=\"https://github.com/VapiAI/flutter\"\n    >\n      Add a Vapi assistant to your Flutter app.\n    </Card>\n    <Card\n      title=\"Vapi React Native\"\n      icon=\"mobile-notch\"\n      color=\"#f4ff54\"\n      href=\"https://github.com/VapiAI/react-native-sdk\"\n    >\n      Add a Vapi assistant to your React Native app.\n    </Card>\n    <Card\n      title=\"Vapi Python\"\n      icon=\"fa-brands python\"\n      color=\"#f4ff54\"\n      href=\"https://github.com/VapiAI/python\"\n    >\n      Multi-platform. Mac, Windows, and Linux.\n    </Card>\n  </CardGroup>\n\n## FAQ\n\nCommon questions asked by other users:\n\n<AccordionGroup>\n  <Accordion title=\"Is Vapi right for my usecase?\" icon=\"hammer\" iconType=\"regular\" defaultOpen={true}>\n\nIf you are **a developer building a voice AI application simulating human conversation** (w/ LLMs — to whatever degree of application complexity) — Vapi is built for you.\n\nWhether you are building for a completely \"turn-based\" use case (like appointment setting), all the way to robust agentic voice applications (like virtual assistants), Vapi is tooled to solve for your voice AI workflow.\n\nVapi runs on any platform: the web, mobile, or even embedded systems (given network access).\n\n  </Accordion>\n  <Accordion title=\"Sounds good, but I’m building a custom X for Y...\" icon=\"face-monocle\" iconType=\"solid\" defaultOpen={false}>\n\nNot a problem, we can likely already support it. Vapi is designed to be modular at every level of the voice pipeline: Text-to-speech, LLM, Speech-to-text.\n\nYou can bring your own custom models for any part of the pipeline.\n\n- **If they’re hosted with one of our providers:** you just need to add your [provider keys](customization/provider-keys), then specify the custom model in your API requests.\n- **If they are hosted elsewhere:** you can use the `Custom LLM` provider and specify the [URL to your model](customization/custom-llm/fine-tuned-openai-models) in your API request.\n\nEverything is interchangeable, mix & match to suit your usecase.\n\n  </Accordion>\n  <Accordion title=\"Couldn’t I build this myself and save money?\" icon=\"piggy-bank\" iconType=\"solid\" defaultOpen={false}>\n\nYou could (and the person writing this right now did, from scratch) — but there are good reasons for not doing so.\n\nWriting a great realtime voice AI application from scratch is a fairly challenging task (more on those challenges [here](/challenges-of-realtime-conversation)). Most of these challenges are not apparent until you face them, then you realize you are 3 weeks into a rabbit hole that may take months to properly solve out of.\n\nThink of Vapi as hiring a software engineering team for this hard problem, while you focus on what uniquely generates value for your voice AI application.\n\n---\n\nBut to address cost, the vast majority of cost in running your application will come from provider cost (Speect-to-text, LLM, Text-to-speech) direct with vendors (Deepgram, OpenAI, ElevenLabs, etc) — where we add no fee (vendor cost passes-through). These would have to be incurred anyway.\n\nVapi only charges its small fee on top of these for the continuous maintenance & improvement of these hardest components of your system (which would have costed you time to write/maintain).\n\nNo matter what, some cost is inescapable (in money, time, etc) to solve this challenging technical problem.\n\nOur focus is solely on foundational Voice AI orchestration, & it’s what we put our full time and resources into.\n\nTo learn more about Vapi’s pricing, you can visit our [pricing page](/pricing).\n\n  </Accordion>\n  <Accordion title=\"Is it going to be hard to set up?\" icon=\"gear\" iconType=\"solid\" defaultOpen={false}>\n\n    No — in fact, the setup could not be easier:\n    - **Web Dashboard:** It can take minutes to get up & running with our [dashboard](https://dashboard.vapi.ai/).\n    - **Client SDKs:** You can start calls with 1 line of code with any of our [client SDKs](/sdks).\n\n    For more advanced features like function calling, you will have to set up a [Server URL](/server-url) to receive and respond to messages.\n\n  </Accordion>\n  <Accordion title=\"How is Vapi different from other Voice AI services?\" icon=\"bowling-pins\" iconType=\"solid\" defaultOpen={false}>\n\n    Vapi focuses on developers. Giving developers modular, simple, & robust tooling to build any voice AI application imaginable.\n\n    Vapi also has some of the lowest latency & (equally important) highest reliability amongst any other voice AI platform built for developers.\n\n  </Accordion>\n</AccordionGroup>\n\n\n## Get Support\n\nJoin our Discord to connect with other developers & connect with our team:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Join Our Discord\"\n    icon=\"fa-brands fa-discord\"\n    iconType=\"solid\"\n    color=\"#5A65EA\"\n    href=\"https://discord.gg/pUFNcf2WmH\"\n  >\n    Connect with our team & other developers using Vapi.\n  </Card>\n  <Card\n    title=\"Email Support\"\n    icon=\"mailbox\"\n    iconType=\"solid\"\n    color=\"#7a7f85\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send our support team an email.\n  </Card>\n</CardGroup>\n"
    },
    "quickstart.mdx": {
      "markdown": "---\ntitle: Core Models\nsubtitle: The three core components to Vapi's voice AI pipeline.\nslug: quickstart\n---\n\n\nAt it's core, Vapi is an orchestration layer over three modules: the **transcriber**, the **model**, and the **voice**.\n\n<Frame>\n  <img src=\"file:6331f69d-6898-4521-90ff-6123bdcb7597\" />\n</Frame>\n\nThese three modules can be swapped out with **any provider** of your choosing; OpenAI, Groq, Deepgram, ElevenLabs, PlayHT, etc. You can even plug in your server to act as the LLM.\n\nVapi takes these three modules, optimizes the latency, manages the scaling & streaming, and orchestrates the conversation flow to make it sound human.\n\n<Steps titleSize=\"h3\">\n  <Step title=\"Listen (intake raw audio)\">\n    <div>\n      When a person speaks, the client device (whether it is a laptop, phone,\n      etc) will record raw audio (1’s & 0’s at the core of it).\n    </div>\n    <div>\n      This raw audio will have to either be transcribed on the client device\n      itself, or get shipped off to a server somewhere to turn into\n      transcription text.\n    </div>\n  </Step>\n  <Step title=\"Run an LLM\">\n    <div>\n      That transcript text will then get fed into a prompt & run through an LLM\n      ([LLM inference](/glossary#inference)). The LLM is the core intelligence\n      that simulates a person behind-the-scenes.\n    </div>\n  </Step>\n  <Step title=\"Speak (text → raw audio)\">\n    <div>\n      The LLM outputs text that now must be spoken. That text is turned back\n      into raw audio (again, 1’s & 0’s), that is playable back at the user’s\n      device.\n    </div>\n    <div>\n      This process can also either happen on the user’s device itself, or on a\n      server somewhere (then the raw speech audio be shipped back to the user).\n    </div>\n  </Step>\n</Steps>\n\n<Info>The idea is to perform each phase in realtime (sensitive down to 50-100ms level), streaming between every layer. Ideally the whole flow [voice-to-voice](/glossary#voice-to-voice) clocks in at \\<500-700ms.</Info>\n\nVapi pulls all these pieces together, ensuring a smooth & responsive conversation (in addition to providing you with a simple set of tools to manage these inner-workings).\n"
    },
    "how-vapi-works.mdx": {
      "markdown": "---\ntitle: Orchestration Models\nsubtitle: All the fancy stuff Vapi does on top of the core models.\nslug: how-vapi-works\n---\n\n\nVapi also runs a suite of audio and text models that make it's latency-optimized Speech-to-Text (STT), Large Language Model (LLM), & Text-to-Speech (TTS) pipeline feel human.\n\nHere's a high-level overview of the Vapi architecture:\n\n<Frame>\n  <img src=\"file:d9d9a2ae-deb5-496e-8474-7665b81a92e8\" />\n</Frame>\n\nThese are some of the models that are part of the Orchestration suite. We currently have lots of other models in the pipeline that will be added to the orchestration suite soon. The ultimate goal is to achieve human performance.\n\n### Endpointing\n\nEndpointing is a fancy word for knowing when the user is done speaking. Traditional methods use silence detection with a timeout. Unfortunately, if we want sub-second response-times, that's not going to work.\n\nVapi's uses a custom fusion audio-text model to know when a user has completed their turn. Based on both the user's tone and what they're saying, it decides how long to pause before hitting the LLM.\n\nThis is critical to make sure the user isn't interrupted mid-thought while still providing sub-second response times when they're done speaking.\n\n### Interruptions (Barge-in)\n\nInterruptions (aka. barge-in in research circles) is the ability to detect when the user would like to interject and stop the assistant's speech.\n\nVapi uses a custom model to distinguish when there is a true interruption, like \"stop\", \"hold up\", \"that's not what I mean, and when there isn't, like \"yeah\", \"oh gotcha\", \"okay.\"\n\nIt also keeps track of where the assistant was cut off, so the LLM knows what it wasn't able to say.\n\n### Background Noise Filtering\n\nMany of our models, including the transcriber, are audio-based. In the real world, things like music and car horns can interfere with model performance.\n\nWe use a proprietary real-time noise filtering model to ensure the audio is cleaned without sacrificing latency, before it reaches the inner models of the pipeline.\n\n### Background Voice Filtering\n\nWe rely quite heavily on the transcription model to know what's going on, for interruptions, endpointing, backchanneling, and for the user's statement passed to the LLM.\n\nTranscription models are built to pick up everything that sounds like speech, so this can be a problem. As you can imagine, having a TV on in the background or echo coming back into the mic can severely impact the conversation ability of a system like Vapi.\n\nBackground noise cancellation is a well-researched problem. Background voice cancellation is not. To solve this, we built proprietary audio filtering model that's able to **focus in** on the primary speaker and block everything else out.\n\n### Backchanneling\n\nHumans like to affirm each other while they speak with statements like \"yeah\", \"uh-huh\", \"got it\", \"oh no!\"\n\nThey're not considered interruptions, they're just used to let the speaker know that their statement has been understood, and encourage the user to continue their statement.\n\nA backchannel cue used at the wrong moment can derail a user's statement. Vapi uses a proprietary fusion audio text model to determine the best moment to backchannel and to decide which backchannel cue is most appropriate to use.\n\n### Emotion Detection\n\nHow a person says something is just as important as what they're saying. So we've trained a real-time audio model to extract the emotional inflection of the user's statement.\n\nThis emotional information is then fed into the LLM, so knows to behave differently if the user is angry, annoyed, or confused.\n\n### Filler Injection\n\nThe output of LLMs tends to be formal, and not conversational. People speak with phrases like \"umm\", \"ahh\", \"i mean\", \"like\", \"so\", etc.\n\nYou can prompt the model to output like this, but we treat our user's prompts as **sacred**. Making a change like this to a prompt can change the behavior in unintended ways.\n\nTo ensure we don't add additional latency transforming the output, we've built a custom model that's able to convert streaming input and make it sound conversational in real-time.\n"
    },
    "knowledgebase.mdx": {
      "markdown": "---\ntitle: Creating Custom Knowledge Bases for Your Voice AI Assistants\nsubtitle: >-\n  Learn how to create and integrate custom knowledge bases into your voice AI\n  assistants.\nslug: knowledgebase\n---\n \n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/i5mvqC5sZxU\"\n  title=\"Vapi's Knowledge Base\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowfullscreen\n></iframe>\n\n## **What is Vapi's Knowledge Base?**\nOur Knowledge Base is a collection of custom documents that contain information on specific topics or domains. By integrating a Knowledge Base into your voice AI assistant, you can enable it to provide more accurate and informative responses to user queries.\n\n### **Why Use a Knowledge Base?**\nUsing a Knowledge Base with your voice AI assistant offers several benefits:\n\n* **Improved accuracy**: By integrating custom documents into your assistant, you can ensure that it provides accurate and up-to-date information to users.\n* **Enhanced capabilities**: A Knowledge Base enables your assistant to answer complex queries and provide detailed responses to user inquiries.\n* **Customization**: With a Knowledge Base, you can tailor your assistant's responses to specific domains or topics, making it more effective and informative.\n\n## **How to Create a Knowledge Base**\n\nTo create a Knowledge Base, follow these steps:\n\n### **Step 1: Upload Your Documents**\n\nNavigate to Overview > Documents and upload your custom documents in Markdown, PDF, plain text, or Microsoft Word (.doc and .docx) format to Vapi's Knowledge Base.\n\n<Frame caption=\"Adding documents to your Knowledge Base\">\n<img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1715628063841/rSrWDQ6YM.png\" alt=\"Adding documents to your Knowledge Base\" />\n</Frame>\n\n### **Step 2: Create an Assistant**\n\nCreate a new assistant in Vapi and, on the right sidebar menu, select the document you've just added to the Knowledge Base feature.\n\n<Frame caption=\"Adding documents to your assistant\">\n<img src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1715628223370/e18L04yRk-.png\" alt=\"Adding documents to your assistant\" />\n</Frame>\n\n\n### **Step 3: Configure Your Assistant**\n\nCustomize your assistant's system prompt to utilize the Knowledge Base for responding to user queries.\n\n## **Best Practices for Creating Effective Knowledge Bases**\n\n* **Organize Your documents**: Organize your documents by topic or category to ensure that your assistant can quickly retrieve relevant information.\n* **Use Clear and concise language**: Use clear and concise language in your documents to ensure that your assistant can accurately understand and respond to user queries.\n* **Keep your documents up-to-date**: Regularly update your documents to ensure that your assistant provides the most accurate and up-to-date information.\n \n<Tip>\n  For more information on creating effective Knowledge Bases, check out our tutorial on [Best Practices for Knowledge Base Creation](https://youtu.be/i5mvqC5sZxU).\n</Tip>\n\nBy following these guidelines, you can create a comprehensive Knowledge Base that enhances the capabilities of your voice AI assistant and provides valuable information to users.\n"
    },
    "pricing.mdx": {
      "markdown": "---\ntitle: Pricing Overview\nsubtitle: Only pay for the minutes you use.\nslug: pricing\n---\n\n\n<Frame caption=\"Vapi charges a flat 5¢/min for calls.\">\n  <img src=\"file:adde559a-870e-4373-9ef3-09f8bb07d6ee\" />\n</Frame>\n\n<br />\n\n<CardGroup cols={2}>\n  <Card title=\"5¢/min for Calls\" icon=\"cent-sign\" iconType=\"solid\">\n    Vapi itself charges $0.05 per minute for calls. Prorated to the second.\n  </Card>\n  <Card\n    title=\"At-Cost for Providers\"\n    icon=\"chart-network\"\n    iconType=\"sharp-solid\"\n    color=\"#5BBFF1\"\n  >\n    Transcriber, model, voice, & telephony costs charged at-cost.\n  </Card>\n  <Card\n    title=\"Bring Your Own Keys\"\n    icon=\"key-skeleton-left-right\"\n    iconType=\"solid\"\n    color=\"#AD81F2\"\n  >\n    Bring your own API keys for providers, Vapi makes requests on your behalf.\n  </Card>\n  <Card\n    title=\"$2/mo for Phone Numbers\"\n    icon=\"phone-office\"\n    iconType=\"solid\"\n    color=\"#fcba03\"\n  >\n    Phone numbers purchased through Vapi bill at $2/mo.\n  </Card>\n</CardGroup>\n\n### Starter Credits\n\nEvery new account is granted **$10 in free credits** to begin testing voice workflows. You can [begin using Vapi](/quickstart/dashboard) without a credit card.\n\n---\n\n## Enterprise\n\nHandling a large volume of calls? You can find more information on our Enterprise plans [here](/enterprise).\n\n- Higher concurrency and rate limits\n- Hands-on 24/7 support\n- Shared Slack channel with our team\n- Included minutes with volume pricing\n- Calls with our engineering team 2-3 times per week\n\n## Further Reading\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Routing Provider Cost\"\n    icon=\"route\"\n    iconType=\"solid\"\n    href=\"/billing/cost-routing\"\n  >\n    Learn more about how Vapi routes provider costs.\n  </Card>\n  <Card\n    title=\"Estimating Costs\"\n    icon=\"equals\"\n    iconType=\"solid\"\n    href=\"/billing/estimating-costs\"\n  >\n    Learn more about estimating costs for your voice pipeline.\n  </Card>\n  <Card\n    title=\"Billing Limits\"\n    icon=\"meter\"\n    iconType=\"solid\"\n    href=\"/billing/billing-limits\"\n  >\n    Learn how to set billing limits for your account.\n  </Card>\n  <Card\n    title=\"Billing Examples\"\n    icon=\"books\"\n    iconType=\"solid\"\n    href=\"/billing/examples\"\n  >\n    Read full end-to-end billing breakdowns to better understand how Vapi bills.\n  </Card>\n</CardGroup>\n"
    },
    "billing/cost-routing.mdx": {
      "markdown": "---\ntitle: Cost Routing\nsubtitle: Learn more about how your Vapi account is billed for provider expenses.\nslug: billing/cost-routing\n---\n\n\n<Frame caption=\"Provider expenses can either end up provider-side, or at-cost in your Vapi account.\">\n  <img src=\"file:e516a5ae-85e2-4aa5-9ec5-de353c98f659\" />\n</Frame>\n\nDuring calls, requests will be made to different providers in the voice pipeline:\n\n- **transcription providers:** providers conducting speech-to-text\n- **model providers:** LLM providers\n- **voice providers:** providers conducting text-to-speech\n- **telephony providers:** providers like [Twilio](https://www.twilio.com)/[Vonage](https://www.vonage.com) that facilitate phone calls\n\n<Info>\n  Per-minute telephony costs only occur during inbound/outbound phone calling. Web calls do not\n  incur this cost.\n</Info>\n\n## Where Provider Costs End-up\n\nThere are 2 places these charges can end up:\n\n1. **Provider-side:** in the account you have with the provider.\n2. **With Vapi:** in your Vapi account.\n\n<AccordionGroup>\n  <Accordion title=\"Billed Provider-side\" icon=\"washing-machine\" iconType=\"solid\" defaultOpen={true}>\n    If we have [provider keys](customization/provider-keys) on file for a provider, the cost will be seen directly\n    in your account with the provider. Vapi will have made the request on your behalf with your provider key.\n\n    No charge will be made to your Vapi account.\n\n    Charges for inbound/outbound phone calling (telephony) will always end up where the phone number\n    was provisioned. If you import a phone number from Twilio or Vonage, per-minute charges for calling\n    those numbers will appear with them.\n\n  </Accordion>\n  <Accordion title=\"Billed with Vapi\" icon=\"v\" iconType=\"solid\" defaultOpen={true}>\n    If no key is found on-file for the provider, Vapi will make the API request itself (with Vapi's own keys, at Vapi's expense). This expense is then passed on [**at-cost**](/glossary#at-cost) to be billed directly to your Vapi account.\n\n    No charge will show up provider-side.\n\n  </Accordion>\n</AccordionGroup>\n\n## Billing That \"Just Works\"\n\nThe central idea is that everything is designed to \"just work\".\n\nWhether you are billed provider-side, or on Vapi's side, you will never be charged with any margin for provider fees incurred during calls.\n"
    },
    "billing/billing-limits.mdx": {
      "markdown": "---\ntitle: Billing Limits\nsubtitle: Set billing limits on your Vapi account.\nslug: billing/billing-limits\n---\n\n\nYou can set billing limits in the billing section of your dashboard.\n\n<Note>\n  You can access your billing settings at\n  [dashboard.vapi.ai/billing](https://dashboard.vapi.ai/billing)\n</Note>\n\n### Setting a Monthly Billing Limit\n\nIn your billing settings you can set a monthly billing limit:\n\n<Frame>\n  <img src=\"file:fbf84b40-499d-4f63-8dee-c7dcdce341fe\" />\n</Frame>\n\n### Exceeding Billing Limits\n\nOnce you have used all of your starter credits, or exceeded your set monthly usage limit, you will start seeing errors in your dashboard & via the API mentioning `Billing Limits Exceeded`.\n\n<Frame caption=\"Once you have gone over on your monthly billing limits, the API & dashboard will throw billing-related errors.\">\n  <img src=\"file:1b83d54f-319e-4b5f-8193-002566616528\" />\n</Frame>\n"
    },
    "billing/estimating-costs.mdx": {
      "markdown": "---\ntitle: Estimating Costs\nsubtitle: Get information on your voice pipeline's projected costs.\nslug: billing/estimating-costs\n---\n\n\nSince there are so many moving parts to the voice pipeline that can incur cost, it would be ideal for us to get a good estimate of our final projected per-minute cost for calls.\n\n### Dashboard Cost Estimates\n\nThe Vapi dashboard provides static cost projections on a per-assistant basis, so you can get a rough idea of the costs your assistant will incur during live execution.\n\n<Tip>\n  You can view your dashboard at [dashboard.vapi.ai](https://dashboard.vapi.ai/)\n  & get started with our [dashboard quickstart](/quickstart/dashboard).\n</Tip>\n\n<Frame caption=\"Vapi-computed per-minute cost projection, as well as latency projection.\">\n  <img\n    src=\"file:d50a0410-d5a5-4f26-9c29-bd22d716933a\"\n  />\n</Frame>\n\n### General Provider Estimates\n\nThe provider costs listed below are subject to change as we get more data, but they will always reflect our best estimate of the provider costs per minute:\n\n<AccordionGroup>\n  <Accordion\n    title=\"Transcription Provider Estimates\"\n    icon=\"microphone\"\n    iconType=\"solid\"\n    defaultOpen={true}\n  >\n    | Provider |   \\$/min (≈)   |  \\$/hour  |\n    | -------- | -------------- | --------- |\n    | Deepgram | **\\$0.01/min** | \\$0.60/hr |\n  </Accordion>\n  <Accordion title=\"Model Provider Estimates\" icon=\"microchip\" iconType=\"solid\" defaultOpen={true}>\n    | Provider                | $/min (≈)       | $/hour        |\n    | ----------------------- | --------------- | ------------- |\n    | OpenAI (gpt-4-turbo)    | **$0.20/min**   | $12.00/hr     |\n    | OpenAI (gpt-3.5-turbo)  | **$0.02/min**   | $1.20/hr      |\n\n  </Accordion>\n  <Accordion title=\"Voice Provider Estimates\" icon=\"person\" iconType=\"solid\" defaultOpen={true}>\n    | Provider   | $/min (≈)       | $/hour     |\n    | ---------- | --------------- | ---------- |\n    | ElevenLabs | **$0.04/min**   | $2.40/hr   |\n    | PlayHT     | **$0.07/min**   | $4.20/hr   |\n    | Deepgram   | **$0.02/min**   | $1.20/hr   |\n    | OpenAI     | **$0.02/min**   | $1.20/hr   |\n    | RimeAI     | **$0.03/min**   | $1.80/hr   |\n    | Azure      | **$0.02/min**   | $1.20/hr   |\n    | Neets      | **$0.005/min**  | $0.30/hr   |\n    | LMNT       | **$0.03/min**   | $1.80/hr   |\n\n  </Accordion>\n  <Accordion\n    title=\"Telephony Provider Estimates\"\n    icon=\"phone-arrow-up-right\"\n    iconType=\"solid\"\n    defaultOpen={true}\n  >\n    | Provider | $/min (≈)       | $/hour     |\n    | -------- | --------------- | ---------- |\n    | Twilio   | **$0.01/min**   | $0.60/hr   |\n    | Vonage   | **$0.01/min**   | $0.60/hr   |\n\n  </Accordion>\n</AccordionGroup>\n\n### Provider Pricings\n\nHere are direct links to different provider's pricing pages to assist in estimating cost:\n\n<AccordionGroup>\n  <Accordion\n    title=\"Transcription Providers\"\n    icon=\"microphone\"\n    iconType=\"solid\"\n    defaultOpen={true}\n  >\n    <CardGroup cols={2}>\n      <Card\n        title=\"Deepgram\"\n        href=\"https://deepgram.com/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        Deepgram transcription pricing.\n      </Card>\n    </CardGroup>\n  </Accordion>\n  <Accordion\n    title=\"Model Providers\"\n    icon=\"microchip\"\n    iconType=\"solid\"\n    defaultOpen={true}\n  >\n    <CardGroup cols={2}>\n      <Card\n        title=\"OpenAI\"\n        href=\"https://openai.com/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        OpenAI model pricing.\n      </Card>\n    </CardGroup>\n  </Accordion>\n  <Accordion\n    title=\"Voice Providers\"\n    icon=\"person\"\n    iconType=\"solid\"\n    defaultOpen={true}\n  >\n    <CardGroup cols={3}>\n      <Card\n        title=\"ElevenLabs\"\n        href=\"https://elevenlabs.io/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        ElevenLabs voice pricing.\n      </Card>\n      <Card\n        title=\"PlayHT\"\n        href=\"https://play.ht/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        PlayHT voice pricing.\n      </Card>\n      <Card\n        title=\"Deepgram\"\n        href=\"https://deepgram.com/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        Deepgram voice pricing.\n      </Card>\n      <Card\n        title=\"OpenAI\"\n        href=\"https://openai.com/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        OpenAI voice pricing.\n      </Card>\n      <Card\n        title=\"RimeAI\"\n        href=\"https://rime.ai/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        RimeAI voice pricing.\n      </Card>\n      <Card\n        title=\"Azure\"\n        href=\"https://azure.microsoft.com/en-us/pricing/details/cognitive-services/speech-services\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        Azure voice pricing.\n      </Card>\n      <Card\n        title=\"Neets\"\n        href=\"https://neets.ai/#pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        Neets voice pricing.\n      </Card>\n      <Card\n        title=\"LMNT\"\n        href=\"https://app.lmnt.com/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        LMNT voice pricing.\n      </Card>\n    </CardGroup>\n  </Accordion>\n  <Accordion\n    title=\"Telephony Providers\"\n    icon=\"phone-arrow-up-right\"\n    iconType=\"solid\"\n    defaultOpen={true}\n  >\n    <CardGroup cols={2}>\n      <Card\n        title=\"Twilio\"\n        href=\"https://www.twilio.com/en-us/voice/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        Twilio phone call pricing.\n      </Card>\n      <Card\n        title=\"Vonage\"\n        href=\"https://www.vonage.com/communications-apis/voice/pricing\"\n        icon=\"arrow-up-right-from-square\"\n              >\n        Vonage phone call pricing.\n      </Card>\n    </CardGroup>\n  </Accordion>\n</AccordionGroup>\n\n### Calling Your Assistant\n\nOne good way to get an empirical per-minute cost on your whole voice pipeline is to actually call in, use it for a few minutes, & observe the average cost/minute at the call level.\n\n<Note>\n  You can view a breakdown of your cost per call in your dashboard at\n  [dashboard.vapi.ai/calls](https://dashboard.vapi.ai/calls)\n</Note>\n\nYour call cost breakdowns will look something like this:\n\n<Frame caption=\"Cost breakdown for a call viewed in the Vapi web dashboard.\">\n  <img src=\"file:404dd489-8a21-4fe1-b1c6-4e0e25b612bf\" />\n</Frame>\n\nHere is what each line item corresponds to:\n\n- `STT`: Speech-to-text (providers often bill per-minute, prorated)\n- `LLM`: LLM inference (providers often bill per-million or per-thousand tokens)\n- `TTS`: Text-to-speech (providers often bill per-character)\n- `Vapi`: the Vapi platform fee of 5¢/minute (prorated per-second)\n- `Transport`: telephony costs (incurred for inbound/outbound phone calls to/from a phone number) (providers often bill per-minute)\n\nThis method can be effective because **per-minute costs will not scale** with the amount of call minutes you consume. The cost for the 1st minute will be the same as the 10,000th minute.\n\n<Tip>\n  Volume pricing is available on enterprise plans. Check out\n  [enterprise](/enterprise) to learn more.\n</Tip>\n"
    },
    "billing/examples.mdx": {
      "markdown": "---\ntitle: Billing Examples\nsubtitle: End-to-end examples estimating voice workflow cost on Vapi.\nslug: billing/examples\n---\n\n\n<Frame>\n  <img src=\"file:ab4e9cb7-5df5-40f1-b64b-2509072a0ef9\" />\n</Frame>\n\n## Case Examples\n\nHere are a few case-examples of what billing would look like on Vapi for different voice pipeline configurations.\n\n<AccordionGroup>\n  <Accordion title=\"Call Center (Inbound Phone)\" icon=\"phone-arrow-down-left\" iconType=\"solid\" defaultOpen={true}>\n    A customer is looking to use Vapi to assist their call center staff taking phone calls inbound:\n\n    <Steps>\n      <Step title=\"Scenario\">\n        <div>\n          \"I want to use Vapi voice assistants to support my human customer service reps in a call\n          center. However, I have a custom LLM I would prefer to use instead of the ones offered through\n          the platform.\n        </div>\n\n        <div>\n          Expected monthly usage will be 10,000 calls, with an average of 2 minutes per call. For Voice,\n          PlayHT will suit our needs.\n        </div>\n\n        <div>What is my pricing breakdown?\"</div>\n\n      </Step>\n      <Step title=\"Providers\">\n        The providers used will determine per-minute cost. The following providers will be involved:\n\n        <div>**Transcriber:** Deepgram</div>\n        <div>**Model:** custom model</div>\n        <div>**Voice:** PlayHT</div>\n        <div>**Telephony:** Twilio (receiving inbound phone calls)</div>\n\n        <br/>\n        <Frame caption=\"The customer will be using a custom model & taking inbound phone calls.\">\n          <img src=\"file:3b2ecede-ee4f-41cd-b5f3-d3bf7fff4480\" />\n        </Frame>\n        <br/>\n\n      </Step>\n      <Step title=\"Cost Breakdown\">\n        We will break down the costs of each piece of the voice pipeline, then later multiply by call volume:\n\n        <div>**Deepgram:** ≈ \\$0.01/min</div>\n        <div>**Custom Model:** ≈ \\$0.04/min (vague assumption, can vary widely)</div>\n        <div>**PlayHT:** ≈ \\$0.07/min</div>\n        <div>**Twilio:** ≈ \\$0.02/min (inbound, toll-free) (see Twilio [phone call pricing](https://www.twilio.com/en-us/voice/pricing))</div>\n        <div>**Vapi:** \\$0.05/min</div>\n\n        <Tip>Our [estimating costs](/billing/estimating-costs) guide can help you determine these values.</Tip>\n\n      </Step>\n      <Step title=\"Final Estimate\">\n        Call Minutes / Month: 10,000 calls x 2 min/call = **20,000 call minutes**\n\n        <div>**Transcription:** \\$0.01/min x 20,000 = **\\$200**</div>\n        <div>**Custom Model:** \\$0.04/min x 20,000 = **\\$800**</div>\n        <div>**Voice:** ≈ \\$0.07/min x 20,000 = **\\$1,400**</div>\n        <div>**Telephony:** ≈ \\$0.02/min x 20,000 = **\\$400**</div>\n        <div>**Vapi:** \\$0.05/min x 20,000 = **\\$1,000**</div>\n\n        **Total**: **\\$3,800**/mo\n\n      </Step>\n    </Steps>\n\n  </Accordion>\n  <Accordion title=\"Real Estate Lead Generation (Outbound Phone)\" icon=\"phone-arrow-up-right\" iconType=\"solid\">\n    A customer doing real estate lead generation is looking to use Vapi to automate parts of their sales calling operation:\n\n    <Steps>\n      <Step title=\"Scenario\">\n        \"I have a company that does real estate lead generation, and would like to use Vapi voice\n        assistants to automate parts of my sales process.\n\n        Calls would average ~4 minutes, for Model I want to use GPT-3.5-turbo through your platform, and for Voice I will be using a ElevenLabs.\n\n        I’d like a breakdown based on sending 1,000 outbound calls in one month.\"\n\n      </Step>\n      <Step title=\"Providers\">\n        <div>**Transcriber:** Deepgram</div>\n        <div>**Model:** OpenAI (GPT-3.5 Turbo)</div>\n        <div>**Voice:** ElevenLabs</div>\n        <div>**Telephony:** Vonage (sending outbound phone calls)</div>\n\n        <br/>\n        <Frame caption=\"The customer will be using GPT-4 & making outbound phone calls.\">\n          <img src=\"file:dd0b4692-a514-4974-b234-a2f89fa8dec8\" />\n        </Frame>\n        <br/>\n\n      </Step>\n      <Step title=\"Cost Breakdown\">\n        <div>**Deepgram:** ≈ \\$0.01/min</div>\n        <div>**OpenAI (GPT-3.5 Turbo):** ≈ \\$0.02/min</div>\n        <div>**ElevenLabs:** ≈ \\$0.04/min</div>\n        <div>**Vonage:** ≈ \\$0.01/min (outbound call) (see Vonage's [phone call pricing](https://www.vonage.com/communications-apis/voice/pricing))</div>\n        <div>**Vapi:** \\$0.05/min</div>\n\n        <Tip>Our [estimating costs](/billing/estimating-costs) guide can help you determine these values.</Tip>\n\n      </Step>\n      <Step title=\"Final Estimate\">\n        Call Minutes / Month: 1,000 calls x 4 min/call = **4,000 call minutes**\n\n        <div>**Transcription:** \\$0.01/min x 4,000 = **\\$40**</div>\n        <div>**Model:** \\$0.02/min x 4,000 = **\\$80**</div>\n        <div>**Voice:** ≈ \\$0.04/min x 4,000 = **\\$160**</div>\n        <div>**Telephony:** ≈ \\$0.01/min x 4,000 = **\\$40**</div>\n        <div>**Vapi:** \\$0.05/min x 4,000 = **\\$200**</div>\n\n        **Total**: **\\$520**/mo\n\n      </Step>\n    </Steps>\n\n  </Accordion>\n  <Accordion title=\"Mock Intervieweing Application (Web)\" icon=\"browser\" iconType=\"solid\">\n    A web engineer is looking to develop a website that helps job candidates practice for job interviews. They are looking to use Vapi for their virtual interviewers:\n\n    <Steps>\n      <Step title=\"Scenario\">\n        \"Hi, I'm looking to develop a web application for mock interviews. Users will be able to practice for a variety\n        of job interviews with AI interviewers.\n\n        Interviews will be 30-minutes each (at max), for model I'll be using a custom open-source model hosted with Baseten & for voice I'll be using PlayHT.\n\n        How much would this cost me each month if I service 1,000 interviews per month?\"\n\n      </Step>\n      <Step title=\"Providers\">\n        <div>**Transcriber:** Deepgram</div>\n        <div>**Model:** custom model</div>\n        <div>**Voice:** PlayHT</div>\n\n        <br/>\n        <Frame caption=\"The customer will be using a custom model & making web calls (no telephony will be involved).\">\n          <img src=\"file:26e63dd7-80a1-460b-a858-1fb4d48abf07\" />\n        </Frame>\n        <br/>\n\n      </Step>\n      <Step title=\"Cost Breakdown\">\n        <div>**Deepgram:** ≈ \\$0.01/min</div>\n        <div>**Custom Model:** ≈ \\$0.02/min (vague assumption, can vary widely)</div>\n        <div>**PlayHT:** ≈ \\$0.07/min</div>\n        <div>**Vapi:** \\$0.05/min</div>\n\n        <Tip>Our [estimating costs](/billing/estimating-costs) guide can help you determine these values.</Tip>\n\n      </Step>\n      <Step title=\"Final Estimate\">\n        Call Minutes / Month: 1,000 calls x 30 min/call = **30,000 call minutes**\n\n        <div>**Transcription:** \\$0.01/min x 30,000 = **\\$300**</div>\n        <div>**Model:** \\$0.02/min x 30,000 = **\\$600**</div>\n        <div>**Voice:** ≈ \\$0.07/min x 30,000 = **\\$2,100**</div>\n        <div>**Vapi:** \\$0.05/min x 30,000 = **\\$1,500**</div>\n\n        **Total**: **\\$4,500**/mo\n\n      </Step>\n    </Steps>\n\n  </Accordion>\n</AccordionGroup>\n\n### Further Reading\n\n<CardGroup cols={2}>\n  <Card title=\"Provider Costs\" icon=\"route\" iconType=\"solid\" href=\"/billing/cost-routing\">\n    Learn more about where provider costs end up getting billed.\n  </Card>\n  <Card title=\"Estimating Costs\" icon=\"equals\" iconType=\"solid\" href=\"/billing/estimating-costs\">\n    Learn more about determining per-minute costs for providers.\n  </Card>\n</CardGroup>\n"
    },
    "enterprise/plans.mdx": {
      "markdown": "---\ntitle: Vapi Enterprise\nsubtitle: Build and scale with Vapi.\nslug: enterprise/plans\n---\n\n\nIf you're building a production application on Vapi, we can help you every step of the way from idea to full-scale deployment.\n\nOn the Pay-As-You-Go plan, there is a limit of **10 concurrent calls**. On Enterprise, we reserve GPUs for you on our Enterprise cluster so you can scale up to **millions of calls**.\n\n#### Enterprise Plans include:\n\n- Reserved concurrency and higher rate limits\n- Hands-on 24/7 support\n- Shared Slack channel with our team\n- Included minutes with volume pricing\n- Calls with our engineering team 2-3 times per week\n- Access to the Vapi SIP trunk for telephony\n\n## Contact us\n\nTo get started on Vapi Enterprise, [fill out this form](https://book.vapi.ai).\n"
    },
    "enterprise/onprem.mdx": {
      "markdown": "---\ntitle: On-Prem Deployments\nsubtitle: Deploy Vapi in your private cloud.\nslug: enterprise/onprem\n---\n\n\nVapi On-Prem allows you to deploy Vapi's best in class enterprise voice infrastructure AI directly in your own cloud. It can be deployed in a dockerized format on any cloud provider, in any geographic location, running on your GPUs.\n\nWith On-Prem, your audio and text data stays in your cloud. Data never passes through Vapi's servers. If you're are handling sensitive data (e.g. health, financial, legal) and are under strict data requirements, you should consider deploying on-prem.\n\nYour device regularly sends performance and usage information to Vapi's cloud. This data helps adjust your device's GPU resources and is also used for billing. All network traffic from your device is tracked in an audit log, letting your engineering or security team see what the device is doing at all times.\n\n## Frequently Asked Questions\n\n#### Can the appliance adjust to my needs?\n\nYes, the Vapi On-Prem appliance automatically adjusts its GPU resources to handle your workload as required by our service agreement. It can take a few minutes to adjust to changes in your workload. If you need quicker adjustments, you might want to ask for more GPUs by contacting support@vapi.ai.\n\n#### What if I can’t get enough GPUs from my cloud provider?\n\nIf you're struggling to get more GPUs from your provider, contact support@vapi.ai for help.\n\n#### Can I access Vapi's AI models?\n\nNo, our AI models are on secure machines in your Isolated VPC and you can’t log into these machines or check their files.\n\n#### How can I make sure my data stays within my cloud?\n\nYour device operates in VPCs that you control. You can check the network settings and firewall rules, and look at traffic logs to make sure everything is as it should be. The Control VPC uses open source components, allowing you to make sure the policies are being followed. Performance data and model updates are sent to Vapi, but all other traffic leaving your device is logged, except for the data sent back to your API clients.\n\n## Contact us\n\nFor more information about Vapi On-Prem, please contact us at support@vapi.ai\n"
    },
    "changelog.mdx": {
      "markdown": "---\ntitle: Changelog\nsubtitle: 'New features, improvements, and fixes every few days'\nslug: changelog\n---\n\n\n# October 7 to October 8, 2024\n\n1. **New GPT-4o Model Support for Azure OpenAI**: You can now specify the `gpt-4o-2024-08-06` model in the `models` field when configuring Azure OpenAI credentials. Use this model to access the latest GPT-4 operational capabilities in your applications.\n\n2. **Specify Timestamps as Strings in `/logs`**: We now expect timestamps as strings when working with logs. Please make sure to handle this accordingly in your applications.\n\n\n# October 6 to October 7, 2024\n\n1. **Add Structured Outputs for OpenAI Functions in Assistant Tools**: You can use [OpenAI Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs) by specifying a new parameter called `strict` as true or false when creating or using `OpenAIFunction`s in `assistant.model.tools[type=function]`. Set the `name`, provide a `description` (up to 1000 characters), and specify `parameters` as a [JSON Schema object](https://json-schema.org/understanding-json-schema). See the [OpenAI guide](https://platform.openai.com/docs/guides/function-calling) for examples.\n\n2. **Secure Incoming SIP Phone Calls to Vapi Provided SIP Numbers**: You can now specify a `username`, `password`, and optional `realm` in SIP Invite AuthZ header, through digest authentication. Create this secure SIP number by specifying an \"authentication\" object with the username and password fields inside `POST /phone-number` request body. Example:\n```bash\ncurl --location 'https://api.vapi.ai/phone-number' \\\n--header 'Content-Type: application/json' \\\n--header 'Authorization: Bearer {}API_KEY}}' \\\n--data-raw '{\n  \"provider\": \"vapi\",\n  \"sipUri\": \"sip:{{USERNAME}}@sip.vapi.ai\",\n  \"assistantId\": \"{{ASSISTANT_ID}}\",\n  \"name\": \"example phone number label for your reference\",\n  \"authentication\": {\n    \"realm\": \"sip.vapi.ai\",\n    \"username\": \"test@example.com\",\n    \"password\": \"example_password\"\n  }\n}'\n```\n\n3. **Use Updated `handoff`, `callback` Steps in Blocks**: You can now use `assistant.model.steps[type=handoff]` and `assistant.model.steps[type=callback]` to control conversation flow in your assistant. Use `HandoffStep` to move to the next step linearly without returning to the previous step, ideal for sequential tasks like forms. Use `CallbackStep` to spawn a new conversation thread and return to the previous step once done, good for handling interruptions or sub-tasks within a conversation.\n\n4. **Use Step Destinations and Assignment Mutation in Blocks**: Specify destination nodes for each step with `assistant.model.steps[type=handoff].destinations[type=step]` to direct the workflow to specific steps based on certain conditions. Update context variables in each callback step with `mutations[type=assignment]`, for example: `assistant.model.steps[type=callback].mutations[type=assignment]`\n"
    },
    "support.mdx": {
      "markdown": "---\ntitle: Support\nsubtitle: >-\n  We are open to all kinds of help inquiry, feedback and feature request, help\n  inquiry.\nslug: support\n---\n\n\n## Join Vapi community\n\n- To take part in community discussion join our [Discord](https://discord.com/invite/pUFNcf2WmH) server to collaborate with other developers.\n- For quick support: Visit #support channel to submit support requests.\n"
    },
    "quickstart/dashboard.mdx": {
      "markdown": "---\ntitle: Dashboard\nsubtitle: Quickstart with the Vapi web dashboard.\nslug: quickstart/dashboard\n---\n\n<Frame>\n  <div class=\"video-embed-wrapper\">\n    <iframe\n      src=\"https://www.youtube.com/embed/sFXaTsmMR8s?si=aV-mAdjwkpHchHfT\"\n      title='An embedded YouTube video titled \"Quickstart: VAPI Dashboard\"'\n      frameborder=\"0\"\n      allow=\"fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n      allowfullscreen\n      referrerpolicy=\"strict-origin-when-cross-origin\"\n    />\n  </div>\n</Frame>\n\n### The Web Dashboard\n\nOne of the easiest ways to get started with Vapi is by using the web dashboard.\n\n<Note>You can visit your dashboard by going to [dashboard.vapi.ai](https://dashboard.vapi.ai)</Note>\n\nThe web dashboard gives you the ability to:\n\n- **view, create, & modify [assistants](/assistants)** associated with your account\n- **provision & manage phone numbers** assistants can dial outbound from or receive inbound calls to\n- **review conversation data** (such as audio recordings, call metadata, etc)\n- **manage your [provider keys](customization/provider-keys)** (used in communication with external [TTS](/glossary#tts), LLM, & [STT](/glossary#stt) vendors)\n\nWe will be walking through the core necessities you need to get up and running in this guide.\n\n<Tip>\n  The web dashboard wraps over much of the realtime call functionality of Vapi. The dashboard\n  actually uses the [web SDK](/sdk/web) beneath-the-hood to make web calls.\n</Tip>\n\n## Vapi’s Pizzeria\n\nIn this guide we will be implementing a simple order-taking assistant at a pizza shop called “Vapi’s Pizzeria”.\n\nVapi’s has 3 types of menu items: `pizza`, `side`s, & `drink`s. Customers will be ordering 1 of each.\n\n<Frame caption=\"Customers will order 3 items: 1 pizza, 1 side, & 1 drink. The assistant will handle the full order taking conversation.\">\n  <img src=\"file:88a31f93-503c-4aa0-9d30-8d50060af0f3\" />\n</Frame>\n\n## Assistant Setup\n\nFirst we're going to set up our assistant in the dashboard. Once our assistant’s **transcriber**, **model**, & **voice** are set up, we can call it to place our order.\n\n<Note>You can visit your dashboard at [dashboard.vapi.ai](https://dashboard.vapi.ai/)</Note>\n\n<AccordionGroup>\n  <Accordion title=\"Sign-up or Log-in to Vapi\" icon=\"user-plus\" iconType=\"solid\">\n    If you haven't already signed-up, you're going to need an account before you can use the web dashboard. When you visit [dashboard.vapi.ai](https://dashboard.vapi.ai) you may see something like this:\n\n    <Frame>\n      <img src=\"file:2a36b470-9983-4270-aa74-bb2fccc56af4\" />\n    </Frame>\n\n    Sign-up for an account (or log-in to your existing account) — you will then find yourself inside the web dashboard. It will look something like this:\n\n    <Frame caption=\"Your dashboard may look a bit different if you already have an account with assistants in it. The main idea is that we’re in the dashboard now.\">\n      <img src=\"file:20f9dc5a-cf0e-4750-ad63-82f8a16c8a0d\" />\n    </Frame>\n\n  </Accordion>\n  <Accordion title=\"Create an Assistant\" icon=\"layer-plus\" iconType=\"solid\">\n    Now that you're in your dashboard, we're going to create an [assistant](/assistants).\n\n    Assistants are at the heart of how Vapi models AI voice agents — we will be setting certain properties on a new assistant to model an order-taking agent.\n\n    Once in the \"Assistants\" dashboard tab (you should be in it by-default after log-in), you will see a button to create a new assistant.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"file:0eb3cc5f-5aa4-435f-8a56-fd05ae3ff398\" />\n    </Frame>\n\n    After clicking the create new assistant button, you will see a pop-up modal that asks you to pick a starter template. For our example we will start from a blank slate so choose the `Blank Template` option.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"file:80f913e8-fd91-46e6-9cff-f574bd100f10\" />\n    </Frame>\n\n    You will then be able to name your assistant — you can name it whatever you'd like (`Vapi’s Pizza Front Desk`, for example):\n\n    <Info>\n      This name is only for internal labeling use. It is not an identifier, nor will the assistant be\n      aware of this name.\n    </Info>\n\n    <Frame caption=\"Name your assistant.\">\n      <img src=\"file:a4fc51f3-6f6d-43c8-b577-da34b255198a\" />\n    </Frame>\n\n    Once you have named your assistant, you can hit \"Create\" to create it. You will then see something like this:\n\n    <Frame caption=\"The assistant overview. You can edit your assistant’s transcriber, model, & voice — and edit other advanced configuration.\">\n      <img src=\"file:300206c9-ee39-484c-910e-e45e3a28567d\" />\n    </Frame>\n\n    This is the assistant overview view — it gives you the ability to edit different attributes about your assistant, as well as see **cost** & **latency** projection information for each portion of it’s voice pipeline (this is very important data to have handy when building out your assistants).\n\n  </Accordion>\n  <Accordion title=\"Model Setup\" icon=\"microchip\" iconType=\"solid\">\n    Now we’re going to set the \"brains\" of the assistant, the large language model. We're going to be using `GPT-4` (from [OpenAI](https://openai.com/)) for this demo (though you're free to use `GPT-3.5`, or any one of your favorite LLMs).\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your OpenAI Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        Before we proceed, we can set our [provider key](customization/provider-keys) for OpenAI (this is just your OpenAI secret key).\n\n        <Note>\n          You can see all of your provider keys in the \"Provider Keys\" dashboard tab. You can also go\n          directly to [dashboard.vapi.ai/keys](https://dashboard.vapi.ai/keys).\n        </Note>\n\n        Vapi uses [provider keys](customization/provider-keys) you provide to communicate with LLM, TTS, & STT vendors on your behalf. It is most ideal that we set keys for the vendors we intend to use ahead of time.\n\n        <Frame caption=\"We set our provider key for OpenAI so Vapi can make requests to their API.\">\n          <img src=\"file:d9e19883-176c-4849-ab6f-91eebf3615e1\" />\n        </Frame>\n\n        While we're here it'd be ideal for you to go & set up provider keys for other providers you're familiar with & intend to use later.\n      </Accordion>\n      <Accordion title=\"Set a First Message\" icon=\"message\" iconType=\"light\">\n        Assistants can **optionally** be configured with a `First Message`. This first message will be spoken by your assistant when either:\n\n        - **A Web Call Connects:** when a web call is started with your assistant\n        - **An Inbound Call is Picked-up:** an [inbound call](/glossary#inbound-call) is picked-up & answered by your assistant\n        - **An Outbound Call is Dialed & Picked-up:** an [outbound call](/glossary#outbound-call) is dialed by your assistant & a person picks up\n\n        <Warning>\n          Note that this first message cannot be interrupted & is guaranteed to be spoken. Certain use cases\n          need a first message, while others do not.\n        </Warning>\n\n        For our use case, we will want a first message. It would be ideal for us to have a first message like this:\n\n        ```text\n        Vappy’s Pizzeria speaking, how can I help you?\n        ```\n\n        <Info>\n          Some text-to-speech voices may struggle to pronounce 'Vapi' correctly, compartmentalizing it to be\n          spoken letter by letter \"V. A. P. I.\"\n\n        Some aspects of configuring your voice pipeline will require tweaks like this to get the target\n        behaviour you want.\n\n        </Info>\n\n        This will be spoken by the assistant when a web or inbound phone call is received.\n      </Accordion>\n      <Accordion title=\"Set the System Prompt\" icon=\"message\" iconType=\"solid\">\n        We will now set the `System Prompt` for our assistant. If you're familiar with OpenAI's API, this is the first prompt in the message list that we feed our LLM (learn more about prompt engineering on the [OpenAI docs](https://platform.openai.com/docs/guides/prompt-engineering)).\n\n        The system prompt can be used to configure the context, role, personality, instructions and so on for the assistant. In our case, a system prompt like this will give us the behaviour we want:\n\n        ```text\n        You are a voice assistant for Vappy’s Pizzeria,\n        a pizza shop located on the Internet.\n\n        Your job is to take the order of customers calling in. The menu has only 3 types\n        of items: pizza, sides, and drinks. There are no other types of items on the menu.\n\n        1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza\n        (often called \"veggie\" pizza).\n        2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.\n        3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a\n        brand name like \"coca cola\", just let them know that we only offer \"soda\")\n\n        Customers can only order 1 of each item. If a customer tries to order more\n        than 1 item within each category, politely inform them that only 1 item per\n        category may be ordered.\n\n        Customers must order 1 item from at least 1 category to have a complete order.\n        They can order just a pizza, or just a side, or just a drink.\n\n        Be sure to introduce the menu items, don't assume that the caller knows what\n        is on the menu (most appropriate at the start of the conversation).\n\n        If the customer goes off-topic or off-track and talks about anything but the\n        process of ordering, politely steer the conversation back to collecting their order.\n\n        Once you have all the information you need pertaining to their order, you can\n        end the conversation. You can say something like \"Awesome, we'll have that ready\n        for you in 10-20 minutes.\" to naturally let the customer know the order has been\n        fully communicated.\n\n        It is important that you collect the order in an efficient manner (succinct replies\n        & direct questions). You only have 1 task here, and it is to collect the customers\n        order, then end the conversation.\n\n        - Be sure to be kind of funny and witty!\n        - Keep all your responses short and simple. Use casual language, phrases like \"Umm...\", \"Well...\", and \"I mean\" are preferred.\n        - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.\n        ```\n\n        You can copy & paste the above prompt into the `System Prompt` field. Now the model configuration for your assistant should look something like this:\n\n        <Frame caption=\"Note how our model provider is set to OpenAI & the model is set to GPT-4.\">\n          <img src=\"file:8f55b735-a327-428a-af55-e0918f9e7b46\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Transcriber Setup\" icon=\"microphone\" iconType=\"solid\">\n    The transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your Deepgram Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        We will be using [Deepgram](https://deepgram.com) (which provides blazing-fast & accurate Speech-to-Text) as our STT provider.\n\n        We will set our provider key for them in \"Provider Keys\":\n\n        <Frame>\n          <img src=\"file:38df25eb-0169-45e1-82ba-1580d3707aed\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Transcriber\" icon=\"language\" iconType=\"solid\">\n        We will set the model to `Nova 2` & the language to `en` for English. Now your assistant's transcriber configuration should look something like this:\n\n        <Frame caption=\"Note how our transcriber is set to 'deepgram', the model is set to 'Nova 2', & the language is set to English.\">\n          <img src=\"file:f3d0e9b2-5fd5-41bb-9132-39eb0a091218\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Voice Setup\" icon=\"head-side-cough\" iconType=\"solid\">\n    The final portion of the voice pipeline is turning LLM output-text into speech. This process is called \"Text-to-speech\" (or TTS for short).\n\n    We will be using a voice provider called [PlayHT](https://play.ht) (they have very conversational voices), & a voice provided by them labeled `Jennifer` (`female`, `en-US`).\n\n    You are free to use your favorite TTS voice platform here. [ElevenLabs](https://elevenlabs.io/) is\n    another alternative — by now you should get the flow of plugging in vendors into Vapi (add\n    provider key + pick provider in assistant config).\n\n    You can skip the next step(s) if you don't intend to use PlayHT.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your PlayHT Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        If you haven't already, sign up for an account with PlayHT at [play.ht](https://play.ht). Since their flows are liable to change — you can just grab your `API Key` & `User ID` from them.\n\n        <Frame>\n          <img src=\"file:f063fc59-93ea-4a16-bb0a-f973f54c51f3\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Voice\" icon=\"person\" iconType=\"solid\">\n        You will want to select `playht` in the \"provider\" field, & `Jennifer` in the \"voice\" field. We will leave all of the other settings untouched.\n\n        <Frame caption=\"Each voice provider offers a host of settings you can modulate to customize voices. Here we will leave all the defaults alone.\">\n          <img src=\"file:cbcb3b79-84bf-4502-8489-a8e5c9b91d47\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n\n\n## Calling Your Assistant\n\nNow that your assistant is fully setup & configured, we will want to contact it. There are 2 ways to \"call in\" to an assistant:\n\n- **Over the Internet:** Network-enabled devices can contact Vapi via the Internet (i.e. web applications, mobile applications). No phone number is involved.\n- **Via Telephony:** Phones can communicate to Vapi over a cellular network (i.e. phone call). One phone number dials to another phone number.\n\nFor our use case, it is most appropriate that customers will contact our assistant via an inbound\nphone call. Though, we will look at both ways of calling in.\n\n<AccordionGroup>\n  <Accordion title=\"Call in the Dashboard\" icon=\"camera-web\" iconType=\"solid\">\n    The quickest way to contact your new assistant is by simply using the call button on the assistant detail page:\n\n    <Frame caption=\"Call into your assistant via the dashboard.\">\n      <img src=\"file:e2e34f52-7467-4407-8d7d-2ba8575b3418\" />\n    </Frame>\n\n    <Tip>The dashboard uses the [web SDK](/sdk/web) underneath to make web calls.</Tip>\n\n    This will start a web call with your assistant, you can now speak to it to order your pizza & sides!\n\n  </Accordion>\n  <Accordion title=\"Call via Phone\" icon=\"phone-arrow-up-right\" iconType=\"solid\">\n    Since our assistant is meant to take orders over the phone, we will want to set up [inbound calling](/phone-calling) to our assistant. We will need to do 2 things:\n\n    1. **provision a new phone number** to sit our agent behind (it will pick-up calls that come in — hence \"inbound calling\")\n    2. **place our agent behind that phone number**\n\n    If you already have your own phone numbers (purchased via Twilio or Vonage, etc), you can import\n    them into Vapi for use. Learn more about [telephony](/phone-calling) on Vapi.\n\n    <AccordionGroup>\n      <Accordion title=\"Provision a Phone Number\" icon=\"hashtag\" iconType=\"solid\">\n        The quickest way to secure a phone number for your assistant is to purchase a phone number directly through Vapi.\n        \n        <Info>\n          Ensure you have a card on file that Vapi can bill before proceeding, you can add your billing\n          information in your dashboard at [dashboard.vapi.ai/billing](https://dashboard.vapi.ai/billing)\n        </Info>\n        \n        Navigate to the \"Phone Numbers\" section & click the \"Buy number\" button:\n        \n        <Frame caption=\"Make sure you are in the 'Phone Numbers' dashboard tab.\">\n          <img src=\"file:5aacd386-b019-4352-8f21-e232e1e883c0\" />\n        </Frame>\n        \n        We will use the area code `415` for our phone number (these are area codes domestic to the US & Canada).\n        \n        <Frame caption=\"Choose an area code for your phone number.\">\n          <img src=\"file:ff861b6e-dcd7-439c-9463-2f0b1a6f42b9\" />\n        </Frame>\n        \n        <Info>\n          Currently, only US & Canada phone numbers can be directly purchased through Vapi. Phone numbers in\n          other regions must be imported, see our [phone calling](/phone-calling) guide.\n        </Info>\n        \n        Click \"Buy\", after purchasing a phone number you should see something like this:\n        \n        <Frame caption=\"Here we can attach an assistant to the number for inbound calls (or perform an outbound call, with a select assistant).\">\n          <img src=\"file:4fd9c589-942f-4474-8df7-562255d342d1\" />\n        </Frame>\n        \n        The phone number is now ready to be used (either for inbound or outbound calling).\n        \n      </Accordion>\n      <Accordion title=\"Attach Your Assistant\" icon=\"user-robot\" iconType=\"solid\">\n        In the `Inbound` area of the phone number detail view, select your assistant in the dropdown under `Assistant`.\n\n        <Frame caption=\"Your assistant will now pick-up calls made to this phone number.\">\n          <img src=\"file:f5182f5d-8fea-4e6a-8e0e-98ac685885f0\" />\n        </Frame>\n\n        This will put your assistant behind the phone number for inbound calls. Your assistant is now ready to take calls.\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n\nYour assistant should be able to accept calls & maintain a basic conversation. Happy ordering!\n\n<Tip>\n  Your assistant won't yet be able to hang-up the phone at the end of the call. We will learn more\n  about configuring call end behaviour in later guides.\n</Tip>\n"
    },
    "quickstart/inbound.mdx": {
      "markdown": "---\ntitle: Inbound Calling\nsubtitle: Quickstart handling inbound calls with Vapi.\nslug: quickstart/phone/inbound\n---\n\n<Frame>\n  <div class=\"video-embed-wrapper\">\n    <iframe\n      src=\"https://www.youtube.com/embed/e7q4p8Gg1Tg?si=ZvFumklEMubfbZi7\"\n      title='An embedded YouTube video titled \"Quickstart: Inbound Calling\"'\n      frameborder=\"0\"\n      allow=\"fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n      allowfullscreen\n      referrerpolicy=\"strict-origin-when-cross-origin\"\n    />\n  </div>\n</Frame>\n\nAn inbound call is a phone call that comes **\"in\"** towards a phone number, & in our case, our AI assistant will be there to pick up the phone call.\n\nThere are **4 steps** we will cover to handle our first inbound phone call:\n\n1. **Create an Assistant:** we will create an [assistant](/assistants) & instruct it on how to conduct the call\n2. **Get a Phone Number:** we can either import existing numbers we own, or purchase one through Vapi\n3. **Attach Our Assistant:** we will put our assistant behind the phone number to pick up calls\n4. **Call the Number:** we can then call the number & talk to our assistant\n\n## Vapi’s Pizzeria\n\nWe will be implementing a simple order-taking assistant that receives customer calls at a pizza shop called “Vapi’s Pizzeria”.\n\nVapi’s has 3 types of menu items: `pizza`, `side`s, & `drink`s. Customers will be ordering 1 of each.\n\n<Frame caption=\"Customers will order 3 items: 1 pizza, 1 side, & 1 drink. The assistant will pick up the phone & take the customer's order.\">\n  <img src=\"file:88a31f93-503c-4aa0-9d30-8d50060af0f3\" />\n</Frame>\n\n## Assistant Setup\n\nFirst we're going to set up our assistant in the dashboard. Once our assistant’s **transcriber**, **model**, & **voice** are set up, we can call it to place our order.\n\n<Info>\n  You can visit your dashboard by going to\n  [dashboard.vapi.ai](https://dashboard.vapi.ai)\n</Info>\n\n<AccordionGroup>\n  <Accordion title=\"Sign-up or Log-in to Vapi\" icon=\"user-plus\" iconType=\"solid\">\n    If you haven't already signed-up, you're going to need an account before you can use the web dashboard. When you visit [dashboard.vapi.ai](https://dashboard.vapi.ai) you may see something like this:\n\n    <Frame>\n      <img src=\"file:2a36b470-9983-4270-aa74-bb2fccc56af4\" />\n    </Frame>\n\n    Sign-up for an account (or log-in to your existing account) — you will then find yourself inside the web dashboard. It will look something like this:\n\n    <Frame caption=\"Your dashboard may look a bit different if you already have an account with assistants in it. The main idea is that we’re in the dashboard now.\">\n      <img src=\"file:20f9dc5a-cf0e-4750-ad63-82f8a16c8a0d\" />\n    </Frame>\n\n  </Accordion>\n  <Accordion title=\"Create an Assistant\" icon=\"layer-plus\" iconType=\"solid\">\n    Now that you're in your dashboard, we're going to create an [assistant](/assistants).\n\n    Assistants are at the heart of how Vapi models AI voice agents — we will be setting certain properties on a new assistant to model an order-taking agent.\n\n    Once in the \"Assistants\" dashboard tab (you should be in it by-default after log-in), you will see a button to create a new assistant.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"file:0eb3cc5f-5aa4-435f-8a56-fd05ae3ff398\" />\n    </Frame>\n\n    After clicking the create new assistant button, you will see a pop-up modal that asks you to pick a starter template. For our example we will start from a blank slate so choose the `Blank Template` option.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"file:80f913e8-fd91-46e6-9cff-f574bd100f10\" />\n    </Frame>\n\n    You will then be able to name your assistant — you can name it whatever you'd like (`Vapi’s Pizza Front Desk`, for example):\n\n    <Info>\n      This name is only for internal labeling use. It is not an identifier, nor will the assistant be\n      aware of this name.\n    </Info>\n\n    <Frame caption=\"Name your assistant.\">\n      <img src=\"file:a4fc51f3-6f6d-43c8-b577-da34b255198a\" />\n    </Frame>\n\n    Once you have named your assistant, you can hit \"Create\" to create it. You will then see something like this:\n\n    <Frame caption=\"The assistant overview. You can edit your assistant’s transcriber, model, & voice — and edit other advanced configuration.\">\n      <img src=\"file:300206c9-ee39-484c-910e-e45e3a28567d\" />\n    </Frame>\n\n    This is the assistant overview view — it gives you the ability to edit different attributes about your assistant, as well as see **cost** & **latency** projection information for each portion of it’s voice pipeline (this is very important data to have handy when building out your assistants).\n\n  </Accordion>\n  <Accordion title=\"Model Setup\" icon=\"microchip\" iconType=\"solid\">\n    Now we’re going to set the \"brains\" of the assistant, the large language model. We're going to be using `GPT-4` (from [OpenAI](https://openai.com/)) for this demo (though you're free to use `GPT-3.5`, or any one of your favorite LLMs).\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your OpenAI Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        Before we proceed, we can set our [provider key](customization/provider-keys) for OpenAI (this is just your OpenAI secret key).\n\n        <Note>\n          You can see all of your provider keys in the \"Provider Keys\" dashboard tab. You can also go\n          directly to [dashboard.vapi.ai/keys](https://dashboard.vapi.ai/keys).\n        </Note>\n\n        Vapi uses [provider keys](customization/provider-keys) you provide to communicate with LLM, TTS, & STT vendors on your behalf. It is most ideal that we set keys for the vendors we intend to use ahead of time.\n\n        <Frame caption=\"We set our provider key for OpenAI so Vapi can make requests to their API.\">\n          <img src=\"file:d9e19883-176c-4849-ab6f-91eebf3615e1\" />\n        </Frame>\n\n        While we're here it'd be ideal for you to go & set up provider keys for other providers you're familiar with & intend to use later.\n      </Accordion>\n      <Accordion title=\"Set a First Message\" icon=\"message\" iconType=\"light\">\n        Assistants can **optionally** be configured with a `First Message`. This first message will be spoken by your assistant when either:\n\n        - **A Web Call Connects:** when a web call is started with your assistant\n        - **An Inbound Call is Picked-up:** an [inbound call](/glossary#inbound-call) is picked-up & answered by your assistant\n        - **An Outbound Call is Dialed & Picked-up:** an [outbound call](/glossary#outbound-call) is dialed by your assistant & a person picks up\n\n        <Warning>\n          Note that this first message cannot be interrupted & is guaranteed to be spoken. Certain use cases\n          need a first message, while others do not.\n        </Warning>\n\n        For our use case, we will want a first message. It would be ideal for us to have a first message like this:\n\n        ```text\n        Vappy’s Pizzeria speaking, how can I help you?\n        ```\n\n        <Info>\n          Some text-to-speech voices may struggle to pronounce 'Vapi' correctly, compartmentalizing it to be\n          spoken letter by letter \"V. A. P. I.\"\n\n        Some aspects of configuring your voice pipeline will require tweaks like this to get the target\n        behaviour you want.\n\n        </Info>\n\n        This will be spoken by the assistant when a web or inbound phone call is received.\n      </Accordion>\n      <Accordion title=\"Set the System Prompt\" icon=\"message\" iconType=\"solid\">\n        We will now set the `System Prompt` for our assistant. If you're familiar with OpenAI's API, this is the first prompt in the message list that we feed our LLM (learn more about prompt engineering on the [OpenAI docs](https://platform.openai.com/docs/guides/prompt-engineering)).\n\n        The system prompt can be used to configure the context, role, personality, instructions and so on for the assistant. In our case, a system prompt like this will give us the behaviour we want:\n\n        ```text\n        You are a voice assistant for Vappy’s Pizzeria,\n        a pizza shop located on the Internet.\n\n        Your job is to take the order of customers calling in. The menu has only 3 types\n        of items: pizza, sides, and drinks. There are no other types of items on the menu.\n\n        1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza\n        (often called \"veggie\" pizza).\n        2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.\n        3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a\n        brand name like \"coca cola\", just let them know that we only offer \"soda\")\n\n        Customers can only order 1 of each item. If a customer tries to order more\n        than 1 item within each category, politely inform them that only 1 item per\n        category may be ordered.\n\n        Customers must order 1 item from at least 1 category to have a complete order.\n        They can order just a pizza, or just a side, or just a drink.\n\n        Be sure to introduce the menu items, don't assume that the caller knows what\n        is on the menu (most appropriate at the start of the conversation).\n\n        If the customer goes off-topic or off-track and talks about anything but the\n        process of ordering, politely steer the conversation back to collecting their order.\n\n        Once you have all the information you need pertaining to their order, you can\n        end the conversation. You can say something like \"Awesome, we'll have that ready\n        for you in 10-20 minutes.\" to naturally let the customer know the order has been\n        fully communicated.\n\n        It is important that you collect the order in an efficient manner (succinct replies\n        & direct questions). You only have 1 task here, and it is to collect the customers\n        order, then end the conversation.\n\n        - Be sure to be kind of funny and witty!\n        - Keep all your responses short and simple. Use casual language, phrases like \"Umm...\", \"Well...\", and \"I mean\" are preferred.\n        - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.\n        ```\n\n        You can copy & paste the above prompt into the `System Prompt` field. Now the model configuration for your assistant should look something like this:\n\n        <Frame caption=\"Note how our model provider is set to OpenAI & the model is set to GPT-4.\">\n          <img src=\"file:8f55b735-a327-428a-af55-e0918f9e7b46\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Transcriber Setup\" icon=\"microphone\" iconType=\"solid\">\n    The transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your Deepgram Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        We will be using [Deepgram](https://deepgram.com) (which provides blazing-fast & accurate Speech-to-Text) as our STT provider.\n\n        We will set our provider key for them in \"Provider Keys\":\n\n        <Frame>\n          <img src=\"file:38df25eb-0169-45e1-82ba-1580d3707aed\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Transcriber\" icon=\"language\" iconType=\"solid\">\n        We will set the model to `Nova 2` & the language to `en` for English. Now your assistant's transcriber configuration should look something like this:\n\n        <Frame caption=\"Note how our transcriber is set to 'deepgram', the model is set to 'Nova 2', & the language is set to English.\">\n          <img src=\"file:f3d0e9b2-5fd5-41bb-9132-39eb0a091218\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Voice Setup\" icon=\"head-side-cough\" iconType=\"solid\">\n    The final portion of the voice pipeline is turning LLM output-text into speech. This process is called \"Text-to-speech\" (or TTS for short).\n\n    We will be using a voice provider called [PlayHT](https://play.ht) (they have very conversational voices), & a voice provided by them labeled `Jennifer` (`female`, `en-US`).\n\n    You are free to use your favorite TTS voice platform here. [ElevenLabs](https://elevenlabs.io/) is\n    another alternative — by now you should get the flow of plugging in vendors into Vapi (add\n    provider key + pick provider in assistant config).\n\n    You can skip the next step(s) if you don't intend to use PlayHT.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your PlayHT Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        If you haven't already, sign up for an account with PlayHT at [play.ht](https://play.ht). Since their flows are liable to change — you can just grab your `API Key` & `User ID` from them.\n\n        <Frame>\n          <img src=\"file:f063fc59-93ea-4a16-bb0a-f973f54c51f3\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Voice\" icon=\"person\" iconType=\"solid\">\n        You will want to select `playht` in the \"provider\" field, & `Jennifer` in the \"voice\" field. We will leave all of the other settings untouched.\n\n        <Frame caption=\"Each voice provider offers a host of settings you can modulate to customize voices. Here we will leave all the defaults alone.\">\n          <img src=\"file:cbcb3b79-84bf-4502-8489-a8e5c9b91d47\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n\n\n## Get a Phone Number\n\nNow that we've configured how our assistant will behave, we want to figure out how to call it. We will need a phone number that we can make phone calls to.\n\nThere are **2 ways** we can get a phone number into our Vapi account:\n\n1. **Purchase a Number Through Vapi:** we can directly purchase phone numbers through Vapi.\n\n   - Vapi will provision the phone number for us via Twilio\n   - This can be done in the dashboard, or via the API (we will use the dashboard)\n\n2. **Import from Twilio or Vonage:** if we already have a phone number with an external telephony provider (like Twilio or Vonage), we can import them into our Vapi account.\n\n<AccordionGroup>\n  <Accordion title=\"Provision via Vapi (faster)\" icon=\"v\" iconType=\"solid\">\n    The quickest way to secure a phone number for your assistant is to purchase a phone number directly through Vapi.\n\n    <Info>\n      Ensure you have a card on file that Vapi can bill before proceeding, you can add your billing\n      information in your dashboard at [dashboard.vapi.ai/billing](https://dashboard.vapi.ai/billing)\n    </Info>\n\n    Navigate to the \"Phone Numbers\" section & click the \"Buy number\" button:\n\n    <Frame caption=\"Make sure you are in the 'Phone Numbers' dashboard tab.\">\n      <img src=\"file:5aacd386-b019-4352-8f21-e232e1e883c0\" />\n    </Frame>\n\n    We will use the area code `415` for our phone number (these are area codes domestic to the US & Canada).\n\n    <Frame caption=\"Choose an area code for your phone number.\">\n      <img src=\"file:ff861b6e-dcd7-439c-9463-2f0b1a6f42b9\" />\n    </Frame>\n\n    <Info>\n      Currently, only US & Canada phone numbers can be directly purchased through Vapi. Phone numbers in\n      other regions must be imported, see our [phone calling](/phone-calling) guide.\n    </Info>\n\n    Click \"Buy\", after purchasing a phone number you should see something like this:\n\n    <Frame caption=\"Here we can attach an assistant to the number for inbound calls (or perform an outbound call, with a select assistant).\">\n      <img src=\"file:4fd9c589-942f-4474-8df7-562255d342d1\" />\n    </Frame>\n\n    The phone number is now ready to be used (either for inbound or outbound calling).\n  </Accordion>\n  <Accordion title=\"Import from Twilio or Vonage\" icon=\"hashtag\" iconType=\"regular\">\n    We can also import an existing phone number we already own with either Twilio or Vonage.\n\n    For example's sake, we will proceed with [**Twilio**](https://twilio.com) (though the steps are the same for Vonage as\n    well).\n\n    <AccordionGroup>\n      <Accordion title=\"Buy a Phone Number via Twilio (optional)\" icon=\"hashtag\" iconType=\"solid\">\n        If you don't already have a number in Twilio, you can purchase one by going to your Twilio console's \"Buy a number\" section:\n\n        <Frame caption=\"The Twilio 'Buy a Number' page in the Twilio console.\">\n          <img src=\"file:2c08fcf6-7b50-4b04-a0f6-3645a686037b\" />\n        </Frame>\n\n        Once you've purchased a number, it will immediately be ready for import into Vapi.\n      </Accordion>\n      <Accordion title=\"Locate Twilio Account SID & Auth Token\" icon=\"key\" iconType=\"solid\">\n        To complete the import on Vapi's side, we will need to grab our Twilio **\"Account SID\"** & **\"Auth Token\"**.\n\n        You should see a section for \"API keys & tokens\", the credentials we will need for the import will live here.\n\n        <Frame caption=\"We will want to navigate to the credentials section of our account.\">\n          <img src=\"file:63cef21d-bdb4-4f87-b361-c488563018ac\" />\n        </Frame>\n\n        Once we are in our \"API keys & tokens\" section, we will grab the Account SID & Auth Token:\n\n        <Frame>\n          <img src=\"file:b015946b-4e29-47eb-b82f-f47e6d2dd139\" />\n        </Frame>\n\n        We will use both of these credentials in the next step of importing via the Dashboard.\n      </Accordion>\n      <Accordion title=\"Import via Dashboard\" icon=\"v\" iconType=\"solid\">\n        Navigate to the “Phone Numbers” section & click the “Import” button:\n\n        <Frame caption=\"Click 'Import' in the 'Phone Numbers' tab of your dashboard.\">\n          <img src=\"file:6f315749-3171-4de8-af7a-52819b782f50\" />\n        </Frame>\n\n        There you will input your phone number, as well as the credentials you retrieved in the previous step:\n\n        <Frame >\n          <img src=\"file:c1b7453e-aff6-4fe0-a8eb-6d4b5fe4da49\" />\n        </Frame>\n\n        Hit \"Import\" & you will come to the phone number detail page:\n\n        <Frame caption=\"The phone number detail page, we can configure our phone number here.\">\n          <img src=\"file:03d9b5e3-7716-42eb-ae73-5b3a1249ce15\" />\n        </Frame>\n\n        Your number is now ready to be attached to an assistant for inbound or outbound phone calling.\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n\n\n## Attach Your Assistant\n\nNow that we have a configured assistant & a phone number, we will put our assistant behind the phone number to pick up incoming phone calls.\n\nIn the `Inbound` area of the phone number detail view, select your assistant in the dropdown under `Assistant`.\n\n<Frame caption=\"Your assistant will now pick-up calls made to this phone number.\">\n  <img src=\"file:f5182f5d-8fea-4e6a-8e0e-98ac685885f0\" />\n</Frame>\n\n## Call the Number\n\nYou can now make a phone call to the number. Your assistant will pick up the phone & manage the order-taking conversation. Happy ordering!\n\n<Tip>\n  Your assistant won't yet be able to hang-up the phone at the end of the call.\n  We will learn more about configuring call end behaviour in later guides.\n</Tip>\n"
    },
    "quickstart/outbound.mdx": {
      "markdown": "---\ntitle: Outbound Calling\nsubtitle: Quickstart sending outbound calls with Vapi.\nslug: quickstart/phone/outbound\n---\n\n<Frame>\n  <div class=\"video-embed-wrapper\">\n    <iframe\n      src=\"https://www.youtube.com/embed/xMjTHCBCSbI?si=GlE8Ei78OrOh-ZrR\"\n      title='An embedded YouTube video titled \"Quickstart: Outbound Calling\"'\n      frameborder=\"0\"\n      allow=\"fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n      allowfullscreen\n      referrerpolicy=\"strict-origin-when-cross-origin\"\n    />\n  </div>\n</Frame>\n\nAn outbound call is a phone call that is dialed and goes **\"out\"** from a phone number, & in our case, our AI assistant will be doing the dialing.\n\nThere are **3 steps** we will cover to send our first outbound phone call:\n\n1. **Create an Assistant:** we will create an [assistant](/assistants) & instruct it on how to conduct itself during the call\n2. **Get a Phone Number:** we can either import existing numbers we own, or purchase one through Vapi\n3. **Call Your Number:** we will set our assistant as the dialer, set the destination phone number, then make the call\n\nWe can then send the outbound call, hopefully someone picks up!\n\n<Warning>\n  It is a violation of FCC law to dial phone numbers without consent in an\n  automated manner. See [Telemarketing Sales\n  Rule](/glossary#telemarketing-sales-rule) to learn more.\n</Warning>\n\n## Vapi’s Pizzeria\n\nWe will be implementing a simple order-taking assistant for a pizza shop called “Vapi’s Pizzeria”.\n\nVapi’s has 3 types of menu items: `pizza`, `side`s, & `drink`s. Customers will be ordering 1 of each.\n\n**Outbound Scenario:** We will imagine we are calling back a customer who originally called in to place an order. Our assistant is calling back to complete the ordering process with the customer.\n\n<Frame caption=\"Customers can order 3 items: 1 pizza, 1 side, & 1 drink. The assistant will call the customer (who got disconnected) & finish the ordering process.\">\n  <img src=\"file:88a31f93-503c-4aa0-9d30-8d50060af0f3\" />\n</Frame>\n\n## Assistant Setup\n\nFirst we're going to set up our assistant in the dashboard. Once our assistant’s **transcriber**, **model**, & **voice** are set up, we can have it call the customer to finish the order.\n\n<Info>\n  You can visit your dashboard by going to\n  [dashboard.vapi.ai](https://dashboard.vapi.ai)\n</Info>\n\n<AccordionGroup>\n  <Accordion title=\"Sign-up or Log-in to Vapi\" icon=\"user-plus\" iconType=\"solid\">\n    If you haven't already signed-up, you're going to need an account before you can use the web dashboard. When you visit [dashboard.vapi.ai](https://dashboard.vapi.ai) you may see something like this:\n\n    <Frame>\n      <img src=\"file:2a36b470-9983-4270-aa74-bb2fccc56af4\" />\n    </Frame>\n\n    Sign-up for an account (or log-in to your existing account) — you will then find yourself inside the web dashboard. It will look something like this:\n\n    <Frame caption=\"Your dashboard may look a bit different if you already have an account with assistants in it. The main idea is that we’re in the dashboard now.\">\n      <img src=\"file:20f9dc5a-cf0e-4750-ad63-82f8a16c8a0d\" />\n    </Frame>\n\n  </Accordion>\n  <Accordion title=\"Create an Assistant\" icon=\"layer-plus\" iconType=\"solid\">\n    Now that you're in your dashboard, we're going to create an [assistant](/assistants).\n\n    Assistants are at the heart of how Vapi models AI voice agents — we will be setting certain properties on a new assistant to model an order-taking agent.\n\n    Once in the \"Assistants\" dashboard tab (you should be in it by-default after log-in), you will see a button to create a new assistant.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"file:0eb3cc5f-5aa4-435f-8a56-fd05ae3ff398\" />\n    </Frame>\n\n    After clicking the create new assistant button, you will see a pop-up modal that asks you to pick a starter template. For our example we will start from a blank slate so choose the `Blank Template` option.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"file:80f913e8-fd91-46e6-9cff-f574bd100f10\" />\n    </Frame>\n\n    You will then be able to name your assistant — you can name it whatever you'd like (`Vapi’s Pizza Front Desk`, for example):\n\n    <Info>\n      This name is only for internal labeling use. It is not an identifier, nor will the assistant be\n      aware of this name.\n    </Info>\n\n    <Frame caption=\"Name your assistant.\">\n      <img src=\"file:a4fc51f3-6f6d-43c8-b577-da34b255198a\" />\n    </Frame>\n\n    Once you have named your assistant, you can hit \"Create\" to create it. You will then see something like this:\n\n    <Frame caption=\"The assistant overview. You can edit your assistant’s transcriber, model, & voice — and edit other advanced configuration.\">\n      <img src=\"file:300206c9-ee39-484c-910e-e45e3a28567d\" />\n    </Frame>\n\n    This is the assistant overview view — it gives you the ability to edit different attributes about your assistant, as well as see **cost** & **latency** projection information for each portion of it’s voice pipeline (this is very important data to have handy when building out your assistants).\n\n  </Accordion>\n  <Accordion title=\"Model Setup\" icon=\"microchip\" iconType=\"solid\">\n    Now we’re going to set the \"brains\" of the assistant, the large language model. We're going to be using `GPT-4` (from [OpenAI](https://openai.com/)) for this demo (though you're free to use `GPT-3.5`, or any one of your favorite LLMs).\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your OpenAI Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        Before we proceed, we can set our [provider key](customization/provider-keys) for OpenAI (this is just your OpenAI secret key).\n\n        <Note>\n          You can see all of your provider keys in the \"Provider Keys\" dashboard tab. You can also go\n          directly to [dashboard.vapi.ai/keys](https://dashboard.vapi.ai/keys).\n        </Note>\n\n        Vapi uses [provider keys](customization/provider-keys) you provide to communicate with LLM, TTS, & STT vendors on your behalf. It is most ideal that we set keys for the vendors we intend to use ahead of time.\n\n        <Frame caption=\"We set our provider key for OpenAI so Vapi can make requests to their API.\">\n          <img src=\"file:d9e19883-176c-4849-ab6f-91eebf3615e1\" />\n        </Frame>\n\n        While we're here it'd be ideal for you to go & set up provider keys for other providers you're familiar with & intend to use later.\n      </Accordion>\n      <Accordion title=\"Set a First Message\" icon=\"message\" iconType=\"light\">\n        Assistants can **optionally** be configured with a `First Message`. This first message will be spoken by your assistant when either:\n\n        - **A Web Call Connects:** when a web call is started with your assistant\n        - **An Inbound Call is Picked-up:** an [inbound call](/glossary#inbound-call) is picked-up & answered by your assistant\n        - **An Outbound Call is Dialed & Picked-up:** an [outbound call](/glossary#outbound-call) is dialed by your assistant & a person picks up\n\n        <Warning>\n          Note that this first message cannot be interrupted & is guaranteed to be spoken. Certain use cases\n          need a first message, while others do not.\n        </Warning>\n\n        For our use case, we will want a first message. Since we are calling the customer back it would be ideal for us to have a first message like this:\n\n        ```text\n        Hi this is Jennifer from Vappy’s Pizzeria giving you a call back since we got disconnected. Would you like to finish your order with us?\n        ```\n\n        <Info>\n          Some text-to-speech voices may struggle to pronounce 'Vapi' correctly, compartmentalizing it to be\n          spoken letter by letter \"V. A. P. I.\"\n\n        Some aspects of configuring your voice pipeline will require tweaks like this to get the target\n        behaviour you want.\n\n        </Info>\n\n        This will be spoken by the assistant when a web or inbound phone call is received.\n      </Accordion>\n      <Accordion title=\"Set the System Prompt\" icon=\"message\" iconType=\"solid\">\n        We will now set the `System Prompt` for our assistant. If you're familiar with OpenAI's API, this is the first prompt in the message list that we feed our LLM (learn more about prompt engineering on the [OpenAI docs](https://platform.openai.com/docs/guides/prompt-engineering)).\n\n        The system prompt can be used to configure the context, role, personality, instructions and so on for the assistant.\n\n        Since we are calling the customer back, we will tweak the base prompt a bit so the model understands the situation & new goal (recovering the order).\n\n        A system prompt like this will give us the behaviour we want:\n\n        ```text\n        You are a voice assistant for Vappy’s Pizzeria,\n        a pizza shop located on the Internet.\n\n        Your job is to take the order of customers calling in. The menu has only 3 types\n        of items: pizza, sides, and drinks. There are no other types of items on the menu.\n\n        1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza\n        (often called \"veggie\" pizza).\n        2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.\n        3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a\n        brand name like \"coca cola\", just let them know that we only offer \"soda\")\n\n        Customers can only order 1 of each item. If a customer tries to order more\n        than 1 item within each category, politely inform them that only 1 item per\n        category may be ordered.\n\n        Customers must order 1 item from at least 1 category to have a complete order.\n        They can order just a pizza, or just a side, or just a drink.\n\n        Be sure to introduce the menu items, don't assume that the caller knows what\n        is on the menu (most appropriate at the start of the conversation).\n\n        If the customer goes off-topic or off-track and talks about anything but the\n        process of ordering, politely steer the conversation back to collecting their order.\n\n        Once you have all the information you need pertaining to their order, you can\n        end the conversation. You can say something like \"Awesome, we'll have that ready\n        for you in 10-20 minutes.\" to naturally let the customer know the order has been\n        fully communicated.\n\n        It is important that you collect the order in an efficient manner (succinct replies\n        & direct questions). You only have 1 task here, and it is to collect the customers\n        order, then end the conversation.\n\n        - Be sure to be kind of funny and witty!\n        - Keep all your responses short and simple. Use casual language, phrases like \"Umm...\", \"Well...\", and \"I mean\" are preferred.\n        - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.\n\n        You are calling back a customer after the call got disconnected while they were\n        ordering. Your job is to help them complete their order.\n        ```\n\n        You can copy & paste the above prompt into the `System Prompt` field. Now the model configuration for your assistant should look something like this:\n\n        <Frame caption=\"Note how our model provider is set to OpenAI & the model is set to GPT-4.\">\n          <img src=\"file:2019a4c3-9877-4419-b1d0-23d9b3f37fec\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Transcriber Setup\" icon=\"microphone\" iconType=\"solid\">\n    The transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your Deepgram Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        We will be using [Deepgram](https://deepgram.com) (which provides blazing-fast & accurate Speech-to-Text) as our STT provider.\n\n        We will set our provider key for them in \"Provider Keys\":\n\n        <Frame>\n          <img src=\"file:38df25eb-0169-45e1-82ba-1580d3707aed\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Transcriber\" icon=\"language\" iconType=\"solid\">\n        We will set the model to `Nova 2` & the language to `en` for English. Now your assistant's transcriber configuration should look something like this:\n\n        <Frame caption=\"Note how our transcriber is set to 'deepgram', the model is set to 'Nova 2', & the language is set to English.\">\n          <img src=\"file:f3d0e9b2-5fd5-41bb-9132-39eb0a091218\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Voice Setup\" icon=\"head-side-cough\" iconType=\"solid\">\n    The final portion of the voice pipeline is turning LLM output-text into speech. This process is called \"Text-to-speech\" (or TTS for short).\n\n    We will be using a voice provider called [PlayHT](https://play.ht) (they have very conversational voices), & a voice provided by them labeled `Jennifer` (`female`, `en-US`).\n\n    You are free to use your favorite TTS voice platform here. [ElevenLabs](https://elevenlabs.io/) is\n    another alternative — by now you should get the flow of plugging in vendors into Vapi (add\n    provider key + pick provider in assistant config).\n\n    You can skip the next step(s) if you don't intend to use PlayHT.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your PlayHT Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        If you haven't already, sign up for an account with PlayHT at [play.ht](https://play.ht). Since their flows are liable to change — you can just grab your `API Key` & `User ID` from them.\n\n        <Frame>\n          <img src=\"file:f063fc59-93ea-4a16-bb0a-f973f54c51f3\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Voice\" icon=\"person\" iconType=\"solid\">\n        You will want to select `playht` in the \"provider\" field, & `Jennifer` in the \"voice\" field. We will leave all of the other settings untouched.\n\n        <Frame caption=\"Each voice provider offers a host of settings you can modulate to customize voices. Here we will leave all the defaults alone.\">\n          <img src=\"file:cbcb3b79-84bf-4502-8489-a8e5c9b91d47\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n\n## Get a Phone Number\n\nNow that we've configured how our assistant will behave, we want to figure out how to dial calls with it. We will need a phone number that we can call from.\n\nThere are **2 ways** we can get a phone number into our Vapi account:\n\n1. **Purchase a Number Through Vapi:** we can directly purchase phone numbers through Vapi.\n\n   - Vapi will provision the phone number for us via Twilio\n   - This can be done in the dashboard, or via the API (we will use the dashboard)\n\n2. **Import from Twilio or Vonage:** if we already have a phone number with an external telephony provider (like Twilio or Vonage), we can import them into our Vapi account.\n\n<AccordionGroup>\n  <Accordion title=\"Provision via Vapi (faster)\" icon=\"v\" iconType=\"solid\">\n    The quickest way to secure a phone number for your assistant is to purchase a phone number directly through Vapi.\n\n    <Info>\n      Ensure you have a card on file that Vapi can bill before proceeding, you can add your billing\n      information in your dashboard at [dashboard.vapi.ai/billing](https://dashboard.vapi.ai/billing)\n    </Info>\n\n    Navigate to the \"Phone Numbers\" section & click the \"Buy number\" button:\n\n    <Frame caption=\"Make sure you are in the 'Phone Numbers' dashboard tab.\">\n      <img src=\"file:5aacd386-b019-4352-8f21-e232e1e883c0\" />\n    </Frame>\n\n    We will use the area code `415` for our phone number (these are area codes domestic to the US & Canada).\n\n    <Frame caption=\"Choose an area code for your phone number.\">\n      <img src=\"file:ff861b6e-dcd7-439c-9463-2f0b1a6f42b9\" />\n    </Frame>\n\n    <Info>\n      Currently, only US & Canada phone numbers can be directly purchased through Vapi. Phone numbers in\n      other regions must be imported, see our [phone calling](/phone-calling) guide.\n    </Info>\n\n    Click \"Buy\", after purchasing a phone number you should see something like this:\n\n    <Frame caption=\"Here we can attach an assistant to the number for inbound calls (or perform an outbound call, with a select assistant).\">\n      <img src=\"file:4fd9c589-942f-4474-8df7-562255d342d1\" />\n    </Frame>\n\n    The phone number is now ready to be used (either for inbound or outbound calling).\n  </Accordion>\n  <Accordion title=\"Import from Twilio or Vonage\" icon=\"hashtag\" iconType=\"regular\">\n    We can also import an existing phone number we already own with either Twilio or Vonage.\n\n    For example's sake, we will proceed with [**Twilio**](https://twilio.com) (though the steps are the same for Vonage as\n    well).\n\n    <AccordionGroup>\n      <Accordion title=\"Buy a Phone Number via Twilio (optional)\" icon=\"hashtag\" iconType=\"solid\">\n        If you don't already have a number in Twilio, you can purchase one by going to your Twilio console's \"Buy a number\" section:\n\n        <Frame caption=\"The Twilio 'Buy a Number' page in the Twilio console.\">\n          <img src=\"file:2c08fcf6-7b50-4b04-a0f6-3645a686037b\" />\n        </Frame>\n\n        Once you've purchased a number, it will immediately be ready for import into Vapi.\n      </Accordion>\n      <Accordion title=\"Locate Twilio Account SID & Auth Token\" icon=\"key\" iconType=\"solid\">\n        To complete the import on Vapi's side, we will need to grab our Twilio **\"Account SID\"** & **\"Auth Token\"**.\n\n        You should see a section for \"API keys & tokens\", the credentials we will need for the import will live here.\n\n        <Frame caption=\"We will want to navigate to the credentials section of our account.\">\n          <img src=\"file:63cef21d-bdb4-4f87-b361-c488563018ac\" />\n        </Frame>\n\n        Once we are in our \"API keys & tokens\" section, we will grab the Account SID & Auth Token:\n\n        <Frame>\n          <img src=\"file:b015946b-4e29-47eb-b82f-f47e6d2dd139\" />\n        </Frame>\n\n        We will use both of these credentials in the next step of importing via the Dashboard.\n      </Accordion>\n      <Accordion title=\"Import via Dashboard\" icon=\"v\" iconType=\"solid\">\n        Navigate to the “Phone Numbers” section & click the “Import” button:\n\n        <Frame caption=\"Click 'Import' in the 'Phone Numbers' tab of your dashboard.\">\n          <img src=\"file:6f315749-3171-4de8-af7a-52819b782f50\" />\n        </Frame>\n\n        There you will input your phone number, as well as the credentials you retrieved in the previous step:\n\n        <Frame >\n          <img src=\"file:c1b7453e-aff6-4fe0-a8eb-6d4b5fe4da49\" />\n        </Frame>\n\n        Hit \"Import\" & you will come to the phone number detail page:\n\n        <Frame caption=\"The phone number detail page, we can configure our phone number here.\">\n          <img src=\"file:03d9b5e3-7716-42eb-ae73-5b3a1249ce15\" />\n        </Frame>\n\n        Your number is now ready to be attached to an assistant for inbound or outbound phone calling.\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n\n\n## Call Your Number\n\nWe can now make outbound calls to phone numbers, setting our assistant as the one doing the dialing.\n\nIn the phone numbers section of the dashboard, go to your phone number detail page. We will:\n\n1. fill out **our own phone number** as the number to dial\n2. set our assistant as the one doing the calling\n\n<Frame caption=\"When we hit the call button, our assistant will make the outbound call to the phone number.\">\n  <img src=\"file:92bd7abc-7dee-40b6-8ed9-5a7dee6f7a87\" />\n</Frame>\n\nYou can now hit the call button to make the outbound call. Your assistant will dial the phone number & manage the order recovery process.\n\n<Tip>\n  Your assistant won't yet be able to hang-up the phone at the end of the call.\n  We will learn more about configuring call end behaviour in later guides.\n</Tip>\n"
    },
    "quickstart/web.mdx": {
      "markdown": "---\ntitle: Web Calling\nsubtitle: Get started with Vapi on the Web.\nslug: quickstart/web\n---\n\nimport { quickstartDemoLink } from \"../snippets/quickstart/web/links.tsx\";\n\n<Frame>\n  <div class=\"video-embed-wrapper\">\n    <iframe\n      src=\"https://www.youtube.com/embed/PUb-cseRNr4?si=NP-GO8tU46hibfuW\"\n      title='An embedded YouTube video titled \"Quickstart: Web\"'\n      frameborder=\"0\"\n      allow=\"fullscreen; accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n      allowfullscreen\n      referrerpolicy=\"strict-origin-when-cross-origin\"\n    />\n  </div>\n</Frame>\n\nAnywhere you can run client-side JavaScript, you can run Vapi. All the way from vanilla to complex component-based applications with React and Next.js.\n\n<CardGroup cols={2}>\n  <Card title=\"The Web SDK\" icon=\"window\" iconType=\"duotone\" href=\"/sdk/web\">\n    Explore the full Vapi Web SDK.\n  </Card>\n  <Card\n    title=\"Live React Demo\"\n    icon=\"arrow-up-right-from-square\"\n        color=\"#f25130\"\n    href={quickstartDemoLink}\n  >\n    Follow along as you read.\n  </Card>\n</CardGroup>\n\n## Installation\n\nInstall the package:\n\n```bash\nyarn add @vapi-ai/web\n```\n\nor w/ npm:\n\n```bash\nnpm install @vapi-ai/web\n```\n\n\nImport the package:\n\n```javascript\nimport Vapi from \"@vapi-ai/web\";\n```\n\nThen, create a new instance of the Vapi class, passing your **Public Key** as a parameter to the constructor:\n\n```javascript\nconst vapi = new Vapi(\"your-public-key\");\n```\n\nYou can find your public key in the [Vapi Dashboard](https://dashboard.vapi.ai/account).\n\n\n## Starting a Call\n\nAssistants can either be created on the fly (temporary) or created & persisted to your account (persistent).\n\n### Option 1: Temporary Assistant\n\nIf you want to customize properties from the frontend on the fly, you can create an assistant configuration object and pass it to the `.start()` method.\n\n<AccordionGroup>\n  <Accordion title=\"Assistant Configuration\" icon=\"brackets-curly\" iconType=\"solid\">\n    Here are the options we will pass to `.start()`:\n\n    ```javascript\n      const assistantOptions = {\n        name: \"Vapi’s Pizza Front Desk\",\n        firstMessage: \"Vappy’s Pizzeria speaking, how can I help you?\",\n        transcriber: {\n          provider: \"deepgram\",\n          model: \"nova-2\",\n          language: \"en-US\",\n        },\n        voice: {\n          provider: \"playht\",\n          voiceId: \"jennifer\",\n        },\n        model: {\n          provider: \"openai\",\n          model: \"gpt-4\",\n          messages: [\n            {\n              role: \"system\",\n              content: `You are a voice assistant for Vappy’s Pizzeria, a pizza shop located on the Internet.\n\n      Your job is to take the order of customers calling in. The menu has only 3 types\n      of items: pizza, sides, and drinks. There are no other types of items on the menu.\n\n      1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza\n      (often called \"veggie\" pizza).\n      2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.\n      3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a\n      brand name like \"coca cola\", just let them know that we only offer \"soda\")\n\n      Customers can only order 1 of each item. If a customer tries to order more\n      than 1 item within each category, politely inform them that only 1 item per\n      category may be ordered.\n\n      Customers must order 1 item from at least 1 category to have a complete order.\n      They can order just a pizza, or just a side, or just a drink.\n\n      Be sure to introduce the menu items, don't assume that the caller knows what\n      is on the menu (most appropriate at the start of the conversation).\n\n      If the customer goes off-topic or off-track and talks about anything but the\n      process of ordering, politely steer the conversation back to collecting their order.\n\n      Once you have all the information you need pertaining to their order, you can\n      end the conversation. You can say something like \"Awesome, we'll have that ready\n      for you in 10-20 minutes.\" to naturally let the customer know the order has been\n      fully communicated.\n\n      It is important that you collect the order in an efficient manner (succinct replies\n      & direct questions). You only have 1 task here, and it is to collect the customers\n      order, then end the conversation.\n\n      - Be sure to be kind of funny and witty!\n      - Keep all your responses short and simple. Use casual language, phrases like \"Umm...\", \"Well...\", and \"I mean\" are preferred.\n      - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.`,\n            },\n          ],\n        },\n      };\n    ```\n    Let's break down the configuration options we passed:\n\n    - **name:** the display name for the assistant in our dashboard (for internal purposes only)\n    - **firstMessage:** the first message that our assistant will say when it picks up the web call\n    - **transcriber:** the transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline. We are using Deepgram for transcription, specifically, their `Nova 2` model. We also set the language to be transcribed as English.\n    - **voice:** the final portion of the voice pipeline is turning LLM output-text into speech. This process is called \"Text-to-speech\" (or TTS for short). We use a voice provider called PlayHT, & a voice provided by them called `jennifer`.\n    - **model:** for our LLM, we use `gpt-4` (from OpenAI) & set our system prompt for the assistant. The system prompt configures the context, role, personality, instructions and so on for the assistant. In our case, the system prompt above will give us the behaviour we want.\n\n   </Accordion>\n</AccordionGroup>\n\nNow we can call `.start()`, passing the temporary assistant configuration:\n\n```javascript\nvapi.start(assistantOptions);\n```\n\nMore configuration options can be found in the [Assistant](/api-reference/assistants/create-assistant) API reference.\n\n### Option 2: Persistent Assistant\n\nIf you want to create an assistant that you can reuse across multiple calls, you can create a persistent assistant in the [Vapi Dashboard](https://dashboard.vapi.ai). Here's how you can do that:\n\n<AccordionGroup>\n  <Accordion title=\"Sign-up or Log-in to Vapi\" icon=\"user-plus\" iconType=\"solid\">\n    If you haven't already signed-up, you're going to need an account before you can use the web dashboard. When you visit [dashboard.vapi.ai](https://dashboard.vapi.ai) you may see something like this:\n\n    <Frame>\n      <img src=\"file:2a36b470-9983-4270-aa74-bb2fccc56af4\" />\n    </Frame>\n\n    Sign-up for an account (or log-in to your existing account) — you will then find yourself inside the web dashboard. It will look something like this:\n\n    <Frame caption=\"Your dashboard may look a bit different if you already have an account with assistants in it. The main idea is that we’re in the dashboard now.\">\n      <img src=\"file:20f9dc5a-cf0e-4750-ad63-82f8a16c8a0d\" />\n    </Frame>\n\n  </Accordion>\n  <Accordion title=\"Create an Assistant\" icon=\"layer-plus\" iconType=\"solid\">\n    Now that you're in your dashboard, we're going to create an [assistant](/assistants).\n\n    Assistants are at the heart of how Vapi models AI voice agents — we will be setting certain properties on a new assistant to model an order-taking agent.\n\n    Once in the \"Assistants\" dashboard tab (you should be in it by-default after log-in), you will see a button to create a new assistant.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"file:0eb3cc5f-5aa4-435f-8a56-fd05ae3ff398\" />\n    </Frame>\n\n    After clicking the create new assistant button, you will see a pop-up modal that asks you to pick a starter template. For our example we will start from a blank slate so choose the `Blank Template` option.\n\n    <Frame caption=\"Ensure you are in the 'Assistants' dashboard tab, then this button will allow you to begin the assistant creation flow.\">\n      <img src=\"file:80f913e8-fd91-46e6-9cff-f574bd100f10\" />\n    </Frame>\n\n    You will then be able to name your assistant — you can name it whatever you'd like (`Vapi’s Pizza Front Desk`, for example):\n\n    <Info>\n      This name is only for internal labeling use. It is not an identifier, nor will the assistant be\n      aware of this name.\n    </Info>\n\n    <Frame caption=\"Name your assistant.\">\n      <img src=\"file:a4fc51f3-6f6d-43c8-b577-da34b255198a\" />\n    </Frame>\n\n    Once you have named your assistant, you can hit \"Create\" to create it. You will then see something like this:\n\n    <Frame caption=\"The assistant overview. You can edit your assistant’s transcriber, model, & voice — and edit other advanced configuration.\">\n      <img src=\"file:300206c9-ee39-484c-910e-e45e3a28567d\" />\n    </Frame>\n\n    This is the assistant overview view — it gives you the ability to edit different attributes about your assistant, as well as see **cost** & **latency** projection information for each portion of it’s voice pipeline (this is very important data to have handy when building out your assistants).\n\n  </Accordion>\n  <Accordion title=\"Model Setup\" icon=\"microchip\" iconType=\"solid\">\n    Now we’re going to set the \"brains\" of the assistant, the large language model. We're going to be using `GPT-4` (from [OpenAI](https://openai.com/)) for this demo (though you're free to use `GPT-3.5`, or any one of your favorite LLMs).\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your OpenAI Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        Before we proceed, we can set our [provider key](customization/provider-keys) for OpenAI (this is just your OpenAI secret key).\n\n        <Note>\n          You can see all of your provider keys in the \"Provider Keys\" dashboard tab. You can also go\n          directly to [dashboard.vapi.ai/keys](https://dashboard.vapi.ai/keys).\n        </Note>\n\n        Vapi uses [provider keys](customization/provider-keys) you provide to communicate with LLM, TTS, & STT vendors on your behalf. It is most ideal that we set keys for the vendors we intend to use ahead of time.\n\n        <Frame caption=\"We set our provider key for OpenAI so Vapi can make requests to their API.\">\n          <img src=\"file:d9e19883-176c-4849-ab6f-91eebf3615e1\" />\n        </Frame>\n\n        While we're here it'd be ideal for you to go & set up provider keys for other providers you're familiar with & intend to use later.\n      </Accordion>\n      <Accordion title=\"Set a First Message\" icon=\"message\" iconType=\"light\">\n        Assistants can **optionally** be configured with a `First Message`. This first message will be spoken by your assistant when either:\n\n        - **A Web Call Connects:** when a web call is started with your assistant\n        - **An Inbound Call is Picked-up:** an [inbound call](/glossary#inbound-call) is picked-up & answered by your assistant\n        - **An Outbound Call is Dialed & Picked-up:** an [outbound call](/glossary#outbound-call) is dialed by your assistant & a person picks up\n\n        <Warning>\n          Note that this first message cannot be interrupted & is guaranteed to be spoken. Certain use cases\n          need a first message, while others do not.\n        </Warning>\n\n        For our use case, we will want a first message. It would be ideal for us to have a first message like this:\n\n        ```text\n        Vappy’s Pizzeria speaking, how can I help you?\n        ```\n\n        <Info>\n          Some text-to-speech voices may struggle to pronounce 'Vapi' correctly, compartmentalizing it to be\n          spoken letter by letter \"V. A. P. I.\"\n\n        Some aspects of configuring your voice pipeline will require tweaks like this to get the target\n        behaviour you want.\n\n        </Info>\n\n        This will be spoken by the assistant when a web or inbound phone call is received.\n      </Accordion>\n      <Accordion title=\"Set the System Prompt\" icon=\"message\" iconType=\"solid\">\n        We will now set the `System Prompt` for our assistant. If you're familiar with OpenAI's API, this is the first prompt in the message list that we feed our LLM (learn more about prompt engineering on the [OpenAI docs](https://platform.openai.com/docs/guides/prompt-engineering)).\n\n        The system prompt can be used to configure the context, role, personality, instructions and so on for the assistant. In our case, a system prompt like this will give us the behaviour we want:\n\n        ```text\n        You are a voice assistant for Vappy’s Pizzeria,\n        a pizza shop located on the Internet.\n\n        Your job is to take the order of customers calling in. The menu has only 3 types\n        of items: pizza, sides, and drinks. There are no other types of items on the menu.\n\n        1) There are 3 kinds of pizza: cheese pizza, pepperoni pizza, and vegetarian pizza\n        (often called \"veggie\" pizza).\n        2) There are 3 kinds of sides: french fries, garlic bread, and chicken wings.\n        3) There are 2 kinds of drinks: soda, and water. (if a customer asks for a\n        brand name like \"coca cola\", just let them know that we only offer \"soda\")\n\n        Customers can only order 1 of each item. If a customer tries to order more\n        than 1 item within each category, politely inform them that only 1 item per\n        category may be ordered.\n\n        Customers must order 1 item from at least 1 category to have a complete order.\n        They can order just a pizza, or just a side, or just a drink.\n\n        Be sure to introduce the menu items, don't assume that the caller knows what\n        is on the menu (most appropriate at the start of the conversation).\n\n        If the customer goes off-topic or off-track and talks about anything but the\n        process of ordering, politely steer the conversation back to collecting their order.\n\n        Once you have all the information you need pertaining to their order, you can\n        end the conversation. You can say something like \"Awesome, we'll have that ready\n        for you in 10-20 minutes.\" to naturally let the customer know the order has been\n        fully communicated.\n\n        It is important that you collect the order in an efficient manner (succinct replies\n        & direct questions). You only have 1 task here, and it is to collect the customers\n        order, then end the conversation.\n\n        - Be sure to be kind of funny and witty!\n        - Keep all your responses short and simple. Use casual language, phrases like \"Umm...\", \"Well...\", and \"I mean\" are preferred.\n        - This is a voice conversation, so keep your responses short, like in a real conversation. Don't ramble for too long.\n        ```\n\n        You can copy & paste the above prompt into the `System Prompt` field. Now the model configuration for your assistant should look something like this:\n\n        <Frame caption=\"Note how our model provider is set to OpenAI & the model is set to GPT-4.\">\n          <img src=\"file:8f55b735-a327-428a-af55-e0918f9e7b46\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Transcriber Setup\" icon=\"microphone\" iconType=\"solid\">\n    The transcriber is what turns user speech into processable text for our LLM. This is the first step in the end-to-end voice pipeline.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your Deepgram Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        We will be using [Deepgram](https://deepgram.com) (which provides blazing-fast & accurate Speech-to-Text) as our STT provider.\n\n        We will set our provider key for them in \"Provider Keys\":\n\n        <Frame>\n          <img src=\"file:38df25eb-0169-45e1-82ba-1580d3707aed\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Transcriber\" icon=\"language\" iconType=\"solid\">\n        We will set the model to `Nova 2` & the language to `en` for English. Now your assistant's transcriber configuration should look something like this:\n\n        <Frame caption=\"Note how our transcriber is set to 'deepgram', the model is set to 'Nova 2', & the language is set to English.\">\n          <img src=\"file:f3d0e9b2-5fd5-41bb-9132-39eb0a091218\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Voice Setup\" icon=\"head-side-cough\" iconType=\"solid\">\n    The final portion of the voice pipeline is turning LLM output-text into speech. This process is called \"Text-to-speech\" (or TTS for short).\n\n    We will be using a voice provider called [PlayHT](https://play.ht) (they have very conversational voices), & a voice provided by them labeled `Jennifer` (`female`, `en-US`).\n\n    You are free to use your favorite TTS voice platform here. [ElevenLabs](https://elevenlabs.io/) is\n    another alternative — by now you should get the flow of plugging in vendors into Vapi (add\n    provider key + pick provider in assistant config).\n\n    You can skip the next step(s) if you don't intend to use PlayHT.\n\n    <AccordionGroup>\n      <Accordion title=\"Set Your PlayHT Provider Key (optional)\" icon=\"key\" iconType=\"solid\">\n        If you haven't already, sign up for an account with PlayHT at [play.ht](https://play.ht). Since their flows are liable to change — you can just grab your `API Key` & `User ID` from them.\n\n        <Frame>\n          <img src=\"file:f063fc59-93ea-4a16-bb0a-f973f54c51f3\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Set Voice\" icon=\"person\" iconType=\"solid\">\n        You will want to select `playht` in the \"provider\" field, & `Jennifer` in the \"voice\" field. We will leave all of the other settings untouched.\n\n        <Frame caption=\"Each voice provider offers a host of settings you can modulate to customize voices. Here we will leave all the defaults alone.\">\n          <img src=\"file:cbcb3b79-84bf-4502-8489-a8e5c9b91d47\" />\n        </Frame>\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n\n\nTo customize additional fields, this can be done via the [Assistant](/api-reference/assistants/create-assistant) API instead.\n\nThen, you can copy the assistant's ID at the top of the assistant detail page:\n\n<Frame>\n  <img src=\"file:b52d2012-5fdb-43ea-bf04-aafe59cbf3e6\" />\n</Frame>\n\nNow we can call `.start()`, passing the persistent assistant's ID:\n\n```javascript\nvapi.start(\"79f3XXXX-XXXX-XXXX-XXXX-XXXXXXXXce48\");\n```\n\nIf you need to override any assistant settings or set template variables, you can pass `assistantOverrides` as the second argument.\n\nFor example, if the first message is \"Hello `{{name}}`\", you can set `assistantOverrides` to replace `{{name}}` with `John`:\n\n```javascript\nconst assistantOverrides = {\n  transcriber: {\n    provider: \"deepgram\",\n    model: \"nova-2\",\n    language: \"en-US\",\n  },\n  recordingEnabled: false,\n  variableValues: {\n    name: \"John\",\n  },\n};\n\nvapi.start(\"79f3XXXX-XXXX-XXXX-XXXX-XXXXXXXXce48\", assistantOverrides);\n```\n"
    },
    "sdks.mdx": {
      "markdown": "---\ntitle: Client SDKs\nsubtitle: Put Vapi assistants on every platform.\nslug: sdks\n---\n\nThe Vapi Client SDKs automatically configure audio streaming to and from the client, and provide a simple interface for starting calls. The interface is equivalent across all the SDKs.\n\nThe SDKs are open source, and available on GitHub:\n\n<CardGroup cols={3}>\n  <Card title=\"Vapi Web\" icon=\"window\" iconType=\"duotone\" href=\"/sdk/web\">\n    Add a Vapi assistant to your web application.\n  </Card>\n  <Card\n    title=\"Vapi iOS\"\n    icon=\"mobile-notch\"\n    href=\"https://github.com/VapiAI/ios\"\n  >\n    Add a Vapi assistant to your iOS app.\n  </Card>\n  <Card\n    title=\"Vapi Flutter\"\n    icon=\"mobile-notch\"\n    href=\"https://github.com/VapiAI/flutter\"\n  >\n    Add a Vapi assistant to your Flutter app.\n  </Card>\n  <Card\n    title=\"Vapi React Native\"\n    icon=\"mobile-notch\"\n    href=\"https://github.com/VapiAI/react-native-sdk\"\n  >\n    Add a Vapi assistant to your React Native app.\n  </Card>\n  <Card\n    title=\"Vapi Python\"\n    icon=\"fa-brands fa-python\"\n    href=\"https://github.com/VapiAI/python\"\n  >\n    Multi-platform. Mac, Windows, and Linux.\n  </Card>\n</CardGroup>\n\n---\n\n<Accordion title=\"Events\" defaultOpen={true}>\n  - `speech-start`, `speech-end`, and `volume-level` for creating animations. -\n  `message` for receiving messages sent to the [Server URL](/server-url) locally\n  on the client, so you can show live transcriptions and use function calls to\n  perform actions on the client.\n</Accordion>\n"
    },
    "sdk/web.mdx": {
      "markdown": "---\ntitle: Web SDK\nsubtitle: Integrate Vapi into your web application.\nslug: sdk/web\n---\n\nThe Vapi Web SDK provides web developers a simple API for interacting with the realtime call functionality of Vapi.\n\n### Installation\n\nInstall the package:\n\n```bash\nyarn add @vapi-ai/web\n```\n\nor w/ npm:\n\n```bash\nnpm install @vapi-ai/web\n```\n\n\n### Importing\n\nImport the package:\n\n```javascript\nimport Vapi from \"@vapi-ai/web\";\n```\n\nThen, create a new instance of the Vapi class, passing your **Public Key** as a parameter to the constructor:\n\n```javascript\nconst vapi = new Vapi(\"your-public-key\");\n```\n\nYou can find your public key in the [Vapi Dashboard](https://dashboard.vapi.ai/account).\n\n\n---\n\n## Usage\n\n### `.start()`\n\nYou can start a web call by calling the `.start()` function. The `start` function can either accept:\n\n1. **a string**, representing an assistant ID\n2. **an object**, representing a set of assistant configs (see [Create Assistant](/api-reference/assistants/create-assistant))\n\n#### Passing an Assistant ID\n\nIf you already have an assistant that you created (either via [the Dashboard](/quickstart/dashboard) or [the API](/api-reference/assistants/create-assistant)), you can start the call with the assistant's ID:\n\n```javascript\nvapi.start(\"79f3XXXX-XXXX-XXXX-XXXX-XXXXXXXXce48\");\n```\n\n#### Passing Assistant Configuration Inline\n\nYou can also specify configuration for your assistant inline.\n\nThis will not create a [persistent assistant](/assistants/persistent-assistants) that is saved to your account, rather it will create an ephemeral assistant only used for this call specifically.\n\nYou can pass the assistant's configuration in an object (see [Create Assistant](/api-reference/assistants/create-assistant) for a list of acceptable fields):\n\n```javascript\nvapi.start({\n  transcriber: {\n    provider: \"deepgram\",\n    model: \"nova-2\",\n    language: \"en-US\",\n  },\n  model: {\n    provider: \"openai\",\n    model: \"gpt-3.5-turbo\",\n    messages: [\n      {\n        role: \"system\",\n        content: \"You are a helpful assistant.\",\n      },\n    ],\n  },\n  voice: {\n    provider: \"playht\",\n    voiceId: \"jennifer\",\n  },\n  name: \"My Inline Assistant\",\n  ...\n});\n```\n\n#### Overriding Assistant Configurations\n\nTo override assistant settings or set template variables, you can pass `assistantOverrides` as the second argument.\n\nFor example, if the first message is \"Hello `{{name}}`\", set `assistantOverrides` to the following to replace `{{name}}` with `John`:\n\n```javascript\nconst assistantOverrides = {\n  transcriber: {\n    provider: \"deepgram\",\n    model: \"nova-2\",\n    language: \"en-US\",\n  },\n  recordingEnabled: false,\n  variableValues: {\n    name: \"Alice\",\n  },\n};\n\nvapi.start(\"79f3XXXX-XXXX-XXXX-XXXX-XXXXXXXXce48\", assistantOverrides);\n```\n\n### `.send()`\n\nDuring the call, you can send intermediate messages to the assistant (like [background messages](/assistants/background-messages)).\n\n- `type` will always be `\"add-message\"`\n- the `message` field will have 2 items, `role` and `content`.\n\n```javascript\nvapi.send({\n  type: \"add-message\",\n  message: {\n    role: \"system\",\n    content: \"The user has pressed the button, say peanuts\",\n  },\n});\n```\n\n<Info>\n  Possible values for role are `system`, `user`, `assistant`, `tool` or\n  `function`.\n</Info>\n\n### `.stop()`\n\nYou can stop the call session by calling the `stop` method:\n\n```javascript\nvapi.stop();\n```\n\nThis will stop the recording and close the connection.\n\n### `.isMuted()`\n\nCheck if the user's microphone is muted:\n\n```javascript\nvapi.isMuted();\n```\n\n### `.setMuted(muted: boolean)`\n\nYou can mute & unmute the user's microphone with `setMuted`:\n\n```javascript\nvapi.isMuted(); // false\nvapi.setMuted(true);\nvapi.isMuted(); // true\n```\n\n### `say(message: string, endCallAfterSpoken?: boolean)`\n\nThe `say` method can be used to invoke speech and gracefully terminate the call if needed\n\n```javascript\nvapi.say(\"Our time's up, goodbye!\", true)\n```\n\n## Events\n\nYou can listen on the `vapi` instance for events. These events allow you to react to changes in the state of the call or user speech.\n\n#### `speech-start`\n\nOccurs when your AI assistant has started speaking.\n\n```javascript\nvapi.on(\"speech-start\", () => {\n  console.log(\"Assistant speech has started.\");\n});\n```\n\n#### `speech-end`\n\nOccurs when your AI assistant has finished speaking.\n\n```javascript\nvapi.on(\"speech-end\", () => {\n  console.log(\"Assistant speech has ended.\");\n});\n```\n\n#### `call-start`\n\nOccurs when the call has connected & begins.\n\n```javascript\nvapi.on(\"call-start\", () => {\n  console.log(\"Call has started.\");\n});\n```\n\n#### `call-end`\n\nOccurs when the call has disconnected & ended.\n\n```javascript\nvapi.on(\"call-end\", () => {\n  console.log(\"Call has ended.\");\n});\n```\n\n#### `volume-level`\n\nRealtime volume level updates for the assistant. A floating-point number between `0` & `1`.\n\n```javascript\nvapi.on(\"volume-level\", (volume) => {\n  console.log(`Assistant volume level: ${volume}`);\n});\n```\n\n#### `message`\n\nVarious assistant messages can be sent back to the client during the call. These are the same messages that your [server](/server-url) would receive.\n\nAt [assistant creation time](/api-reference/assistants/create-assistant), you can specify on the `clientMessages` field the set of messages you'd like the assistant to send back to the client.\n\nThose messages will come back via the `message` event:\n\n```javascript\n// Various assistant messages can come back (like function calls, transcripts, etc)\nvapi.on(\"message\", (message) => {\n  console.log(message);\n});\n```\n\n#### `error`\n\nHandle errors that occur during the call.\n\n```javascript\nvapi.on(\"error\", (e) => {\n  console.error(e);\n});\n```\n\n---\n\n## Resources\n\n<CardGroup cols={2}>\n  <Card\n    title=\"NPM\"\n    icon=\"fa-brands fa-npm\"\n    iconType=\"solid\"\n    href=\"https://www.npmjs.com/package/@vapi-ai/web\"\n  >\n    View the package on NPM.\n  </Card>\n  <Card\n    title=\"GitHub\"\n    icon=\"fa-brands fa-github\"\n    iconType=\"solid\"\n    href=\"https://github.com/VapiAI/web\"\n  >\n    View the package on GitHub.\n  </Card>\n</CardGroup>\n<CardGroup cols={1}>\n  <Card\n    title=\"Try Our Quickstart\"\n    icon=\"bolt\"\n    iconType=\"solid\"\n    href=\"/quickstart/web\"\n  >\n    Get up and running quickly with the Web SDK.\n  </Card>\n</CardGroup>\n"
    },
    "examples/voice-widget.mdx": {
      "markdown": "---\ntitle: Web Snippet\nsubtitle: >-\n  Easily integrate the Vapi Voice Widget into your website for enhanced user\n  interaction.\nslug: examples/voice-widget\n---\n\n\nImprove your website's user interaction with the Vapi Voice Widget. This robust tool enables your visitors to engage with a voice assistant for support and interaction, offering a smooth and contemporary way to connect with your services.\n\n## Steps for Installation\n\n<Steps>\n  <Step title=\"Insert the Widget Snippet\">\n    Copy the snippet below and insert it into your website's HTML, ideally before the closing `</body>` tag.\n\n    ```html\n    <script>\n      var vapiInstance = null;\n      const assistant = \"<assistant_id>\"; // Substitute with your assistant ID\n      const apiKey = \"<your_public_api_key>\"; // Substitute with your Public key from Vapi Dashboard.\n      const buttonConfig = {}; // Modify this as required\n\n      (function (d, t) {\n        var g = document.createElement(t),\n          s = d.getElementsByTagName(t)[0];\n        g.src =\n          \"https://cdn.jsdelivr.net/gh/VapiAI/html-script-tag@latest/dist/assets/index.js\";\n        g.defer = true;\n        g.async = true;\n        s.parentNode.insertBefore(g, s);\n\n        g.onload = function () {\n          vapiInstance = window.vapiSDK.run({\n            apiKey: apiKey, // mandatory\n            assistant: assistant, // mandatory\n            config: buttonConfig, // optional\n          });\n        };\n      })(document, \"script\");\n\n    </script>\n    ```\n\n  </Step>\n  <Step title=\"Generate Your Assistant\">\n    From your Vapi dashboard, create an assistant to get the assistant ID. Alternatively, define an assistant configuration directly in your website's code as demonstrated in the example below.\n    ```javascript\n    const assistant = {\n      model: {\n        provider: \"openai\",\n        model: \"gpt-3.5-turbo\",\n        systemPrompt:\n          \"You're a versatile AI assistant named Vapi who is fun to talk with.\",\n      },\n      voice: {\n        provider: \"11labs\",\n        voiceId: \"paula\",\n      },\n      firstMessage: \"Hi, I am Vapi how can I assist you today?\",\n    };\n    ```\n\n  </Step>\n  <Step title=\"Modify the Button\">\n    Modify the `buttonConfig` object to align with your website's style and branding. Choose between a pill or round button and set colors, positions, and icons.\n    ```javascript\n    const buttonConfig = {\n      position: \"bottom-right\", // \"bottom\" | \"top\" | \"left\" | \"right\" | \"top-right\" | \"top-left\" | \"bottom-left\" | \"bottom-right\"\n      offset: \"40px\", // decide how far the button should be from the edge\n      width: \"50px\", // min-width of the button\n      height: \"50px\", // height of the button\n      idle: { // button state when the call is not active.\n        color: `rgb(93, 254, 202)`, \n        type: \"pill\", // or \"round\"\n        title: \"Have a quick question?\", // only required in case of Pill\n        subtitle: \"Talk with our AI assistant\", // only required in case of pill\n        icon: `https://unpkg.com/lucide-static@0.321.0/icons/phone.svg`,\n      },\n      loading: { // button state when the call is connecting\n        color: `rgb(93, 124, 202)`,\n        type: \"pill\", // or \"round\"\n        title: \"Connecting...\", // only required in case of Pill\n        subtitle: \"Please wait\", // only required in case of pill\n        icon: `https://unpkg.com/lucide-static@0.321.0/icons/loader-2.svg`,\n      },\n      active: { // button state when the call is in progress or active.\n        color: `rgb(255, 0, 0)`,\n        type: \"pill\", // or \"round\"\n        title: \"Call is in progress...\", // only required in case of Pill\n        subtitle: \"End the call.\", // only required in case of pill\n        icon: `https://unpkg.com/lucide-static@0.321.0/icons/phone-off.svg`,\n      },\n    };\n    ```\n  </Step>\n\n  <Step title=\"Add Functionality to Vapi Instance\">\n    You can use the `vapiInstance` returned from the run function in the snippet to further customize the behaviour. For instance, you might want to listen to various EventSource, or even send some messages to the bot programmatically.\n\n    ```js\n      vapiInstance.on('speech-start', () => {\n        console.log('Speech has started');\n      });\n\n      vapiInstance.on('speech-end', () => {\n        console.log('Speech has ended');\n      });\n\n      vapiInstance.on('call-start', () => {\n        console.log('Call has started');\n      });\n\n      vapiInstance.on('call-end', () => {\n        console.log('Call has stopped');\n      });\n\n      vapiInstance.on('volume-level', (volume) => {\n        console.log(`Assistant volume level: ${volume}`);\n      });\n\n      // Function calls and transcripts will be sent via messages\n      vapiInstance.on('message', (message) => {\n        console.log(message);\n      });\n\n      vapiInstance.on('error', (e) => {\n        console.error(e)\n      });\n    ```\n\n  </Step>\n</Steps>\n\n## Customization\n\nModify your assistant's behavior and the initial message users will see. Refer to the provided examples to customize the assistant's model, voice, and initial greeting.\n\n## UI Customization\n\nFor advanced styling, target the exposed CSS and other classes to ensure the widget's appearance aligns with your website's design. Here is a list of the classes you can customize:\n\n- `.vapi-btn`: The primary class for the Vapi button.\n- `.vapi-btn-is-idle`: The class for the Vapi button when the call is disconnected.\n- `.vapi-btn-is-active`: The class for the Vapi button when the call is active.\n- `.vapi-btn-is-loading`: The class for the Vapi button when the call is connecting.\n- `.vapi-btn-is-speaking`: The class for the Vapi button when the bot is speaking.\n- `.vapi-btn-pill`: The class for Vapi button to set pill variant.\n- `.vapi-btn-round`: The class for Vapi button to set round variant.\n"
    },
    "examples/outbound-sales.mdx": {
      "markdown": "---\ntitle: \"Outbound Sales Example \\U0001F4DE\"\nsubtitle: Let's build an outbound sales agent that can schedule appointments.\nslug: examples/outbound-sales\n---\n\n\nWe want this agent to be able to call a list of leads and schedule appointments. We'll create our assistant, create a phone number for it, then we'll configure our server for function calling to book the appointments.\n\n<Steps>\n  <Step title=\"Create an assistant\">\n    We'll start by taking a look at the [Assistant API\n    reference](/api-reference/assistants/create-assistant) and define our\n    assistant:\n\n    ```json\n    {\n      \"transcriber\":{\n        \"provider\": \"deepgram\",\n        \"keywords\": [\"Bicky:1\"]\n      },\n      \"model\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4\",\n        \"messages\": [\n          {\n              \"role\": \"system\",\n              \"content\": \"You're a sales agent for a Bicky Realty. You're calling a list of leads to schedule appointments to show them houses...\"\n          }\n        ],\n        \"functions\": [\n          {\n            \"name\": \"bookAppointment\",\n            \"description\": \"Used to book the appointment.\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"datetime\": {\n                  \"type\": \"string\",\n                  \"description\": \"The date and time of the appointment in ISO format.\"\n                }\n              }\n            }\n          }\n        ]\n      },\n      \"voice\": {\n        \"provider\": \"openai\",\n        \"voiceId\": \"onyx\"\n      },\n      \"forwardingPhoneNumber\": \"+16054440129\",\n      \"voicemailMessage\": \"Hi, this is Jennifer from Bicky Realty. We were just calling to let you know...\",\n      \"firstMessage\": \"Hi, this Jennifer from Bicky Realty. We're calling to schedule an appointment to show you a house. When would be a good time for you?\",\n      \"endCallMessage\": \"Thanks for your time.\",\n      \"endCallFunctionEnabled\": true,\n      \"recordingEnabled\": false,\n    }\n    ```\n    Let's break this down:\n    - `transcriber` - We're defining this to make sure the transcriber picks up the custom word \"Bicky\"\n    - `model` - We're using the OpenAI GPT-4 model, which is better at function calling.\n    - `messages` - We're defining the assistant's instructions for how to run the call.\n    - `functions` - We're providing a bookAppointment function with a datetime parameter. The assistant can call this during the conversation to book the appointment.\n    - `voice` - We're using the Onyx voice from OpenAI.\n    - `forwardingPhoneNumber` - Since we've added this, the assistant will be provided the [transferCall](/assistants#transfer-call) function to use.\n    - `voicemailMessage` - If the call goes to voicemail, this message will be played.\n    - `firstMessage` - This is the first message the assistant will say when the user picks up.\n    - `endCallMessage` - This is the message the assistant will deciding to hang up.\n    - `endCallFunctionEnabled` - This will give the assistant the [endCall](/assistants#end-call) function.\n    - `recordingEnabled` - We've disabled recording, since we don't have the user's consent to record the call.\n\n    We'll then make a POST request to the [Create Assistant](/api-reference/assistants/create-assistant) endpoint to create the assistant.\n\n  </Step>\n  <Step title=\"Buy a phone number\">\n    We'll buy a phone number for outbound calls using the [Phone Numbers API](/phone-calling#set-up-a-phone-number).\n\n    ```json\n    {\n      \"id\": \"c86b5177-5cd8-447f-9013-99e307a8a7bb\",\n      \"orgId\": \"aa4c36ba-db21-4ce0-9c6e-99e307a8a7bb\",\n      \"number\": \"+11234567890\",\n      \"createdAt\": \"2023-09-29T21:44:37.946Z\",\n      \"updatedAt\": \"2023-12-08T00:57:24.706Z\",\n    }\n    ```\n\n    Great, let's take note of that `id` field- we'll need it later.\n\n  </Step>\n  <Step title=\"Configure your Server URL\">\n    When the assistant calls that `bookAppointment` function, we'll want to handle that function call and actually book the appointment. We also want to let the user know if booking the appointment was unsuccessful.\n\n    First, we'll create an endpoint on our server for Vapi to hit. It'll receive messages as shown in the [Function Calling](/server-url#function-calling) docs. Once created, we'll add that endpoint URL to the **Server URL** field in the Account page on the [Vapi Dashboard](https://dashboard.vapi.ai).\n\n  </Step>\n    <Step title=\"Handle function calls\">\n    So now, when the assistant decides to call `bookAppointment`, our server will get something like this:\n\n    ```json\n    {\n      \"message\": {\n        \"type\": \"function-call\",\n        \"call\": { Call Object },\n        \"functionCall\": {\n          \"name\": \"bookAppointment\",\n          \"parameters\": \"{ \\\"datetime\\\": \\\"2023-09-29T21:44:37.946Z\\\"}\"\n        }\n      }\n    }\n    ```\n\n    We'll do our own logic to book the appointment, then we'll respond to the request with the result to let the assistant know it was booked:\n\n    ```json\n    { \"result\": \"The appointment was booked successfully.\" }\n    ```\n\n    or, if it failed:\n\n    ```json\n    { \"result\": \"The appointment time is unavailable, please try another time.\" }\n    ```\n\n    So, when the assistant calls this function, these results will be appended to the conversation, and the assistant will respond to the user knowing the result.\n\n    Great, now we're ready to start calling leads!\n    </Step>\n\n  <Step title=\"Place a call\">\n    We'll use the [Create Phone Call](/api-reference/calls/create-phone-call) endpoint to place a call to a lead:\n\n    ```json\n    {\n      \"phoneNumberId\": \"c86b5177-5cd8-447f-9013-99e307a8a7bb\",\n      \"assistantId\": \"d87b5177-5cd8-447f-9013-99e307a8a7bb\",\n      \"customer\": {\n        \"number\": \"+11234567890\"\n      }\n    }\n    ```\n\n    Since we also defined a `forwardingPhoneNumber`, when the user asks to speak to a human, the assistant will transfer the call to that number automatically.\n\n    We can then check the [Dashboard](https://dashboard.vapi.ai) to see the call logs and read the transcripts.\n\n  </Step>\n</Steps>\n"
    },
    "examples/inbound-support.mdx": {
      "markdown": "---\ntitle: Inbound Support Example ⚙️\nsubtitle: Let's build a technical support assistant that remembers where we left off.\nslug: examples/inbound-support\n---\n\n\nWe want a phone number we can call to get technical support. We want the assistant to use a provided set of troubleshooting guides to help walk the caller through solving their issue.\n\nAs a bonus, we also want the assistant to remember by the phone number of the caller where we left off if we get disconnected.\n\n<Steps>\n  <Step title=\"Create an assistant\">\n    We'll start by taking a look at the [Assistant API\n    reference](/api-reference/assistants/create-assistant) and define our\n    assistant:\n\n    ```json\n    {\n      \"transcriber\":{\n        \"provider\": \"deepgram\",\n        \"keywords\": [\"iPhone:1\", \"MacBook:1.5\", \"iPad:1\", \"iMac:0.8\", \"Watch:1\", \"TV:1\", \"Apple:2\"],\n      },\n      \"model\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4\",\n        \"messages\": [\n          {\n              \"role\": \"system\",\n              \"content\": \"You're a technical support assistant. You're helping a customer troubleshoot their Apple device. You can ask the customer questions, and you can use the following troubleshooting guides to help the customer solve their issue: ...\"\n          }\n        ]\n      },\n      \"forwardingPhoneNumber\": \"+16054440129\",\n      \"firstMessage\": \"Hey, I'm an A.I. assistant for Apple. I can help you troubleshoot your Apple device. What's the issue?\",\n      \"recordingEnabled\": true,\n    }\n    ```\n    <Card title=\"Let's break this down\">\n    - `transcriber` - We're defining this to make sure the transcriber picks up the custom words related to our devices.\n    - `model` - We're using the OpenAI GPT-3.5-turbo model. It's much faster and preferred if we don't need GPT-4.\n    - `messages` - We're defining the assistant's instructions for how to run the call.\n    - `forwardingPhoneNumber` - Since we've added this, the assistant will be provided the [transferCall](/assistants#transfer-call) function to use if the caller asks to be transferred to a person.\n    - `firstMessage` - This is the first message the assistant will say when the user picks up.\n    - `recordingEnabled` - We're recording the call so we can hear the conversation later.\n\n</Card>\n\n    Since we want the assistant to remember where we left off, its configuration is going to change based on the caller. So, we're not going to use [temporary assistants](/assistants/persistent-assistants).\n\n    For this example, we're going to store the conversation on our server between calls and use the [Server URL's `assistant-request`](/server-url#retrieving-assistants) to fetch a new configuration based on the caller every time someone calls.\n\n  </Step>\n  <Step title=\"Buy a phone number\">\n    We'll buy a phone number for inbound calls using the [Phone Numbers API](/api-reference/phone-numbers/buy-phone-number).\n\n    ```json\n    {\n      \"id\": \"c86b5177-5cd8-447f-9013-99e307a8a7bb\",\n      \"orgId\": \"aa4c36ba-db21-4ce0-9c6e-99e307a8a7bb\",\n      \"number\": \"+11234567890\",\n      \"createdAt\": \"2023-09-29T21:44:37.946Z\",\n      \"updatedAt\": \"2023-12-08T00:57:24.706Z\",\n    }\n    ```\n\n  </Step>\n  <Step title=\"Configure your Server URL\">\n    When someone calls our number, we want to fetch the assistant configuration from our server. We'll use the [Server URL's `assistant-request`](/server-url#retrieving-assistants) to do this.\n\n    First, we'll create an endpoint on our server for Vapi to hit. It'll receive messages as shown in the [Assistant Request](/server-url#retrieving-assistants-calling) docs. Once created, we'll add that endpoint URL to the **Server URL** field in the Account page on the [Vapi Dashboard](https://dashboard.vapi.ai).\n\n  </Step>\n   <Step title=\"Save the conversation at the end of the call\">\n        We'll want to save the conversation at the end of the call for the next time they call. We'll use the [Server URL's `end-of-call-report`](/server-url#end-of-call-report) message to do this.\n\n        At the end of each call, we'll get a message like this:\n\n        ```json\n        {\n            \"message\": {\n                \"type\": \"end-of-call-report\",\n                \"endedReason\": \"hangup\",\n                \"call\": { Call Object },\n                \"recordingUrl\": \"https://vapi-public.s3.amazonaws.com/recordings/1234.wav\",\n                \"summary\": \"The user mentioned they were having an issue with their iPhone restarting randomly. They restarted their phone, but the issue persisted. They mentioned they were using an iPhone 12 Pro Max. They mentioned they were using iOS 15.\",\n                \"transcript\": \"Hey, I'm an A.I. assistant for Apple...\",\n                \"messages\":[\n                {\n                    \"role\": \"assistant\",\n                    \"message\": \"Hey, I'm an A.I. assistant for Apple. I can help you troubleshoot your Apple device. What's the issue?\",\n                },\n                {\n                    \"role\": \"user\",\n                    \"message\": \"Yeah I'm having an issue with my iPhone restarting randomly.\",\n                },\n                ...\n                ]\n            }\n        }\n        ```\n\n        We'll save the `call.customer.number` and `summary` fields to our database for the next time they call.\n    </Step>\n    <Step title=\"Handle assistant requests.\">\n    When our number receives a call, Vapi will also hit our server's endpoint with a message like this:\n\n    ```json\n    {\n        \"message\": {\n            \"type\": \"assistant-request\",\n            \"call\": { Call Object },\n        }\n    }\n    ```\n\n    We'll check our database to see if we have a conversation for this caller. If we do, we'll create an assistant configuration like in Step 1 and respond with it:\n\n    ```json\n    {\n        \"assistant\": {\n            ...\n            \"model\": {\n                \"provider\": \"openai\",\n                \"model\": \"gpt-4\",\n                \"messages\": [\n                  {\n                    \"role\": \"system\",\n                    \"content\": \"You're a technical support assistant. Here's where we left off: ...\"\n                  }\n                ]\n            },\n            ...\n        }\n    }\n    ```\n\n    If we don't, we'll just respond with the assistant configuration from Step 1.\n\n    </Step>\n\n  <Step title=\"Try calling it!\">\n    We'll call our number and see if it works. Give it a call, and tell it you're having an issue with your iPhone restarting randomly.\n\n    Hang up, and call back. Then ask what the issue was. The assistant should remember where we left off.\n\n  </Step>\n</Steps>\n"
    },
    "examples/pizza-website.mdx": {
      "markdown": "---\ntitle: \"Pizza Website Example \\U0001F355\"\nsubtitle: Let's build a pizza ordering assistant for our website.\nslug: examples/pizza-website\n---\n\n\nIn this example, we'll be using the [Web SDK](https://github.com/VapiAI/web) to create an assistant that can take a pizza order. Since all the [Client SDKs](/sdks) have equivalent functionality, you can use this example as a guide for any Vapi client.\n\nWe want to add a button to the page to start a call, update our UI with the call status, and display what the user's saying while they say it. When the user mentions a topping, we should add it to the pizza. When they're done, we should redirect them to checkout.\n\n<Steps>\n  <Step title=\"Create an assistant\">\n    We'll start by taking a look at the [Assistant API\n    reference](/api-reference/assistants/create-assistant) and define our\n    assistant:\n\n    ```json\n    {\n      \"model\": {\n        \"provider\": \"openai\",\n        \"model\": \"gpt-4\",\n        \"messages\": [\n          {\n              \"role\": \"system\",\n              \"content\": \"You're a pizza ordering assistant. The user will ask for toppings, you'll add them. When they're done, you'll redirect them to checkout.\"\n          }\n        ],\n        \"functions\": [\n          {\n            \"name\": \"addTopping\",\n            \"description\": \"Used to add a topping to the pizza.\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"topping\": {\n                  \"type\": \"string\",\n                  \"description\": \"The name of the topping. For example, 'pepperoni'.\"\n                }\n              }\n            }\n          },\n           {\n            \"name\": \"goToCheckout\",\n            \"description\": \"Redirects the user to checkout and order their pizza.\",\n            \"parameters\": {\"type\": \"object\", \"properties\": {}}\n          }\n        ]\n      },\n      \"firstMessage\": \"Hi, I'm the pizza ordering assistant. What toppings would you like?\",\n    }\n    ```\n    Let's break this down:\n    - `model` - We're using the OpenAI GPT-4 model, which is better at function calling.\n    - `messages` - We're defining the assistant's instructions for how to run the call.\n    - `functions` - We're providing a addTopping function with a topping parameter. The assistant can call this during the conversation to add a topping. We're also adding goToCheckout, with an empty parameters object. The assistant can call this to redirect the user to checkout.\n    - `firstMessage` - This is the first message the assistant will say when the user starts the call.\n\n    We'll then make a POST request to the [Create Assistant](/api-reference/assistants/create-assistant) endpoint to create the assistant.\n\n  </Step>\n  <Step title=\"Set up the Web SDK\">\n    We'll follow the `README` for the [Web SDK](https://github.com/VapiAI/web) to get it installed.\n\n    We'll then get our **Public Key** from the [Vapi Dashboard](https://dashboard.vapi.ai) and initialize the SDK:\n\n    ```js\n    import Vapi from '@vapi-ai/web';\n\n    const vapi = new Vapi('your-web-token');\n    ```\n\n  </Step>\n  <Step title=\"Add the call buttons\">\n    We'll add a button to the page that starts the call when clicked:\n\n    ```html\n    <button id=\"start-call\">Start Call</button>\n    <button id=\"stop-call\">Stop Call</button>\n    ```\n\n    ```js\n    const startCallButton = document.getElementById('start-call');\n\n    startCallButton.addEventListener('click', async () => {\n      await vapi.start('your-assistant-id');\n    });\n\n    const stopCallButton = document.getElementById('stop-call');\n\n    stopCallButton.addEventListener('click', async () => {\n      await vapi.stop();\n    });\n    ```\n\n  </Step>\n  <Step title=\"Handle call status events\">\n    ```js\n    vapi.on('call-start', () => {\n      // Update UI to show that the call has started\n    });\n\n    vapi.on('call-end', () => {\n      // Update UI to show that the call has ended\n    });\n    ```\n\n  </Step>\n\n<Step title=\"Handle speaking events\">\n  ```js\n  vapi.on('speech-start', () => {\n    // Update UI to show that the assistant is speaking\n  });\n\nvapi.on('speech-end', () => {\n// Update UI to show that the assistant is done speaking\n});\n\n````\n\n</Step>\n\n<Step title=\"Handle transcription events\">\n  All messages send to the [Server URL](/server-url), including `transcript` and `function-call` messages, are also sent to the client as `message` events. We'll need to check the `type` of the message to see what type it is.\n\n```js\nvapi.on(\"message\", (msg) => {\n  if (msg.type !== \"transcript\") return;\n\n  if (msg.transcriptType === \"partial\") {\n    // Update UI to show the live partial transcript\n  }\n\n  if (msg.transcriptType === \"final\") {\n    // Update UI to show the final transcript\n  }\n});\n````\n\n</Step>\n\n<Step title=\"Handle function call events\">\n```javascript\nvapi.on('message', (msg) => {\n  if (msg.type !== \"function-call\") return;\n\nif (msg.functionCall.name === \"addTopping\") {\nconst topping = msg.functionCall.parameters.topping;\n// Add the topping to the pizza\n}\n\nif (msg.functionCall.name === \"goToCheckout\") {\n// Redirect the user to checkout\n}\n});\n\n```\n</Step>\n<Step title=\"Order your pizza!\">\nYou should now have a working pizza ordering assistant! 🍕\n</Step></Steps>\n\n\n```\n"
    },
    "examples/outbound-call-python.mdx": {
      "markdown": "---\ntitle: \"Outbound Calls from Python \\U0001F4DE\"\nsubtitle: Some sample code for placing an outbound call using Python\nslug: examples/outbound-call-python\n---\n\n\n```python\nimport requests\n\n# Your Vapi API Authorization token\nauth_token = '<YOUR AUTH TOKEN>'\n# The Phone Number ID, and the Customer details for the call\nphone_number_id = '<PHONE NUMBER ID FROM DASHBOARD>'\ncustomer_number = \"+14151231234\"\n\n# Create the header with Authorization token\nheaders = {\n    'Authorization': f'Bearer {auth_token}',\n    'Content-Type': 'application/json',\n}\n\n# Create the data payload for the API request\ndata = {\n    'assistant': {\n        \"firstMessage\": \"Hey, what's up?\",\n        \"model\": {\n            \"provider\": \"openai\",\n            \"model\": \"gpt-3.5-turbo\",\n            \"messages\": [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are an assistant.\"\n                }\n            ]\n        },\n        \"voice\": \"jennifer-playht\"\n    },\n    'phoneNumberId': phone_number_id,\n    'customer': {\n        'number': customer_number,\n    },\n}\n\n# Make the POST request to Vapi to create the phone call\nresponse = requests.post(\n    'https://api.vapi.ai/call/phone', headers=headers, json=data)\n\n# Check if the request was successful and print the response\nif response.status_code == 201:\n    print('Call created successfully')\n    print(response.json())\nelse:\n    print('Failed to create call')\n    print(response.text)\n```\n"
    },
    "resources.mdx": {
      "markdown": "---\ntitle: Code Resources\nsubtitle: Find all of our resources here.\nslug: resources\n---\n\n\n{/* Use this LInk to modify the content -> https://onecompiler.com/ejs/425khha82 */}\n\n<table>\n\n<thead><tr><th colSpan=\"2\">Vapi AI Ecosystem</th></tr></thead>\n<tbody>\n<tr><td>Real-time SDKs</td><td><a target=\"_blank\" href=\"https://github.com/VapiAI/web\">Web</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/flutter\">Flutter</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/react-native-sdk\">React Native</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/ios\">iOS</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/python\">Python</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/html-script-tag\">Vanilla</a></td></tr>\n<tr><td>Client Examples</td><td><a target=\"_blank\" href=\"https://github.com/VapiAI/client-side-example-javascript-next\">Next.js</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/client-side-example-javascript-react\">React</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/flutter/tree/main/example\">Flutter</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/client-side-example-react-native\">React Native</a></td></tr>\n<tr><td>Server Examples</td><td><a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-serverless-vercel\">Vercel</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-serverless-cloudflare\">Cloudflare</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-serverless-supabase\">Supabase</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-javascript-node\">Node</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-javascript-bun\">Bun</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-javascript-deno\">Deno</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-python-flask\">Flask</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-php-laravel\">Laravel</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-go-gin\">Go</a> · <a target=\"_blank\" href=\"https://github.com/VapiAI/server-side-example-rust-actix\">Rust</a></td></tr>\n<tr><td>Resources</td><td><a target=\"_blank\" href=\"https://docs.vapi.ai/\">Official Docs</a> · <a target=\"_blank\" href=\"https://api.vapi.ai/api\">API Reference</a></td></tr>\n<tr><td>Community</td><td><a target=\"_blank\" href=\"/community/videos\">Videos</a> . <a target=\"_blank\" href=\"https://www.vapiblocks.com/\">UI Library</a></td></tr>\n</tbody>\n</table>\n"
    },
    "customization/provider-keys.mdx": {
      "markdown": "---\ntitle: Provider Keys\nsubtitle: Bring your own API keys to Vapi.\nslug: customization/provider-keys\n---\n\n\nHave a custom model or voice with one of the providers? Or an enterprise account with volume pricing?\n\nNo problem! You can bring your own API keys to Vapi. You can add them in the [Dashboard](https://dashboard.vapi.ai) under the **Provider Keys** tab. Once your API key is validated, you won't be charged when using that provider through Vapi. Instead, you'll be charged directly by the provider.\n\n## Transcription Providers\n\nCurrently, the only available transcription provider is `deepgram`. To use a custom model, you can specify the deepgram model ID in the `transcriber.model` parameter of the [Assistant](/api-reference/assistants/create-assistant).\n\n## Model Providers\n\nWe are currently have support for any OpenAI-compatible endpoint. This includes services like [OpenRouter](https://openrouter.ai/), [AnyScale](https://www.anyscale.com/), [Together AI](https://www.together.ai/), or your own server.\n\nTo use one of these providers, you can specify the `provider` and `model` in the `model` parameter of the [Assistant](/api-reference/assistants/create-assistant).\n\nYou can find more details in the [Custom LLMs](customization/custom-llm/fine-tuned-openai-models) section of the documentation.\n\n## Voice Providers\n\nAll voice providers are supported. Once you've validated your API through the [Dashboard](https://dashboard.vapi.ai), any voice ID from your provider can be used in the `voice.voiceId` field of the [Assistant](/api-reference/assistants/create-assistant).\n"
    },
    "customization/custom-llm/fine-tuned-openai-models.mdx": {
      "markdown": "---\ntitle: Fine-tuned OpenAI models\nsubtitle: Use Another LLM or Your Own Server\nslug: customization/custom-llm/fine-tuned-openai-models\n---\n\n\nVapi supports using any OpenAI-compatible endpoint as the LLM. This includes services like [OpenRouter](https://openrouter.ai/), [AnyScale](https://www.anyscale.com/), [Together AI](https://www.together.ai/), or your own server.\n\n<Accordion title=\"When to Use Custom LLMs\">\n  - For an open-source LLM, like Mixtral\n  - To update the context during the conversation\n  - To customize the messages before they're sent to an LLM\n</Accordion>\n\n## Using an LLM provider\n\nYou'll first want to POST your API key via the `/credential` endpoint:\n\n```json\n{\n  \"provider\": \"openrouter\",\n  \"apiKey\": \"<YOUR OPENROUTER KEY>\"\n}\n```\n\nThen, you can create an assistant with the model provider:\n\n```json\n{\n  \"name\": \"My Assistant\",\n  \"model\": {\n    \"provider\": \"openrouter\",\n    \"model\": \"cognitivecomputations/dolphin-mixtral-8x7b\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are an assistant.\"\n      }\n    ],\n    \"temperature\": 0.7\n  }\n}\n```\n## Using Fine-Tuned OpenAI Models\n\nTo set up your OpenAI Fine-Tuned model, you need to follow these steps:\n\n1. Set the custom llm URL to `https://api.openai.com/v1`.\n2. Assign the custom llm key to the OpenAI key.\n3. Update the model to their model.\n4. Execute a PATCH request to the `/assistant` endpoint and ensure that `model.metadataSendMode` is set to off.\n\n## Using your server\n\nTo set up your server to act as the LLM, you'll need to create an endpoint that is compatible with the [OpenAI Client](https://platform.openai.com/docs/api-reference/making-requests). For best results, your endpoint should also support streaming completions.\n\nIf your server is making calls to an OpenAI compatble API, you can pipe the requests directly back in your response to Vapi.\n\nIf you'd like your OpenAI-compatible endpoint to be authenticated, you can POST your server's API key and URL via the `/credential` endpoint:\n\n```json\n{\n  \"provider\": \"custom-llm\",\n  \"apiKey\": \"<YOUR SERVER API KEY>\"\n}\n```\n\nIf your server isn't authenticated, you can skip this step.\n\nThen, you can create an assistant with the `custom-llm` model provider:\n\n```json\n{\n  \"name\": \"My Assistant\",\n  \"model\": {\n    \"provider\": \"custom-llm\",\n    \"url\": \"<YOUR OPENAI COMPATIBLE ENDPOINT BASE URL>\",\n    \"model\": \"my-cool-model\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are an assistant.\"\n      }\n    ],\n    \"temperature\": 0.7\n  }\n}\n```\n"
    },
    "customization/custom-llm/using-your-server.mdx": {
      "markdown": "---\ntitle: 'Connecting Your Custom LLM to Vapi: A Comprehensive Guide'\nslug: customization/custom-llm/using-your-server\n---\n\n\nThis guide provides a comprehensive walkthrough on integrating Vapi with OpenAI's gpt-3.5-turbo-instruct model using a custom LLM configuration. We'll leverage Ngrok to expose a local development environment for testing and demonstrate the communication flow between Vapi and your LLM.\n## Prerequisites\n\n- **Vapi Account**: Access to the Vapi Dashboard for configuration.\n- **OpenAI API Key**: With access to the gpt-3.5-turbo-instruct model.\n- **Python Environment**: Set up with the OpenAI library (`pip install openai`).\n- **Ngrok**: For exposing your local server to the internet.\n- **Code Reference**: Familiarize yourself with the `/openai-sse/chat/completions` endpoint function in the provided Github repository: [Server-Side Example Python Flask](https://github.com/VapiAI/server-side-example-python-flask/blob/main/app/api/custom_llm.py).\n\n## Step 1: Setting Up Your Local Development Environment\n\n**1. Create a Python Script (app.py):**\n\n```python\nfrom flask import Flask, request, jsonify\nimport openai\n\napp = Flask(__name__)\nopenai.api_key = \"YOUR_OPENAI_API_KEY\"  # Replace with your actual API key\n\n@app.route(\"/chat/completions\", methods=[\"POST\"])\ndef chat_completions():\n    data = request.get_json()\n    # Extract relevant information from data (e.g., prompt, conversation history)\n    # ...\n    \n    response = openai.ChatCompletion.create(\n        model=\"gpt-3.5-turbo-instruct\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            # ... (Add messages from conversation history and current prompt)\n        ]\n    )\n    # Format response according to Vapi's structure\n    # ...\n    return jsonify(formatted_response)\n\nif __name__ == \"__main__\":\n    app.run(debug=True, port=5000)  # You can adjust the port if needed\n```\n**2. Run the Script:**\nExecute the Python script using python app.py in your terminal. This will start the Flask server on the specified port (5000 in this example).\n\n**3. Expose with Ngrok:**\nOpen a new terminal window and run ngrok http 5000 (replace 5000 with your chosen port) to create a public URL that tunnels to your local server.\n\n## Step 2: Configuring Vapi with Custom LLM\n**1. Access Vapi Dashboard:**\nLog in to your Vapi account and navigate to the \"Model\" section.\n\n**2. Select Custom LLM:**\nChoose the \"Custom LLM\" option to set up the integration.\n\n**3. Enter Ngrok URL:**\nPaste the public URL generated by ngrok (e.g., https://your-unique-id.ngrok.io) into the endpoint field. This will be the URL Vapi uses to communicate with your local server.\n\n**4. Test the Connection:**\nSend a test message through the Vapi interface to ensure it reaches your local server and receives a response from the OpenAI API. Verify that the response is displayed correctly in Vapi.\n\n## Step 3: Understanding the Communication Flow\n**1. Vapi Sends POST Request:**\nWhen a user interacts with your Vapi application, Vapi sends a POST request containing conversation context and metadata to the configured endpoint (your ngrok URL).\n\n**2. Local Server Processes Request:**\nYour Python script receives the POST request and the chat_completions function is invoked.\n\n**3. Extract and Prepare Data:**\nThe script parses the JSON data, extracts relevant information (prompt, conversation history), and builds the prompt for the OpenAI API call.\n\n**4. Call to OpenAI API:**\nThe constructed prompt is sent to the gpt-3.5-turbo-instruct model using the openai.ChatCompletion.create method.\n\n**5. Receive and Format Response:**\nThe response from OpenAI, containing the generated text, is received and formatted according to Vapi's expected structure.\n\n**6. Send Response to Vapi:**\nThe formatted response is sent back to Vapi as a JSON object.\n\n**7. Vapi Displays Response:**\nVapi receives the response and displays the generated text within the conversation interface to the user.\n\nBy following these detailed steps and understanding the communication flow, you can successfully connect Vapi to OpenAI's gpt-3.5-turbo-instruct model and create powerful conversational experiences within your Vapi applications. The provided code example and reference serve as a starting point for you to build and customize your integration based on your specific needs.\n\n**Video Tutorial:**\n  <iframe\n    src=\"https://www.youtube.com/embed/-1xWhYmOT0A?si=8qB6FLzcmmrmduT-\"\n    title=\"Loom video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    width=\"100%\"\n    height=\"400px\"\n    allowfullscreen\n  />\n"
    },
    "customization/custom-voices/custom-voice.mdx": {
      "markdown": "---\ntitle: Introduction\nsubtitle: Use Custom Voice with your favourite provider instead of the preset ones.\nslug: customization/custom-voices/custom-voice\n---\n\n\nVapi lets you use various providers with some preset voice. At the same time you can also create your own custom voices in the supported providers and use them with Vapi.\n\nYou can update the `voice` property in the assistant configuration when you are creating the assistant to use your custom voice.\n\n```json\n{\n  \"voice\": {\n    \"provider\": \"deepgram\",\n    \"voiceId\": \"your-voice-id\"\n  }\n}\n```\n"
    },
    "customization/custom-voices/elevenlabs.mdx": {
      "markdown": "---\ntitle: Elevenlabs\nsubtitle: 'Quickstart: Setup Elevenlabs Custom Voice'\nslug: customization/custom-voices/elevenlabs\n---\n\n\nThis guide outlines the procedure for integrating your cloned voice with 11labs through the VAPI platform.\n\n<Note>An subscription is required for this process to work.</Note>\n\nTo integrate your cloned voice with 11labs using the VAPI platform, follow these steps. \n\n1. **Obtain an 11labs API Subscription:** Visit the [11labs pricing page](https://elevenlabs.io/pricing) and subscribe to an API plan that suits your needs.\n2. **Retrieve Your API Key:** Go to the 'Profile + Keys' section on the 11labs website to get your API key.\n3. **Enter Your API Key in VAPI:** Navigate to the [VAPI Provider Key section](https://dashboard.vapi.ai/keys) and input your 11labs API key under the 11labs section.\n4. **Sync Your Cloned Voice:** From the [Voice Library](https://dashboard.vapi.ai/voice-library) in VAPI, select 11labs as your voice provider and click on \"Sync with 11labs.\"\n5. **Search and Use Your Cloned Voice:** After syncing, you can search for your cloned voice within the voice library and directly use it with your assistant.\n\nBy following these steps, you will successfully integrate your cloned voice from 11labs with VAPI.\n\n**Video Tutorial:**\n  <iframe\n    src=\"https://www.loom.com/embed/91568c17289740889c278f458f0d291c?sid=a0c812fa-5809-4ffa-8391-6a578c3e0608\"\n    title=\"Loom video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    width=\"100%\"\n    height=\"400px\"\n    allowfullscreen\n  />\n"
    },
    "customization/custom-voices/playht.mdx": {
      "markdown": "---\ntitle: PlayHT\nsubtitle: 'Quickstart: Setup PlayHT Custom Voice'\nslug: customization/custom-voices/playht\n---\n\n\nThis guide outlines the procedure for integrating your cloned voice with Play.ht through the VAPI platform.\n\n<Note>An API subscription is required for this process to work.</Note>\n\nTo integrate your cloned voice with [Play.ht](http://play.ht/) using the VAPI platform, follow these steps.\n\n1. **Obtain a Play.ht API Subscription:** Visit the [Play.ht pricing page](https://play.ht/studio/pricing) and subscribe to an API plan.\n2. **Retrieve Your User ID and Secret Key:** Go to the [API Access section](https://play.ht/studio/api-access) on Play.ht to get your User ID and Secret Key.\n3. **Enter Your API Keys in VAPI:** Navigate to the [VAPI Provider Key section](https://dashboard.vapi.ai/keys) and input your Play.ht API keys under the Play.ht section.\n4. **Sync Your Cloned Voice:** From the [Voice Library](https://dashboard.vapi.ai/voice-library) in VAPI, select Play.ht as your voice provider and click on \"Sync with Play.ht.\"\n5. **Search and Use Your Cloned Voice:** After syncing, you can search for your cloned voice within the voice library and directly use it with your assistant.\n\n**Video Tutorial:**\n  <iframe\n    src=\"https://www.loom.com/embed/45a6e43ae03945a783385f771ea9203d?sid=268071d7-d37f-43aa-843a-13c221af3ed5\"\n    title=\"Loom video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    width=\"100%\"\n    height=\"400px\"\n    allowfullscreen\n  />\n"
    },
    "customization/custom-keywords.mdx": {
      "markdown": "---\ntitle: Custom Keywords\nsubtitle: Enhanced transcription accuracy guide\nslug: customization/custom-keywords\n---\n\n\nVAPI allows you to improve the accuracy of your transcriptions by leveraging Deepgram's keyword boosting feature. This is particularly useful when dealing with specialized terminology or uncommon proper nouns. By providing specific keywords to the Deepgram model, you can enhance transcription quality directly through VAPI.\n\n### Why Use Keyword Boosting?\n\nKeyword boosting is beneficial for:\n\n- Enhancing the recognition of specialized terms and proper nouns.\n- Improving transcription accuracy without the need for a custom-trained model.\n- Quickly updating the model's vocabulary with new or uncommon words.\n\n### Important Notes\n\n- Keywords should be uncommon words or proper nouns not frequently recognized by the model.\n- Custom model training is the most effective way to ensure accurate keyword recognition.\n- For more than 50 keywords, consider custom model training by contacting Deepgram.\n\n## Enabling Keyword Boosting in VAPI\n\n### API Call Integration\n\nTo enable keyword boosting, you need to add a `keywords` parameter to your VAPI assistant's transcriber section. This parameter should include the keywords and their respective intensifiers.\n\n### Example of POST Request\n\nTo create an assistant with keyword boosting enabled, you can make the following POST request to VAPI:\n\n```bash\nbashCopy code\ncurl \\\n  --request POST \\\n  --header 'Authorization: Bearer <token>' \\\n  --header 'Content-Type: application/json' \\\n  --data '{\n    \"name\": \"Emma\",\n    \"model\": {\n        \"model\": \"gpt-4o\",\n        \"provider\": \"openai\"\n    },\n    \"voice\": {\n        \"voiceId\": \"emma\",\n        \"provider\": \"azure\"\n    },\n    \"transcriber\": {\n        \"provider\": \"deepgram\",\n        \"model\": \"nova-2\",\n        \"language\": \"bg\",\n        \"smartFormat\": true,\n        \"keywords\": [\n            \"snuffleupagus:1\"\n        ]\n    },\n    \"firstMessage\": \"Hi, I am Emma, what is your name?\",\n    \"firstMessageMode\": \"assistant-speaks-first\"\n  }' \\\n  https://api.vapi.ai/assistant\n\n```\n\nIn this configuration:\n\n- **name**: The name of the assistant.\n- **model**: Specifies the model and provider for the assistant's conversational capabilities.\n- **voice**: Specifies the voice and provider for the assistant's speech.\n- **transcriber**: Specifies Deepgram as the transcription provider, along with the model, language, smart formatting, and keywords for boosting.\n- **firstMessage**: The initial message the assistant will speak.\n- **firstMessageMode**: Specifies that the assistant speaks first.\n\n### Intensifiers\n\nIntensifiers are exponential factors that boost or suppress the likelihood of the specified keyword being recognized. The default intensifier is `1`. Higher values increase the likelihood, while `0` is equivalent to not specifying a keyword.\n\n- **Boosting Example:** `keywords=snuffleupagus:5`\n- **Suppressing Example:** `keywords=kansas:-10`\n\n### Best Practices for Keyword Boosting\n\n1. **Send Uncommon Keywords:** Focus on keywords not successfully transcribed by the model.\n2. **Send Keywords Once:** Avoid repeating keywords.\n3. **Use Individual Keywords:** Prefer individual terms over phrases.\n4. **Use Proper Spelling:** Spell proper nouns as you want them to appear in transcripts.\n5. **Moderate Intensifiers:** Start with small increments to avoid false positives.\n6. **Custom Model Training:** For extensive vocabulary needs, consider custom model training.\n\n### Additional Resources\n\nFor more detailed information on Deepgram's keyword boosting feature, refer to the Deepgram Keyword Boosting Documentation.\n\nBy following these guidelines, you can effectively utilize Deepgram's keyword boosting feature within your VAPI assistant, ensuring enhanced transcription accuracy for specialized terminology and uncommon proper nouns.\n"
    },
    "customization/knowledgebase.mdx": {
      "markdown": "---\ntitle: Creating Custom Knowledge Bases for Your Voice AI Assistants\nsubtitle: >-\n  Learn how to create and integrate custom knowledge bases into your voice AI\n  assistants.\nslug: customization/knowledgebase\n---\n \n<iframe\n  width=\"560\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/i5mvqC5sZxU\"\n  title=\"Vapi's Knowledge Base\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\"\n  allowfullscreen\n></iframe>\n\n## **What is Vapi's Knowledge Base?**\nOur Knowledge Base is a collection of custom documents that contain information on specific topics or domains. By integrating a Knowledge Base into your voice AI assistant, you can enable it to provide more accurate and informative responses to user queries.\n\n### **Why Use a Knowledge Base?**\nUsing a Knowledge Base with your voice AI assistant offers several benefits:\n\n* **Improved accuracy**: By integrating custom documents into your assistant, you can ensure that it provides accurate and up-to-date information to users.\n* **Enhanced capabilities**: A Knowledge Base enables your assistant to answer complex queries and provide detailed responses to user inquiries.\n* **Customization**: With a Knowledge Base, you can tailor your assistant's responses to specific domains or topics, making it more effective and informative.\n\n## **How to Create a Knowledge Base**\n\nTo create a Knowledge Base, follow these steps:\n\n### **Step 1: Upload Your Documents**\n\nNavigate to Overview > Documents and upload your custom documents in Markdown, PDF, plain text, or Microsoft Word (.doc and .docx) format to Vapi's Knowledge Base.\n\n<Image src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1715628063841/rSrWDQ6YM.png\" alt=\"Adding documents to your Knowledge Base\" />\n\n### **Step 2: Create an Assistant**\n\nCreate a new assistant in Vapi and, on the right sidebar menu, select the document you've just added to the Knowledge Base feature.\n<Image src=\"https://cdn.hashnode.com/res/hashnode/image/upload/v1715628223370/e18L04yRk-.png\" alt=\"Adding documents to your assistant\" />\n\n\n### **Step 3: Configure Your Assistant**\n\nCustomize your assistant's system prompt to utilize the Knowledge Base for responding to user queries.\n\n## **Best Practices for Creating Effective Knowledge Bases**\n\n* **Organize Your documents**: Organize your documents by topic or category to ensure that your assistant can quickly retrieve relevant information.\n* **Use Clear and concise language**: Use clear and concise language in your documents to ensure that your assistant can accurately understand and respond to user queries.\n* **Keep your documents up-to-date**: Regularly update your documents to ensure that your assistant provides the most accurate and up-to-date information.\n \n<Tip>\n  For more information on creating effective Knowledge Bases, check out our tutorial on [Best Practices for Knowledge Base Creation](https://youtu.be/i5mvqC5sZxU).\n</Tip>\n\nBy following these guidelines, you can create a comprehensive Knowledge Base that enhances the capabilities of your voice AI assistant and provides valuable information to users.\n"
    },
    "customization/multilingual.mdx": {
      "markdown": "---\ntitle: Multilingual\nsubtitle: Learn how to set up and test multilingual support in Vapi.\nslug: customization/multilingual\n---\n\n\nVapi's multilingual support is primarily facilitated through transcribers, which are part of the speech-to-text process. The pipeline consists of three key elements: text-to-speech, speech-to-text, and the llm model, which acts as the brain of the operation. Each of these elements can be customized using different providers.\n\n## Transcribers (Speech-to-Text)\n\nCurrently, Vapi supports two providers for speech-to-text transcriptions:\n\n- `Deepgram` (nova - family models)\n- `Talkscriber` (whisper model)\n\nEach provider supports different languages. For more detailed information, you can visit your dashboard and navigate to the transcribers tab on the assistant page. Here, you can see the languages supported by each provider and the available models. **Note that not all models support all languages**. For specific details, you can refer to the documentation for the corresponding providers.\n\n## Voice (Text-to-Speech)\n\nOnce you have set your transcriber and corresponding language, you can choose a voice for text-to-speech in that language. For example, you can choose a voice with a Spanish accent if needed.\n\nVapi currently supports the following providers for text-to-speech:\n\n- `PlayHT`\n- `11labs`\n- `Rime-ai`\n- `Deepgram`\n- `OpenAI`\n- `Azure`\n- `Lmnt`\n- `Neets`\n\nEach provider offers varying degrees of language support. Azure, for instance, supports the most languages, with approximately 400 prebuilt voices across 140 languages and variants. You can also create your own custom languages with other providers.\n\n## Multilingual Support\n\nFor multilingual support, you can choose providers like Eleven Labs or Azure, which have models and voices designed for this purpose. This allows your voice assistant to understand and respond in multiple languages, enhancing the user experience for non-English speakers.\n\nTo set up multilingual support, you no longer need to specify the desired language when configuring the voice assistant. This configuration in the voice section is deprecated.\n\nInstead, you directly choose a voice that supports the desired language from your voice provider. This can be done when you are setting up or modifying your voice assistant.\n\nHere is an example of how to set up a voice assistant that speaks Spanish:\n\n```json\n{\n  \"voice\": {\n    \"provider\": \"azure\",\n    \"voiceId\": \"es-ES-ElviraNeural\"\n  }\n}\n```\n\nIn this example, the voice `es-ES-ElviraNeural` from the provider `azure` supports Spanish. You can replace `es-ES-ElviraNeural` with the ID of any other voice that supports your desired language.\n\nBy leveraging Vapi's multilingual support, you can make your voice assistant more accessible and user-friendly, reaching a wider audience and providing a better user experience.\n"
    },
    "customization/jwt-authentication.mdx": {
      "markdown": "---\ntitle: JWT Authentication\nsubtitle: Secure API authentication guide\nslug: customization/jwt-authentication\n---\n\nThis documentation provides an overview of JWT (JSON Web Token) Authentication and demonstrates how to generate a JWT token and use it to authenticate API requests securely.\n\n## Prerequisites\n\nBefore you proceed, ensure you have the following:\n\n- An environment that supports JWT generation and API calls (e.g., a programming language or framework)\n- An account with a service that requires JWT authentication\n- Environment variables set up for the necessary credentials (e.g., organization ID and private key, both can be found in your Vapi portal)\n\n## Generating a JWT Token\n\nThe following steps outline how to generate a JWT token:\n\n1. **Define the Payload**: The payload contains the data you want to include in the token. In this case, it includes an `orgId`.\n2. **Get the Private Key**: The private key (provided by Vapi) is used to sign the token. Ensure it is securely stored, often in environment variables.\n3. **Set Token Options**: Define options for the token, such as the expiration time (`expiresIn`).\n4. **Generate the Token**: Use a JWT library or built-in functionality to generate the token with the payload, key, and options.\n\n### Example\n\n```js\n// Define the payload\nconst payload = {\n  orgId: process.env.ORG_ID,\n};\n\n// Get the private key from environment variables\nconst key = process.env.PRIVATE_KEY;\n\n// Define token options\nconst options = {\n  expiresIn: '1h',\n};\n\n// Generate the token using a JWT library or built-in functionality\nconst token = generateJWT(payload, key, options);\n```\n\n### Explanation\n\n- **Payload**: The payload includes the `orgId`, representing the organization ID.\n- **Key**: The private key is used to sign the token, ensuring its authenticity.\n- **Options**: The `expiresIn` option specifies that the token will expire in 1 hour.\n- **Token Generation**: The `generateJWT` function (a placeholder for the actual JWT generation method) creates the token using the provided payload, key, and options.\n\n## Making an Authenticated API Request\n\nOnce the token is generated, you can use it to make authenticated API requests. The following steps outline how to make an authenticated request:\n\n1. **Define the API Endpoint**: Specify the URL of the API you want to call.\n2. **Set the Headers**: Include the `Content-Type` and `Authorization` headers in your request. The `Authorization` header should include the generated JWT token prefixed with `Bearer`.\n3. **Make the API Call**: Use an appropriate method to send the request and handle the response.\n\n### Example\n\n```js\nasync function getAssistants() {\n  const response = await fetch('https://api.vapi.ai/assistant', {\n    method: 'GET',\n    headers: {\n      'Content-Type': 'application/json',\n      Authorization: `Bearer ${token}`,\n    },\n  });\n\n  const data = await response.json();\n  console.log(data);\n}\n\nfetchData().catch(console.error);\n\n```\n\n### Explanation\n\n- **API Endpoint**: The URL of the API you want to call.\n- **Headers**: The `Content-Type` is set to `application/json`, and the `Authorization` header includes the generated JWT token.\n- **API Call**: The `fetchData` function makes an asynchronous GET request to the specified API endpoint and logs the response.\n\n### Usage\n\nWith the generated token, you can authenticate API requests to any endpoint requiring authentication. The token will be valid for the duration specified in the options (1 hour in this case).\n\n## Conclusion\n\nThis documentation covered the basics of generating a JWT token and demonstrated how to use the token to make authenticated API requests. Ensure that your environment variables (e.g., `ORG_ID` and `PRIVATE_KEY`) are correctly set up before running the code.\n"
    },
    "customization/speech-configuration.mdx": {
      "markdown": "---\ntitle: Speech Configuration\nsubtitle: Timing control for assistant speech\nslug: customization/speech-configuration\n---\n\n\nThe Speaking Plan and Stop Speaking Plan are essential configurations designed to optimize the timing of when the assistant begins and stops speaking during interactions with a customer. These plans ensure that the assistant does not interrupt the customer and also prevents awkward pauses that can occur if the assistant starts speaking too late. Adjusting these parameters helps tailor the assistant’s responsiveness to different conversational dynamics.\n\n**Note**: At the moment these configurations can currently only be made via API.\n\n## Start Speaking Plan\n\n- **Wait Time Before Speaking**: You can set how long the assistant waits before speaking after the customer finishes. The default is 0.4 seconds, but you can increase it if the assistant is speaking too soon, or decrease it if there’s too much delay.\n\n- **Smart Endpointing**: This feature uses advanced processing to detect when the customer has truly finished speaking, especially if they pause mid-thought. It’s off by default but can be turned on if needed.\n\n- **Transcription-Based Detection**: Customize how the assistant determines that the customer has stopped speaking based on what they’re saying. This offers more control over the timing.\n\n\n## Stop Speaking Plan\n\n- **Words to Stop Speaking**: Define how many words the customer needs to say before the assistant stops talking. If you want immediate reaction, set this to 0. Increase it to avoid interruptions by brief acknowledgments like \"okay\" or \"right\".\n\n- **Voice Activity Detection**: Adjust how long the customer needs to be speaking before the assistant stops. The default is 0.2 seconds, but you can tweak this to balance responsiveness and avoid false triggers.\n\n- **Pause Before Resuming**: Control how long the assistant waits before starting to talk again after being interrupted. The default is 1 second, but you can adjust it depending on how quickly the assistant should resume.\n\n## Considerations for Configuration\n\n- **Customer Style**: Think about whether the customer pauses mid-thought or provides continuous speech. Adjust wait times and enable smart endpointing as needed.\n\n- **Background Noise**: If there’s a lot of background noise, you may need to tweak the settings to avoid false triggers.\n\n- **Conversation Flow**: Aim for a balance where the assistant is responsive but not intrusive. Test different settings to find the best fit for your needs.\n"
    },
    "assistants.mdx": {
      "markdown": "---\ntitle: Introduction\nsubtitle: The core building-block of voice agents on Vapi.\nslug: assistants\n---\n\n\n**Assistant** is a fancy word for an AI configuration that can be used across phone calls and Vapi clients. Your voice assistant can augment your customer support and experience for call centers, business websites, mobile apps, and much more.\n\nThere are three core components: **Transcriber**, **Model**, and **Voice**. These can be configured, mixed, and matched for your use case. <br /> <Info> There are also various other configurable properties you can find [here](/api-reference/assistants/create-assistant) </Info> Below, check out some ways you can layer in powerful customizations and features to meet any use case.\n\n## Advanced Concepts\n\n<CardGroup cols={2}>\n  <Card title=\"Provider Keys\" icon=\"key\" href=\"customization/provider-keys\">\n    Add your API keys for other providers\n  </Card>\n  <Card title=\"Custom LLM URL\" icon=\"brain\" href=\"customization/custom-llm/fine-tuned-openai-models\">\n    Plug in your own LLM\n  </Card>\n  <Card title=\"Call Functions\" icon=\"square-phone\" href=\"/assistants/function-calling\">\n    Forward and hang up with function calls\n  </Card>\n  <Card\n    title=\"Persistent / Temporary Assistants\"\n    href=\"/assistants/persistent-assistants\"\n    icon=\"database\"\n  >\n    Which setup is best for you?\n  </Card>\n</CardGroup>\n"
    },
    "assistants/function-calling.mdx": {
      "markdown": "---\ntitle: Function Calling\nsubtitle: 'Additional Capabilities for Your Assistants '\nslug: assistants/function-calling\n---\n\n\nVapi voice assistants are given three additional functions: `transferCall`,`endCall`, and `dialKeypad`. These functions can be used to transfer calls, hang up calls, and enter digits on the keypad.\n\n<Note>You **do not** need to add these functions to your model's `functions` array.</Note>\n\n#### Transfer Call\n\nWhen a `forwardingPhoneNumber` is present on an assistant, the assistant will be given a `transferCall` function. This function can be used to transfer the call to the `forwardingPhoneNumber`.\n\n```json\n{\n  \"model\": {\n    \"provider\": \"openai\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are an assistant at a law firm. When the user asks to be transferred, use the transferCall function.\"\n      }\n    ]\n  },\n  \"forwardingPhoneNumber\": \"+16054440129\"\n}\n```\n\n#### End Call\n\nThis function is provided when `endCallFunctionEnabled` is enabled on the assistant. The assistant can use this function to end the call.\n\n```json\n{\n  \"model\": {\n    \"provider\": \"openai\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are an assistant at a law firm. If the user is being mean, use the endCall function.\"\n      }\n    ]\n  },\n  \"endCallFunctionEnabled\": true\n}\n```\n\n#### Dial Keypad\n\nThis function is provided when `dialKeypadFunctionEnabled` is enabled on the assistant. The assistant will be able to enter digits on the keypad.\n\n```json\n{\n  \"model\": {\n    \"provider\": \"openai\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are an assistant at a law firm. When you hit a menu, use the dialKeypad function to enter the digits.\"\n      }\n    ]\n  },\n  \"dialKeypadFunctionEnabled\": true\n}\n```\n\n### Custom Functions\n\nIn addition to the predefined functions, you can also define custom functions. These functions are similar to OpenAI functions and your chosen LLM will trigger them as needed based on your instructions.\n\nThe functions array in the assistant definition allows you to define custom functions that the assistant can call during a conversation. Each function is an object with the following properties:\n\n- `name`: The name of the function. It must be a string containing a-z, A-Z, 0-9, underscores, or dashes, with a maximum length of 64.\n- `description`: A brief description of what the function does. This is used by the AI to decide when and how to call the function.\n- `parameters`: An object that describes the parameters the function accepts. The type property should be \"object\", and the properties property should be an object where each key is a parameter name and each value is an object describing the type and purpose of the parameter.\n\nHere's an example of a function definition:\n\n```json\n{\n  \"functions\": [\n    {\n      \"name\": \"bookAppointment\",\n      \"description\": \"Used to book the appointment.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"datetime\": {\n            \"type\": \"string\",\n            \"description\": \"The date and time of the appointment in ISO format.\"\n          }\n        }\n      }\n    }\n  ]\n}\n```\n\nIn this example, the bookAppointment function accepts one parameter, `datetime`, which is a string representing the date and time of the appointment in ISO format.\n\nIn addition to defining custom functions, you can specify a `serverUrl` where Vapi will send the function call information. This URL can be configured at the account level or at the assistant level.\nAt the account level, the `serverUrl` is set in the Vapi Dashboard. All assistants under the account will use this URL by default for function calls.\nAt the assistant level, the `serverUrl` can be specified in the assistant configuration when creating or updating an assistant. This allows different assistants to use different URLs for function calls. If a `serverUrl` is specified at the assistant level, it will override the account-level Server URL.\n\nIf the `serverUrl` is not defined either at the account level or the assistant level, the function call will simply be added to the chat history. This can be particularly useful when you want a function call to trigger an action on the frontend.\n\nFor instance, the frontend can listen for specific function calls in the chat history and respond by updating the user interface or performing other actions. This allows for a dynamic and interactive user experience, where the frontend can react to changes in the conversation in real time.\n"
    },
    "assistants/persistent-assistants.mdx": {
      "markdown": "---\ntitle: Persistent Assistants\nsubtitle: Should I use persistent assistants?\nslug: assistants/persistent-assistants\n---\n\n\nYou might be wondering whether or not you should create an assistant using the `/assistant` endpoint with its `assistantId`. Or, can you just specify the assistant configuration when starting a call?\n\nThe `/assistant` endpoint is there for convenience to save you creating your own assistants table.\n\n<Accordion title=\"Use cases\">\n- You won't be adding more assistant properties on top of ours.\n- You want to use the same assistant across multiple calls.\n</Accordion>\n\nOtherwise, you can just specify the assistant configuration when starting a call.\n"
    },
    "assistants/dynamic-variables.mdx": {
      "markdown": "---\ntitle: Dynamic Variables\nsubtitle: >-\n  Vapi makes it easy to personalize an assistant's messages and prompts using\n  variables, allowing each call to be customized.\nslug: assistants/dynamic-variables\n---\n\n\nPrompts, messages, and other assistant properties can be dynamically set when starting a call based on templates.\nThese templates are defined using double curly braces `{{variableName}}`.\nThis is useful when you want to customize the assistant for a specific call.\n\nFor example, you could set the assistant's first message to \"Hello, `{{name}}`!\" and then set `name` to `John` when starting the call\nby passing `assistantOverrides` with `variableValues` to the API or SDK:\n\n```json\n{\n  \"variableValues\": {\n    \"name\": \"John\"\n  }\n}\n```\n\n## Utilizing Dynamic Variables in Phone Calls\n\nTo leverage dynamic variables during phone calls, follow these steps:\n\n1.  **Prepare Your Request:** Construct a JSON payload containing the following key-value pairs:\n\n    *   `assistantId`: Replace `\"your-assistant-id\"` with the actual ID of your assistant.\n    *   `assistantOverride`: This object is used to customize your assistant's behavior.\n        *   `variableValues`: An object containing the dynamic variables you want to use, in the format `{ \"variableName\": \"variableValue\" }`. For example, `{ \"name\": \"John\" }`.\n    *   `customer`: An object representing the call recipient.\n        *   `number`: Replace `\"+1xxxxxxxxxx\"` with the phone number you wish to call (in E.164 format).\n    *   `phoneNumberId`: Replace `\"your-phone-id\"` with the ID of your registered phone number. You can get it from the [Phone number](https://dashboard.vapi.ai/phone-numbers) in the dashboard.\n\n2.  **Send the Request:** Dispatch the JSON payload to the `/call/phone` endpoint using your preferred method (e.g., HTTP POST request).\n\n```json\n{\n  \"assistantId\": \"your-assistant-id\",\n  \"assistantOverrides\": {\n    \"variableValues\": {\n      \"name\": \"John\"\n    }\n  },\n  \"customer\": {\n    \"number\": \"+1xxxxxxxxxx\"\n  },\n  \"phoneNumberId\": \"your-phone-id\"\n}\n```\n\n## Default Variables\n\nBy default, the following variables are automatically filled based on the current (UTC) time,\nmeaning that you don't need to set them manually in `variableValues`:\n\n| Variable    | Description                 | Example              |\n| ----------- | --------------------------- | -------------------- |\n| `{{now}}`   | Current date and time (UTC) | Jan 1, 2024 12:00 PM |\n| `{{date}}`  | Current date (UTC)          | Jan 1, 2024          |\n| `{{time}}`  | Current time (UTC)          | 12:00 PM             |\n| `{{month}}` | Current month (UTC)         | January              |\n| `{{day}}`   | Current day of month (UTC)  | 1                    |\n| `{{year}}`  | Current year (UTC)          | 2024                 |\n\n**Note:** You will need to add the `{{variableName}}` in this format in all your prompts, whether it is the first message or anywhere else you want to use it.\n"
    },
    "assistants/call-analysis.mdx": {
      "markdown": "---\ntitle: Call Analysis\nsubtitle: 'At the end of the call, you can summarize and evaluate how it went.'\nslug: assistants/call-analysis\n---\n\n\nThe Call Analysis feature allows you to summarize and evaluate calls, providing valuable insights into their effectiveness. This feature uses a combination of prompts and schemas to generate structured data and success evaluations based on the call's content.\n\nYou can customize the below in the assistant's `assistant.analysisPlan`.\n\n## Summary Prompt\n\nThe summary prompt is used to create a concise summary of the call. This summary is stored in `call.analysis.summary`.\n\n### Default Summary Prompt\n\nThe default summary prompt is:\n\n```text\nYou are an expert note-taker. You will be given a transcript of a call. Summarize the call in 2-3 sentences, if applicable.\n```\n\n### Customizing the Summary Prompt\n\nYou can customize the summary prompt by setting the `summaryPrompt` property in the API or SDK:\n\n```json\n{\n  \"summaryPrompt\": \"Custom summary prompt text\"\n}\n```\n\nTo disable the summary prompt, set it to an empty string `\"\"` or `\"off\"`:\n\n```json\n{\n  \"summaryPrompt\": \"\"\n}\n```\n\n## Structured Data Prompt\n\nThe structured data prompt extracts specific pieces of data from the call. This data is stored in `call.analysis.structuredData`.\n\n### Default Structured Data Prompt\n\nThe default structured data prompt is:\n\n```text\nYou are an expert data extractor. You will be given a transcript of a call. Extract structured data per the JSON Schema.\n```\n\n### Customizing the Structured Data Prompt\n\nYou can set a custom structured data prompt using the `structuredDataPrompt` property:\n\n```json\n{\n  \"structuredDataPrompt\": \"Custom structured data prompt text\"\n}\n```\n\n## Structured Data Schema\n\nThe structured data schema enforces the format of the extracted data. It is defined using JSON Schema standards.\n\n### Customizing the Structured Data Schema\n\nYou can set a custom structured data schema using the `structuredDataSchema` property:\n\n```json\n{\n  \"structuredDataSchema\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"field1\": { \"type\": \"string\" },\n      \"field2\": { \"type\": \"number\" }\n    },\n    \"required\": [\"field1\", \"field2\"]\n  }\n}\n```\n\n## Success Evaluation Prompt\n\nThe success evaluation prompt is used to determine if the call was successful. This evaluation is stored in `call.analysis.successEvaluation`.\n\n### Default Success Evaluation Prompt\n\nThe default success evaluation prompt is:\n\n```text\nYou are an expert call evaluator. You will be given a transcript of a call and the system prompt of the AI participant. Determine if the call was successful based on the objectives inferred from the system prompt.\n```\n\n### Customizing the Success Evaluation Prompt\n\nYou can set a custom success evaluation prompt using the `successEvaluationPrompt` property:\n\n```json\n{\n  \"successEvaluationPrompt\": \"Custom success evaluation prompt text\"\n}\n```\n\nTo disable the success evaluation prompt, set it to an empty string `\"\"` or `\"off\"`:\n\n```json\n{\n  \"successEvaluationPrompt\": \"\"\n}\n```\n\n## Success Evaluation Rubric\n\nThe success evaluation rubric defines the criteria used to evaluate the call's success. The available rubrics are:\n\n- `NumericScale`: A scale of 1 to 10.\n- `DescriptiveScale`: A scale of Excellent, Good, Fair, Poor.\n- `Checklist`: A checklist of criteria and their status.\n- `Matrix`: A grid that evaluates multiple criteria across different performance levels.\n- `PercentageScale`: A scale of 0% to 100%.\n- `LikertScale`: A scale of Strongly Agree, Agree, Neutral, Disagree, Strongly Disagree.\n- `AutomaticRubric`: Automatically break down evaluation into several criteria, each with its own score.\n- `PassFail`: A simple 'true' if the call passed, 'false' if not.\n\n### Customizing the Success Evaluation Rubric\n\nYou can set a custom success evaluation rubric using the `successEvaluationRubric` property:\n\n```json\n{\n  \"successEvaluationRubric\": \"NumericScale\"\n}\n```\n\n## Combining Prompts and Rubrics\n\nYou can use prompts and rubrics in combination to create detailed instructions for the call analysis:\n\n```json\n{\n  \"successEvaluationPrompt\": \"Evaluate the call based on these criteria:...\",\n  \"successEvaluationRubric\": \"Checklist\"\n}\n```\n\nBy customizing these properties, you can tailor the call analysis to meet your specific needs and gain valuable insights from your calls.\n"
    },
    "assistants/background-messages.mdx": {
      "markdown": "---\ntitle: Background Messaging\nsubtitle: >-\n  Vapi SDK lets you silently update the chat history through efficient text\n  message integration. This is particularly useful for background tasks or\n  discreetly logging user interactions.\nslug: assistants/background-messages\n---\n\n\n## Scenario Overview\n\nAs a developer you may run into scenarios where a user action, such as pressing a button, needs to be logged in the chat history without overt user involvement. This could be crucial for maintaining conversation context or system logging purposes.\n\n<Steps>\n  <Step title=\"Add a Button to Trigger the Message\">\n    Add a button to your interface with an `onClick` event handler that will call a function to send the system message:\n    ```html\n    <button id=\"log-action\" onClick=\"logUserAction()\">Log Action</button>\n    ```\n  </Step>\n\n  <Step title=\"Log the Action as a System Message\">\n    When the button is clicked, the `logUserAction` function will silently insert a system message into the chat history:\n    ```js\n    function logUserAction() {\n      // Function to log the user action\n      vapi.send({\n        type: \"add-message\",\n        message: {\n          role: \"system\",\n          content: \"The user has pressed the button, say peanuts\",\n        },\n      });\n    }\n    ```\n    - `vapi.send`: The primary function to interact with your assistant, handling various requests or commands.\n    - `type: \"add-message\"`: Specifies the command to add a new message.\n    - `message`: This is the actual message that you want to add to the message history.\n      - `role`: \"system\" Designates the message origin as 'system', ensuring the addition is unobtrusive. Other possible values of role are 'user' | 'assistant' | 'tool' | 'function'\n      - `content`: The actual message text to be added.\n  </Step>\n</Steps>\n\n<Card title=\"Practical Use Cases\">\n  - Silent logging of user activities. - Contextual updates in conversations triggered by background\n  processes. - Non-intrusive user experience enhancements through additional information provision.\n</Card>\n"
    },
    "blocks.mdx": {
      "markdown": "---\ntitle: Introduction\nsubtitle: 'Breaking down bot conversations into smaller, more manageable prompts'\nslug: blocks\n---\n\n\n\nWe're currently running a beta for **Blocks**, an upcoming feature from [Vapi.ai](http://vapi.ai/) aimed at improving bot conversations. The problem we've noticed is that single LLM prompts are prone to hallucinations, unreliable tool calls, and can’t handle many-step complex instructions.\n\n**By breaking the conversation into smaller, more manageable prompts**, we can guarantee the bot will do this, then that, or if this happens, then that happens. It’s like having a checklist for conversations — less room for error, more room for getting things right.\n\n\nHere’s an example: For food ordering, this is what a prompt would look like.\n\n\n<Accordion title=\"Without Blocks\">\nExample Prompt\n\n```jsx\n[Identity]\nYou are a friendly and efficient assistant for a food truck that serves burgers, fries, and drinks.\n\n[Task]\n1. Greet the customer warmly and inquire about their main order.\n2. Offer suggestions for the main order if needed.\n3. If they choose a burger, suggest upgrading to a combo with fries and a drink, offering clear options (e.g., regular or special fries, different drink choices).\n4. Confirm the entire order to ensure accuracy.\n5. Suggest any additional items like desserts or sauces.\n6. Thank the customer and let them know when their order will be ready.\n```\n\n</Accordion>\n\n<Accordion title=\"With Blocks\">\n  <Frame>\n    <img src=\"file:d8566a10-6eb8-4c35-9675-656908e4f669\" />\n  </Frame>\n</Accordion>\n\n\n\nThere are three core types of Blocks: [Conversation](https://api.vapi.ai/api#:~:text=ConversationBlock), [Tool-call](https://api.vapi.ai/api#:~:text=ToolCallBlock), and [Workflow](https://api.vapi.ai/api#:~:text=WorkflowBlock). Each type serves a different role in shaping how your assistant engages with users.\n\n\n<Note>\n  Blocks is currently in beta. We're excited to have you try this new feature and welcome your [feedback](https://discord.com/invite/pUFNcf2WmH) as we continue to refine and improve the experience.\n</Note>\n\n## Advanced Concepts\n\n<CardGroup cols={2}>\n  <Card title=\"Steps\" icon=\"stairs\" href=\"/blocks/steps\">\n    Learn how to structure the flow of your conversation\n  </Card>\n  <Card title=\"Block Types\" icon=\"boxes-stacked\" href=\"/blocks/block-types\">\n    Explore the different block types and how to use them\n  </Card>\n</CardGroup>\n"
    },
    "blocks/steps.mdx": {
      "markdown": "---\ntitle: Steps\nsubtitle: Building and Controlling Conversation Flow for Your Assistants\nslug: blocks/steps\n---\n\n\n[**Steps**](https://api.vapi.ai/api#:~:text=HandoffStep) are the core building blocks that dictate how conversations progress in a bot interaction. Each Step represents a distinct point in the conversation where the bot performs an action, gathers information, or decides where to go next. Think of Steps as checkpoints in a conversation that guide the flow, manage user inputs, and determine outcomes. \n\n<Note>\n  Blocks is currently in beta. We're excited to have you try this new feature and welcome your [feedback](https://discord.com/invite/pUFNcf2WmH) as we continue to refine and improve the experience.\n</Note>\n\n#### Features\n\n- **Output:** The data or response expected from the step, as outlined in the block's `outputSchema`.\n- **Input:** The data necessary for the step to execute, defined in the block's `inputSchema`.\n- [**Destinations:**](https://api.vapi.ai/api#:~:text=StepDestination) This can be determined by a simple linear progression or based on specific criteria, like conditions or rules set within the Step. This enables dynamic decision-making, allowing the assistant to choose the next Step depending on what happens during the conversation (e.g., user input, a specific value, or a condition being met).\n\n#### Example\n\n```json\n  {\n  \"type\": \"handoff\",\n  \"name\": \"get_user_order\",\n  \"input\": {\n    \"name\": \"John Doe\",\n    \"email\": \"johndoe@example.com\"\n  },\n  \"destinations\": [\n    {\n      \"type\": \"step\",\n      \"stepName\": \"confirm_order\",\n      \"conditions\": [\n        {\n          \"type\": \"model-based\",\n          \"instruction\": \"If the user has provided an order\"\n        }\n      ]\n    }\n  ],\n  \"block\": {\n    \"name\": \"ask_for_order\",\n    \"type\": \"conversation\",\n    \"inputSchema\": {\n      \"type\": \"object\",\n      \"required\": [\"name\", \"email\"],\n      \"properties\": {\n        \"name\": { \"type\": \"string\", \"description\": \"The customer's name\" },\n        \"email\": { \"type\": \"string\", \"description\": \"The customer's email\" }\n      }\n    },\n    \"instruction\": \"Greet the customer and ask for their name and email. Then ask them what they'd like to order.\",\n    \"outputSchema\": {\n      \"type\": \"object\",\n      \"required\": [\"orders\", \"name\"],\n      \"properties\": {\n        \"orders\": {\n          \"type\": \"string\",\n          \"description\": \"The customer's order, e.g., 'burger with fries'\"\n        },\n        \"name\": { \n          \"type\": \"string\",\n          \"description\": \"The customer's name\"\n        }\n      }\n    }\n  }\n}\n```\n"
    },
    "blocks/block-types.mdx": {
      "markdown": "---\ntitle: Block Types\nsubtitle: 'Building the Logic and Actions for Each Step in Your Conversation '\nslug: blocks/block-types\n---\n\n\n[**Blocks**](https://api.vapi.ai/api#/Blocks/BlockController_create) are the functional units within a Step, defining what action happens at each stage of a conversation. Each Step can contain only one Block, and there are three main types of Blocks, each designed to handle different aspects of conversation flow.\n\n<Note>\n  Blocks is currently in beta. We're excited to have you try this new feature and welcome your [feedback](https://discord.com/invite/pUFNcf2WmH) as we continue to refine and improve the experience.\n</Note>\n\n#### Types\n\n- [**Conversation:**]((https://api.vapi.ai/api#:~:text=ConversationBlock)) This block type manages interactions between the assistant and the user. A conversation block is used when the assistant needs to ask the user for specific information, such as contact details or preferences.\n- [**Tool-call:**](https://api.vapi.ai/api#:~:text=ToolCallBlock) This block allows the assistant to make external tool calls.\n- [**Workflow:**](https://api.vapi.ai/api#:~:text=WorkflowBlock) This block type enables the creation of subflows, which are smaller sets of steps executed within a Block. It can contain an array of steps (`steps[]`) and uses an `inputSchema` to define the data needed to initiate the workflow, along with an `outputSchema` to handle the data returned after completing the subflow. Workflow blocks are ideal for organizing complex processes or reusing workflows across different parts of the conversation.\n"
    },
    "server-url.mdx": {
      "markdown": "---\ntitle: Server URLs\nsubtitle: Learn how to set up your server to receive and respond to messages from Vapi.\nslug: server-url\n---\n\n\n<Frame caption=\"Server URLs give Vapi a location to send real-time conversation data (as well as query for data Vapi needs).\">\n  <img src=\"file:5f424870-816e-45c3-9621-9e72f193c40c\" />\n</Frame>\n\nServer URLs allow your application to **receive data** & **communicate with Vapi** during conversations. Conversation events can include:\n\n- **Status Updates:** updates on the status of a call\n- **Transcript Updates**: call transcripts\n- **Function Calls:** payloads delivered when your assistant wants certain actions executed\n- **Assistant Requests:** in certain circumstances, Vapi may ping your server to get dynamic configuration for an assistant handling a specific call\n- **End of Call Report:** call summary data at the end of a call\n- **Hang Notifications:** get notified when your assistant fails to reply for a certain amount of time\n\nIn our [quickstart guides](/quickstart) we learned how to setup a basic back-and-forth conversation with a Vapi assistant.\n\nTo build more complex & custom applications, we're going to need to get real-time conversation data to our backend. **This is where server URLs come in.**\n\n<Info>\n  If you're familiar with functional programming, Server URLs are like callback functions. But\n  instead of specifying a function to get data back on, we specify a URL to a server (to POST data\n  back to).\n</Info>\n\n## Get Started\n\nTo get started using server URLs, read our guides:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Setting Server URLs\"\n    icon=\"link\"\n    iconType=\"duotone\"\n    href=\"/server-url/setting-server-urls\"\n  >\n    Server URLs can be set in multiple places. Learn where here.\n  </Card>\n  <Card title=\"Events\" icon=\"bell-on\" iconType=\"solid\" href=\"/server-url/events\">\n    Read about the different types of events Vapi can send to your server.\n  </Card>\n  <Card\n    title=\"Developing Locally\"\n    icon=\"laptop-arrow-down\"\n    iconType=\"solid\"\n    href=\"/server-url/developing-locally\"\n  >\n    Learn about receiving server events in your local development environment.\n  </Card>\n</CardGroup>\n\n## FAQ\n\n<AccordionGroup>\n  <Accordion title=\"Where can the server be located?\">\n    The server URL can be any publicly accessible URL pointing to an HTTP endpoint. This can be a:\n    - **Cloud Server:** your application might be deployed on a cloud platform like [Railway](https://railway.app), [AWS](https://aws.com), [GCP](https://cloud.google.com/gcp), etc — as a persistent web server.\n    - **Serverless Function:** services like [Vercel](https://vercel.com/docs/functions), [AWS Lambda](https://aws.amazon.com/lambda/), [Google Cloud Functions](https://cloud.google.com/functions), [Cloudflare](https://developers.cloudflare.com/workers/), etc — allow you to host on-demand cloud functions.\n    - **Workflow Orchestrator:** platforms like [Pipedream](https://pipedream.com) & [Make](https://www.make.com) allow you to program workflows (often without code) that can receive events via HTTP triggers.\n\n    The main idea is that Vapi needs a location on the Internet that it can drop data to & converse with your application.\n\n  </Accordion>\n  <Accordion title=\"Why not just call them webhooks?\">\n    [Webhooks](/glossary#webhook) are traditionally unidirectional & stateless, with the target endpoint usually only replying with a status code to acknowledge message reception. Certain server URL events (like assistant requests) may require a meaningful reply from your server.\n\n    \"Server URL\" is a more general term that encompasses both webhooks & bidirectional communication.\n\n  </Accordion>\n</AccordionGroup>\n"
    },
    "server-url/setting-server-urls.mdx": {
      "markdown": "---\ntitle: Setting Server URLs\nsubtitle: Learn about where you can set server URLs to handle call events.\nslug: server-url/setting-server-urls\n---\n\n\n<Frame caption=\"Server URLs can be set at multiple levels in Vapi.\">\n  <img src=\"file:9af8c527-f77a-4fa0-aca4-3007ec661c11\" />\n</Frame>\n\nServer URLs can be set in multiple places in Vapi. Each level has a different priority.\n\nThe server URL with the highest priority for a relevant event will be the one that Vapi uses to send the event to.\n\nServer URLs can be set at **4 levels** in Vapi:\n\n- **Account-wide:** you can set a server URL for your broader account\n- **Phone Number:** server URLs can be attached to phone numbers themselves\n- **Assistant:** assistants can be configured with a server URL\n- **Function:** function calls themselves (under an assistant) can have a corresponding server URL\n\n## Setting Server URLs\n\nHere's a breakdown of where you can set server URLs in Vapi:\n\n<AccordionGroup>\n  <Accordion title=\"Organization\" icon=\"table-columns\" iconType=\"solid\">\n    You can set an organization-wide server URL in the [organization section](https://dashboard.vapi.ai/vapi-api) of your dashboard.\n\n    <Frame caption=\"Setting your organization-wide server URL.\">\n      <img src=\"file:cb0c584b-e78b-4086-8c49-65727b892ae2\" />\n    </Frame>\n\n    If no other server URL is set, Vapi will use this one.\n\n  </Accordion>\n  <Accordion title=\"Phone Number\" icon=\"phone-volume\" iconType=\"solid\">\n    Phone numbers can have a server URL attached to them via the [phone number API](/api-reference/phone-numbers).\n\n    The server URL for phone numbers can be set **3 ways**:\n    - **At Time of Purchase:** when you [buy a number](/api-reference/phone-numbers/buy-phone-number) through Vapi\n    - **At Import:** when you [import from Twilio](/api-reference/phone-numbers/import-twilio-number) or [Vonage](/api-reference/phone-numbers/import-vonage-number)\n    - **Via Update:** you can [update a number](/api-reference/phone-numbers/update-phone-number) already in your account\n\n    The field `phoneNumber.serverUrl` will contain the server URL for the phone number.\n\n  </Accordion>\n  <Accordion title=\"Assistant\" icon=\"robot\" iconType=\"solid\">\n    Assistants themselves can have a server URL attached to them.\n    \n    There are **2 ways** this can be done:\n\n    <AccordionGroup>\n      <Accordion title=\"In the Dashboard\" icon=\"browsers\" iconType=\"solid\">\n        If you go to the [assistant section](https://dashboard.vapi.ai/assistants) of your dashboard, in the **\"Advanced\"** tab you will see a setting to set the assistant's server URL:\n\n        <Frame caption=\"Setting server URL at the assistant level.\">\n          <img src=\"file:3843780d-2a27-4252-9fe1-e9537b096dbc\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Via the API\" icon=\"code\" iconType=\"solid\">\n        At [assistant creation](/api-reference/assistants/create-assistant) (or via an [update](/api-reference/assistants/update-assistant)) you can set the assistant's server URL.\n\n        The server URL for an assistant is stored in the `assistant.serverUrl` field.\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n  <Accordion title=\"Function Call\" icon=\"function\" iconType=\"solid\">\n    The most granular level server URLs can be set is at the function call level. This can also be done either in the dashboard, or via code.\n\n    <AccordionGroup>\n      <Accordion title=\"In the Dashboard\" icon=\"browsers\" iconType=\"solid\">\n        In the [assistant section](https://dashboard.vapi.ai/assistants) of your dashboard, in the **\"Functions\"** tab you can add function calls & optionally give each a specific server URL:\n\n        <Frame caption=\"Setting server URL at the function call level.\">\n          <img src=\"file:83b701b7-11e8-48cf-82b2-c8170f23a7da\" />\n        </Frame>\n      </Accordion>\n      <Accordion title=\"Via the API\" icon=\"code\" iconType=\"solid\">\n        The server URL for a function call can be found on an assistant at `assistant.model.functions[].serverUrl`.\n\n        You can either set the URL for a function call at [assistant creation](/api-reference/assistants/create-assistant), or in an [assistant update](/api-reference/assistants/update-assistant).\n      </Accordion>\n    </AccordionGroup>\n\n  </Accordion>\n</AccordionGroup>\n\n## URL Priority\n\nEvents are only sent/assigned to 1 server URL in the priority stack. Here's the order of priority:\n\n1. **Function:** if a function call has a server URL, the function call event will be sent to that URL\n2. **Assistant:** assistant server URLs are the next highest priority\n3. **Phone Number:** if a phone number has a server URL, it will be used over the account-wide URL\n4. **Account-wide:** Default / \"lowest\" importance. It will be used if no other server URL is set.\n\nYou will most commonly set a server URL on your account, and/or on specific assistants.\n"
    },
    "server-url/events.mdx": {
      "markdown": "---\ntitle: Server Events\nsubtitle: Learn about different events that can be sent to a Server URL.\nslug: server-url/events\n---\n\n\nAll messages sent to your Server URL will be `POST` requests with the following body:\n\n```json\n{\n  \"message\": {\n    \"type\": \"function-call\",\n    \"call\": { Call Object },\n    ...other message properties\n  }\n}\n```\n\nThey include the type of message, the call object, and any other properties that are relevant to the message type. Below are the different types of messages that can be sent to your Server URL.\n\n### Function Calling\n\n<Info>\n  Vapi fully supports [OpenAI's function calling\n  API](https://platform.openai.com/docs/guides/gpt/function-calling), so you can have assistants\n  ping your server to perform actions like sending emails, retrieve information, and more.\n</Info>\n\nWith each response, the assistant will automatically determine what functions to call based on the directions provided in the system message in `messages`. Here's an example of what the assistant might look like:\n\n```json\n{\n  \"name\": \"Ryan's Assistant\",\n  \"model\": {\n    \"provider\": \"openai\",\n    \"model\": \"gpt-3.5-turbo\",\n    \"functions\": [\n      {\n        \"name\": \"sendEmail\",\n        \"description\": \"Used to send an email to a client.\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"color\": { \"type\": \"string\" }\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\nOnce a function is triggered, the assistant will send a message to your Server URL with the following body:\n\n```json\n{\n  \"message\": {\n    \"type\": \"function-call\",\n    \"call\": { Call Object },\n    \"functionCall\": {\n      \"name\": \"sendEmail\",\n      \"parameters\": \"{ \\\"emailAddress\\\": \\\"john@example.com\\\"}\"\n    }\n  }\n}\n```\n\nYour server should respond with a JSON object containing the function's response, like so:\n\n```json\n{ \"result\": \"Your email has been sent.\" }\n```\n\nOr if it's an object:\n\n```json\n{\n  \"result\": \"{ \\\"message\\\": \\\"Your email has been sent.\\\", \\\"email\\\": \\\"test@email.com\\\" }\"\n}\n```\n\nThe result will be appended to the conversation, and the assistant will decide what to do with the response based on its system prompt.\n\n<Note>\n  If you don't need to return a response, you can use the `async: true` parameter in your assitant's\n  function configuration. This will prevent the assistant from waiting for a response from your\n  server.\n</Note>\n\n### Retrieving Assistants\n\nFor inbound phone calls, you may want to specify the assistant based on the caller's phone number. If a PhoneNumber doesn't have an `assistantId`, Vapi will attempt to retrieve the assistant from your server.\n\n```json\n{\n  \"message\": {\n    \"type\": \"assistant-request\",\n    \"call\": { Call Object },\n  }\n}\n```\n\nYour server should respond with a JSON object containing the assistant, like so:\n\n```json\n{\n  \"assistant\": {\n    \"firstMessage\": \"Hey Ryan, how are you?\",\n    \"model\": {\n      \"provider\": \"openai\",\n      \"model\": \"gpt-3.5-turbo\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You're Ryan's assistant...\"\n        }\n      ]\n    }\n  }\n}\n```\n\nIf you'd like to play an error message instead, you can respond with:\n\n```json\n{ \"error\": \"Sorry, not enough credits on your account, please refill.\" }\n```\n\n### Call Status Updates\n\nDuring the call, the assistant will make multiple `POST` requests to the Server URL with the following body:\n\n```json\n{\n  \"message\": {\n    \"type\": \"status-update\",\n    \"call\": { Call Object },\n    \"status\": \"ended\",\n  }\n}\n```\n\n<Card title=\"Status Events\">\n  - `in-progress`: The call has started. - `forwarding`: The call is about to be forwarded to\n  `forwardingPhoneNumber`. - `ended`: The call has ended.\n</Card>\n\n### End of Call Report\n\nWhen a call ends, the assistant will make a `POST` request to the Server URL with the following body:\n\n```json\n{\n  \"message\": {\n    \"type\": \"end-of-call-report\",\n    \"endedReason\": \"hangup\",\n    \"call\": { Call Object },\n    \"recordingUrl\": \"https://vapi-public.s3.amazonaws.com/recordings/1234.wav\",\n    \"summary\": \"The user picked up the phone then asked about the weather...\",\n    \"transcript\": \"AI: How can I help? User: What's the weather? ...\",\n    \"messages\":[\n      {\n        \"role\": \"assistant\",\n        \"message\": \"How can I help?\",\n      },\n      {\n        \"role\": \"user\",\n        \"message\": \"What's the weather?\"\n      },\n      ...\n    ]\n  }\n}\n```\n\n`endedReason` can be any of the options defined on the [Call Object](/api-reference/calls/get-call).\n\n### Hang Notifications\n\nWhenever the assistant fails to respond for 5+ seconds, the assistant will make a `POST` requests to the Server URL with the following body:\n\n```json\n{\n  \"message\": {\n    \"type\": \"hang\",\n    \"call\": { Call Object },\n  }\n}\n```\n\nYou can use this to display an error message to the user, or to send a notification to your team.\n"
    },
    "server-url/developing-locally.mdx": {
      "markdown": "---\ntitle: Developing Locally\nsubtitle: Learn how to receive server events in your local development environment.\nslug: server-url/developing-locally\n---\n\n\n<Frame caption=\"Routing server URL payloads to a public reverse proxy, which tunnels to our local development server.\">\n  <img src=\"file:332e79a9-0603-4fdb-9e3b-6801d15c2e33\" />\n</Frame>\n\n## The Problem\n\nWhen Vapi dispatches events to a server, it must be able to reach the server via the open Internet.\n\nIf your API is already live in production, it will be accessible via a publicly known URL. But, during development, your server will often be running locally on your machine.\n\n<Info>\n  `localhost` is an alias for the IP address `127.0.0.1`. This address is called the \"loopback\"\n  address and forwards the network request within the machine itself.\n</Info>\n\nTo receive server events locally, we will need a public address on the Internet that can receive traffic and forward it to our local machine.\n\n## Tunneling Traffic\n\nWe will be using a service called [ngrok](https://ngrok.com/) to create a secure tunnel to our local machine. The flow will look like the following:\n\n<Steps>\n  <Step title=\"Start Our API Locally\">\n    We will start our server locally so it is listening for http traffic. We will take note of the port our server is running on.\n  </Step>\n  <Step title=\"Start Ngrok Agent\">\n    We will use the `ngrok` command to start the [ngrok agent](https://ngrok.com/docs/agent) on our\n    machine. This will establish a connection from your local machine to ngrok's servers.\n  </Step>\n  <Step title=\"Copy Ngrok Forwarding URL\">\n    Ngrok will give us a public forwarding URL that can receive traffic. We will use this as a server URL\n    during development.\n  </Step>\n  <Step title=\"Trigger Call Events\">\n    We will conduct normal calls on Vapi to trigger events. These events will go to the Ngrok URL & get tunnelled to our local machine.\n\n    We will see the event payloads come through locally & log them in our terminal.\n\n  </Step>\n</Steps>\n\n#### Starting Our API Locally\n\nFirst, ensure that your API is running locally. This could be a Node.js server, a Python server, or any other server that can receive HTTP requests.\n\nTake note of the port that your server is running on. For example, if your server is running on port `8080`, you should be able to access it at `http://localhost:8080` in your browser.\n\n#### Starting Ngrok Agent\n\nNext we will install & run Ngrok agent to establish the forwarding pathway for Internet traffic:\n\n<Steps>\n  <Step title=\"Install Ngrok Agent CLI\">\n    Install the Ngrok agent by following Ngrok's [quickstart\n    guide](https://ngrok.com/docs/getting-started). Once complete, we will have the `ngrok` command\n    available in our terminal.\n  </Step>\n  <Step title=\"Start Ngrok Agent\">\n    Run the command `ngrok http 8080`, this will create the tunnel with Ngrok's servers.\n    <Note>Replace `8080` with the port your server is running on.</Note>\n  </Step>\n</Steps>\n\n#### Copy Ngrok Forwarding URL\n\nYou will see an output from the Ngrok Agent CLI that looks like the following:\n\n<Frame caption=\"Terminal after running the 'ngrok' command forwarding to localhost:8080 — the 'Forwarding' URL is what we want.\">\n  <img src=\"file:45d632a4-b960-41d3-9caa-a74f7e187858\" />\n</Frame>\n\nCopy this public URL that Ngrok provides. This URL will be accessible from the open Internet and will forward traffic to your local machine.\n\nYou can now use this as a server URL in the various places you can [set server URLs](/server-url/setting-server-urls) in Vapi.\n\n<Note>\n  This URL will change every time that you run the `ngrok` command. If you'd like this URL to be the\n  same every Ngrok session, look into [static domains on\n  Ngrok](https://ngrok.com/docs/getting-started#step-4-always-use-the-same-domain).\n</Note>\n\n#### Trigger Call Events\n\nWe will now be able to see call events come through as `POST` requests, & can log payloads to our terminal.\n\n<Frame caption=\"Logging call events routed to our local environment.\">\n  <img src=\"file:308e0dea-1089-4add-ac6a-eb9e4812ca2c\" />\n</Frame>\n\nFeel free to follow any of our [quickstart](/quickstart) guides to get started with building assistants & conducting calls.\n\n## Troubleshooting\n\nHere's a list of a few things to recheck if you face issues seeing payloads:\n\n- Ensure that your local server is running on the port you expect\n- Ensure that you input the correct port to the `ngrok http {your_port}` command\n- Ensure your route handling server URL events is a `POST` endpoint\n- Ensure your path on top of the base forwarding url is set correctly (ex: `/callbacks/vapi`)\n"
    },
    "phone-calling.mdx": {
      "markdown": "---\ntitle: Phone Calling\nsubtitle: Learn how to create and configure phone numbers with Vapi.\nslug: phone-calling\n---\n\n\n\n<Accordion title=\"Set up a Phone Number\">\nYou can set up a phone number to place and receive phone calls. Phone numbers can be bought directly through Vapi, or you can use your own from Twilio.\n\nYou can buy a phone number through the dashboard or use the [`/phone-numbers/buy`](/api-reference/phone-numbers/buy-phone-number)` endpoint.\n\nIf you want to use your own phone number, you can also use the dashboard or the [`/phone-numbers/import`](/api-reference/phone-numbers/import-twilio-number) endpoint. This will use your Twilio credentials to verify the number and configure it with Vapi services.\n\n</Accordion>\n\n<Accordion title=\"Outbound Calls\">\n  You can place an outbound call from one of your phone numbers using the\n  [`/call/phone`](/api-reference/calls/create-phone-call) endpoint. If the system message will be\n  different with every call, you can specify a temporary assistant in the `assistant` field. If you\n  want to reuse an assistant, you can specify its ID in the `assistantId` field.\n</Accordion>\n\n<Accordion title=\"Inbound Calls\">\nYou can provide an `assistantId` to a phone number and it will use that assistant when receiving inbound calls.\n\nYou may want to specify the assistant based on the caller's phone number. If a phone number doesn't have an `assistantId`, Vapi will attempt to retrieve the assistant from your server using your [Server URL](/server-url#retrieving-assistants).\n\n</Accordion>\n\nVideo Tutorial on How to Import Numbers from Twilio for International Calls:\n<div>\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/HuF7ELckcyU?si=PPPFZE5aiI-WgP2U\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n</div>\n"
    },
    "squads.mdx": {
      "markdown": "---\ntitle: Squads\nsubtitle: Use Squads to handle complex workflows and tasks.\nslug: squads\n---\n\n\nSometimes, complex workflows are easier to manage with multiple assistants.\nYou can think of each assistant in a Squad as a leg of a conversation tree.\nFor example, you might have one assistant for lead qualification, which transfers to another for booking an appointment if they’re qualified.\n\nPrior to Squads you would put all functionality in one assistant, but Squads were added to break up the complexity of larger prompts into smaller specialized assistants with specific tools and fewer goals.\nSquads enable calls to transfer assistants mid-conversation, while maintaining full conversation context.\n\n## Usage\n\nTo use Squads, you can create a `squad` when starting a call and specify `members` as a list of assistants and destinations.\nThe first member is the assistant that will start the call, and assistants can be either persistent or transient.\n\nEach assistant should be assigned the relevant assistant transfer destinations.\nTransfers are specified by assistant name and are used when the model recognizes a specific trigger.\n\n```json\n{\n    \"squad\": {\n        \"members\": [\n            {\n                \"assistantId\": \"information-gathering-assistant-id\",\n                \"assistantDestinations\": [{\n                    \"type\": \"assistant\",\n                    \"assistantName\": \"Appointment Booking\",\n                    \"message\": \"Please hold on while I transfer you to our appointment booking assistant.\",\n                    \"description\": \"Transfer the user to the appointment booking assistant after they say their name.\"\n                }],\n            },\n            {\n                \"assistant\": {\n                    \"name\": \"Appointment Booking\",\n                    ...\n                },\n            }\n        ]\n    }\n}\n```\n\n\n## Best practices\n\nThe following are some best practices for using Squads to reduce errors:\n\n- Group assistants by closely related tasks\n- Create as few assistants as possible to reduce complexity\n- Make sure descriptions for transfers are clear and concise\n\n"
    },
    "squads-example.mdx": {
      "markdown": "---\ntitle: Configuring Inbound and Outbound Calls for Squads\nsubtitle: Configuring assistants for inbound/outbound calls.\nslug: squads-example\n---\n\n\nThis guide details how to set up and manage inbound and outbound call functionality within Squads, leveraging AI assistants.\n\n### Key Concepts\n* **Transient Assistant:** A temporary assistant configuration passed directly in the request payload.\n* **Assistant ID:** A unique identifier referring to a pre-existing assistant configuration.\n\n<Note>When using Assistant IDs, ensure the `name` property in the payload matches the associated assistant's name accurately.</Note>\n\n### Inbound Call Configuration\n\nWhen your server receives a request of type `assistant-request`, respond with a JSON payload structured as follows:\n\n\n```json\n{\n    \"squad\": {\n        \"members\": [\n            {\n                \"assistant\": { \n                    \"name\": \"Emma\", \n                    \"model\": { \"model\": \"gpt-4o\", \"provider\": \"openai\" },\n                    \"voice\": { \"voiceId\": \"emma\", \"provider\": \"azure\" },\n                    \"transcriber\": { \"provider\": \"deepgram\" },\n                    \"firstMessage\": \"Hi, I am Emma, what is your name?\",\n                    \"firstMessageMode\": \"assistant-speaks-first\"\n                },\n                \"assistantDestinations\": [ \n                    {\n                        \"type\": \"assistant\",\n                        \"assistantName\": \"Mary\", \n                        \"message\": \"Please hold on while I transfer you to our appointment booking assistant Mary.\",\n                        \"description\": \"Transfer the user to the appointment booking assistant.\"\n                    }\n                ]\n            },\n            {\n                \"assistantId\": \"your-assistant-id\" \n            }\n        ]\n    }\n}\n```\n\n**In this example:**\n\n* The first `members` entry is a **transient assistant** (full configuration provided).\n* The second `members` entry uses an **Assistant ID**.\n* `assistantDestinations` defines how to **transfer the call** to another assistant.\n\n### Outbound Call Configuration\n\nTo initiate an outbound call, send a POST request to the API endpoint /call/phone with a JSON payload structured as follows:\n\n```json\n{\n    \"squad\": {\n        \"members\": [\n            {\n                \"assistant\": { \n                    \"name\": \"Emma\", \n                    \"model\": { \"model\": \"gpt-4o\", \"provider\": \"openai\" },\n                    \"voice\": { \"voiceId\": \"emma\", \"provider\": \"azure\" },\n                    \"transcriber\": { \"provider\": \"deepgram\" },\n                    \"firstMessage\": \"Hi, I am Emma, what is your name?\",\n                    \"firstMessageMode\": \"assistant-speaks-first\"\n                },\n                \"assistantDestinations\": [ \n                    {\n                        \"type\": \"assistant\",\n                        \"assistantName\": \"Mary\", \n                        \"message\": \"Please hold on while I transfer you to our appointment booking assistant Mary.\",\n                        \"description\": \"Transfer the user to the appointment booking assistant.\"\n                    }\n                ]\n            },\n            {\n                \"assistantId\": \"your-assistant-id\" \n            }\n        ]\n    },\n    \"customer\": {\n        \"number\": \"your-phone-number\" \n    },\n    \"phoneNumberId\": \"your-phone-number-id\" \n}\n```\n\n**Key points:**\n\n* `customer.number` is the phone number to call.\n* `phoneNumberId` is a unique identifier for the phone number (obtain this from your provider).\n"
    },
    "call-forwarding.mdx": {
      "markdown": "---\ntitle: Call Forwarding\nslug: call-forwarding\n---\n\n\nVapi's call forwarding functionality allows you to redirect calls to different phone numbers based on specific conditions using tools. This guide explains how to set up and use the `transferCall` function for call forwarding.\n\n## Key Concepts\n\n### Call Forwarding Tools\n\n- **`transferCall` Tool**: This tool enables call forwarding to predefined phone numbers with specific messages based on the destination.\n\n### Parameters and Messages\n\n- **Destinations**: A list of phone numbers where the call can be forwarded.\n- **Messages**: Custom messages that inform the caller about the call being forwarded.\n\n## Setting Up Call Forwarding\n\n### 1. Defining Destinations and Messages\n\nThe `transferCall` tool includes a list of destinations and corresponding messages to notify the caller:\n\n```json\n{\n  \"tools\": [\n    {\n      \"type\": \"transferCall\",\n      \"destinations\": [\n        {\n          \"type\": \"number\",\n          \"number\": \"+1234567890\",\n          \"message\": \"I am forwarding your call to Department A. Please stay on the line.\"\n        },\n        {\n          \"type\": \"number\",\n          \"number\": \"+0987654321\",\n          \"message\": \"I am forwarding your call to Department B. Please stay on the line.\"\n        },\n        {\n          \"type\": \"number\",\n          \"number\": \"+1122334455\",\n          \"message\": \"I am forwarding your call to Department C. Please stay on the line.\"\n        }\n      ],\n      \"function\": {\n        \"name\": \"transferCall\",\n        \"description\": \"Use this function to transfer the call. Only use it when following instructions that explicitly ask you to use the transferCall function. DO NOT call this function unless you are instructed to do so.\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"destination\": {\n              \"type\": \"string\",\n              \"enum\": [\n                \"+1234567890\",\n                \"+0987654321\",\n                \"+1122334455\"\n              ],\n              \"description\": \"The destination to transfer the call to.\"\n            }\n          },\n          \"required\": [\n            \"destination\"\n          ]\n        }\n      },\n      \"messages\": [\n        {\n          \"type\": \"request-start\",\n          \"content\": \"I am forwarding your call to Department A. Please stay on the line.\",\n          \"conditions\": [\n            {\n              \"param\": \"destination\",\n              \"operator\": \"eq\",\n              \"value\": \"+1234567890\"\n            }\n          ]\n        },\n        {\n          \"type\": \"request-start\",\n          \"content\": \"I am forwarding your call to Department B. Please stay on the line.\",\n          \"conditions\": [\n            {\n              \"param\": \"destination\",\n              \"operator\": \"eq\",\n              \"value\": \"+0987654321\"\n            }\n          ]\n        },\n        {\n          \"type\": \"request-start\",\n          \"content\": \"I am forwarding your call to Department C. Please stay on the line.\",\n          \"conditions\": [\n            {\n              \"param\": \"destination\",\n              \"operator\": \"eq\",\n              \"value\": \"+1122334455\"\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n### 2. Using the `transferCall` Function\n\nWhen the assistant needs to forward a call, it uses the `transferCall` function with the appropriate destination:\n\n```json\n{\n  \"function\": {\n    \"name\": \"transferCall\",\n    \"parameters\": {\n      \"destination\": \"+1234567890\"\n    }\n  }\n}\n\n```\n\n### 3. Customizing Messages\n\nCustomize the messages for each destination to provide clear information to the caller:\n\n```json\n{\n  \"messages\": [\n    {\n      \"type\": \"request-start\",\n      \"content\": \"I am forwarding your call to Department A. Please stay on the line.\",\n      \"conditions\": [\n        {\n          \"param\": \"destination\",\n          \"operator\": \"eq\",\n          \"value\": \"+1234567890\"\n        }\n      ]\n    }\n  ]\n}\n\n```\n\n## Instructing the Assistant\n\nUse the system prompt to guide the assistant on when to utilize each forwarding number. For example:\n\n- \"If the user asks for sales, call the `transferCall` function with `+1234567890`.\"\n- \"If the user requests technical support, use the `transferCall` function with `+0987654321`.\"\n\n## Troubleshooting\n\n- If calls are not being transferred, check the logs for errors.\n- Ensure that the correct destination numbers are used.\n- Ensure you have written the function description properly to indicate where you want to forward the call\n- Test the call forwarding setup thoroughly to confirm its functionality.\n\n## Call Transfers Mode\n\nVapi supports two types of call transfers:\n\n1. **Blind Transfer** (default): Directly transfers the call to another agent without providing any prior information to the recipient.\n2. **Warm Transfer**: Transfers the call to another agent after providing context about the call. The context can be either a full transcript or a summary, based on your configuration.\n\n### Warm Transfer\n\nTo implement a warm transfer, add a `transferPlan` object to the `transferCall` tool syntax and specify the transfer mode.\n\n#### Modes of Warm Transfer\n\n#### 1. Warm Transfer with Summary\n\nIn this mode, Vapi provides a summary of the call to the recipient before transferring.\n\n* **Configuration:**\n   * Set the `mode` to `\"warm-transfer-with-summary\"`.\n   * Define a `summaryPlan` specifying how the summary should be generated.\n   * Use the `{{transcript}}` variable to include the call transcript.\n\n* **Example:**\n\n```json\n\"transferPlan\": {\n  \"mode\": \"warm-transfer-with-summary\",\n  \"summaryPlan\": {\n    \"enabled\": true,\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"Please provide a summary of the call.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Here is the transcript:\\n\\n{{transcript}}\\n\\n\"\n      }\n    ]\n  }\n}\n```\n\n#### 2. Warm Transfer with Message\n\nIn this mode, Vapi delivers a custom static message to the recipient before transferring the call.\n\n* **Configuration:**\n   * Set the `mode` to `\"warm-transfer-with-message\"`.\n   * Provide the custom message in the `message` property.\n   * Note that the `{{transcript}}` variable is not available in this mode.\n\n* **Example:**\n\n```json\n\"transferPlan\": {\n  \"mode\": \"warm-transfer-with-message\",\n  \"message\": \"Hey, this call has been forwarded through Vapi.\"\n}\n```\n\n#### Complete Example\n\nHere is a full example of a `transferCall` payload using the warm transfer with summary mode:\n\n```json\n{\n  \"type\": \"transferCall\",\n  \"messages\": [\n    {\n      \"type\": \"request-start\",\n      \"content\": \"I'll transfer you to someone who can help.\"\n    }\n  ],\n  \"destinations\": [\n    {\n      \"type\": \"number\",\n      \"number\": \"+918936850777\",\n      \"description\": \"Transfer the call\",\n      \"transferPlan\": {\n        \"mode\": \"warm-transfer-with-summary\",\n        \"summaryPlan\": {\n          \"enabled\": true,\n          \"messages\": [\n            {\n              \"role\": \"system\",\n              \"content\": \"Please provide a summary of the call.\"\n            },\n            {\n              \"role\": \"user\",\n              \"content\": \"Here is the transcript:\\n\\n{{transcript}}\\n\\n\"\n            }\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\n**Note:** In all warm transfer modes, the `{{transcript}}` variable contains the full transcript of the call and can be used within the `summaryPlan`.\n"
    },
    "calls/call-ended-reason.mdx": {
      "markdown": "---\ntitle: Call Ended Reason\nsubtitle: A guide to understanding all call \"Ended Reason\" types & errors.\nslug: calls/call-ended-reason\n---\n\n\nThis guide will discuss all possible `endedReason`s for a call.\n\nYou can find these under the **\"Ended Reason\"** section of your [call\nlogs](https://dashboard.vapi.ai/calls) (or under the `endedReason` field on the [Call\nObject](/api-reference/calls/get-call)).\n\n#### **Assistant-Related**\n\n- **assistant-ended-call**: The assistant intentionally ended the call based on the user's response.\n- **assistant-error**: This general error occurs within the assistant's logic or processing due to bugs, misconfigurations, or unexpected inputs.\n- **assistant-forwarded-call**: The assistant successfully transferred the call to another number or service.\n- **assistant-join-timed-out**: The assistant failed to join the call within the expected timeframe.\n- **assistant-not-found**: The specified assistant cannot be located or accessed, possibly due to an incorrect assistant ID or configuration issue.\n- **assistant-not-invalid**: The assistant ID provided is not valid or recognized by the system.\n- **assistant-not-provided**: No assistant ID was specified in the request, causing the system to fail.\n- **assistant-request-returned-error**: Communicating with the assistant resulted in an error, possibly due to network issues or problems with the assistant itself.\n- **assistant-request-returned-forwarding-phone-number**: The assistant triggered a call forwarding action, ending the current call.\n- **assistant-request-returned-invalid-assistant**: The assistant returned an invalid response or failed to fulfill the request properly.\n- **assistant-request-returned-no-assistant**: The assistant didn't provide any response or action to the request.\n- **assistant-said-end-call-phrase**: The assistant recognized a phrase or keyword triggering call termination.\n\n#### **Pipeline and LLM**\n\nThese relate to issues within the AI processing pipeline or the Large Language Models (LLMs) used for understanding and generating text:\n\n- **pipeline-error-\\***: Various error codes indicate specific failures within the processing pipeline, such as function execution, LLM responses, or external service integration. Examples include OpenAI, Azure OpenAI, Together AI, and several other LLMs or voice providers.\n- **pipeline-error-first-message-failed:** The system failed to deliver the first message. This issue usually occurs when you add your own provider key in the voice section. It may be due to exceeding your subscription or quota limit.\n- **pipeline-no-available-llm-model**: No suitable LLM was available to process the request.\n\n#### **Phone Calls and Connectivity**\n\n- **customer-busy**: The customer's line was busy.\n- **customer-ended-call**: The customer(end human user) ended the call for both inbound and outbound calls.\n- **customer-did-not-answer**: The customer didn't answer the call. If you're looking to build a usecase where you need the bot to talk to automated IVRs, set `assistant.voicemailDetectionEnabled=false`.\n- **customer-did-not-give-microphone-permission**: The user didn't grant the necessary microphone access for the call.\n- **phone-call-provider-closed-websocket**: The connection with the call provider was unexpectedly closed.\n- **twilio-failed-to-connect-call**: The Twilio service, responsible for managing calls, failed to establish a connection.\n- **vonage-disconnected**: The call was disconnected by Vonage, another call management service.\n- **vonage-failed-to-connect-call**: Vonage failed to establish the call connection.\n- **vonage-rejected**: The call was rejected by Vonage due to an issue or configuration problem.\n\n#### **Other Reasons**\n\n- **exceeded-max-duration**: The call reached its maximum allowed duration and was automatically terminated.\n- **silence-timed-out**: The call was ended due to prolonged silence, indicating inactivity.\n- **voicemail**: The call was diverted to voicemail.\n\n#### **Unknown**\n\n- **unknown-error**: An unexpected error occurred, and the cause is unknown. For this, please [contact support](/support) with your `call_id` and account email address, & we will investigate.\n"
    },
    "advanced/calls/sip.mdx": {
      "markdown": "---\ntitle: SIP\nsubtitle: You can make SIP calls to Vapi Assistants.\nslug: advanced/calls/sip\n---\n\n\n<Info>This instruction is solely for testing purposes. To productionize a SIP implementation, contact Sales to inquire about an Enterprise plan.</Info>\n\n<Accordion title=\"1. Create an Assistant.\">\n\n## 1. Create an Assistant\n\nWe'll create an assistant with `POST /assistant` endpoint. This is no different than creating an assistant for other transports.\n\n```json\n{\n\t\"name\": \"My SIP Assistant\",\n\t\"firstMessage\": \"Hello {{first_name}}, you've reached me over SIP. How can I help you today?\"\n}\n\n```\n\n</Accordion>\n\n<Accordion title=\"2. Create a SIP Phone Number.\">\n\n## 2. Create A SIP Phone Number\n\nWe'll create a SIP phone number with `POST /phone-number` endpoint. \n\n```json\n{\n\t\"provider\": \"vapi\",\n\t\"sipUri\": \"sip:your_unique_user_name@sip.vapi.ai\",\n\t\"assistantId\": \"your_assistant_id\"\n}\n\n```\n\n`sipUri` is the SIP URI of the phone number. It must be in the format `sip:username@sip.vapi.ai`. You are free to choose any username you like.\n\n</Accordion>\n\n\n\n<Accordion title=\"3. Start a SIP call.\">\n\n## 3. Start a SIP call.\n\nYou can use any SIP softphone to test the Assistant. Examples include [Zoiper](https://www.zoiper.com/) or [Linphone](https://www.linphone.org/). \n\nYou just need to dial `sip:your_unique_user_name@sip.vapi.ai` and the Assistant will answer your call. \n\nThere is no authentication or SIP registration required.\n\n</Accordion>\n\n<Accordion title=\"4. Send SIP Headers to Fill Template Variables.\">\n\n## 4. Send SIP Headers to Fill Template Variables.\n\nTo fill your template variables, you can send custom SIP headers. \n\nFor example, to fill the `first_name` variable, you can send a SIP header `x-first_name: John`. \n\nThe header name is case insensitive. So, `X-First_Name`, `x-first_name`, and `X-FIRST_NAME` are all the same.\n\n</Accordion>\n\n<Accordion title=\"5. Use a Custom Assistant For Each Call.\">\n\n## 5. Use a Custom Assistant For Each Call.\n\nYou can use a custom assistant for SIP calls same as phone calls.\n\nSet the `assistantId` to `null` and the `serverUrl` to the URL of your server which will respond to the `assistant-request`.\n\n`PATCH /phone-number/:id`\n```json\n{\n\t\"assistantId\": null,\n\t\"serverUrl\": \"https://your_server_url\"\n}\n```\n\nNow, every time you make a call to this phone number, the server will receive a `assistant-request` event.\n\n</Accordion>\n\n"
    },
    "calls/call-features.mdx": {
      "markdown": "---\ntitle: Live Call Control\nslug: calls/call-features\n---\n\nVapi offers two main features that provide enhanced control over live calls:\n\n1. **Call Control**: This feature allows you to inject conversation elements dynamically during an ongoing call.\n2. **Call Listen**: This feature enables real-time audio data streaming using WebSocket connections.\n\nTo use these features, you first need to obtain the URLs specific to the live call. These URLs can be retrieved by triggering a `/call` endpoint, which returns the `listenUrl` and `controlUrl` within the `monitor` object.\n\n## Obtaining URLs for Call Control and Listen\n\nTo initiate a call and retrieve the `listenUrl` and `controlUrl`, send a POST request to the `/call` endpoint.\n\n### Sample Request\n\n```bash\ncurl 'https://api.vapi.ai/call/phone' \n-H 'authorization: Bearer YOUR_API_KEY' \n-H 'content-type: application/json' \n--data-raw '{\n  \"assistantId\": \"5b0a4a08-133c-4146-9315-0984f8c6be80\",\n  \"customer\": {\n    \"number\": \"+12345678913\"\n  },\n  \"phoneNumberId\": \"42b4b25d-031e-4786-857f-63b346c9580f\"\n}'\n\n```\n\n### Sample Response\n\n```json\n{\n  \"id\": \"7420f27a-30fd-4f49-a995-5549ae7cc00d\",\n  \"assistantId\": \"5b0a4a08-133c-4146-9315-0984f8c6be80\",\n  \"phoneNumberId\": \"42b4b25d-031e-4786-857f-63b346c9580f\",\n  \"type\": \"outboundPhoneCall\",\n  \"createdAt\": \"2024-09-10T11:14:12.339Z\",\n  \"updatedAt\": \"2024-09-10T11:14:12.339Z\",\n  \"orgId\": \"eb166faa-7145-46ef-8044-589b47ae3b56\",\n  \"cost\": 0,\n  \"customer\": {\n    \"number\": \"+12345678913\"\n  },\n  \"status\": \"queued\",\n  \"phoneCallProvider\": \"twilio\",\n  \"phoneCallProviderId\": \"CA4c6793d069ef42f4ccad69a0957451ec\",\n  \"phoneCallTransport\": \"pstn\",\n  \"monitor\": {\n    \"listenUrl\": \"wss://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/transport\",\n    \"controlUrl\": \"<https://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/control>\"\n  }\n}\n\n```\n\n## Call Control Feature\n\nOnce you have the `controlUrl`, you can inject a message into the live call using a POST request. This can be done by sending a JSON payload to the `controlUrl`.\n\n### Example: Injecting a Message\n\n```bash\ncurl -X POST 'https://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/control' \n-H 'content-type: application/json' \n--data-raw '{\n  \"type\": \"say\",\n  \"message\": \"Welcome to Vapi, this message was injected during the call.\"\n}'\n\n```\n\nThe message will be spoken in real-time during the ongoing call.\n\n## Call Listen Feature\n\nThe `listenUrl` allows you to connect to a WebSocket and stream the audio data in real-time. You can either process the audio directly or save the binary data to analyze or replay later.\n\n### Example: Saving Audio Data from a Live Call\n\nHere is a simple implementation for saving the audio buffer from a live call using Node.js:\n\n```jsx\nconst WebSocket = require('ws');\nconst fs = require('fs');\n\nlet pcmBuffer = Buffer.alloc(0);\n\nconst ws = new WebSocket(\"wss://aws-us-west-2-production1-phone-call-websocket.vapi.ai/7420f27a-30fd-4f49-a995-5549ae7cc00d/transport\");\n\nws.on('open', () => console.log('WebSocket connection established'));\n\nws.on('message', (data, isBinary) => {\n  if (isBinary) {\n    pcmBuffer = Buffer.concat([pcmBuffer, data]);\n    console.log(`Received PCM data, buffer size: ${pcmBuffer.length}`);\n  } else {\n    console.log('Received message:', JSON.parse(data.toString()));\n  }\n});\n\nws.on('close', () => {\n  if (pcmBuffer.length > 0) {\n    fs.writeFileSync('audio.pcm', pcmBuffer);\n    console.log('Audio data saved to audio.pcm');\n  }\n});\n\nws.on('error', (error) => console.error('WebSocket error:', error));\n\n```\n"
    },
    "GHL.mdx": {
      "markdown": "---\ntitle: How to Connect Vapi with Make & GHL\nslug: GHL\n---\n\n\nVapi's GHL/Make Tools integration allows you to directly import your GHL workflows and Make scenarios into Vapi as Tools. This enables you to create voicebots that can trigger your favorite app integrations and automate complex workflows using voice commands.\n\n## What are GHL/Make Tools?\n\nGHL (GoHighLevel) workflows and Make scenarios are powerful automation tools that allow you to connect and integrate various apps and services. With the GHL/Make Tools integration, you can now bring these automations into Vapi and trigger them using voice commands.\n\n## How does the integration work?\n\n1. **Import workflows and scenarios**: Navigate to the [Tools section](https://dashboard.vapi.ai/tools) in your Vapi dashboard and import your existing GHL workflows and Make scenarios.\n\n2. **Add Tools to your assistants**: Once imported, you can add these Tools to your AI assistants, enabling them to trigger the automations based on voice commands.\n\n3. **Trigger automations with voice**: Your AI assistants can now understand voice commands and execute the corresponding GHL workflows or Make scenarios, allowing for seamless voice-enabled automation.\n\n## Setting up the GHL/Make Tools integration\n\n1. **Create a GHL workflow or Make scenario**: Design your automation in GHL or Make, connecting the necessary apps and services.\n\n2. **Import the workflow/scenario into Vapi**: In the Vapi dashboard, navigate to the Tools section and click on \"Import.\" Select the GHL workflow or Make scenario you want to import.\n\n3. **Configure the Tool**: Provide a name and description for the imported Tool, and map any required input variables to the corresponding Vapi entities (e.g., extracted from user speech).\n\n4. **Add the Tool to your assistant**: Edit your AI assistant and add the newly imported Tool to its capabilities. Specify the voice commands that should trigger the Tool.\n\n5. **Test the integration**: Engage with your AI assistant using the specified voice commands and verify that the corresponding GHL workflow or Make scenario is triggered successfully.\n\n## Use case examples\n\n### Booking appointments with AI callers\n\n- Import a GHL workflow that handles appointment booking\n- Configure the workflow to accept appointment details (date, time, user info) from Vapi\n- Add the Tool to your AI assistant, allowing it to book appointments based on voice commands\n\n### Updating CRMs with voice-gathered data\n\n- Import a Make scenario that updates your CRM with customer information\n- Map the scenario's input variables to entities extracted from user speech\n- Enable your AI assistant to gather customer information via voice and automatically update your CRM\n\n### Real Estate: Automated Property Information Retrieval\n\n- Import a Make scenario that retrieves property information from your MLS (Multiple Listing Service) or real estate database\n- Configure the scenario to accept a property address or MLS ID as input\n- Add the Tool to your AI assistant, allowing potential buyers to request property details using voice commands\n- Your AI assistant can then provide key information about the property, such as price, square footage, number of bedrooms/bathrooms, and amenities\n\n### Healthcare/Telehealth: Appointment Reminders and Prescription Refills\n\n- Import a GHL workflow that sends appointment reminders and handles prescription refill requests\n- Configure the workflow to accept patient information and appointment/prescription details from Vapi\n- Add the Tool to your AI assistant, enabling patients to request appointment reminders or prescription refills using voice commands\n- Your AI assistant can confirm the appointment details, send reminders via SMS or email, and forward prescription refill requests to the appropriate healthcare provider\n\n### Restaurant Ordering: Custom Order Placement and Delivery Tracking\n\n- Import a Make scenario that integrates with your restaurant's online ordering system and delivery tracking platform\n- Configure the scenario to accept customer information, order details, and delivery preferences from Vapi\n- Add the Tool to your AI assistant, allowing customers to place custom orders and track their delivery status using voice commands\n- Your AI assistant can guide customers through the ordering process, suggest menu items based on preferences, and provide real-time updates on the order status and estimated delivery time\n\n## Best practices\n\n- Break down complex automations into smaller, focused workflows or scenarios for better maintainability\n- Use clear and concise naming conventions for your imported Tools and their input variables\n- Thoroughly test the integration to ensure reliable performance and accurate data passing\n- Keep your GHL workflows and Make scenarios up to date to reflect any changes in the connected apps or services\n\n## Troubleshooting\n\n- If a Tool is not triggering as expected, verify that the voice commands are correctly configured and the input variables are properly mapped\n- Check the Vapi logs and the GHL/Make execution logs to identify any errors or issues in the automation flow\n- Ensure that the necessary API credentials and permissions are correctly set up in both Vapi and the integrated apps/services\n\nBy leveraging Vapi's GHL/Make Tools integration, you can create powerful voice-enabled automations and streamline your workflows, all without extensive coding. Automate tasks, connect your favorite apps, and unlock the full potential of voice AI with Vapi.\n\n## Get Support\n\nJoin our Discord to connect with other developers & connect with our team:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Join Our Discord\"\n    icon=\"fa-brands fa-discord\"\n    iconType=\"solid\"\n    color=\"#5A65EA\"\n    href=\"https://discord.gg/pUFNcf2WmH\"\n  >\n    Connect with our team & other developers using Vapi.\n  </Card>\n  <Card\n    title=\"Email Support\"\n    icon=\"mailbox\"\n    iconType=\"solid\"\n    color=\"#7a7f85\"\n    href=\"mailto:support@vapi.ai\"\n  >\n    Send our support team an email.\n  </Card>\n</CardGroup>\n\nHere are some video tutorials that will guide you on how to use Vapi with services like Make and GoHighLevel:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/PVP1P2nak4M?si=vGGAMZVI3Fzzik9X\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/3LSXJECXpkc?si=hhWsXZeFYC6wM-cq\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.loom.com/embed/026d1c1a2cc64e479044619842ce8bd1?sid=c5934ff7-2f58-4cf3-952a-140938079ca0\"\n    title=\"GoHighLevel Loom Video\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.loom.com/embed/210579f412bd47de8964683b2f28c3ec?sid=1f289c56-d4bc-4923-85bb-6cbc0fe55361\"\n    title=\"GoHighLevel Bulk call Loom Video\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/KwQmJbIOov4?si=x5ep0ziyIM5ueuvG\"\n    title=\"GoHighLevel Bulk call Loom Video\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n</div>\n"
    },
    "tools-calling.mdx": {
      "markdown": "---\ntitle: Tools Calling\nsubtitle: Learn how to create and configure Tools Calling with Vapi.\nslug: tools-calling\n---\n\n\nThis guide focuses on configuring tools within your Vapi assistant using the provided sample payload as a reference. We'll explore how to adapt the existing structure to fit your specific needs and desired functionalities.\n\n## Understanding the Tool Structure\n\nThe sample payload demonstrates the configuration for a \"function\" type tool. Let's break down its key components:\n\n```json\n{\n    \"type\": \"function\",\n    \"messages\": [ ... ],\n    \"function\": { ... },\n    \"async\": false,\n    \"server\": { ... }\n}\n```\n1. **type:** This remains as \"function\" if you're setting up an API call.\n2. **messages:** This array holds the messages the AI will communicate to the user at different stages of the tool's execution.\n3. **function:** Defines the function details:\n    - **name:** The unique identifier for your function.\n    - **parameters:** (Optional) An object describing the parameters expected by the function.\n    - **description:** (Optional) A description of the function's purpose.\n4. **async:** Set to \"true\" if the function call should be asynchronous, allowing the AI to continue without waiting for the response. Use \"false\" for synchronous calls.\n5. **server:** Provides the URL of the server where the function is hosted.\n\n## Adapting the Payload for Your Needs\n\n1. **Modify Function Details:**\n    - Change the \"name\" to reflect your specific function.\n    - Adjust the \"parameters\" object according to the data your function requires.\n    - Update the \"description\" to accurately explain the function's purpose.\n2. **Customize Messages:**\n    - Edit the content of each message within the \"messages\" array to align with your desired communication style and information.\n    - Add or remove messages as needed. You can include messages like \"request-start\", \"request-response-delayed\", \"request-complete\", and \"request-failed\" to inform the user about the tool's progress.\n3. **Set Server URL:**\n    - Replace the existing URL in the \"server\" object with the endpoint of your server hosting the function.\n4. **Choose Asynchronous Behavior (Optional):**\n    - Change \"async\" to \"true\" if you want the AI to continue without waiting for the function's response.\n\n### Example Modification\n\nLet's say you want to create a tool that fetches the weather for a given location. You would modify the payload as follows:\n\n```json\n{\n    \"type\": \"function\",\n    \"messages\": [\n        {\n            \"type\": \"request-start\",\n            \"content\": \"Checking the weather forecast. Please wait...\"\n        },\n        {\n            \"type\": \"request-complete\",\n            \"content\": \"The weather in location is\"\n        },\n        {\n            \"type\": \"request-failed\",\n            \"content\": \"I couldn't get the weather information right now.\"\n        },\n        {\n            \"type\": \"request-response-delayed\",\n            \"content\": \"It appears there is some delay in communication with the weather API.\",\n            \"timingMilliseconds\": 2000\n        }\n    ],\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\"\n                }\n            }\n        },\n        \"description\": \"Retrieves the current weather for a specified location.\"\n    },\n    \"async\": false,\n    \"server\": {\n        \"url\": \"https://your-weather-api.com/weather\"\n    }\n}\n```\n\n### Adding More Tools\n\nSimply create additional tool objects within the \"tools\" array, following the same structure and modifying the details as needed. Each tool can have its own unique configuration and messages.\n\n## Server Response Format: Providing Results and Context\n\nWhen your Vapi assistant calls a tool (via the server URL you configured), your server will receive an HTTP request containing information about the tool call. Upon processing the request and executing the desired function, your server needs to send back a response in the following JSON format:\n\n\n```json\n{\n    \"results\": [\n        {\n            \"toolCallId\": \"X\",\n            \"result\": \"Y\"\n        }\n    ]\n}\n```\n\n**Breaking down the components:**\n\n- **toolCallId (X):** This is a unique identifier included in the initial request from Vapi. It allows the assistant to match the response with the corresponding tool call, ensuring accurate processing and context preservation.\n- **result (Y):** This field holds the actual output or result of your tool's execution. The format and content of \"result\" will vary depending on the specific function of your tool. It could be a string, a number, an object, an array, or any other data structure that is relevant to the tool's purpose.\n\n**Example:**\n\nLet's revisit the weather tool example from before. If the tool successfully retrieves the weather for a given location, the server response might look like this:\n\n```json\n{\n  \"results\": [\n    {\n      \"toolCallId\": \"call_VaJOd8ZeZgWCEHDYomyCPfwN\",\n      \"result\": \"San Francisco's weather today is 62°C, partly cloudy.\"\n    }\n  ]\n}\n```\n\n**Some Key Points:**\n\n- Pay attention to the required parameters and response format of your functions.\n- Ensure your server is accessible and can handle the incoming requests from Vapi.\n- Make sure to add \"Tools Calls\" in both the Server and Client messages and remove the function calling from it.\n\nBy following these guidelines and adapting the sample payload, you can easily configure a variety of tools to expand your Vapi assistant's capabilities and provide a richer, more interactive user experience.\n\n\n**Video Tutorial:**\n<iframe\n        src=\"https://www.youtube.com/embed/4fLe3sRH_uI?si=rAE5sWS2f8Ky5Zgx\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        width=\"100%\"\n        height=\"400px\"\n        allowfullscreen\n/>\n"
    },
    "prompting-guide.mdx": {
      "markdown": "---\ntitle: Prompting Guide\nslug: prompting-guide\n---\n\n\nPrompt engineering is the art of crafting effective instructions for AI agents, directly influencing their performance and reliability. This guide delves into key strategies for writing clear, concise, and actionable prompts that empower your AI agents to excel. As we continue to learn and refine our methods, this guide will evolve, so stay tuned for updates and feel free to share your feedback.\n\n### Building Blocks of Effective Prompts: Sectional Organization\n\nTo enhance clarity and maintainability, it's recommended to break down system prompts into distinct sections, each focusing on a specific aspect:\n\n- **Identity:** Define the persona and role of the AI agent, setting the tone for interactions.\n- **Style:** Establish stylistic guidelines, such as conciseness, formality, or humor, to ensure consistent communication.\n- **Response Guidelines:** Specify formatting preferences, question limits, or other structural elements for responses.\n- **Task & Goals:** Outline the agent's objectives and the steps it should take to achieve them.\n\n**Example:**\n\n```jsx\n[Identity]\nYou are a helpful and knowledgeable virtual assistant for a travel booking platform.\n\n[Style]\n- Be informative and comprehensive.\n- Maintain a professional and polite tone.\n- Be concise, as you are currently operating as a Voice Conversation.\n\n[Response Guideline]\n- Present dates in a clear format (e.g., January 15, 2024).\n- Offer up to three travel options based on user preferences.\n\n[Task]\n1. Greet the user and inquire about their desired travel destination.\n2. Ask about travel dates and preferences (e.g., budget, interests).\n3. Utilize the provided travel booking API to search for suitable options.\n4. Present the top three options to the user, highlighting key features.\n\n```\n\n\n### Task Breakdown: Step-by-Step Instructions\nFor complex interactions, breaking down the task into a sequence of steps enhances the agent's understanding and ensures a structured conversation flow. Incorporate conditional logic to guide the agent's responses based on user input.\nExample:\n\n```jsx\n[Task]\n1. Welcome the user to the technical support service.\n2. Inquire about the nature of the technical issue.\n3. If the issue is related to software, ask about the specific software and problem details.\n4. If the issue is hardware-related, gather information about the device and symptoms.\n5. Based on the collected information, provide troubleshooting steps or escalate to a human technician if necessary.\n```\n\n### Controlling Response Timing\nTo prevent the agent from rushing through the conversation, explicitly indicate when to wait for the user's response before proceeding to the next step.\n\n```jsx\n[Task]\n1. Inform the user about the purpose of the call.\n2. Ask for the user's name and account information.\n<wait for user response>\n3. Inquire about the reason for the call and offer assistance options....\n```\n\n### Explicit Tool Integration\nSpecify when and how the agent should utilize external tools or APIs. Reference the tools by their designated names and describe their functions to ensure accurate invocation.\nExample:\n\n```jsx\n[Task]\n...\n3. If the user wants to know about something, use the get_data function with the parameter 'query', which will contain the user's question to initiate the process.\n4. Guide the user through the password reset steps provided by the API....\n```\n\n### Some Common Issues\n**1. Numbers sound very robotic, how can we improve it?**\n\n⇒ When working with numbers that need to be expressed in writing, it's often preferable to spell them out. This approach usually sounds more natural.\n\n**2. How to make assistants sound more human-like?**\n\n⇒ Add personality and provide tone to the assistant to make it sound more like a human being. Here are some examples:\n\n```jsx\n- Tone: conversational, spartan, use less corporate jargon\n- Act as Iva, a sales agent who is a human being and has real feelings.\n```\n"
    },
    "glossary.mdx": {
      "markdown": "---\ntitle: Definitions\nsubtitle: Useful terms and definitions for Vapi & voice AI applications.\nslug: glossary\n---\n\n\n## A\n\n### At-cost\n\n\"At-cost\" is often use when discussing pricing. It means \"without profit to the seller\". Vapi charges at-cost for requests made to [STT](/glossary#stt), [LLM](/glossary#large-language-model), & [TTS](/glossary#tts) providers.\n\n## B\n\n### Backchanneling\n\nA [backchannel](<https://en.wikipedia.org/wiki/Backchannel_(linguistics)>) occurs when a listener provides verbal or non-verbal feedback to a speaker during a conversation.\n\nExamples of backchanneling in English include such expressions as \"yeah\", \"OK\", \"uh-huh\", \"hmm\", \"right\", and \"I see\".\n\nThis feedback is often not semantically significant to the conversation, but rather serves to signify the listener's attention, understanding, sympathy, or agreement.\n\n## E\n\n### Endpointing\n\nSee [speech endpointing](/glossary#speech-endpointing).\n\n## I\n\n### Inbound Call\n\nThis is a call received by an assistant **_from_** another phone number (w/ the assistant being the \"person\" answering). The call comes **\"in\"**-ward to a number (from an external caller) — hence the term \"inbound call\".\n\n### Inference\n\nYou may often hear the term \"run inference\" when referring to running a large language model against an input prompt to receive text output back out.\n\nThe process of running a prompt against an LLM for output is called \"inference\".\n\n## L\n\n### Large Language Model\n\nLarge Language Models (or \"LLM\", for short) are machine learning models trained on large amounts of text, & later used to generate text in a probabilistic manner, \"token-by-token\".\n\nFor further reading see [large language model wiki](https://en.wikipedia.org/wiki/Large_language_model).\n\n### LLM\n\nSee [Large Language Model](/glossary#large-language-model).\n\n## O\n\n### Outbound Call\n\nThis is a call made by an assistant **_to_** another target phone number (w/ the assistant being the \"person\" dialing). The call goes **\"out\"**-ward to another number — hence the term \"outbound call\".\n\n## S\n\n### Server URL\n\nA \"server url\" is an endpoint you expose to Vapi to receive conversation data in real-time. Server urls can reply with meaningful responses, distinguishing them from traditional [webhooks](/glossary#webhook).\n\nSee our [server url](/server-url) guide to learn more.\n\n### SDK\n\nStands for \"Software Development Kit\" — these are pre-packaged libraries & platform-specific building tools that a software publisher creates to expedite & increase the ease of integration for developers.\n\n### Speech Endpointing\n\nSpeech endpointing is the process of detecting the start and end of (a line of) speech in an audio signal. This is an important function in conversation turn detection.\n\nA starting heuristic for the end of a user's speech is the detection of silence. If someone does not speak for a certain amount of milliseconds, the utterance can be considered complete.\n\nA more robust & ideal approach is to actually understand what the user is saying (as well as the current conversation's state & the speech turn's intent) to determine if the user is just pausing for effect, or actually finished speaking.\n\nVapi uses a combination of silence detection and machine learning models to properly endpoint conversation speech (to prevent improper interruption & encourage proper [backchanneling](/glossary#backchanneling)).\n\nAdditional reading on speech endpointing can be found [here](https://en.wikipedia.org/wiki/Speech_segmentation) & on [Deepgram's docs](https://developers.deepgram.com/docs/endpointing).\n\n### STT\n\nAn abbreviation used for \"Speech-to-text\". The process of converting physical sound waves into raw transcript text (a process called \"transcription\").\n\n## T\n\n### Telemarketing Sales Rule\n\nThe Telemarketing Sales Rule (or \"TSR\" for short) is a regulation established by the Federal Trade Commission ([ftc.gov](https://www.ftc.gov/)) in the United States to protect consumers from deceptive and abusive telemarketing practices.\n\n**You may only conduct outbound calls to phone numbers which you have consent to contact.** Violating TSR rules can result in significant civil (or even criminal) penalties.\n\nLearn more on the [FCC website](https://www.ftc.gov/legal-library/browse/rules/telemarketing-sales-rule).\n\n### TTS\n\nAn abbreviation used for \"Text-to-speech\". The process of converting raw text into playable audio data.\n\n## V\n\n### Voice-to-Voice\n\n\"Voice-to-voice\" is often a term brought up in discussing voice AI system latency — the time it takes to go from a user finishing their speech (however that endpoint is computed) → to the AI agent's first speech chunk/byte being played back on a client’s device.\n\nIdeally, this process should happen in \\<1s, better if closer to 500-700ms (responding too quickly can be an issue as well). Voice AI applications must closely watch this metric to ensure their applications stay responsive & usable.\n\n## W\n\n### Webhook\n\nA webhook is a server endpoint you expose to external services with the intention of receiving external data in real-time. Your exposed URL is essentially a \"drop-bin\" for data to come in from external providers to update & inform your systems.\n\nTraditionally, webhooks are unidirectional & stateless. Endpoints only reply with status code to signal acknowledgement.\n\n<Info>\n  To make the distinction clear, Vapi calls these \"[server urls](/server-url)\".\n  Certain requests made to your server (like assistant requests) require a reply\n  with meaningful data.\n</Info>\n"
    },
    "faq.mdx": {
      "markdown": "---\ntitle: Frequently Asked Questions\nsubtitle: Frequently asked questions about Vapi.\nslug: faq\n---\n\n<AccordionGroup>\n  <Accordion title=\"Is Vapi right for my usecase?\" icon=\"hammer\" iconType=\"regular\" defaultOpen={true}>\n\nIf you are **a developer building a voice AI application simulating human conversation** (w/ LLMs — to whatever degree of application complexity) — Vapi is built for you.\n\nWhether you are building for a completely \"turn-based\" use case (like appointment setting), all the way to robust agentic voice applications (like virtual assistants), Vapi is tooled to solve for your voice AI workflow.\n\nVapi runs on any platform: the web, mobile, or even embedded systems (given network access).\n\n  </Accordion>\n  <Accordion title=\"Sounds good, but I’m building a custom X for Y...\" icon=\"face-monocle\" iconType=\"solid\" defaultOpen={false}>\n\nNot a problem, we can likely already support it. Vapi is designed to be modular at every level of the voice pipeline: Text-to-speech, LLM, Speech-to-text.\n\nYou can bring your own custom models for any part of the pipeline.\n\n- **If they’re hosted with one of our providers:** you just need to add your [provider keys](customization/provider-keys), then specify the custom model in your API requests.\n- **If they are hosted elsewhere:** you can use the `Custom LLM` provider and specify the [URL to your model](customization/custom-llm/fine-tuned-openai-models) in your API request.\n\nEverything is interchangeable, mix & match to suit your usecase.\n\n  </Accordion>\n  <Accordion title=\"Couldn’t I build this myself and save money?\" icon=\"piggy-bank\" iconType=\"solid\" defaultOpen={false}>\n\nYou could (and the person writing this right now did, from scratch) — but there are good reasons for not doing so.\n\nWriting a great realtime voice AI application from scratch is a fairly challenging task (more on those challenges [here](/challenges-of-realtime-conversation)). Most of these challenges are not apparent until you face them, then you realize you are 3 weeks into a rabbit hole that may take months to properly solve out of.\n\nThink of Vapi as hiring a software engineering team for this hard problem, while you focus on what uniquely generates value for your voice AI application.\n\n---\n\nBut to address cost, the vast majority of cost in running your application will come from provider cost (Speect-to-text, LLM, Text-to-speech) direct with vendors (Deepgram, OpenAI, ElevenLabs, etc) — where we add no fee (vendor cost passes-through). These would have to be incurred anyway.\n\nVapi only charges its small fee on top of these for the continuous maintenance & improvement of these hardest components of your system (which would have costed you time to write/maintain).\n\nNo matter what, some cost is inescapable (in money, time, etc) to solve this challenging technical problem.\n\nOur focus is solely on foundational Voice AI orchestration, & it’s what we put our full time and resources into.\n\nTo learn more about Vapi’s pricing, you can visit our [pricing page](/pricing).\n\n  </Accordion>\n  <Accordion title=\"Is it going to be hard to set up?\" icon=\"gear\" iconType=\"solid\" defaultOpen={false}>\n\n    No — in fact, the setup could not be easier:\n    - **Web Dashboard:** It can take minutes to get up & running with our [dashboard](https://dashboard.vapi.ai/).\n    - **Client SDKs:** You can start calls with 1 line of code with any of our [client SDKs](/sdks).\n\n    For more advanced features like function calling, you will have to set up a [Server URL](/server-url) to receive and respond to messages.\n\n  </Accordion>\n  <Accordion title=\"How is Vapi different from other Voice AI services?\" icon=\"bowling-pins\" iconType=\"solid\" defaultOpen={false}>\n\n    Vapi focuses on developers. Giving developers modular, simple, & robust tooling to build any voice AI application imaginable.\n\n    Vapi also has some of the lowest latency & (equally important) highest reliability amongst any other voice AI platform built for developers.\n\n  </Accordion>\n</AccordionGroup>\n\n"
    },
    "community/appointment-scheduling.mdx": {
      "markdown": "---\ntitle: Appointment Scheduling\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/appointment-scheduling\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n    <iframe\n        src=\"https://www.youtube.com/embed/F57M9ljJkMU?si=wCtu42Jzu6fC6th_\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/kDX9JlPxGzE?si=XBxwUcskqGliBBSX\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/5p6rML68cKE?si=0wHuyQCQFHvkRk66\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/FMYc0KRYRPM?si=72tA_8NiYjkkLNYS\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/uqBG7nf1vZk?si=hH8UooV1l6hIvNXG\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/hCJb11EOdME?si=nMmUXuOnT6psbNBP\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/fS3OIrAHWwQ\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/2btAigfVxIE\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/y-GMretSYEw?si=xt_IdlLbRxXazCvB\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/p-Ns7kgZALk?si=6pkNV2zWo-TrwxV-\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/hPtvSolWJgU?si=4CcU3TLDPe3AnhXW\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/hwneiBSbC4k?si=Z_YOVnXQw3YaExdj\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/comparisons.mdx": {
      "markdown": "---\ntitle: Comparisons\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/comparisons\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/KloYd6cANkM?si=ssM9ouDeCeyFe1hv\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/iEi_zion9jM?si=J5Ik5GJ24SUt-PDr&amp;start=19\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/5pKInI77zNk?si=N7Xdit2mTN7kbm6o&amp;start=1508\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/nykm4h0gFV4?si=5mlDF4uGRwKbNn5l\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/hO6hoJGheLA?si=onLVeZhYWmo1hG3B\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/rc0XGjI4QKM\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/Cp_lmp_05ww?si=jQhA1y0vnuuQseBR\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  \n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/7K8vSheVnic?si=pa1VVij50Q6Znrac\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  \n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/GMCDPIxVIPg?si=OHgOrNn9QBZHAdNh\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/conferences.mdx": {
      "markdown": "---\ntitle: Conferences\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/conferences\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/jag7NjaROck?si=OLFbkgF9YDBw0ufs&amp;start=415\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n</div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/demos.mdx": {
      "markdown": "---\ntitle: Demos\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/demos\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/H6qym392wFg?si=GC2anHDMMbPcG7xF\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n\n  <iframe\n    src=\"https://www.youtube.com/embed/Gda0Le__n8g?si=jNEMbdr7WIbf_PQk\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n\n</div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/ghl.mdx": {
      "markdown": "---\ntitle: GoHighLevel\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/ghl\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n    <iframe\n        src=\"https://www.youtube.com/embed/PVP1P2nak4M?si=vGGAMZVI3Fzzik9X\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/DpnC8NX4tas\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/guide.mdx": {
      "markdown": "---\ntitle: Guide\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/guide\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n    <iframe\n        src=\"https://www.youtube.com/embed/NyZ0AYdTKVw?si=F-2ZxlOpC5rxv7Wh\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/3LSXJECXpkc?si=hhWsXZeFYC6wM-cq\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/cuQI3UH2lDE?si=chPXGUvgN7FoQQFb\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/RMOHpWAPan8?si=44FZKzxfxxXndR3i\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/HuF7ELckcyU?si=PPPFZE5aiI-WgP2U\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/jg1j1Rv8SkA?si=V9IZJObqPF6-iXSS\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/u-8xdblVY_4?si=_8YYFR44n3SevrSb\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/jDPXWMUVUPE?si=FrNKyc2oTZIuNysu\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/AH5oS2w3EcQ?si=PrskTE6KXHA0i9v2\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/gN6CsDtLnMs?si=SkGG6LzS9wIXrh_-\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/3y7GJ9Vozkk?si=0lBabiKaMjMAOz8Y\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/w7yHxLSvXnU?si=LNsPkkk0K5F2ccy8\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/QzO22FT2W9Q?si=JuLipGu1spTZY3pM\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/STzxtEf8Fu0?si=mCtn52wCM8m8eIfo\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/J24SNZNjB6o?si=k1vi5RsYTpzzuzjp\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/iWZD_HBk9zQ?si=ngKqJu4NIecL9TTT\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/nyzSRnuPAS8?si=w89PVdgvenwFbkZj\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/EFJoQ6wwf9I?si=0CnSsAcW1zNshh01\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/FQjWcxfIo2g\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n    <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/2VA2mvzugts\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n\n/>\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/Nj5nirt_m6c\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n{\" \"}\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/Nj5nirt_m6c?si=tXeyFGEOwEVM3W_7\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/INQC85uTID4?si=jjHc8bzk0S6agfzZ\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n<iframe width=\"100%\" height=\"315\" src=\"https://www.youtube.com/embed/sricQothccU?si=5ZdC9RZVrILIE4YV\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen />\n\n    <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/_pawGoj0Re8?si=zJDTyY25LKlCvITl\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n\n/>\n\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/inbound.mdx": {
      "markdown": "---\ntitle: Inbound\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/inbound\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n    <iframe\n        src=\"https://www.youtube.com/embed/ai32iXHj8fc?si=z8PKMD8Dklpg_j0B\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/mPC-YOmidqE\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n    <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/xsDc8ALGaeE\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n\n/>\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/NCjdEREIyR8?si=bKiyp65Qisbbq_4r\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/Kg1sOISqKiE?si=kdpFMfFq6w13c__Z\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/knowledgebase.mdx": {
      "markdown": "---\ntitle: Knowledgebase\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/knowledgebase\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/9MD1VM7038Q?si=G1at__w9gAm0dqEA\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/xCdfLSrwbjc?si=xn1jIHmV4J7VTsTr\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/g0SKKwBpp7g?si=b8uXyopKo9fZYowX\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/4_9IOCiC7hc\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/outbound.mdx": {
      "markdown": "---\ntitle: Outbound\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/outbound\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n    <iframe\n        src=\"https://www.youtube.com/embed/gi5Qa0Z2iqQ?si=l1FLFu5TvuTxYyIc\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/IbgPEG8l09Y?si=jIFLBN_SKPShurfy\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/jJjD5UsO46o?si=ATafEm5RDmt-f13I\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n\n{\" \"}\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/Sj-OOK11Nac?si=laIZ3JasRWIF1vKh\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/t_35BMnOTDY?si=SO4m9QEqg4sxT9BY\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n    <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/BMjSnRfcL7g?si=5ZW1Qr1tEecBCNgt\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n\n/>\n\n<iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/d7OQJ83XsBE?si=ebPYoX04ImtZl92U\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n\n/>\n\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/podcast.mdx": {
      "markdown": "---\ntitle: Podcast\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/podcast\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/kOhr047QFFA?si=8-2uY09fni5195tx&amp;start=1245\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n</div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/snippets-sdks-tutorials.mdx": {
      "markdown": "---\ntitle: Snippets & SDKs Tutorials\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/snippets-sdks-tutorials\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n    <iframe\n        src=\"https://www.youtube.com/embed/wEiiDEGECb4?si=uuJZ3gcr9iQDv9tZ\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/yMkaa-15CWI?si=5dW45BVC-IteS18E\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/XOVt1qHmrlg?si=NXypsXXAetLkgZU9\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n    <iframe\n        src=\"https://www.youtube.com/embed/6O-8LimQST4?si=4ePsmegoe2CVBnwy\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        allowfullscreen\n    />\n\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/aM79mkF6UkA?si=PYe7zAwKoA0wzrEE\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/special-mentions.mdx": {
      "markdown": "---\ntitle: Special Mentions\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/special-mentions\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/WCYf2Agml-s?si=dAMT_Xf7vPHmjqKJ&amp;start=929\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/rc0XGjI4QKM\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n\n<iframe\n  width=\"100%\"\n  height=\"315\"\n  src=\"https://www.youtube.com/embed/1VzKEEbTYUQ?si=rZjcTPNe4Ro1leQ9\"\n  title=\"YouTube video player\"\n  frameborder=\"0\"\n  allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n  referrerpolicy=\"strict-origin-when-cross-origin\"\n  allowfullscreen\n/>\n\n<iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/nYKFuI6sagw?si=oh9pcwKVnUamEIdV\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n</div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/squads.mdx": {
      "markdown": "---\ntitle: Squads\nslug: community/squads\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/uT7mW61H0nw?si=eN_n2c2umNNRtxIw\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/n8oFkp0_2qE?si=Egsv56Nfx-Dkl_b4\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/uJ52-EqBscQ?si=GeJJCUcptinCqRRg\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/V308U_5syiA?si=DQycim7-WhzVOsQp\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n\n  <iframe\n    width=\"100%\"\n    height=\"315\"\n    src=\"https://www.youtube.com/embed/gdNkUESKC5k?si=13psOtVhyjWd6Ww8\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  </div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/television.mdx": {
      "markdown": "---\ntitle: Television\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/television\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/pRUddK6sxDg?si=pvlgT0ban0lkvHTL\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n</div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/usecase.mdx": {
      "markdown": "---\ntitle: Usecase\nsubtitle: Videos showcasing Vapi out in the wild.\nslug: community/usecase\n---\n\n\nHere are some videos made by people in our community showcasing what Vapi can do:\n\n<div class=\"video-grid\">\n  <iframe\n    src=\"https://www.youtube.com/embed/WS4QJF9Bn7U?si=yrK1ErRpFZAKbpYW\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/ZvPQU1VKmi8?si=neKdm1K8T-Tex56r\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/CvhonBxoJ00?si=POAHHoKAvKQx7__O\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/lmHMAJxlD0I?si=sHfjjxJxTah21VTR\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/5ElOMT7cyJM?si=Pc_XE9CLAZs5qfkZ\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n  <iframe\n    src=\"https://www.youtube.com/embed/VVKyogARy6A?si=KbKsdCact_ucdKQB\"\n    title=\"YouTube video player\"\n    frameborder=\"0\"\n    allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n    referrerpolicy=\"strict-origin-when-cross-origin\"\n    allowfullscreen\n  />\n</div>\n\n## Send Us Your Video\n\nHave a video showcasing Vapi that you want us to feature? Let us know:\n\n<CardGroup cols={2}>\n  <Card\n    title=\"Send Us Your Video\"\n    icon=\"video-arrow-up-right\"\n    iconType=\"solid\"\n    href=\"https://tally.so/r/3yD9Wx\"\n  >\n    Send us your video showcasing what Vapi can do, we'd like to feature it.\n  </Card>\n</CardGroup>\n"
    },
    "community/myvapi.mdx": {
      "markdown": "---\ntitle: My Vapi\nslug: community/myvapi\n---\n\n\nHere is the updated MyVapi User Guide, including the customer endpoints and noting that MyVapi uses 27 out of the 33 available VAPI APIs:\n# MyVapi User Guide\n\nWelcome to MyVapi! This guide will help you get started with using MyVapi, your custom GPT, to enhance your productivity and streamline your tasks. Follow the steps below to make the most out of this powerful tool.\n\n## Table of Contents\n- [Introduction to MyVapi](#introduction-to-myvapi)\n- [Getting Started](#getting-started)\n- [Accessing MyVapi](#accessing-myvapi)\n- [Using MyVapi](#using-myvapi)\n  - [Basic Commands](#basic-commands)\n- [Tips and Best Practices](#tips-and-best-practices)\n- [Troubleshooting](#troubleshooting)\n- [FAQ](#faq)\n\n## Introduction to MyVapi\n\n### What is MyVapi?\nMyVapi is a custom GPT designed to allow users to manage their Vapi accounts with ease. While the Vapi Dashboard provides limited functionality and using PostMan can be cumbersome, MyVapi offers a streamlined solution to interact with the Vapi API directly. This eliminates the back-and-forth usually associated with manual API interactions and JSON validation, making the process more efficient and user-friendly. The reason MyVapi was created is to help users understand the power of using VAPI's API. MyVapi uses 27 out of the 33 available VAPI APIs.\n\n### Key Features\n- **Full API Access:** Leverage the full power of the Vapi API without the limitations of the Dashboard.\n- **Efficient Workflow:** Avoid the tedious back-and-forth of using PostMan and JSON validators.\n- **Voice Assistant Creation:** Simplify the process of creating voice assistants with the Vapi API.\n- **Troubleshooting:** Get real-time help and troubleshooting advice from ChatGPT.\n\n### Benefits of Using MyVapi\n- **Streamlined Management:** Manage your Vapi account more effectively and efficiently.\n- **Increased Productivity:** Save time and reduce effort in creating and managing voice assistants.\n- **Enhanced Support:** Receive guidance and support directly from ChatGPT to resolve any issues you encounter.\n\n### Additional Information\nMyVapi is not connected to a user's account but will help with almost anything you need help with. This includes creating transient assistants, creating tools, getting information about a call, and more.\n\n## Getting Started\n\n### Accessing MyVapi\nMyVapi can be accessed in the following ways:\n- Visit [https://chatgpt.com/g/g-3luI9WIdj-myvapi](https://chatgpt.com/g/g-3luI9WIdj-myvapi)\n- Search for \"MyVapi\" in the GPT Store\n\nMyVapi is available to both free and paid ChatGPT accounts.\n\n## Using MyVapi\n\n### Basic Commands\nMyVapi provides a range of commands to interact with your Vapi account efficiently. Below are the basic commands and their functions:\n\n#### Assistant Management\n- **Get Assistants**\n  - **Method:** GET\n  - **Endpoint:** /assistant\n  - **Description:** Retrieve a list of all assistants.\n\n- **Create Assistant**\n  - **Method:** POST\n  - **Endpoint:** /assistant\n  - **Description:** Create a new assistant.\n\n- **Get Assistant by ID**\n  - **Method:** GET\n  - **Endpoint:** /assistant/{id}\n  - **Description:** Retrieve details of a specific assistant using its ID.\n\n- **Update Assistant by ID**\n  - **Method:** PATCH\n  - **Endpoint:** /assistant/{id}\n  - **Description:** Update details of a specific assistant using its ID.\n\n- **Delete Assistant by ID**\n  - **Method:** DELETE\n  - **Endpoint:** /assistant/{id}\n  - **Description:** Delete a specific assistant using its ID.\n\n#### Phone Call Management\n- **Get Phone Calls**\n  - **Method:** GET\n  - **Endpoint:** /call\n  - **Description:** Retrieve a list of all phone calls.\n\n- **Get Phone Call by ID**\n  - **Method:** GET\n  - **Endpoint:** /call/{id}\n  - **Description:** Retrieve details of a specific phone call using its ID.\n\n- **Create Phone Call**\n  - **Method:** POST\n  - **Endpoint:** /call/phone\n  - **Description:** Create a new phone call.\n\n- **Update Phone Call by ID**\n  - **Method:** PATCH\n  - **Endpoint:** /call/{id}\n  - **Description:** Update the details of a specific phone call by its ID.\n\n- **Delete Phone Call by ID**\n  - **Method:** DELETE\n  - **Endpoint:** /call/{id}\n  - **Description:** Delete a specific phone call by its ID.\n\n- **Get Call Logs**\n  - **Method:** GET\n  - **Endpoint:** /log\n  - **Description:** Retrieve call logs.\n\n#### Squad Management\n- **Get Squads**\n  - **Method:** GET\n  - **Endpoint:** /squad\n  - **Description:** Retrieve a list of all squads.\n\n- **Create Squad**\n  - **Method:** POST\n  - **Endpoint:** /squad\n  - **Description:** Create a new squad.\n\n- **Get Squad by ID**\n  - **Method:** GET\n  - **Endpoint:** /squad/{id}\n  - **Description:** Retrieve details of a specific squad using its ID.\n\n- **Update Squad by ID**\n  - **Method:** PATCH\n  - **Endpoint:** /squad/{id}\n  - **Description:** Update details of a specific squad using its ID.\n\n- **Delete Squad by ID**\n  - **Method:** DELETE\n  - **Endpoint:** /squad/{id}\n  - **Description:** Delete a specific squad using its ID.\n\n#### Metrics Management\n- **Get Metrics**\n  - **Method:** GET\n  - **Endpoint:** /metrics\n  - **Description:** Retrieve metrics data.\n\n#### Tool Management\n- **List Tools**\n  - **Method:** GET\n  - **Endpoint:** /tool\n  - **Description:** Retrieve a list of all tools.\n\n- **Create Tool**\n  - **Method:** POST\n  - **Endpoint:** /tool\n  - **Description:** Create a new tool.\n\n- **Get Tool by ID**\n  - **Method:** GET\n  - **Endpoint:** /tool/{id}\n  - **Description:** Retrieve details of a specific tool using its ID.\n\n- **Update Tool by ID**\n  - **Method:** PATCH\n  - **Endpoint:** /tool/{id}\n  - **Description:** Update details of a specific tool using its ID.\n\n- **Delete Tool by ID**\n  - **Method:** DELETE\n  - **Endpoint:** /tool/{id}\n  - **Description:** Delete a specific tool using its ID.\n\n#### Customer Management\n- **Get Customers**\n  - **Method:** GET\n  - **Endpoint:** /customer\n  - **Description:** Retrieve a list of all customers.\n\n- **Create Customer**\n  - **Method:** POST\n  - **Endpoint:** /customer\n  - **Description:** Create a new customer.\n\n- **Get Customer by ID**\n  - **Method:** GET\n  - **Endpoint:** /customer/{id}\n  - **Description:** Retrieve details of a specific customer using its ID.\n\n- **Update Customer by ID**\n  - **Method:** PATCH\n  - **Endpoint:** /customer/{id}\n  - **Description:** Update details of a specific customer using its ID.\n\n- **Delete Customer by ID**\n  - **Method:** DELETE\n  - **Endpoint:** /customer/{id}\n  - **Description:** Delete a specific customer using its ID.\n\n## Tips and Best Practices\n- **Be Specific:** The more specific your request, the better MyVapi can assist you.\n- **Explore Features:** Take time to explore all the features and find what works best for you.\n- **Regular Updates:** Keep your account information and settings up-to-date for the best experience.\n\n## Troubleshooting\nIf you encounter any issues while using MyVapi, try the following steps:\n1. **Check Internet Connection:** Ensure you have a stable internet connection.\n2. **Clear Cache:** Sometimes clearing your browser cache can resolve issues.\n3. **Restart Browser:** Close and reopen your browser to refresh the session.\n\n## FAQ\n**Q:** Is MyVapi free to use?  \n**A:** MyVapi is available to both free and paid ChatGPT accounts.\n\n**Q:** How secure is my data?  \n**A:** We prioritize your data security and use advanced encryption methods to protect your information.\n"
    },
    "community/expert-directory.mdx": {
      "markdown": "---\ntitle: Expert Directory\nsubtitle: Certified Voice AI Expert - Vapi\nslug: community/expert-directory\n---\n\n\nWant to maximize your Voice AI? Vapi, a certified consultant, specializes in building Voice AI bots.\n\nWhether you need help deciding what to automate or assistance in building it, Vapi Experts have proven their expertise by supporting users and creating valuable video content for the community. Find the right fit here.\n\n<div class=\"video-grid\">\n<Card\n  href=\"https://www.6omb.ai/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://media.licdn.com/dms/image/v2/C560BAQGqQcUUy_uaxg/company-logo_200_200/company-logo_200_200/0/1630629573081?e=2147483647&v=beta&t=6jjsYZqRd9p7K_IBntckGVkVfMm51PxMy060U2AP3dA\"\n  />\n  <div className=\"card-content\">\n    <h3>6omb</h3>\n    <p>Voice agents and custom product development.</p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://www.aitoflo.com/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://i.ibb.co/ckdM9nK/aitoflo-b200x-p-1080.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Aitoflo</h3>\n    <p>\n      At Aitoflo, we specialize in Voice AI and RPA services, seamlessly flowing\n      to streamline business operations and enhance customer interactions with\n      Realistic Voice AI.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://amplifyvoice.ai/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://cdn.prod.website-files.com/667a2d87749c53c580ad32de/668cca9c1617b98b5375ad1e_AMPLIFAI%20(8).png\"\n  />\n  <div className=\"card-content\">\n    <h3>Amplify Voice</h3>\n    <p>\n      Our hyper-focus on User Experience will WOW your customers. Click to Book\n      a Strategy Session.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://arose.ai/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://i.imgur.com/FQvbwsZ.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Arose AI</h3>\n    <p>\n      Arose AI creates custom Inbound Voice AI solutions for small businesses.\n      Our founder Tommy Chryst also provides 1-on-1 coaching.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"aiplaygrounds.xyz\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://i.imgur.com/nXXWCIc.png\"\n  />\n  <div className=\"card-content\">\n    <h3>AIP</h3>\n    <p>\n      Created debt collector, appointment book, customer service, website\n      assistant, etc. I, Valentino M., offer full integration and/or\n      consultation services.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://boldwaveagency.com/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://images.leadconnectorhq.com/image/f_webp/q_80/r_1200/u_https://assets.cdn.filesafe.space/y30mqCqdNhLjPaC2MefY/media/66711e425d2409e431102e40.svg\"\n  />\n  <div className=\"card-content\">\n    <h3>Boldwave</h3>\n    <p>\n      We implement voice agents to streamline appointment booking, enhance lead\n      conversion, and provide superior 24/7 customer service.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://www.brisklogic.co/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://www.brisklogic.co/wp-content/uploads/2022/03/brisk-logic-pvt-ltd-logo.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Brisk Logic </h3>\n    <p>\n      We are a AI Automation Agency that specializes in designing advanced AI\n      voice assistants capable of automating various tasks through phone\n      calls.{\" \"}\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://cold-calls.ai/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"http://cold-calls.ai/wp-content/uploads/2024/07/Cold-Calls-AI-logo.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Cold-Calls.AI</h3>\n    <p>\n      We specialize in helping companies within the German market integrate\n      Voice Agents for both inbound and outbound calls.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"dontrunoff.com\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://dontrunoff.podia.com/content-assets/public/eyJhbGciOiJIUzI1NiJ9.eyJvYmplY3Rfa2V5IjoiZGNkNmgwZTRwbm9veW5nM3lqYWc1NW5hcHYwMiIsImRvbWFpbiI6ImRvbnRydW5vZmYucG9kaWEuY29tIn0.yrT0vbilNFZX_MYiQqxflIjQq_bncWA0I-bHkZPZ1VU\"\n  />\n  <div className=\"card-content\">\n    <h3>Don't Run Off AI</h3>\n    <p>Telephone voice AI systems, prompt engineering, integrations</p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://flowzen.ai/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://uploads-ssl.webflow.com/66c61bce25dde805b35e3396/66c6b40ff7af9a4b1457422b_Logo-1%403x-100.jpg\"\n  />\n  <div className=\"card-content\">\n    <h3>Flowzen</h3>\n    <p>\n      Our agency offers Voice AI solutions using VAPI, in English and Spanish,\n      integrated with platforms like GoHighLevel, Airtable, and Make.com.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://globeai-s96libq.gamma.site/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://i.imgur.com/m8kzdXK.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Globe AI</h3>\n    <p>\n      I'm Aryan, founder of Globe AI. We build Inbound Voice Assistants for any\n      industry at any scale.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://inflate.agency/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://cdn.prod.website-files.com/66150d67f058b33ff02872c9/6620a7cd66604e301711d1ab_transpng-p-500.png\"\n  />\n  <div className=\"card-content\">\n    <h3>INFLATE AI Automation Development Services</h3>\n    <p>Building voice systems for any industry from $3k USD minimum.</p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://integraticus.com/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://integraticus.com/wp-content/uploads/2024/07/integraticus-ai-voice-agents.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Integraticus</h3>\n    <p>\n      We build AI appointment setters for Real Estate Agencies to qualify more\n      leads, handle tailored outreach, and make sure you have higher margins\n      than ever before.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://klen.ai\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://drq7swnhvxeif.cloudfront.net/static/assets/media/logos/Klen-Light.svg\"\n  />\n  <div className=\"card-content\">\n    <h3>Klen AI</h3>\n    <p>\n      Custom AI voice assistants to handle calls, pre-qualify leads, schedule\n      appointments, etc. Utilizing Vapi for seamless integration and\n      productivity\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"lunarisai.com\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://i.imgur.com/d4jMx0p.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Lunaris AI</h3>\n    <p>We create Voice Agents for all types of businesses.</p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://www.upwork.com/freelancers/~01790044d9e078fd70\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://assets.static-upwork.com/org-logo/1809692048083431424?date=1721430511424\"\n  />\n  <div className=\"card-content\">\n    <h3>NukyLabs.AI</h3>\n    <p>All Services for VAPI.ai Automation </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://otakusolutions.io/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://storage.googleapis.com/msgsndr/dBaRxB72IyESkQWGMkmq/media/669a84c7b9609d1964adbae7.jpeg\"\n  />\n  <div className=\"card-content\">\n    <h3>Otaku Solutions</h3>\n    <p>\n      Handle the creation of voice assistants, automations, tracking, and\n      training.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://shadow-ai.co/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://i.imgur.com/5Tt41iJ.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Shadow AI</h3>\n    <p>\n      Specializing in AI-powered inbound and outbound calling operations for all\n      types of businesses using industry expertise.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://synthiq.io/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://lh3.googleusercontent.com/u/0/drive-viewer/AKGpihYpbDmAlKpgyRePNNsCrR08AUFfqRavaJfDxG-BGicVkIe958Ot8dctBsAg1I_KN-mVPdXgWZ6TcqQWFy3s7AWel2jZKAVHZgk=w2880-h1626\"\n  />\n  <div className=\"card-content\">\n    <h3>Synthiq</h3>\n    <p>\n      Multilingual AI Voice Agents (20+ countries). Any industry. Use your\n      existing number. Expert AI consulting available.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"Https://www.TemporalLab.com\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://assets.zyrosite.com/cdn-cgi/image/format=auto,w=450,fit=crop,q=95/A1aoblXx2KSKGq4r/tlclogorawnameonly-Awvk565gvMfLKVr4.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Temporal Labs LLC </h3>\n    <p>\n      Temporal Labs LLC offers a unique solution through our parametrized,\n      development community, with industry specific solutions and engagement.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"vatech.io\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://cdn.prod.website-files.com/662a910cb8589f49140e52f6/662a923b33ada78dc4b8946a_6141bc0adf30793a0a262fd8_Group%2013.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Value Added Tech</h3>\n    <p>\n      Top-notch automation company. We specialise in Make.com (Silver partner),\n      multiple CRMs and VAPI.\n    </p>\n  </div>\n</Card>\n\n<Card\n  href=\"https://www.strinq.com/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://cdn.prod.website-files.com/6618155a2af8783e967b6895/669f26093eb73315245fcb95_Add%20a%20heading.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Strinq</h3>\n    <p>\n    Strinq develops custom voice AI solutions for enterprises, offering bespoke software and high-quality human voice models.\n    </p>\n  </div>\n</Card>\n<Card\n  href=\"https://www.iffort.ai/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://i.imgur.com/M6z8aS9.png\"\n  />\n  <div className=\"card-content\">\n    <h3>iffort.ai</h3>\n    <p>\n  We revolutionize your business communication with our conversational agents, turning traditional chats and calls into effortless conversations.\n    </p>\n  </div>\n</Card>\n<Card\n  href=\"https://www.msquare.pro/\"\n  className=\"ed-card\"\n  style={{\n    alignItems: \"center\",\n    display: \"flex\",\n    flexDirection: \"column\",\n    width: \"auto\",\n  }}\n>\n  <img\n    className=\"card-img bg:white\"\n    noZoom\n    src=\"https://static.wixstatic.com/media/cfe1a7_b6db3651b9cc43ee9d64edb1bfa674c0~mv2.png/v1/crop/x_520,y_485,w_5293,h_1131/fill/w_428,h_92,al_c,q_95,enc_auto/M%20square%20Logo_RGB-01.png\"\n  />\n  <div className=\"card-content\">\n    <h3>Msquare Automation</h3>\n    <p>\n  Experts in AI voice assistants and business automation. Affordable, quality service from India. Gold partners of Make.com\n  </p>\n  </div>\n</Card>\n</div>\n"
    },
    "providers/voice/elevenlabs.mdx": {
      "markdown": "---\ntitle: ElevenLabs\nsubtitle: How Vapi Integrates Text-to-Speech Platforms?\nslug: providers/voice/elevenlabs\n---\n\n\n# How Vapi Integrates Text-to-Speech Platforms: ElevenLabs\n\nIn the realm of voice AI development, integrating cutting-edge text-to-speech (TTS) platforms is crucial for creating natural and engaging conversational experiences. This guide explores how developers can leverage our voice AI platform to seamlessly incorporate advanced TTS services like ElevenLabs, enabling the creation of sophisticated voice-driven applications with remarkable efficiency.\n\n## Understanding the Voice AI Platform\n\nOur platform serves as a comprehensive toolkit for developers, designed to simplify the complexities inherent in voice AI development. By abstracting intricate technical details, it allows developers to focus on crafting the core business logic of their applications rather than grappling with low-level implementation challenges.\n\n### Key Components of the Voice AI Architecture\n\nAt the heart of our platform lies a robust architecture comprising three essential components:\n\n1. Automatic Speech Recognition (ASR)\n2. Large Language Model (LLM) processing\n3. Text-to-Speech (TTS) integration\n\nThese components work in concert to facilitate seamless voice interactions. The ASR module captures and processes audio inputs, converting spoken words into digital data. The LLM processing unit analyzes this data, interpreting context and generating appropriate responses. Finally, the TTS integration transforms these responses back into natural-sounding speech.\n\n## Integration with Text-to-Speech Platforms\n\nOur approach to integrating external TTS services, such as ElevenLabs, is designed to be both flexible and powerful. By incorporating advanced TTS platforms, developers can significantly enhance the quality and versatility of their voice AI applications.\n\n### ElevenLabs Integration: A Technical Deep Dive\n\nThe integration with ElevenLabs' AI speech synthesis exemplifies our commitment to providing developers with state-of-the-art tools. This integration process involves several key technical aspects:\n\n1. **API Integration**: Our platform seamlessly connects with ElevenLabs' API, allowing for efficient data exchange and real-time speech synthesis.\n\n2. **Voice Model Selection**: Developers can choose from a range of voice models provided by ElevenLabs, each with unique characteristics and tonal qualities.\n\n3. **Parameter Control**: Fine-tuning of speech parameters such as speed, pitch, and emphasis is made accessible through our intuitive interface.\n\n4. **Data Flow Optimization**: We've implemented efficient data handling mechanisms to ensure smooth transmission between our platform and ElevenLabs' servers, minimizing latency and maintaining high-quality output.\n\n## Advanced Features of the Integration\n\nThe integration of ElevenLabs' technology brings forth a suite of advanced features that elevate the capabilities of voice AI applications.\n\n### Contextual Awareness in Speech Synthesis\n\nBy leveraging ElevenLabs' sophisticated algorithms, our platform enables AI-generated speech that demonstrates a high degree of contextual awareness. This results in more natural-sounding conversations that can adapt to the nuances of different scenarios and user interactions.\n\n### Enhanced Voice Modulation and Emotional Expression\n\nThe integration allows for precise control over voice modulation and emotional expression. Developers can craft AI voices that convey a wide range of emotions, from excitement to empathy, enhancing the overall user experience and making interactions more engaging and human-like.\n\n### Real-time Audio Streaming Capabilities\n\nOne of the most compelling features of our integration is the ability to leverage ElevenLabs' streaming capabilities for real-time applications. This functionality is crucial for creating responsive voice AI systems that can engage in dynamic, live interactions.\n\nImplementing low-latency voice synthesis presents several technical challenges, including:\n\n- **Network Latency Management**: Minimizing delays in data transmission between our platform, ElevenLabs' servers, and the end-user's device.\n- **Buffer Optimization**: Balancing audio quality with real-time performance through careful buffer management.\n- **Adaptive Bitrate Streaming**: Implementing techniques to adjust audio quality based on network conditions, ensuring consistent performance across various environments.\n\nOur platform addresses these challenges through advanced streaming protocols and optimized data handling, enabling developers to create voice AI applications that respond with near-human speed and fluidity.\n\n## Developer Tools and Resources\n\nTo facilitate the integration process, we provide a comprehensive set of developer tools and resources:\n\n- **SDKs**: Open-source software development kits available on GitHub, supporting multiple programming languages.\n- **Documentation**: Detailed API references and conceptual guides covering key aspects of voice AI development.\n- **Quickstart Guides**: Step-by-step tutorials to help developers get up and running quickly.\n- **End-to-End Examples**: Sample implementations of common voice workflows, including outbound sales calls, inbound support interactions, and web-based voice interfaces.\n\n### Building Custom Voice AI Applications\n\nDevelopers can follow these steps to create voice AI applications with integrated TTS:\n\n1. **Define the Use Case**: Clearly outline the objectives and scope of the voice AI application.\n2. **Select the Appropriate Voice Model**: Choose an ElevenLabs voice that aligns with the application's tone and purpose.\n3. **Implement Core Logic**: Utilize our SDKs to implement the application's business logic and conversation flow.\n4. **Configure TTS Parameters**: Fine-tune speech synthesis settings to achieve the desired voice characteristics.\n5. **Test and Iterate**: Conduct thorough testing to ensure natural conversation flow and appropriate responses.\n6. **Optimize Performance**: Leverage our platform's analytics tools to identify and address any performance bottlenecks.\n\nBest practices for optimizing voice AI performance and user experience include:\n\n- Implementing effective error handling and fallback mechanisms\n- Designing clear and concise conversation flows\n- Regularly updating and refining language models based on user interactions\n- Optimizing for low-latency responses to maintain natural conversation cadence\n\n## Use Cases and Applications\n\nThe integration of advanced TTS platforms opens up a myriad of possibilities across various industries:\n\n- **Customer Service**: Creating empathetic and efficient AI-powered support agents.\n- **Education**: Developing interactive language learning tools with native-speaker quality pronunciation.\n- **Healthcare**: Building voice-based assistants for patient engagement and medical information delivery.\n- **Entertainment**: Crafting immersive storytelling experiences with dynamically generated character voices.\n\nDevelopers can leverage this integration to create unique voice-based solutions that were previously challenging or impossible to implement with traditional TTS technologies.\n\n## Future Developments and Potential\n\nAs the field of voice AI continues to advance, our platform is poised to incorporate new features and improvements in TTS integration capabilities. Upcoming developments may include:\n\n- Enhanced multilingual support for global applications\n- More sophisticated emotional intelligence in voice synthesis\n- Improved personalization capabilities, allowing for voice adaptation based on user preferences\n\nThe future of voice AI development is likely to see increased focus on natural language understanding, context-aware responses, and seamless multi-modal interactions. Our platform is well-positioned to address these trends, providing developers with the tools they need to stay at the forefront of voice technology innovation.\n\n## Conclusion\n\nThe integration of advanced text-to-speech platforms like ElevenLabs into our voice AI development ecosystem represents a significant leap forward for developers seeking to create sophisticated, natural-sounding voice applications. By abstracting complex technical challenges and providing robust tools and resources, we enable developers to focus on innovation and creativity in their voice AI projects. As the technology continues to evolve, our platform will remain at the cutting edge, empowering developers to build the next generation of voice-driven experiences.\n"
    },
    "providers/voice/playht.mdx": {
      "markdown": "---\ntitle: PlayHT\nsubtitle: What is PlayHT?\nslug: providers/voice/playht\n---\n\n\n**What is PlayHT?**\n\nIn the dynamic world of artificial intelligence, PlayHT emerges as a leading provider of voice AI solutions. Specializing in text-to-speech (TTS) and voice cloning technologies, PlayHT delivers highly realistic and versatile AI-generated voices that cater to a wide array of applications. From enhancing marketing videos to making content more accessible, PlayHT’s innovative tools empower users to create engaging and professional-grade audio content effortlessly.\n\n**The Evolution of AI Voice Technology:**\n\nAI voice technology has significantly evolved over the past decade. Initially limited to robotic and monotone outputs, advancements in machine learning and neural networks have paved the way for natural and expressive voice synthesis. PlayHT has harnessed these advancements to offer superior AI voices that are nearly indistinguishable from human speech, setting a new standard in the industry.\n\n**Overview of PlayHT’s Offerings:**\n\nPlayHT provides a robust suite of voice AI tools designed to meet diverse needs:\n\n**Text to Speech:**\n\n- PlayHT’s TTS technology converts written text into highly realistic speech, making it ideal for creating voiceovers, audiobooks, and other spoken content. This technology supports over 142 languages and accents, allowing users to generate audio content that is not only clear and engaging but also linguistically diverse.\n\n**Voice Cloning:**\n\n- PlayHT’s voice cloning feature enables users to create digital replicas of voices with high accuracy. This is particularly useful for preserving voices, personalizing digital assistants, and generating unique character voices for media and entertainment. The cloned voices maintain the nuances and emotional expressiveness of the original, ensuring a lifelike audio experience.\n\n**Voice Generation API:**\n\n- PlayHT offers a Voice Generation API that allows developers to integrate AI voice capabilities into their applications. This API supports real-time voice synthesis and cloning, providing a flexible and powerful solution for various interactive applications, including chatbots, virtual assistants, and gaming.\n\n**Use Cases for PlayHT:**\n\n- The applications of PlayHT’s technology are extensive and impactful:\n\n**Marketing:**\n\n- In the marketing sector, PlayHT’s realistic AI voices enhance the quality of promotional videos, explainer videos, and advertisements. Brands can create consistent and professional voiceovers that captivate audiences and convey messages effectively.\n\n**E-Learning:**\n\n- For educational content, PlayHT provides voices capable of pronouncing complex terminologies and acronyms, making e-learning materials more engaging and easier to understand. This helps in creating comprehensive and interactive training modules.\n\n**Accessibility:**\n\n- PlayHT’s TTS technology is a boon for accessibility, converting text into speech to assist individuals with visual impairments or reading difficulties. This promotes inclusivity and ensures that information is accessible to all.\n\n**Gaming:**\n\n- In the gaming industry, PlayHT’s voice cloning and TTS capabilities bring characters to life, enhancing the overall gaming experience. Developers can quickly generate high-quality voiceovers for dialogues, narration, and character interactions.\n\n**Impact on Content Creation:**\n\n- PlayHT is revolutionizing content creation by offering tools that are both powerful and user-friendly. By enabling creators to produce high-quality audio content quickly and efficiently, PlayHT reduces the time and costs associated with traditional recording methods. This democratizes access to professional-grade audio production, fostering innovation and creativity across various domains.\n\n**Innovation and Research:**\n\nCommitted to pushing the boundaries of voice AI, PlayHT invests in continuous research and development. Their team of experts focuses on enhancing the quality, expressiveness, and versatility of AI-generated voices, exploring new applications, and refining existing technologies.\n\n**AI Safety and Ethics:**\n\nPlayHT prioritizes the ethical use of AI technology. They have implemented stringent safeguards to prevent misuse and are actively engaged in discussions about the responsible development and deployment of AI. Ensuring the privacy and security of users’ data is a core aspect of their operations.\n\n**Integrations and Compatibility:**\n\nPlayHT’s Voice Generation API enables seamless integration with various platforms and applications. This flexibility ensures that users can incorporate PlayHT’s voice AI capabilities into their existing systems without any hassle, streamlining workflows and enhancing functionality.\n"
    },
    "providers/voice/azure.mdx": {
      "markdown": "---\ntitle: Azure\nsubtitle: What is Microsoft Azure?\nslug: providers/voice/azure\n---\n\n\n**What is Microsoft Azure?**\n\nMicrosoft Azure is a comprehensive cloud computing platform that provides a wide array of services, including computing power, storage solutions, and advanced analytics. As a leader in cloud technology, Azure enables businesses and developers to build, manage, and deploy applications on a global network. With its robust infrastructure and extensive suite of tools, Azure supports diverse workloads, from simple web apps to complex AI and machine learning models.\n\n**The Evolution of Cloud Computing:**\n\nCloud computing has revolutionized how businesses operate by providing scalable and flexible IT resources over the internet. Early cloud solutions were limited in scope and performance, but advances in virtualization, networking, and storage have transformed the cloud into a versatile and powerful platform. Microsoft Azure has been at the forefront of this evolution, continually enhancing its capabilities to meet the growing demands of modern enterprises.\n\n**Overview of Azure’s Offerings:**\n\nAzure offers a broad range of services designed to support various business needs:\n\n**Cloud Services:**\n\n- Azure’s cloud services include virtual machines, storage solutions, and databases, allowing businesses to host applications, store data, and perform complex computations. These services provide the scalability and flexibility needed to handle dynamic workloads and ensure business continuity.\n\n\n**AI and Machine Learning:**\n\n- Azure’s AI and machine learning offerings include Azure AI, Cognitive Services, and Azure Machine Learning. These tools enable developers to build intelligent applications that can see, hear, speak, and understand. Azure AI provides pre-built models and APIs for tasks like natural language processing, computer vision, and speech recognition.\n\n\n**DevOps and Development:**\n\n- Azure DevOps integrates with GitHub and other development tools to streamline the software development lifecycle. It offers continuous integration and continuous delivery (CI/CD) pipelines, version control, and project management tools, helping teams collaborate more effectively and deliver high-quality software faster.\n\n**Cloud Services:**\n\n- Azure’s cloud services are designed to meet the needs of businesses of all sizes:\n\n**Compute:**\n\n- Azure provides a range of compute options, including virtual machines, containers, and serverless computing. These services allow businesses to run applications and workloads in the cloud without worrying about underlying hardware.\n\n**Storage:**\n\n- Azure offers scalable and secure storage solutions, including Blob Storage, Disk Storage, and File Storage. These services ensure that businesses can store and manage large volumes of data efficiently and reliably.\n\n**Databases:**\n\n- Azure’s database services include SQL Database, Cosmos DB, and Database for PostgreSQL. These managed database solutions offer high availability, security, and performance, making it easy to build and scale data-driven applications.\n\n**AI and Machine Learning:**\n\n- Azure’s AI and machine learning services empower developers to create intelligent applications:\n\n**Azure AI:**\n\n- Azure AI provides a suite of pre-built AI models and APIs that can be easily integrated into applications. These models cover a wide range of AI capabilities, from language understanding to image recognition.\n\n**Cognitive Services:**\n\n- Azure Cognitive Services offer APIs for vision, speech, language, and decision-making. These services allow developers to add sophisticated AI features to their applications with minimal effort.\n\n**Azure Machine Learning:**\n\n- Azure Machine Learning is a cloud-based service that enables data scientists and developers to build, train, and deploy machine learning models. It supports a variety of frameworks and tools, making it a versatile solution for AI projects.\n\n**DevOps and Development:**\n\n- Azure provides comprehensive tools for DevOps and software development:\n\n**Azure DevOps:**\n\n- Azure DevOps offers a suite of tools for managing the entire software development lifecycle. It includes services for version control, build automation, release management, and project tracking, enabling teams to deliver software more efficiently.\n\n**GitHub Integration:**\n\nAzure integrates seamlessly with GitHub, allowing developers to use their preferred version control platform while taking advantage of Azure’s CI/CD capabilities. This integration supports collaboration and accelerates the development process.\n\n**Development Tools:**\n\n- Azure offers a range of development tools, including Visual Studio and Visual Studio Code. These tools provide robust features for coding, debugging, and deploying applications, making it easier for developers to build high-quality software.\n\n**Use Cases for Azure:**\n\n- Azure’s versatile platform supports a wide range of use cases:\n\n**Business Solutions:**\n\n- Azure provides solutions for various business needs, including enterprise resource planning (ERP), customer relationship management (CRM), and business intelligence (BI). These solutions help businesses optimize operations and make data-driven decisions.\n\n**Application Development:**\n\n- Azure supports the development of modern applications with services like App Service, Kubernetes Service, and Functions. These services enable developers to build, deploy, and scale applications quickly and efficiently.\n\n**Data Analytics:**\n\n- Azure offers powerful data analytics tools, including Synapse Analytics, Data Factory, and Databricks. These tools help businesses process and analyze large volumes of data, uncovering insights and driving better outcomes.\n\n**Impact on Digital Transformation**\n\nAzure plays a crucial role in digital transformation by providing the tools and infrastructure needed to innovate and stay competitive. Its cloud services enable businesses to scale rapidly, improve operational efficiency, and deliver new products and services to market faster. Azure’s AI and machine learning capabilities also drive innovation by enabling businesses to leverage advanced analytics and automation.\n\n**Innovation and Research:**\n\nMicrosoft is committed to continuous innovation and research in cloud computing and AI. Azure regularly introduces new features and enhancements, ensuring that businesses have access to the latest technologies. Microsoft also invests in research to advance the state of the art in AI, security, and cloud infrastructure.\n\n**AI Safety and Ethics:**\n\nEnsuring the safe and ethical use of AI is a top priority for Microsoft. Azure implements robust security measures to protect data and prevent misuse. Microsoft is also actively involved in developing ethical guidelines and best practices for AI, promoting transparency, accountability, and fairness in AI development and deployment.\n\n**Integrations and Compatibility:**\n\nAzure’s API and platform integrations ensure compatibility with a wide range of applications and services. This flexibility allows businesses to integrate Azure’s capabilities into their existing workflows and systems, enhancing functionality and improving user experience.\n"
    },
    "providers/voice/openai.mdx": {
      "markdown": "---\ntitle: OpenAI\nsubtitle: What is OpenAI?\nslug: providers/voice/openai\n---\n\n\n**What is OpenAI?**\n\nOpenAI is a leading artificial intelligence research and deployment company dedicated to ensuring that artificial general intelligence (AGI) benefits all of humanity. Founded with the mission to create safe and highly capable AI systems, OpenAI has made significant strides in AI research, producing groundbreaking models like GPT-4, DALL-E, and Codex. These innovations have not only advanced the field of AI but also transformed various industries by providing powerful tools for natural language processing, image generation, and programming assistance.\n\n**The Evolution of AI Research:**\n\nThe field of AI has evolved rapidly over the past few decades. From early rule-based systems to modern deep learning models, AI technology has made significant progress in mimicking human intelligence. OpenAI has been at the forefront of this evolution, pushing the boundaries of what AI can achieve through continuous research and development. Their work on large language models and neural networks has set new benchmarks for performance and capability in AI systems.\n\n**Overview of OpenAI’s Offerings:**\n\nOpenAI offers a range of AI-driven products and services designed to meet diverse needs:\n\n**GPT Models:**\n\n- OpenAI’s Generative Pre-trained Transformer (GPT) models, including the latest GPT-4, are state-of-the-art in natural language processing. These models can generate human-like text, answer questions, summarize information, and perform various language tasks with high accuracy. GPT-4, in particular, represents a significant leap in AI capabilities, offering improved coherence, context understanding, and creativity.\n\n**DALL-E:**\n\n- DALL-E is OpenAI’s revolutionary image generation model that creates detailed and imaginative images from text descriptions. By combining deep learning with creative processes, DALL-E can produce unique artwork, design concepts, and visual content that aligns with the given textual input. This technology opens new possibilities for artists, designers, and content creators.\n\n**Codex:**\n\n\n- Codex is an AI model that assists with programming by understanding and generating code. Integrated into tools like GitHub Copilot, Codex can help developers write code faster and more efficiently by suggesting code snippets, debugging errors, and automating repetitive tasks. This enhances productivity and reduces the barrier to entry for learning programming languages.\n\n**GPT-4 Technology:**\n\n- GPT-4 is the latest and most advanced language model developed by OpenAI. It excels in generating coherent and contextually relevant text, making it a powerful tool for various applications, including chatbots, content creation, and automated customer support. With enhanced capabilities for understanding and generating human language, GPT-4 sets a new standard in AI-driven communication.\n\n**DALL-E Image Generation:**\n\n- DALL-E takes text-to-image generation to new heights, allowing users to create visually stunning and highly specific images based on textual descriptions. This technology is particularly valuable for creative industries, where generating unique visuals quickly and accurately is essential. From concept art to marketing materials, DALL-E provides a versatile tool for visual content creation.\n\n**Codex and Programming Assistance:**\n\n- Codex transforms the way developers interact with code by providing intelligent suggestions and automating routine programming tasks. This AI-powered assistant understands multiple programming languages and can generate code snippets, making coding more accessible and efficient. Integrated into platforms like GitHub Copilot, Codex helps streamline the development process and accelerates software production.\n\n**Use Cases for OpenAI:**\n\nOpenAI’s technologies are versatile and applicable across various sectors:\n\n**Education:**\n\nIn education, GPT-4 and Codex can enhance learning experiences by providing personalized tutoring, generating educational content, and assisting with coding exercises. These tools help students grasp complex concepts and improve their programming skills.\n\n**Business:**\n\nBusinesses leverage OpenAI’s models for automating customer support, generating marketing content, and analyzing large volumes of text data. GPT-4’s ability to understand and generate human-like text enhances customer interactions and drives operational efficiency.\n\n**Creative Industries:**\n\nIn the creative sector, DALL-E and GPT-4 enable artists and writers to generate new ideas, create unique visuals, and produce high-quality content. These tools expand creative possibilities and streamline content production workflows.\n\n**Innovation and Research:**\n\nOpenAI is committed to advancing AI through continuous research and innovation. Their team of researchers and engineers works on developing new models, improving existing technologies, and exploring novel applications of AI. This commitment to innovation ensures that OpenAI remains at the cutting edge of the field.\n\n**AI Safety and Ethics:**\n\nEnsuring the safe and ethical use of AI is a core principle at OpenAI. They implement rigorous safety measures to prevent misuse and ensure that their technologies are used responsibly. OpenAI is also involved in global discussions about AI ethics and governance, contributing to the development of best practices and standards for the industry.\n\n**Integrations and Compatibility:**\n\nOpenAI’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate OpenAI’s AI capabilities into their existing systems, enhancing functionality and improving user experience. The API is designed to be flexible and user-friendly, accommodating a wide range of use cases.\n"
    },
    "providers/voice/neets.mdx": {
      "markdown": "---\ntitle: Neets\nsubtitle: What is Neets.ai?\nslug: providers/voice/neets\n---\n\n\n**What is Neets.ai?**\n\nNeets.ai is an innovative AI platform specializing in ultrafast text-to-speech (TTS) and voice cloning technology. Offering competitive pricing and extensive multilingual support, Neets.ai enables businesses and developers to generate high-quality, natural-sounding speech quickly and efficiently. Their cutting-edge solutions cater to a wide range of applications, from marketing and real-time conversations to content creation and beyond.\n\n**The Evolution of AI Speech Technology:**\n\nAI speech technology has evolved from basic, robotic outputs to sophisticated, human-like voices. Advances in deep learning and neural networks have paved the way for realistic and expressive speech synthesis. Neets.ai leverages these advancements to provide ultrafast, high-quality TTS and voice cloning capabilities, revolutionizing how we create and interact with voice content.\n\n**Overview of Neets.ai’s Offerings:**\n\nNeets.ai offers a comprehensive suite of AI-driven TTS tools designed to meet diverse needs:\n\n**Ultrafast Text-to-Speech:**\n\n- Neets.ai’s TTS technology is optimized for speed and efficiency, delivering low-latency streaming that is perfect for real-time applications such as virtual assistants and interactive games. This technology ensures immediate and engaging user interactions.\n\n\n**Voice Cloning:**\n\n- Neets.ai provides advanced voice cloning capabilities, enabling users to create high-quality, expressive voice replicas with minimal input. This feature is ideal for personalizing digital experiences and generating unique audio content.\n\n\n**Multilingual Support:**\n\n- With support for over 80 languages, Neets.ai ensures that users can generate speech in a wide range of languages, catering to global audiences. This extensive language support makes Neets.ai a versatile solution for international applications.\n\n**Ultrafast Text-to-Speech:**\n\n- Neets.ai’s ultrafast TTS technology offers several key features and benefits:\n\n**Features:**\n\n- Low Latency Streaming: Ideal for real-time applications, ensuring quick response times.\n- High Availability: Reliable performance even under high loads.\n- Expressive Voices: Capable of conveying a wide range of emotions and nuances.\n\n**Benefits:**\n\n- Engagement: Immediate and natural interactions enhance user engagement.\n- Scalability: Handles large volumes of requests without compromising performance or quality.\n- Versatility: Suitable for various applications, from customer service to entertainment.\n\n**Voice Cloning:**\n\n- Neets.ai’s voice cloning technology sets a new standard in creating high-quality, expressive voices:\n\n**Creating High-Quality Voices:**\n\n- Minimal Input: Generate high-fidelity voice clones with minimal recording input.\n- Expressive Replicas: Maintain the nuances and emotional expressiveness of the original voice.\n\n**Applications:**\n\n- Personalization: Create unique voices for digital assistants and characters.\n- Content Creation: Generate consistent and professional voiceovers for various media.\n\n**Multilingual Support:**\n\n- Neets.ai supports a wide range of languages, ensuring global accessibility:\n\n**Supporting 80+ Languages:**\n\n- Extensive Language Options: Generate speech in over 80 languages, catering to diverse audiences.\n- Global Reach: Ideal for international applications and content localization.\n\n**Developer API:**\n\nNeets.ai provides a robust API for easy integration:\n\n\n**Integration:**\n\n- SDKs: Ready-to-use SDKs for various programming languages.\n- Low Latency: Real-time speech synthesis for interactive applications.\n- Documentation: Comprehensive guides and support for seamless implementation.\n\n**Use Cases:**\n\n- Interactive Applications: Real-time voice generation for chatbots and virtual assistants.\n- On-demand Voice Generation: Seamless integration into content creation workflows.\n\n**Use Cases for Neets.ai:**\n\nNeets.ai’s versatile platform supports a wide range of applications:\n\n**Marketing:**\n\nCreate engaging marketing videos with captivating voiceovers, transforming scripts into high-quality audio content quickly and efficiently.\n\n\n**Real-time Applications:**\n\nBuild real-time conversational experiences with ultrafast TTS, ensuring every interaction is instant and engaging.\n\n\n**Content Creation:**\n\nSimplify content creation and produce high-quality audio for videos and other media at scale, reducing the time and effort required for traditional recording methods.\n\n**Impact on Content Creation:**\n\nNeets.ai is revolutionizing content creation by providing tools that enhance productivity and engagement. By automating voice generation, creators can focus on producing high-quality content without the time-consuming task of manual voice recording. This boosts productivity and allows for greater creative freedom and innovation.\n\n**Innovation and Research:**\n\nNeets.ai is committed to continuous innovation and research in the field of AI speech technology. Their team of experts is dedicated to improving the naturalness, expressiveness, and versatility of AI-generated voices. By exploring new applications and refining existing technologies, Neets.ai aims to stay at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Neets.ai. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nNeets.ai’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Neets.ai’s voice synthesis capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/voice/cartesia.mdx": {
      "markdown": "---\ntitle: Cartesia\nsubtitle: What is Cartesia.ai?\nslug: providers/voice/cartesia\n---\n\n\n**What is Cartesia.ai?**\n\nCartesia.ai is an advanced AI platform dedicated to developing real-time multimodal intelligence that operates across various devices. Specializing in ultrafast, realistic speech synthesis and voice API solutions, Cartesia.ai combines state-of-the-art AI technology with practical applications, empowering users to create high-quality, interactive voice content efficiently.\n\n**The Evolution of Multimodal AI:**\n\nAI technology has evolved from single-modal applications to sophisticated multimodal systems capable of processing and generating text, audio, video, and images. These advancements have paved the way for more integrated and interactive AI solutions. Cartesia.ai leverages these developments to offer comprehensive AI services that cater to diverse needs.\n\n**Overview of Cartesia.ai’s Offerings:**\n\nCartesia.ai provides a range of AI-driven tools designed to support various applications:\n\n**Real-time Voice API:**\n\nCartesia.ai’s real-time voice API is engineered for speed and efficiency, offering low latency and high-quality voice generation. This makes it ideal for applications requiring immediate feedback, such as virtual assistants, interactive games, and live conversations.\n\n**Multimodal Intelligence:**\n\nCartesia.ai’s multimodal intelligence capabilities extend beyond voice synthesis, encompassing text, audio, video, and images. This enables users to create more interactive and engaging content by integrating multiple forms of media into a single platform.\n\n**Ultrafast Voice Synthesis:**\n\nCartesia.ai’s ultrafast voice synthesis technology offers several key features and benefits:\n\n**Features:**\n\n- Low Latency Streaming: Ensures quick response times for real-time applications.\n- High Availability: Delivers reliable performance even under heavy loads.\n- Expressive Voices: Provides a wide range of emotions and nuances, enhancing the naturalness of generated speech.\n\n**Benefits:**\n\n- Engagement: Enhances user interactions with immediate and natural responses.\n- Scalability: Manages large volumes of requests without compromising quality.\n- Versatility: Suitable for various applications, from customer service to entertainment.\n\n**Multimodal Intelligence:**\n\nCartesia.ai’s multimodal intelligence capabilities provide comprehensive solutions for creating interactive and engaging content:\n\nText, Audio, Video, Images\n\n- Integrated Media: Combine text, audio, video, and images for more immersive experiences.\n- Advanced AI Models: Utilize state-of-the-art AI models for high-quality media processing.\n\n**Developer API:**\n\nCartesia.ai offers a robust API with comprehensive documentation and SDKs, facilitating seamless integration:\n\n**Integration:**\n\n- SDKs: Available for multiple programming languages.\n- Low Latency: Supports real-time applications with quick response times.\n- Documentation: Detailed guides and support for easy implementation.\n\n**Use Cases:**\n\n- Interactive Applications: Real-time voice generation for chatbots and virtual assistants.\n- On-demand Voice Generation: Seamlessly integrate into content creation workflows.\n\n**Use Cases for Cartesia.ai:**\n\nCartesia.ai’s versatile platform supports a wide range of applications:\n\n**Marketing:**\n\nCreate engaging marketing content with high-quality voiceovers, transforming scripts into professional audio quickly and efficiently.\n\n\n**Real-time Applications:**\n\nBuild real-time conversational experiences with ultrafast voice synthesis, ensuring every interaction is instant and engaging.\n\n\n**Content Creation:**\n\nSimplify content creation and produce high-quality audio for videos and other media at scale, reducing the time and effort required for traditional recording methods.\n\n**Impact on Content Creation:**\n\nCartesia.ai is revolutionizing content creation by providing tools that enhance productivity and engagement. By automating voice generation and integrating multimodal intelligence, creators can focus on producing high-quality content without the time-consuming task of manual media creation. This boosts productivity and allows for greater creative freedom and innovation.\n\n**Innovation and Research:**\n\nCartesia.ai is committed to continuous innovation and research in AI technology. Their team of experts focuses on advancing the capabilities of multimodal AI, exploring new applications, and refining existing technologies to stay at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Cartesia.ai. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nCartesia.ai’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Cartesia.ai’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/voice/imnt.mdx": {
      "markdown": "---\ntitle: LMNT\nsubtitle: What is LMNT?\nslug: providers/voice/imnt\n---\n\n\n**What is LMNT?**\n\nLMNT is a cutting-edge AI platform that specializes in ultrafast and lifelike speech synthesis. By leveraging advanced AI technology, LMNT offers solutions for creating high-quality, natural-sounding speech from text. Their innovative voice cloning technology allows users to generate studio-quality voice replicas with minimal input, transforming the way businesses and developers create and use voice content.\n\n**The Evolution of AI Speech Synthesis:**\n\nAI speech synthesis has come a long way from its early, rudimentary forms to the sophisticated, lifelike voices we have today. Advances in deep learning, neural networks, and data processing have enabled the creation of speech that is virtually indistinguishable from human voices. LMNT has harnessed these advancements to provide fast, reliable, and highly expressive speech synthesis solutions.\n\n**Overview of LMNT’s Offerings:**\n\nLMNT provides a range of AI-driven speech synthesis tools designed to meet diverse needs:\n\n**Ultrafast Speech Synthesis:**\n\n- LMNT’s speech synthesis technology is designed for speed and efficiency, delivering low latency streaming that is ideal for conversational applications, virtual agents, and interactive games. This technology ensures that every interaction is immediate and engaging, enhancing user experience and operational efficiency.\n\n\n**Voice Cloning:**\n\n- LMNT offers advanced voice cloning capabilities, allowing users to create lifelike and expressive voice replicas with as little as a 5-minute recording. Instant voice clones can be generated from just 15 seconds of audio. Users can also choose from a library of pre-built voices, making it easy to find the perfect match for any project.\n\n**Developer API:**\n\n- LMNT provides a robust API with SDKs for Python and Node.js, enabling seamless integration of their voice synthesis capabilities into various applications. The API supports ultrafast, low-latency streaming, making it perfect for real-time voice generation and playback scenarios. Comprehensive documentation and support are available to assist developers through every step of the integration process.\n\n**Ultrafast Speech Synthesis:**\n\n- LMNT’s ultrafast speech synthesis technology offers several key features and benefits:\n\n**Features:**\n\n- Low Latency Streaming: Designed for real-time applications, ensuring quick response times.\n- High Availability: Reliable performance under high loads, making it suitable for large-scale deployments.\n- Expressive Voices: Capable of conveying a wide range of emotions and nuances, enhancing the naturalness of generated speech.\n\n**Benefits:**\n\n- Engagement: Immediate and natural interactions improve user engagement and satisfaction.\n- Scalability: Handle large volumes of requests without compromising on performance or quality.\n- Versatility: Suitable for a wide range of applications, from customer service to entertainment.\n\n**Voice Cloning:**\n\n- LMNT’s voice cloning technology sets a new standard in creating lifelike and expressive voices:\n\n**Creating Lifelike Voices:**\n\n- Studio-quality Cloning: Generate high-fidelity voice clones with minimal recording input.\n- Instant Voice Cloning: Create usable voice clones from just a few seconds of audio.\n- Voice Library: Access a diverse library of pre-built voices for immediate use.\n\n**Applications:**\n\n- Personalization: Create unique voices for digital assistants, characters, and branding.\n- Content Creation: Generate consistent and professional voiceovers for videos, podcasts, and more.\n\n**Developer API:**\n\n- LMNT’s developer API simplifies the integration of voice synthesis capabilities into various projects:\n\n**Integration:**\n\n- SDKs: Ready-to-use SDKs for Python and Node.js.\n- Low Latency: Real-time voice synthesis for interactive applications.\n- Documentation: Comprehensive guides and support for easy implementation.\n\n**Use Cases:**\n\n- Interactive Applications: Real-time voice generation for chatbots and virtual assistants.\n- On-demand Voice Generation: Seamless integration into content creation workflows.\n\n**Use Cases for LMNT:**\n\n- LMNT’s versatile platform supports a wide range of applications:\n\n**Marketing:**\n\nCreate engaging product marketing videos with captivating voiceovers, turning scripts into high-quality audio content quickly and efficiently.\n\n**Real-time Conversations:**\n\nBuild lightning-fast conversational experiences with ultrafast speech synthesis, ensuring every interaction is instant and engaging.\n\n**Content Creation:**\n\nSimplify content creation and produce high-quality audio for videos and avatars at scale, reducing the time and effort required for traditional recording methods.\n\n**Impact on Content Creation:**\n\nLMNT is revolutionizing content creation by providing tools that enhance productivity and engagement. By automating voice generation, creators can focus on producing high-quality content without the time-consuming task of manual voice recording. This not only boosts productivity but also allows for more creative freedom and innovation.\n\n**Innovation and Research:**\n\nLMNT is committed to continuous innovation and research in the field of AI speech synthesis. Their team of experts is dedicated to improving the naturalness, expressiveness, and versatility of AI-generated voices. By exploring new applications and refining existing technologies, LMNT aims to stay at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at LMNT. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nLMNT’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate LMNT’s voice synthesis capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/voice/rimeai.mdx": {
      "markdown": "---\ntitle: RimeAI\nsubtitle: What is Rime.ai?\nslug: providers/voice/rimeai\n---\n\n\n**What is Rime.ai?**\n\nRime.ai is a pioneering platform in the field of speech synthesis, offering real-time, lifelike voice generation. Specializing in creating natural-sounding voices tailored to demographic specifics, Rime.ai provides tools that allow businesses and developers to engage their audiences more effectively. By leveraging advanced AI, Rime.ai delivers high-quality audio that is indistinguishable from human speech, setting a new standard in the industry.\n\n**The Evolution of AI Speech Synthesis:**\n\nAI speech synthesis has come a long way from its early days of robotic-sounding outputs. Advances in machine learning, neural networks, and data processing have transformed synthetic speech into highly realistic and expressive audio. Rime.ai has harnessed these technological advancements to create voices that sound natural and convey the desired emotions and nuances.\n\n**Overview of Rime.ai’s Offerings:**\n\nRime.ai provides a comprehensive suite of speech synthesis tools designed to meet various needs:\n\n**Real-time Speech Synthesis:**\n\n- Rime.ai’s real-time speech synthesis technology enables instant generation of lifelike voices. This is particularly useful for applications requiring immediate feedback, such as interactive voice response (IVR) systems, live virtual assistants, and real-time translation services. The technology boasts sub-300 millisecond response times, ensuring seamless and efficient communication.\n\n**Demographically Specific Voice Control:**\n\n- One of Rime.ai’s standout features is its ability to generate voices that are demographically specific. This means businesses can tailor their audio output to match the cultural, regional, and social characteristics of their target audience. With over 200 distinct voices available, Rime.ai allows for precise customization, enhancing user engagement and relatability.\n\n**Use Cases for Rime.ai:**\n\n\n- Rime.ai’s technology is versatile and applicable across multiple sectors:\n\n**IVR Systems:**\n\n- Interactive voice response systems benefit greatly from Rime.ai’s real-time speech synthesis. By providing natural and clear voices, IVR systems can improve user interactions, reduce call handling times, and enhance overall customer satisfaction.\n\n**Newsreading:**\n\nIn the media industry, Rime.ai’s lifelike voices can be used for automated newsreading, delivering news updates in a natural and engaging manner. This ensures consistency and professionalism in audio content delivery.\n\n\n**Narration:**\n\n- For audiobooks, educational materials, and other forms of narration, Rime.ai offers high-quality voice generation that enhances the listening experience. The ability to match voices to the content’s demographic audience further adds to the personalization and effectiveness of the narration.\n\n**Impact on Content Creation:**\n\nRime.ai is revolutionizing content creation by providing tools that allow for quick and efficient production of high-quality audio. By eliminating the need for traditional recording methods, creators can save time and resources while still producing professional-grade content. This democratization of audio production opens up new opportunities for innovation and creativity.\n\n**Innovation and Research:**\n\nRime.ai is committed to continuous innovation and research in speech synthesis technology. Their team of experts is dedicated to improving the naturalness, expressiveness, and versatility of AI-generated voices. By exploring new applications and refining existing technologies, Rime.ai aims to stay at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a top priority for Rime.ai. They have implemented robust safeguards to prevent misuse of their technology and are actively involved in discussions about responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their approach.\n\n**Integrations and Compatibility:**\n\nRime.ai’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Rime.ai’s speech synthesis capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/voice/deepgram.mdx": {
      "markdown": "---\ntitle: Deepgram\nsubtitle: What is Deepgram?\nslug: providers/voice/deepgram\n---\n\n\n**What is Deepgram?**\n\nDeepgram is a leading AI company specializing in advanced speech recognition and transcription technology. With their state-of-the-art speech-to-text solutions, Deepgram provides fast, accurate, and scalable transcription services for various applications. By leveraging deep learning and neural networks, Deepgram delivers unparalleled audio intelligence capabilities, transforming how we interact with and analyze spoken content.\n\n**The Evolution of Speech Recognition:**\n\nSpeech recognition technology has dramatically evolved from its early, basic forms to the sophisticated systems we have today. Initially, speech recognition systems were limited by their ability to accurately transcribe spoken language. However, advancements in machine learning, particularly deep learning, have revolutionized this field. Deepgram has harnessed these technological advancements to offer highly accurate and efficient speech recognition solutions that set a new industry standard.\n\n**Overview of Deepgram’s Offerings:**\n\nDeepgram offers a comprehensive suite of AI-driven speech recognition tools designed to meet diverse needs:\n\n**Speech-to-Text:**\n\n- Deepgram’s core offering is its speech-to-text technology, which converts spoken language into written text with high accuracy. This technology supports real-time transcription and batch processing, making it ideal for various applications, including media transcription, customer service, and accessibility enhancements.\n\n**Audio Intelligence:**\n\n- Deepgram’s audio intelligence capabilities go beyond simple transcription. Their technology can analyze audio to detect sentiment, intent, and topics, providing deeper insights into conversations and spoken content. This feature is particularly useful for businesses seeking to understand customer interactions and improve service delivery.\n\n**Speech-to-Text Technology:**\n\n- Deepgram’s speech-to-text technology stands out for its precision and speed. By utilizing end-to-end deep learning models, Deepgram achieves higher accuracy rates than traditional transcription methods. This technology can handle diverse accents, dialects, and noisy environments, ensuring reliable performance in real-world scenarios.\n\n**Real-time Transcription:**\n\n- Real-time transcription is one of Deepgram’s key features, enabling instant conversion of speech to text. This is particularly advantageous for applications such as live captioning, real-time customer support, and interactive voice response (IVR) systems. The ability to transcribe speech in real-time enhances user experience and operational efficiency.\n\n**Audio Intelligence:**\n\n- Deepgram’s audio intelligence features allow for advanced analysis of audio content. By detecting sentiment, intent, and topics within conversations, businesses can gain valuable insights into customer behavior and preferences. This information can be used to improve customer service, tailor marketing strategies, and enhance overall business intelligence.\n\n**Use Cases for Deepgram:**\n\nDeepgram’s technology is versatile and applicable across multiple industries:\n\n**Media Transcription:**\n\nIn the media sector, Deepgram’s speech-to-text solutions are used to transcribe interviews, podcasts, and video content. This makes content searchable, accessible, and easier to manage, enhancing the efficiency of media production workflows.\n\n**Contact Centers:**\n\n- For contact centers, Deepgram provides real-time transcription and audio analysis to improve customer interactions. By transcribing calls and analyzing sentiment, businesses can identify trends, monitor agent performance, and enhance customer satisfaction.\n\n**Healthcare:**\n\n- In healthcare, accurate transcription is critical for maintaining patient records and documenting medical consultations. Deepgram’s speech-to-text technology ensures precise and timely transcription, aiding in effective communication and record-keeping.\n\n**Impact on Content Creation:**\n\nDeepgram is transforming content creation by providing tools that streamline the transcription process. By automating transcription, content creators can focus on producing high-quality material without the time-consuming task of manual transcription. This boosts productivity and opens new avenues for creative and professional work.\n\n**Innovation and Research:**\n\nDeepgram is committed to continuous innovation and research in the field of speech recognition and AI. Their team of experts is dedicated to enhancing the capabilities of their technology, exploring new applications, and pushing the boundaries of what speech AI can achieve.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Deepgram. They implement robust safeguards to prevent misuse of their technology and are actively engaged in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\nIntegrations and Compatibility\n\nDeepgram’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Deepgram’s speech recognition capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/model/openai.mdx": {
      "markdown": "---\ntitle: OpenAI\nsubtitle: What is OpenAI?\nslug: providers/model/openai\n---\n\n\n**What is OpenAI?**\n\nOpenAI is a leading artificial intelligence research and deployment company dedicated to ensuring that artificial general intelligence (AGI) benefits all of humanity. Founded with the mission to create safe and highly capable AI systems, OpenAI has made significant strides in AI research, producing groundbreaking models like GPT-4, DALL-E, and Codex. These innovations have not only advanced the field of AI but also transformed various industries by providing powerful tools for natural language processing, image generation, and programming assistance.\n\n**The Evolution of AI Research:**\n\nThe field of AI has evolved rapidly over the past few decades. From early rule-based systems to modern deep learning models, AI technology has made significant progress in mimicking human intelligence. OpenAI has been at the forefront of this evolution, pushing the boundaries of what AI can achieve through continuous research and development. Their work on large language models and neural networks has set new benchmarks for performance and capability in AI systems.\n\n**Overview of OpenAI’s Offerings:**\n\nOpenAI offers a range of AI-driven products and services designed to meet diverse needs:\n\n**GPT Models:**\n\n- OpenAI’s Generative Pre-trained Transformer (GPT) models, including the latest GPT-4, are state-of-the-art in natural language processing. These models can generate human-like text, answer questions, summarize information, and perform various language tasks with high accuracy. GPT-4, in particular, represents a significant leap in AI capabilities, offering improved coherence, context understanding, and creativity.\n\n**DALL-E:**\n\n- DALL-E is OpenAI’s revolutionary image generation model that creates detailed and imaginative images from text descriptions. By combining deep learning with creative processes, DALL-E can produce unique artwork, design concepts, and visual content that aligns with the given textual input. This technology opens new possibilities for artists, designers, and content creators.\n\n**Codex:**\n\n\n- Codex is an AI model that assists with programming by understanding and generating code. Integrated into tools like GitHub Copilot, Codex can help developers write code faster and more efficiently by suggesting code snippets, debugging errors, and automating repetitive tasks. This enhances productivity and reduces the barrier to entry for learning programming languages.\n\n**GPT-4 Technology:**\n\n- GPT-4 is the latest and most advanced language model developed by OpenAI. It excels in generating coherent and contextually relevant text, making it a powerful tool for various applications, including chatbots, content creation, and automated customer support. With enhanced capabilities for understanding and generating human language, GPT-4 sets a new standard in AI-driven communication.\n\n**DALL-E Image Generation:**\n\n- DALL-E takes text-to-image generation to new heights, allowing users to create visually stunning and highly specific images based on textual descriptions. This technology is particularly valuable for creative industries, where generating unique visuals quickly and accurately is essential. From concept art to marketing materials, DALL-E provides a versatile tool for visual content creation.\n\n**Codex and Programming Assistance:**\n\n- Codex transforms the way developers interact with code by providing intelligent suggestions and automating routine programming tasks. This AI-powered assistant understands multiple programming languages and can generate code snippets, making coding more accessible and efficient. Integrated into platforms like GitHub Copilot, Codex helps streamline the development process and accelerates software production.\n\n**Use Cases for OpenAI:**\n\nOpenAI’s technologies are versatile and applicable across various sectors:\n\n**Education:**\n\nIn education, GPT-4 and Codex can enhance learning experiences by providing personalized tutoring, generating educational content, and assisting with coding exercises. These tools help students grasp complex concepts and improve their programming skills.\n\n**Business:**\n\nBusinesses leverage OpenAI’s models for automating customer support, generating marketing content, and analyzing large volumes of text data. GPT-4’s ability to understand and generate human-like text enhances customer interactions and drives operational efficiency.\n\n**Creative Industries:**\n\nIn the creative sector, DALL-E and GPT-4 enable artists and writers to generate new ideas, create unique visuals, and produce high-quality content. These tools expand creative possibilities and streamline content production workflows.\n\n**Innovation and Research:**\n\nOpenAI is committed to advancing AI through continuous research and innovation. Their team of researchers and engineers works on developing new models, improving existing technologies, and exploring novel applications of AI. This commitment to innovation ensures that OpenAI remains at the cutting edge of the field.\n\n**AI Safety and Ethics:**\n\nEnsuring the safe and ethical use of AI is a core principle at OpenAI. They implement rigorous safety measures to prevent misuse and ensure that their technologies are used responsibly. OpenAI is also involved in global discussions about AI ethics and governance, contributing to the development of best practices and standards for the industry.\n\n**Integrations and Compatibility:**\n\nOpenAI’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate OpenAI’s AI capabilities into their existing systems, enhancing functionality and improving user experience. The API is designed to be flexible and user-friendly, accommodating a wide range of use cases.\n"
    },
    "providers/model/groq.mdx": {
      "markdown": "---\ntitle: Groq\nsubtitle: What is Groq?\nslug: providers/model/groq\n---\n\n\n**What is Groq?**\n\nGroq is a pioneering technology company specializing in high-performance AI inference solutions. Known for its innovative GroqChip, Groq delivers unparalleled speed and efficiency in AI processing. Their platform is designed to handle complex AI workloads with low latency, making it ideal for a variety of applications, including autonomous systems, data centers, and real-time AI inference.\n\n\n**The Evolution of AI Inference:**\n\nAI inference has evolved significantly, from simple rule-based systems to advanced neural networks that require substantial computational power. Groq has harnessed cutting-edge advancements in hardware and software to create solutions that meet the growing demands for speed and accuracy in AI processing.\n\n\n**Overview of Groq’s Offerings:**\n\nGroq offers a suite of high-performance AI tools and solutions designed to support various industries:\n\n**GroqChip:**\n\nThe GroqChip is the cornerstone of Groq’s offerings, providing unmatched performance for AI inference tasks. Its architecture is optimized for low latency and high throughput, making it ideal for demanding AI applications.\n\n**GroqWare:**\n\nGroqWare is a suite of software tools that support the deployment and management of AI models on Groq hardware. It includes drivers, compilers, and libraries designed to optimize performance and simplify integration.\n\n**GroqCloud:**\n\nGroqCloud offers cloud-based access to Groq’s high-performance computing resources. This service allows businesses to leverage Groq’s powerful AI infrastructure without the need for significant capital investment in hardware.\n\n**High-Performance AI Inference:**\n\nGroq’s AI inference technology offers several key features and benefits:\n\n**Features:**\n\n- Low Latency: Ensures rapid processing of AI workloads.\n- High Throughput: Capable of handling large volumes of data efficiently.\n- Scalability: Easily scales to meet the demands of growing applications.\n\n**Benefits:**\n\n- Efficiency: Reduces the time required to process AI tasks.\n- Reliability: Delivers consistent performance even under heavy loads.\n- Cost-Effectiveness: Offers a competitive advantage with efficient resource utilization.\n\n**GroqChip Technology:**\n\nThe GroqChip stands out for its advanced architecture and performance capabilities:\n\n**Architecture:**\n\n- Optimized Design: Tailored for AI inference with specialized processing units.\n- High Efficiency: Maximizes performance per watt, reducing energy consumption.\n\n**Performance:**\n\n- Unmatched Speed: Delivers top-tier performance for real-time AI applications.\n- Low Latency: Ensures minimal delay in processing, critical for time-sensitive tasks.\n\n**Applications:**\n\n- Autonomous Systems: Enhances the performance of self-driving cars and drones.\n- Data Centers: Boosts the efficiency and capacity of AI data centers.\n\n\n**Low Latency AI:**\n\nGroq’s low-latency AI solutions provide real-time processing capabilities essential for various applications:\n\n**Real-time Processing:**\n\n- Immediate Response: Critical for applications requiring instant decision-making.\n- High Reliability: Ensures consistent performance with minimal delays.\n\n**Use Cases:**\n\n- AI Research: Accelerates experimentation and model testing.\n- Autonomous Systems: Improves the safety and efficiency of autonomous vehicles.\n\n**Developer API and Tools:**\n\nGroq offers a comprehensive API and a range of development tools to facilitate integration and optimization:\n\n**Integration:**\n\n- SDKs: Available for multiple programming languages.\n- Comprehensive Documentation: Guides and support for seamless implementation.\n\n**Use Cases:**\n\nApplication Development: Streamline the integration of AI capabilities into applications.\n\n- Application Development: Streamline the integration of AI capabilities into applications.\n- Research and Experimentation: Provides tools for efficient AI model deployment and testing.\n\n\n**Use Cases for Groq:**\n\nGroq’s platform supports a wide range of applications across various industries:\n\n**AI Research:**\n\nFacilitates cutting-edge research with high-performance AI infrastructure.\n\n\n**Autonomous Systems:**\n\nEnhances the capabilities of autonomous vehicles and robotics with low-latency processing.\n\n\n**Data Centers:**\n\nImproves the efficiency and capacity of data centers handling AI workloads.\n\n\n**Impact on AI Development:**\n\nGroq is transforming AI development by providing tools that enhance productivity and efficiency. By automating and optimizing AI inference, developers can focus on innovation and application rather than infrastructure management.\n\n\n**Innovation and Research:**\n\nGroq is committed to continuous innovation and research in AI inference technology. Their team of experts focuses on advancing the capabilities of AI hardware and software, exploring new applications, and refining existing technologies to stay at the forefront of the industry.\n\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Groq. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nGroq’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Groq’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/model/deepinfra.mdx": {
      "markdown": "---\ntitle: DeepInfra\nsubtitle: DeepInfra is a provider for Vapi.\nslug: providers/model/deepinfra\n---\n\n\n**What is DeepInfra?**\n\nDeepInfra is an innovative platform that provides scalable and cost-effective infrastructure for deploying machine learning models. By offering a simple API and autoscaling capabilities, DeepInfra allows businesses and developers to efficiently manage and deploy AI models, ensuring high performance and low latency. This platform supports a wide range of AI applications, making it an ideal solution for diverse industries.\n\n\n**The Evolution of AI Infrastructure:**\n\nAI infrastructure has advanced from on-premises solutions to cloud-based platforms that offer flexibility, scalability, and cost efficiency. DeepInfra leverages these advancements to provide robust and scalable infrastructure, enabling seamless deployment and management of machine learning models.\n\n**Overview of DeepInfra’s Offerings:**\n\nDeepInfra offers a comprehensive suite of tools and services designed to support various AI applications:\n\n**Machine Learning Models:**\n\nDeepInfra provides access to a wide range of pre-trained machine learning models, including text generation, text-to-image, automatic speech recognition, and embeddings. These models are optimized for performance and can be easily integrated into various applications.\n\n\n**API:**\n\nDeepInfra’s API allows for easy integration of machine learning models into applications, offering low latency and high availability. The API supports various programming languages, making it accessible to a broad range of developers.\n\n**Scalability:**\n\nDeepInfra’s infrastructure is designed to scale automatically based on demand, ensuring optimal performance and cost efficiency. This scalability is crucial for handling large volumes of requests and maintaining low latency.\n\n**Machine Learning Model Deployment:**\n\nDeepInfra’s model deployment capabilities offer several key features and benefits:\n\n**Features:**\n\n- Low Latency Streaming: Ensures quick response times for real-time applications.\n- High Availability: Delivers reliable performance even under heavy loads.\n- Expressive Models: Provides high-quality outputs for various AI tasks.\n\n**Benefits:**\n\n- Efficiency: Reduces the time and resources needed for model deployment.\n- Scalability: Handles large volumes of requests without compromising performance.\n- Cost-Effectiveness: Offers pay-per-use pricing, minimizing upfront costs.\n\n**Scalable Infrastructure:**\n\nDeepInfra’s scalable infrastructure provides several advantages:\n\n**Autoscaling:**\n\n- Dynamic Resource Allocation: Automatically adjusts resources based on demand.\n- Consistent Performance: Maintains low latency and high availability during peak usage.\n\n**Low Latency:**\n\n- Optimized Network: Ensures fast data transmission and processing.\n- Regional Deployment: Deploys models close to users for reduced latency.\n\n\n**Cost Efficiency:**\n\n- Pay-per-Use Pricing: Charges based on actual usage, avoiding unnecessary costs.\n- Resource Sharing: Maximizes infrastructure utilization, reducing overall expenses.\n\n**Developer API:**\n\nDeepInfra offers a robust API with comprehensive documentation and SDKs, facilitating seamless integration:\n\n**Integration:**\n\n- SDKs: Available for multiple programming languages.\n- Low Latency: Supports real-time applications with quick response times.\n- Documentation: Detailed guides and support for easy implementation.\n\n**Use Cases:**\n\n- Research: Efficiently access and analyze vast amounts of data.\n- Application Development: Integrate advanced AI capabilities into applications.\n- Business Intelligence: Gain insights for strategic decision-making.\n\n**Use Cases for DeepInfra:**\n\nDeepInfra’s versatile platform supports a wide range of applications:\n\n**Research:**\n\nFacilitate academic and scientific research with efficient and accurate AI model deployment.\n\n\n**Application Development:**\n\nStreamline the development process by integrating high-performance AI models into applications.\n\n**Business Intelligence:**\n\nEnhance business operations with powerful AI models that provide valuable insights and data analysis.\n\n**Impact on AI Development:**\n\nDeepInfra is revolutionizing AI development by providing tools that enhance productivity and efficiency. By automating the deployment process and offering scalable infrastructure, developers can focus on innovation and optimization rather than infrastructure management.\n\n\n**Innovation and Research:**\n\nDeepInfra is committed to continuous innovation and research in AI infrastructure. Their team of experts focuses on advancing the capabilities of machine learning models and exploring new applications, ensuring that they remain at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at DeepInfra. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nDeepInfra’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate DeepInfra’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/model/perplexity.mdx": {
      "markdown": "---\ntitle: Perplexity\nsubtitle: What is Perplexity.ai?\nslug: providers/model/perplexity\n---\n\n\n**What is Perplexity.ai?**\n\nPerplexity.ai is an advanced AI-powered search engine that delivers precise and real-time answers to user queries. Utilizing state-of-the-art AI algorithms and knowledge graphs, Perplexity.ai enhances the search experience by providing structured and accurate information. This innovative platform is designed to cater to diverse needs, from research and education to business intelligence, making information retrieval more efficient and reliable.\n\n**The Evolution of AI Search Engines:**\n\nSearch engines have significantly evolved from simple keyword-based systems to sophisticated AI-driven platforms capable of understanding and answering complex queries. Advances in natural language processing, machine learning, and data integration have revolutionized how search engines operate. Perplexity.ai leverages these advancements to offer a more intuitive and accurate search experience, setting a new standard in information retrieval.\n\n**Overview of Perplexity.ai’s Offerings:**\n\nPerplexity.ai provides a range of AI-driven tools designed to enhance search capabilities:\n\n**AI-Powered Answers:**\n\nPerplexity.ai’s core offering is its AI-powered search engine, which delivers accurate and relevant answers to user queries. The AI algorithms understand the context and intent behind each query, providing precise and comprehensive results.\n\n**Knowledge Graphs:**\n\nPerplexity.ai integrates knowledge graphs to enhance search results with structured and interconnected information. This feature helps users understand the relationships between different entities and access detailed insights quickly.\n\n**Real-time Information:**\n\nPerplexity.ai ensures that users receive the most up-to-date information by continuously updating its database. This real-time capability is crucial for queries requiring the latest data and developments.\n\n**AI-Powered Search Technology:**\n\nPerplexity.ai’s search technology offers several key features and benefits:\n\n**Features:**\n\n- Contextual Understanding: Interprets the context and intent behind queries for accurate answers.\n- Comprehensive Results: Provides detailed and relevant information, enhancing the search experience.\n- User-Friendly Interface: Intuitive design for easy navigation and quick access to information.\n\n**Benefits:**\n\n- Efficiency: Reduces the time needed to find accurate information.\n- Reliability: Delivers precise and trustworthy results.\n- Enhanced Insights: Offers deeper understanding through structured knowledge graphs.\n\n**Knowledge Graphs:**\n\nPerplexity.ai’s knowledge graphs enrich search results by organizing information into structured entities and relationships:\n\n**Enhancing Search Results:**\n\n- Interconnected Information: Displays related entities and their connections.\n- Detailed Insights: Provides comprehensive information at a glance, improving understanding.\n\n**Real-time Information:**\n\nPerplexity.ai’s real-time information feature ensures users receive the latest data and updates:\n\n**Providing Up-to-date Answers:**\n\n- Continuous Updates: Regularly refreshes data to maintain accuracy.\n- Timely Information: Crucial for time-sensitive queries and decisions.\n\n**Developer API:**\n\nPerplexity.ai offers a robust API for easy integration into various applications:\n\n\n**Integration:**\n\n- SDKs: Available for multiple programming languages.\n- Documentation: Comprehensive guides and support for seamless implementation.\n\n**Use Cases:**\n\n- Research: Efficiently access and analyze vast amounts of data.\n- Business Intelligence: Gain insights for strategic decision-making.\n\nUse Cases for Perplexity.ai\n\nPerplexity.ai’s versatile platform supports a wide range of applications:\n\n\n**Research:**\n\nFacilitate academic and scientific research with accurate and comprehensive information retrieval.\n\n\n**Education:**\n\nEnhance learning experiences by providing students and educators with reliable answers and insights.\n\n\n**Business Intelligence:**\n\nSupport business decisions with precise data and detailed analyses.\n\n\n**Impact on Information Retrieval:**\n\nPerplexity.ai is revolutionizing information retrieval by providing tools that enhance productivity and accuracy. By automating the search process and integrating knowledge graphs, users can quickly access relevant information, reducing the time and effort required for manual data collection.\n\n\n**Innovation and Research:**\n\nPerplexity.ai is committed to continuous innovation and research in AI search technology. Their team of experts focuses on advancing the capabilities of AI algorithms and knowledge graphs, exploring new applications, and refining existing technologies to stay at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Perplexity.ai. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n\n**Integrations and Compatibility:**\n\nPerplexity.ai’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Perplexity.ai’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/model/togetherai.mdx": {
      "markdown": "---\ntitle: TogetherAI\nsubtitle: TogetherAI is a provider for Vapi.\nslug: providers/model/togetherai\n---\n\n\n**What is Together AI?**\n\nTogether AI is a leading cloud platform designed for building and running generative AI models. It provides state-of-the-art AI inference, fine-tuning capabilities, and high-performance GPU clusters, enabling businesses and developers to harness the full potential of AI. Together AI focuses on speed, scalability, and cost-efficiency, making it an ideal solution for various AI applications.\n\n**The Evolution of AI Cloud Platforms:**\n\nAI cloud platforms have significantly evolved, offering more powerful and efficient solutions for AI model deployment and training. Advances in cloud computing, GPU technology, and AI algorithms have enabled platforms like Together AI to provide comprehensive services that cater to modern AI needs.\n\n**Overview of Together AI’s Offerings:**\n\nTogether AI offers a range of AI-driven tools and services:\n\n**AI Inference:**\n\nTogether AI provides the fastest AI inference stack available, ensuring quick and efficient processing of AI tasks. This service supports large-scale deployments and offers significant cost savings.\n\n**Fine-Tuning:**\n\nTogether AI enables users to fine-tune leading open-source models with their private data, achieving greater accuracy for specific tasks. This service supports various models, including LLaMA-2, RedPajama, and more.\n\n**GPU Clusters:**\n\nTogether AI offers high-performance GPU clusters for large-scale training and fine-tuning. These clusters are equipped with top-tier hardware like NVIDIA A100 and H10 GPUs, ensuring optimal performance and scalability.\n\n**AI Inference Technology:**\n\nTogether AI’s inference technology offers several key features and benefits:\n\n**Features:**\n\n- High Speed: Provides the fastest inference on the market.\n- Scalability: Easily scales to handle large volumes of requests.\n- Cost Efficiency: Offers lower costs compared to traditional inference services.\n\n\n**Benefits:**\n\n- Efficiency: Reduces the time required for AI tasks.\n- Reliability: Ensures consistent and high-quality performance.\n- Flexibility: Adapts to various application needs.\n\n**Fine-Tuning and Custom Models:**\n\nTogether AI’s fine-tuning capabilities allow users to personalize AI models with their private data:\n\n**Personalizing AI Models:**\n\n- Custom Data Integration: Fine-tune models with specific datasets for improved accuracy.\n- Wide Model Support: Supports various open-source models for diverse applications.\n\n\n**GPU Clusters:**\n\nTogether AI’s GPU clusters provide high-performance hardware for AI training:\n\n**High-Performance Hardware:**\n\n- NVIDIA A100 and H100 GPUs: Equipped with the latest GPU technology for optimal performance.\n- Scalable Clusters: Available in configurations ranging from 16 to 2048 GPUs.\n\n\n**Developer API:**\n\nTogether AI offers a comprehensive API for easy integration:\n\n\n**Integration:**\n\n- SDKs: Available for multiple programming languages.\n- Comprehensive Documentation: Detailed guides and support for seamless implementation.\n\n**Use Cases:**\n\n- Business Solutions: Enhance operational efficiency and decision-making.\n- Research: Facilitate academic and scientific research with advanced AI tools.\n- Content Creation: Automate and optimize content production processes.\n\n**Use Cases for Together AI:**\n\nTogether AI supports a wide range of applications across various sectors:\n\n**Business Solutions:**\n\nLeverage AI to improve business operations, enhance customer experiences, and drive innovation.\n\n**Research:**\n\nUtilize advanced AI tools to support academic and scientific research.\n\n**Content Creation:**\n\nEnhance content creation with high-quality AI-generated text, images, and more.\n\n**Impact on AI Development:**\n\nTogether AI is transforming AI development by providing tools that enhance productivity and innovation. By offering scalable and cost-effective solutions, developers can focus on creating advanced AI applications without worrying about infrastructure constraints.\n\n**Innovation and Research:**\n\nTogether AI is committed to continuous innovation and research in AI technology. Their team of experts focuses on advancing the capabilities of AI models and exploring new applications to stay at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Together AI. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nTogether AI’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Together AI’s capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/model/openrouter.mdx": {
      "markdown": "---\ntitle: OpenRouter\nsubtitle: What is OpenRouter?\nslug: providers/model/openrouter\n---\n\n\n**What is OpenRouter?**\n\nOpenRouter is a cutting-edge AI platform offering a unified interface for integrating multiple large language models (LLMs). Designed to streamline AI access, OpenRouter provides a comprehensive suite of tools and APIs that enable businesses and developers to leverage a variety of LLMs for diverse applications. This platform focuses on enhancing efficiency, scalability, and cost-effectiveness.\n\n**The Evolution of AI Integration:**\n\nAI integration has significantly evolved from isolated systems to unified platforms that provide seamless access to multiple AI models. Advances in API technology, cloud computing, and machine learning have enabled platforms like OpenRouter to offer comprehensive solutions that cater to modern AI needs.\n\n**Overview of OpenRouter’s Offerings:**\n\nOpenRouter provides a range of AI-driven tools and services:\n\n**LLM Access:**\n\nOpenRouter offers access to a wide variety of LLMs, including models specialized in different tasks such as roleplaying, programming, marketing, and more. This allows users to select the best models for their specific needs.\n\n**APIs:**\n\nOpenRouter’s robust APIs enable developers to integrate LLM capabilities into their applications, ensuring low latency and high availability. The APIs support multiple programming languages, making them accessible to a broad range of developers.\n\n**Unified Interface:**\n\nOpenRouter provides a unified interface that simplifies the process of accessing and managing multiple AI models. This interface enhances usability and efficiency, making it easier to deploy and utilize AI solutions.\n\n\n**AI Integration Technology:**\n\nOpenRouter’s AI integration technology offers several key features and benefits:\n\n**Features:**\n\n- Unified Access: Provides a single interface for managing multiple AI models.\n- High Availability: Ensures reliable performance even under heavy loads.\n- Scalability: Easily scales to meet the demands of growing applications.\n\n\n**Benefits:**\n\n- Efficiency: Reduces the time and resources needed for AI integration.\n- Flexibility: Supports a wide range of applications and use cases.\n- Cost-Effectiveness: Offers competitive pricing compared to traditional solutions.\n\n**Unified Access to LLMs:**\n\nOpenRouter excels in providing unified access to multiple LLMs:\n\n**Combining Multiple Models in One Interface:**\n\n- Streamlined Management: Simplifies the process of accessing and managing different AI models.\n- Diverse Applications: Supports various tasks, from programming to marketing.\n\n**Developer API:**\n\n\nOpenRouter offers a comprehensive API for easy integration:**\n\n**Integration:**\n\n- SDKs: Available for multiple programming languages.\n- Comprehensive Documentation: Detailed guides and support for seamless implementation.\n\n**Use Cases:**\n\n- Business Solutions: Enhance operational efficiency and decision-making.\n- Research: Facilitate academic and scientific research with advanced AI tools.\n- Content Creation: Automate and optimize content production processes.\n\n**Use Cases for OpenRouter:**\n\nOpenRouter supports a wide range of applications across various sectors:\n\n**Business Solutions:**\n\nLeverage AI to improve business operations, enhance customer experiences, and drive innovation.\n\n\n**Research:**\n\nUtilize advanced AI tools to support academic and scientific research.\n\n**Content Creation:**\n\nEnhance content creation with high-quality AI-generated text, images, and more.\n\n**Impact on AI Development:**\n\nOpenRouter is transforming AI development by providing tools that enhance productivity and innovation. By offering scalable and cost-effective solutions, developers can focus on creating advanced AI applications without worrying about infrastructure constraints.\n\n**Innovation and Research:**\n\nOpenRouter is committed to continuous innovation and research in AI integration. Their team of experts focuses on advancing the capabilities of AI models and exploring new applications to stay at the forefront of the industry.\n\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at OpenRouter. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nOpenRouter’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate OpenRouter’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/transcriber/deepgram.mdx": {
      "markdown": "---\ntitle: Deepgram\nsubtitle: What is Deepgram?\nslug: providers/transcriber/deepgram\n---\n\n\n**What is Deepgram?**\n\nDeepgram is a leading AI company specializing in advanced speech recognition and transcription technology. With their state-of-the-art speech-to-text solutions, Deepgram provides fast, accurate, and scalable transcription services for various applications. By leveraging deep learning and neural networks, Deepgram delivers unparalleled audio intelligence capabilities, transforming how we interact with and analyze spoken content.\n\n**The Evolution of Speech Recognition:**\n\nSpeech recognition technology has dramatically evolved from its early, basic forms to the sophisticated systems we have today. Initially, speech recognition systems were limited by their ability to accurately transcribe spoken language. However, advancements in machine learning, particularly deep learning, have revolutionized this field. Deepgram has harnessed these technological advancements to offer highly accurate and efficient speech recognition solutions that set a new industry standard.\n\n**Overview of Deepgram’s Offerings:**\n\nDeepgram offers a comprehensive suite of AI-driven speech recognition tools designed to meet diverse needs:\n\n**Speech-to-Text:**\n\n- Deepgram’s core offering is its speech-to-text technology, which converts spoken language into written text with high accuracy. This technology supports real-time transcription and batch processing, making it ideal for various applications, including media transcription, customer service, and accessibility enhancements.\n\n**Audio Intelligence:**\n\n- Deepgram’s audio intelligence capabilities go beyond simple transcription. Their technology can analyze audio to detect sentiment, intent, and topics, providing deeper insights into conversations and spoken content. This feature is particularly useful for businesses seeking to understand customer interactions and improve service delivery.\n\n**Speech-to-Text Technology:**\n\n- Deepgram’s speech-to-text technology stands out for its precision and speed. By utilizing end-to-end deep learning models, Deepgram achieves higher accuracy rates than traditional transcription methods. This technology can handle diverse accents, dialects, and noisy environments, ensuring reliable performance in real-world scenarios.\n\n**Real-time Transcription:**\n\n- Real-time transcription is one of Deepgram’s key features, enabling instant conversion of speech to text. This is particularly advantageous for applications such as live captioning, real-time customer support, and interactive voice response (IVR) systems. The ability to transcribe speech in real-time enhances user experience and operational efficiency.\n\n**Audio Intelligence:**\n\n- Deepgram’s audio intelligence features allow for advanced analysis of audio content. By detecting sentiment, intent, and topics within conversations, businesses can gain valuable insights into customer behavior and preferences. This information can be used to improve customer service, tailor marketing strategies, and enhance overall business intelligence.\n\n**Use Cases for Deepgram:**\n\nDeepgram’s technology is versatile and applicable across multiple industries:\n\n**Media Transcription:**\n\nIn the media sector, Deepgram’s speech-to-text solutions are used to transcribe interviews, podcasts, and video content. This makes content searchable, accessible, and easier to manage, enhancing the efficiency of media production workflows.\n\n**Contact Centers:**\n\n- For contact centers, Deepgram provides real-time transcription and audio analysis to improve customer interactions. By transcribing calls and analyzing sentiment, businesses can identify trends, monitor agent performance, and enhance customer satisfaction.\n\n**Healthcare:**\n\n- In healthcare, accurate transcription is critical for maintaining patient records and documenting medical consultations. Deepgram’s speech-to-text technology ensures precise and timely transcription, aiding in effective communication and record-keeping.\n\n**Impact on Content Creation:**\n\nDeepgram is transforming content creation by providing tools that streamline the transcription process. By automating transcription, content creators can focus on producing high-quality material without the time-consuming task of manual transcription. This boosts productivity and opens new avenues for creative and professional work.\n\n**Innovation and Research:**\n\nDeepgram is committed to continuous innovation and research in the field of speech recognition and AI. Their team of experts is dedicated to enhancing the capabilities of their technology, exploring new applications, and pushing the boundaries of what speech AI can achieve.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Deepgram. They implement robust safeguards to prevent misuse of their technology and are actively engaged in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\nIntegrations and Compatibility\n\nDeepgram’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Deepgram’s speech recognition capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/transcriber/gladia.mdx": {
      "markdown": "---\ntitle: Gladia\nsubtitle: What is Gladia?\nslug: providers/transcriber/gladia\n---\n\n\n**What is Gladia?**\n\nGladia is an advanced AI platform specializing in real-time transcription, translation, and audio intelligence. By leveraging state-of-the-art ASR (Automatic Speech Recognition), NLP (Natural Language Processing), and GenAI (Generative AI) models, Gladia helps businesses extract valuable insights from unstructured audio data. Their enterprise-grade API offers scalable, secure, and efficient solutions for various applications, from virtual meetings to customer service.\n\n\n**The Evolution of AI Transcription:**\n\nAI transcription has significantly evolved, moving from basic speech recognition systems to advanced platforms capable of real-time transcription, translation, and audio intelligence. Innovations in machine learning and natural language processing have enhanced accuracy and efficiency. Gladia utilizes these advancements to deliver top-tier transcription services tailored for modern business needs.\n\n**Overview of Gladia’s Offerings:**\n\nGladia provides a comprehensive suite of AI-driven tools:\n\n\n**Speech-to-Text:**\n\nGladia’s core offering is its AI-powered speech-to-text technology, delivering highly accurate and real-time transcription. This service supports 99 languages and includes speaker diarization and code-switching.\n\n**Audio Intelligence:**\n\nGladia’s audio intelligence add-ons offer features like summarization, chapterization, and sentiment analysis, providing deeper insights into audio data.\n\n**API:**\n\nGladia’s robust API allows seamless integration of speech-to-text capabilities into applications, ensuring low latency and high availability.\n\n**AI Transcription Technology:**\n\nGladia’s AI transcription technology offers several key features and benefits:\n\n**Features:**\n\n- High Accuracy: Industry-leading transcription accuracy.\n- Real-time and Async Transcription: Instantaneous and batch processing options.\n- Multilingual Support: Supports transcription and translation in 99 languages.\n\n**Benefits:**\n\n- Efficiency: Reduces the time needed for transcription and analysis.\n- Scalability: Handles large volumes of data efficiently.\n- Cost-Effective: Provides high performance at a competitive cost.\n\n**Real-time Transcription and Translation:**\n\nGladia excels in providing real-time transcription and translation:\n\n\n**Multilingual Support:**\n\n- 99 Languages: Supports a wide range of languages and dialects.\n- Real-time Translation: Near-instantaneous translation for diverse applications.\n\n**Use Cases:**\n\n- Virtual Meetings: Provides real-time transcriptions, note-taking, and video captions.\n- Content Creation: Transcribes and translates videos and podcasts for global audiences.\n\n**Developer API:**\n\nGladia offers a comprehensive API for easy integration:\n\n**Integration:**\n\n- SDKs: Available for multiple programming languages.\n- Comprehensive Documentation: Detailed guides and support for seamless implementation.\n\n**Use Cases:**\n\n- Application Development: Enhance applications with advanced AI capabilities.\n- Business Solutions: Improve operational efficiency and customer service.\n\n**Use Cases for Gladia:**\n\nGladia supports a wide range of applications:\n\n**Content Creation:**\n\nEnhance content creation with high-quality transcription, translation, and subtitling.\n\n\n**Customer Service:**\n\nImprove customer service with accurate call transcriptions and emotion detection.\n\n**Market Research:**\n\nGain valuable insights into market trends and customer preferences through advanced speech analysis.\n\n**Impact on Business Operations:**\n\nGladia is revolutionizing business operations by providing tools that enhance productivity and insights. By automating transcription and audio intelligence, businesses can focus on innovation and strategy rather than manual processes.\n\n**Innovation and Research:**\n\nGladia is committed to continuous innovation and research in AI transcription. Their team of experts focuses on advancing the capabilities of ASR and NLP technologies, exploring new applications, and refining existing tools to stay at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Gladia. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nGladia’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Gladia’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/transcriber/talkscriber.mdx": {
      "markdown": "---\ntitle: Talkscriber\nsubtitle: What is Talkscriber?\nslug: providers/transcriber/talkscriber\n---\n\n\n**What is Talkscriber?**\n\nTalkscriber is an advanced AI-powered speech-to-text platform designed to deliver high-accuracy transcription and emotion detection. Focused on enterprise-grade solutions, Talkscriber provides secure, cost-effective, and flexible deployment options. This platform enhances business operations by converting spoken language into text and analyzing customer interactions for deeper insights.\n\n**The Evolution of Speech-to-Text:**\n\nSpeech-to-text technology has significantly evolved, from basic voice recognition to sophisticated AI-driven transcription systems. Innovations in machine learning and natural language processing have paved the way for more accurate and efficient speech-to-text solutions. Talkscriber leverages these advancements to offer state-of-the-art transcription services that cater to modern enterprise needs.\n\n**Overview of Talkscriber’s Offerings:**\n\nTalkscriber offers a suite of AI-driven tools designed to support various applications:\n\n**AI Transcription:**\n\nTalkscriber’s core service is its AI-powered transcription technology, which converts spoken language into text with high accuracy. This technology supports multiple languages and dialects, making it versatile for global applications.\n\n**Emotion Detection:**\n\nTalkscriber includes advanced emotion detection capabilities, identifying emotions such as anger, joy, sadness, and surprise. This feature provides deeper insights into customer interactions and helps businesses understand their clients better.\n\n**API:**\n\nTalkscriber provides a robust API that allows developers to integrate its speech-to-text capabilities into their applications, ensuring low latency and high availability.\n\n**AI Transcription Technology:**\n\nTalkscriber’s AI transcription technology offers several key features and benefits:\n\n**Features:**\n\n- High Accuracy: Industry-leading transcription accuracy with a Word Error Rate (WER) under 4%.\n- Real-time Transcription: Instantaneous conversion of speech to text.\n- Multilingual Support: Supports multiple languages and dialects.\n\n**Benefits:**\n\n- Efficiency: Reduces the time needed to transcribe and analyze speech.\n- Scalability: Handles large volumes of data efficiently.\n- Cost-Effective: Provides high performance at a lower cost compared to other solutions.\n\n**Emotion and Intent Detection:**\n\nTalkscriber’s emotion detection capabilities enhance the analysis of customer interactions:\n\n**Enhancing Interaction Analysis:**\n\n- Emotion Detection: Identifies emotions at the utterance level, providing deeper insights.\n- Purchase Intent Detection: Recognizes customer purchase intent, helping businesses tailor their strategies.\n\n**Developer API:**\n\nTalkscriber offers a comprehensive API for easy integration of their capabilities into various applications:\n\n\n**Integration:**\n\n- SDKs: Available for multiple programming languages.\n- Comprehensive Documentation: Detailed guides and support for seamless implementation.\n\n**Use Cases:**\n\n- Business Solutions: Enhance operational efficiency and customer service.\n- Market Research: Gain insights into customer behavior and preferences.\n\n\n**Use Cases for Talkscriber:**\n\nTalkscriber’s platform supports a wide range of applications across various sectors:\n\n**Business Solutions:**\n\nImprove business operations with accurate transcription and emotion detection.\n\n\n**Customer Service:**\n\nEnhance customer service by understanding and responding to customer emotions and intents.\n\n\n**Market Research:**\n\nGain valuable insights into market trends and customer preferences through advanced speech analysis.\n\n**Impact on Business Operations:**\n\nTalkscriber is revolutionizing business operations by providing tools that enhance productivity and insights. By automating transcription and emotion detection, businesses can focus on innovation and strategy rather than manual processes.\n\n**Innovation and Research:**\n\nTalkscriber is committed to continuous innovation and research in speech AI. Their team of experts focuses on advancing the capabilities of AI transcription and emotion detection, exploring new applications, and refining existing technologies to stay at the forefront of the industry.\n\n**AI Safety and Ethics:**\n\nEnsuring the ethical use of AI is a core principle at Talkscriber. They implement robust safeguards to prevent misuse of their technology and are actively involved in promoting responsible AI development. Protecting user data and maintaining transparency in AI operations are central to their mission.\n\n**Integrations and Compatibility:**\n\nTalkscriber’s API allows seamless integration with various platforms and applications. This ensures that users can incorporate Talkscriber’s AI capabilities into their existing systems effortlessly, enhancing functionality and improving user experience.\n"
    },
    "providers/voiceflow.mdx": {
      "markdown": "---\ntitle: Voiceflow\nsubtitle: Vapi x Voiceflow\nslug: providers/voiceflow\n---\n\n\n## Overview\n\nVoiceflow is a conversational AI platform that helps teams build, manage, and deploy AI agents, especially chatbots, to enhance customer experiences. It enables users to create advanced AI chatbots without coding, using a user-friendly drag-and-drop flow builder. This feature allows businesses to customize chatbot interactions and efficiently automate customer support processes.\n\nTo link VAPI with Voiceflow, host a proxy using Voiceflow's AI features. This proxy handles requests from Voiceflow, sends them to VAPI's text completion API, and returns VAPI's responses to Voiceflow. You'll need to host this proxy on your server to manage communication between VAPI and Voiceflow.\n\n## Workshop\n\nThe workshop conducted by VAPI in collaboration with Voiceflow provided an in-depth exploration of building voice agents using the Voiceflow platform, deployed through VAPI. During this session, participants learned how to create voice agents that leverage Voiceflow's user-friendly design tools alongside VAPI's voice capabilities. The workshop featured a live demonstration, where attendees could see the entire process of building a voice agent in real-time, including designing the agent, setting up necessary integrations, and testing functionality.\n\n<iframe\n        src=\"https://www.youtube.com/embed/PbS9rfopZQA\"\n        title=\"YouTube video player\"\n        frameborder=\"0\"\n        allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"\n        referrerpolicy=\"strict-origin-when-cross-origin\"\n        width=\"100%\"\n        height=\"400px\"\n        allowfullscreen\n/>\nBy the end of the workshop, participants gained insights into building and deploying voice agents, practical skills in designing conversational flows, and an understanding of voice agents.\n"
    },
    "security-and-privacy/hipaa.mdx": {
      "markdown": "---\ntitle: HIPAA Compliance\nsubtitle: Learn how to ensure privacy when using Vapi's voice assistant platform.\nslug: security-and-privacy/hipaa\n---\n\n\n## Introduction to Privacy at Vapi\n\nAt Vapi, we are committed to delivering exceptional voice assistant services while upholding the highest standards of privacy and data protection for our users. We understand the importance of balancing service quality with the need to respect and protect personal and sensitive information. Our privacy policies and practices are designed to give you control over your data while benefiting from the full capabilities of our platform.\n\n## Understanding HIPAA Compliance Basics\n\nThe Health Insurance Portability and Accountability Act (HIPAA) is a United States legislation that provides data privacy and security provisions for safeguarding medical information. HIPAA compliance is crucial for any entity that deals with protected health information (PHI), ensuring that sensitive patient data is handled, stored, and transmitted with the highest standards of security and confidentiality. The key concepts of HIPAA compliance include the Privacy Rule, which protects the privacy of individually identifiable health information; the Security Rule, which sets standards for the security of electronic protected health information (e-PHI); and the Breach Notification Rule, which requires covered entities to notify individuals, HHS, and in some cases, the media of a breach of unsecured PHI. Compliance with these rules is not just about adhering to legal requirements but also about building trust with your customers by demonstrating your commitment to protecting their sensitive data. By enabling the `hipaaEnabled` configuration in Vapi’s voice assistant platform, you are taking a significant step towards aligning your operations with these HIPAA principles, ensuring that your use of technology adheres to these critical privacy and security standards.\n\n## Understanding Default Settings\n\nBy default, Vapi records your calls and stores logs and transcriptions. This practice is aimed at continuously improving the quality of our service, ensuring that you receive the best possible experience. However, we recognize the importance of privacy and provide options for users who prefer more control over their data.\n\n## Opting for Privacy: The HIPAA Compliance Option\n\nFor users prioritizing privacy, particularly in compliance with the Health Insurance Portability and Accountability Act (HIPAA), Vapi offers the flexibility to opt out of our default data recording settings. Choosing HIPAA compliance through our platform ensures that you can still use our voice assistant services without compromising on privacy requirements.\n\n## Enabling HIPAA Compliance\n\nHIPAA compliance can be ensured by enabling the `hipaaEnabled` configuration in your assistant settings. This simple yet effective setting guarantees that no call logs, recordings, or transcriptions are stored during or after your calls. An end-of-call report message will be generated and stored on your server for record-keeping, ensuring compliance without storing sensitive data on Vapi's systems.\n\nTo enable HIPAA compliance, set hipaaEnabled to true within your assistant's configuration:\n\n```JSON\n{\n  \"hipaaEnabled\": true\n}\n```\n\nNote: The default value for hipaaEnabled is false. Activating this setting is a proactive measure to align with HIPAA standards, requiring manual configuration adjustment.\n\n## FAQs\n\n**Q: Will enabling HIPAA compliance affect the quality of Vapi’s service?**\nA: Enabling HIPAA compliance does not degrade the quality of the voice assistant services. However, it limits access to certain features, such as reviewing call logs or transcriptions, that some users may find valuable for quality improvement purposes.\n\n**Q: Who should use the HIPAA compliance feature?**\nA: This feature is particularly useful for businesses and organizations in the healthcare sector or any entity that handles sensitive health information and must comply with HIPAA regulations.\n\n**Q: Can I switch between default and HIPAA-compliant settings?**\nA: Yes, users can toggle the hipaaEnabled setting as needed. However, we recommend carefully considering the implications of each option on your data privacy and compliance requirements.\n\n## Need Further Assistance?\n\nIf you have more questions about privacy, HIPAA compliance, or how to configure your Vapi assistant, our support team is here to help. Contact us at security@vapi.ai for personalized assistance and more information on how to make the most of Vapi’s voice assistant platform while ensuring your data remains protected.\n"
    }
  },
  "search": {
    "type": "singleAlgoliaIndex",
    "value": {
      "type": "unversioned",
      "indexSegment": {
        "id": "seg_vapi.docs.buildwithfern.com_f2c8d6c4-f874-453f-bced-492e8d374153",
        "searchApiKey": "ZWU0NmQyNDU3YTExZjk0ZjE0YjNjYzgxNDViOWE4ZmZlOTAxZmU2YTRlZGYyOGMzNmVjMjRmOWMyMDMxMmIzMWZpbHRlcnM9aW5kZXhTZWdtZW50SWQlM0FzZWdfdmFwaS5kb2NzLmJ1aWxkd2l0aGZlcm4uY29tX2YyYzhkNmM0LWY4NzQtNDUzZi1iY2VkLTQ5MmU4ZDM3NDE1MyZ2YWxpZFVudGlsPTE3Mjk3NTY1MDI="
      }
    }
  },
  "id": "docs_definition_2076fcc5-5050-4063-9305-1b44c36c62de"
}
