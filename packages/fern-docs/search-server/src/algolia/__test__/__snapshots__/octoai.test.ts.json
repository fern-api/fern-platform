[
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.quickstart-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/quickstart",
    "pathname": "/docs/getting-started/quickstart",
    "title": "Quickstart",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Start using our GenAI Solutions in one minute.",
    "content": "Welcome to OctoAI! Our mission is to enable users to harness value from the latest AI innovations by delievering efficient, reliable, and customizable AI systems for your apps. Run your models or checkpoints on our cost-effective API endpoints, or run our optimized GenAI stack in your environment."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.quickstart-get-started-with-inference-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/quickstart",
    "pathname": "/docs/getting-started/quickstart",
    "title": "Get started with inference",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#get-started-with-inference",
    "content": "Sign up for an account - new users get $10 of free credits\n\nRun your first inference:\n\n\nNavigate to a model page and click Get API Token:\n\n\nCopy the code sample to run an inference:",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://text.octoai.run/v1/chat/completions\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello world\"\n            }\n        ],\n        \"model\": \"mixtral-8x7b-instruct\",\n        \"max_tokens\": 512,\n        \"presence_penalty\": 0,\n        \"temperature\": 0.1,\n        \"top_p\": 0.9\n    }'"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Quickstart"
      },
      "h2": {
        "id": "get-started-with-inference",
        "title": "Get started with inference"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.quickstart-next-steps-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/quickstart",
    "pathname": "/docs/getting-started/quickstart",
    "title": "Next steps",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#next-steps",
    "content": "Check out the wide variety of text generation models and media generation models models we support.\n\nLearn more about our Text Gen Solution, Media Gen Solution, or OctoStack.\n\nExplore our demos to see OctoAI in action.",
    "hierarchy": {
      "h0": {
        "title": "Quickstart"
      },
      "h2": {
        "id": "next-steps",
        "title": "Next steps"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.quickstart-additional-resources-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/quickstart",
    "pathname": "/docs/getting-started/quickstart",
    "title": "Additional Resources",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#additional-resources",
    "content": "Pricing & billing\n\nSupport",
    "hierarchy": {
      "h0": {
        "title": "Quickstart"
      },
      "h2": {
        "id": "additional-resources",
        "title": "Additional Resources"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "Pricing & billing",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Only pay for what you use.",
    "content": "At OctoAI you only pay for what you use. Upon sign up you will receive $10 of free credit in your account, and these credits don't expire. That is equivalent of:\nOver a million words of output with the largest Llama 3 70B model and Mixtral 8x7B model.\n\n1,000 SDXL default images and about 66 Stable Video Diffusion animations"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-how-does-billing-work-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "How does billing work?",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#how-does-billing-work",
    "content": "OctoAI uses post-paid billing - add a credit card and pay for your use at the end of each month. All existing credits will remain available within your account and will be used before any post-paid billing is applied.\nOn the 1st day of each month, we’ll send an invoice so you can see the upcoming charge. On the 7th day of each month, we’ll charge the card on file for the prior billing period. If there’s an issue charging your credit card, you can manually pay via the invoice.",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing"
      },
      "h2": {
        "id": "how-does-billing-work",
        "title": "How does billing work?"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-where-can-i-find-my-billing-data-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "Where can I find my billing data?",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#where-can-i-find-my-billing-data",
    "content": "You can view your plan tier, invoices, and itemized usage for all OctoAI services in Billing & Usage in your account at anytime.",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing"
      },
      "h2": {
        "id": "how-does-billing-work",
        "title": "How does billing work?"
      },
      "h3": {
        "id": "where-can-i-find-my-billing-data",
        "title": "Where can I find my billing data?"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-what-are-the-rate-limits-for-each-solution-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "What are the rate limits for each solution?",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#what-are-the-rate-limits-for-each-solution",
    "content": "See rate limits for details, and feel free to contact us to discuss higher limits to meet your needs. You will recieve an HTTP 429 response code if you reach the limit cap.",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing"
      },
      "h2": {
        "id": "how-does-billing-work",
        "title": "How does billing work?"
      },
      "h3": {
        "id": "what-are-the-rate-limits-for-each-solution",
        "title": "What are the rate limits for each solution?"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-media-gen-solution-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "Media Gen Solution",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#media-gen-solution",
    "content": "Below is a full feature breakdown of the Media Gen Solution tiers.\n Trial Pro Enterprise \nSDXL and SD 1.5 text2img, SVD image animation, img2img, Inpainting, ControlNet Cost-optimized Cost-optimized Option for Cost-optimized or Latency-optimized \nCustom Assets (checkpoints, loras, inversions, VAEs) ❌ ✅ ✅ \nUpscaler ✅ ✅ ✅ \nOption for SLA guarantees ❌ ❌ ✅ \nOption for Private Deployment (at higher price) ❌ ❌ ✅ \nDedicated Customer Success Manager ❌ ❌ ✅",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing"
      },
      "h2": {
        "id": "media-gen-solution",
        "title": "Media Gen Solution"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-pro-pricing-for-media-gen-solution-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "Pro pricing for Media Gen Solution",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#pro-pricing-for-media-gen-solution",
    "content": "Pricing for default image features and configurations are below:\nFeature Type Steps Resolution Sampler Price \nSVD 1.1 25 all supported N/A $0.15/animation \nSDXL 30 1024x1024 DDIM (and any not listed below as premium) $0.004/image \nSDXL with Custom Asset (Fine-tuned) 30 1024x1024 DDIM (and any not listed below as premium) $0.008/image \nSDXL Lightning base 4 1024x1024 DDIM (and any not listed below as premium) $0.001/image \nSDXL Lightning Custom Asset (Fine-tuned) 4 1024x1024 DDIM (and any not listed below as premium) $0.005/image \nSDXL Fine-tuning 500 N/A N/A $0.25/tune \nSD 1.5 with Base or Custom Asset (Fine-tuned) 30 512x512 DDIM (and any not listed below as premium) $0.0015/image \nSD1.5 Fine-tuning 500 N/A N/A $0.1/tune \nAsset library (storage) N/A N/A N/A $0.05/GB stored per month, after the first 50GB \nUpscaling N/A N/A N/A $0.004/request \nBackground Removal N/A N/A N/A $0.002/request \nPhoto Merge 30 1024x1024 N/A $0.015/image \nAdetailer N/A N/A N/A $0.0004/object \n\nThe price for each feature type changes as listed below for non-default configurations:\nConfiguration Type Price Formula \nImage Animation Steps Default price * (step_count/25) \nImage Generation Steps Default price * (step_count/30) \nSDXL Resolutions Default price *(pixel_count/(1024*1024)) \nSD1.5 Resolutions Default price * (pixel_count/(512*512)) \nPremium Samplers: DPM_2, DPM_2_ANCESTRAL, DPM_PLUS_PLUS_SDE_KARRAS, HEUN, KLMS Default price *2 \nFine-tuning Steps Default price * (step_count/500) \n\nHere are a few examples to illustrate how this works to assist you in applying to your own use case:\nFeature Type Steps Resolution Sampler Price \nSDXL 40 1024x1024 DDIM (default) $.0053 \nSDXL 40 1024x1024 DPM_2_ANCESTRAL (premium) $.0107 \nSDXL Lightning base 4 1024x1024 DDIM (default) $.001 \nSDXL Lightning with Custom Asset 4 1024x1024 DDIM (default) $.005 \nSDXL with Custom Asset (Fine-tuned) 60 1024x1024 DDIM (default) $.016 \nSDXL with Custom Asset (Fine-tuned) 60 1024x1024 DPM_2 (premium) $.032 \nSDXL Fine-tuning 1000 N/A N/A $.5 \nSD 1.5 40 512x512 DDIM (default) $.002 \nSD1.5 60 1024x1024 DDIM (default) $.003 \nSD1.5 40 1024x1024 DPM_2 (premium) $.009",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing"
      },
      "h2": {
        "id": "media-gen-solution",
        "title": "Media Gen Solution"
      },
      "h4": {
        "id": "pro-pricing-for-media-gen-solution",
        "title": "Pro pricing for Media Gen Solution"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-text-gen-solution-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "Text Gen Solution",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#text-gen-solution",
    "content": "We offer simple, competitive token-based pricing for text gen endpoints, with prices varying depending on parameter size and quantization level:\nModel Sizes Per M Tokens \nMixtral-8x7B models $0.45 \nMixtral-8x22B models $1.20 \n7B and 8B models $0.15 \n13B models $0.20 \n32B models $0.75 \n34B models $0.75 \n70B models $0.90 \nGTE-large $0.05 \n\nIf you would like to explore pricing for other models, quantization levels, or specific fine tunes, contact us.",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing"
      },
      "h2": {
        "id": "text-gen-solution",
        "title": "Text Gen Solution"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-compute-service-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "Compute Service",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#compute-service",
    "content": "Trial Pro Enterprise \nDeploy endpoint from any container (private or public registry) ✅ ✅ ✅ \nExample models from community ✅ ✅ ✅ \nCLI and SDK for containerizing + deploying Python models ✅ ✅ ✅ \nMax endpoints per account 2 10 No limit \nMax replicas per endpoint 3 10 No limit \nAuto-acceleration of PyTorch models ❌ ❌ Early access \nDedicated Customer Success Manager ❌ ❌ ✅",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing"
      },
      "h2": {
        "id": "compute-service",
        "title": "Compute Service"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-pro-pricing-for-compute-service-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "title": "Pro pricing for Compute Service",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#pro-pricing-for-compute-service",
    "content": "Large 80: A100 GPU with 80GB memory @ $0.00145 per second (~$5.20 per hour)\n\nLarge 40: A100 GPU with 40GB memory @ $0.00114 per second (~$4.10 per hour)\n\nMedium: A10 GPU with 24GB memory @ $0.00032 per second (~$1.15 per hour)\n\nSmall: T4 GPU with 16GB memory @ $0.00011 per second (~$0.40 per hour)\n\n\nBilling is by the second of compute usage, starting at the time when the endpoint is ready for inferences. The time when the endpoint is ready for inferences is when either the healtcheck on your end point begins returning 200, or if there is no healthcheck, the time you see the “Replica is running” log line in your events tab.\nYou will be billed for the total inference duration and timeout duration\n\nYou will not be billed for the duration of cold start\n\n\nExample models in the platform have a pre-set hardware / pricing tier. If you create an endpoint from a custom model, you can choose the tier best suited to your needs.",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing"
      },
      "h2": {
        "id": "compute-service",
        "title": "Compute Service"
      },
      "h4": {
        "id": "pro-pricing-for-compute-service",
        "title": "Pro pricing for Compute Service"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.text-gen-rest-api-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/rest-api",
    "pathname": "/docs/text-gen-solution/rest-api",
    "title": "Text Gen REST API",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "All OctoAI text generation models are accessible via REST API. Learn how to implement with easy to follow code examples.",
    "content": "All of our text generation models are accessible via REST API, and we follow the \"Chat Completions\" standard popularized by OpenAI. Below you can see a simple cURL example and JSON response for our endpoint, along with explnations of all parameters."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.text-gen-rest-api-input-parameters-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/rest-api",
    "pathname": "/docs/text-gen-solution/rest-api",
    "title": "Input Parameters",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#input-parameters",
    "content": "model (string): The model to be used for chat completion. Here is the complete list of presently supported model arguments. For more information regarding these models, see this description.\n\n\nmax_tokens (integer, optional): The maximum number of tokens to generate for the chat completion.\n\nmessages (list of objects): A list of chat messages, where each message is an object with properties: role and content. Supported roles are “system”, “assistant”, and “user”.\n\ntemperature (float, optional): A value between 0.0 and 2.0 that controls the randomness of the model's output.\n\ntop_p (float, optional): A value between 0.0 and 1.0 that controls the probability of the model generating a particular token.\n\nstop (list of strings, optional): A list of strings that the model will stop generating text if it encounters any of them.\n\nfrequency_penalty (float, optional): A value between 0.0 and 1.0 that controls how much the model penalizes generating repetitive responses.\n\npresence_penalty (float, optional): A value between 0.0 and 1.0 that controls how much the model penalizes generating responses that contain certain words or phrases.\n\nstream (boolean, optional): Indicates whether the response should be streamed.",
    "code_snippets": [
      {
        "code": "  \"llama-2-13b-chat\",\n  \"llama-2-70b-chat\",\n  \"codellama-7b-instruct\",\n  \"codellama-13b-instruct\",\n  \"codellama-34b-instruct\",\n  \"mistral-7b-instruct\"\n  \"mixtral-8x7b-instruct\"\n  \"nous-hermes-2-mixtral-8x7b-dpo\"\n  \"hermes-2-pro-mistral-7b\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Text Gen REST API"
      },
      "h3": {
        "id": "input-parameters",
        "title": "Input Parameters"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.text-gen-rest-api-streaming-response-sample-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/rest-api",
    "pathname": "/docs/text-gen-solution/rest-api",
    "title": "Streaming Response Sample:",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#streaming-response-sample",
    "content": "Once parsed to JSON, you will see the content of the streaming response similar to below:\nWithout parsing, the text stream will start with data: for each chunk. Below is an example. Please note, the final chunk contains simply data: [DONE] as text which can break JSON parsing if not accounted for.",
    "code_snippets": [
      {
        "lang": "JSON",
        "meta": "JSON",
        "code": "// Starting chunk, note that content is null and finish_reason is also null.\n{\n  \"id\":\"cmpl-994f6307a891454cb0f57b7027f5f113\",\n  \"created\":1700527881,\n  \"model\":\"llama-2-13b-chat\",\n  \"choices\":\n  [\n    {\n      \"index\":0,\n      \"delta\":\n      {\n        \"role\":\"assistant\",\n        \"content\":null\n      },\n      \"finish_reason\":null\n    }\n  ]\n}\n// Ending chunk, note the finish_reason \"length\" instead of null.\n// This means we reached the max tokens allowed in this request.\n// The \"object\" field is \"chat.completion.chunk\" for the body of responses.\n{\n  \"id\":\"cmpl-994f6307a891454cb0f57b7027f5f113\",\n  \"object\":\"chat.completion.chunk\",\n  \"created\":1700527881,\n  \"model\":\"llama-2-13b-chat\",\n  \"choices\":\n  [\n    {\n      \"index\":0,\n      \"delta\":\n      {\n        \"role\":\"assistant\",\n        \"content\":\"\",\n        \"function_call\":null\n      },\n      \"finish_reason\":\"length\"\n    }\n  ]\n}"
      },
      {
        "code": "data: {\"id\": \"cmpl-994f6307a891454cb0f57b7027f5f113\", \"created\": 1700527881, \"model\": \"llama-2-13b-chat\", \"choices\": [{\"index\": 0, \"delta\": {\"role\": \"assistant\", \"content\": null}, \"finish_reason\": null}]}\n\ndata: {\"id\": \"cmpl-994f6307a891454cb0f57b7027f5f113\", \"object\": \"chat.completion.chunk\", \"created\": 1700527881, \"model\": \"llama-2-13b-chat\", \"choices\": [{\"index\": 0, \"delta\": {\"role\": \"assistant\", \"content\": \"\", \"function_call\": null}, \"finish_reason\": null}]}\n\ndata: {\"id\": \"cmpl-994f6307a891454cb0f57b7027f5f113\", \"object\": \"chat.completion.chunk\", \"created\": 1700527881, \"model\": \"llama-2-13b-chat\", \"choices\": [{\"index\": 0, \"delta\": {\"role\": \"assistant\", \"content\": \"Hello\", \"function_call\": null}, \"finish_reason\": null}]}\n\ndata: {\"id\": \"cmpl-994f6307a891454cb0f57b7027f5f113\", \"object\": \"chat.completion.chunk\", \"created\": 1700527881, \"model\": \"llama-2-13b-chat\", \"choices\": [{\"index\": 0, \"delta\": {\"role\": \"assistant\", \"content\": \"!\", \"function_call\": null}, \"finish_reason\": null}]}\n\ndata: {\"id\": \"cmpl-994f6307a891454cb0f57b7027f5f113\", \"object\": \"chat.completion.chunk\", \"created\": 1700527881, \"model\": \"llama-2-13b-chat\", \"choices\": [{\"index\": 0, \"delta\": {\"role\": \"assistant\", \"content\": \"\", \"function_call\": null}, \"finish_reason\": null}]}\n\ndata: {\"id\": \"cmpl-994f6307a891454cb0f57b7027f5f113\", \"object\": \"chat.completion.chunk\", \"created\": 1700527881, \"model\": \"llama-2-13b-chat\", \"choices\": [{\"index\": 0, \"delta\": {\"role\": \"assistant\", \"content\": \"\", \"function_call\": null}, \"finish_reason\": \"stop\"}]}\n\ndata: [DONE]\n"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Text Gen REST API"
      },
      "h3": {
        "id": "streaming-response-sample",
        "title": "Streaming Response Sample:"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.text-gen-rest-api-response-parameters-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/rest-api",
    "pathname": "/docs/text-gen-solution/rest-api",
    "title": "Response Parameters",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#response-parameters",
    "content": "Parameters\nid (string): A unique identifier for the chat completion.\n\nchoices (list of objects):\nThis is a list of chat completion choices, each represented as an object.\n\nEach object within the choices list contains the following fields:\n\n_ index (integer): The position of the choice in the list of generated completions.\n\n_ message (object):\n\n_ An object representing the content of the chat completion, which includes:\n\n_ role (string): The role associated with the message, typically \"assistant\" for the generated response.\n\n_ content (string): The actual text content of the chat completion.\n\n_ function_call (object or null): An optional field that may contain information about a function call made within the message. It's usually null in standard responses.\n\n_ delta (object or null): An optional field that can contain additional metadata about the message, typically null.\n\n_ finish_reason (string): The reason why the message generation was stopped, such as reaching the maximum length (\"length\").\n\n\n\ncreated (integer): The Unix timestamp (in seconds) of when the chat completion was created.\n\nmodel (string): The model used for the chat completion.\n\nobject (string): The object type, which is always chat.completion.\n\nsystem_fingerprint (object or null): An optional field that may contain system-specific metadata.\n\nusage (object):\nUsage statistics for the completion request, detailing token usage in the prompt and completion.",
    "hierarchy": {
      "h0": {
        "title": "Text Gen REST API"
      },
      "h3": {
        "id": "response-parameters",
        "title": "Response Parameters"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "title": "Migrate from OpenAI to OctoAI in 3 lines of code",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "If you've been using GPT-3.5 or GPT-4, switching to Octo AI is easy!",
    "content": "OctoAI LLMs are available to use through our OpenAI compatible API. Additionally, if you have been building or prototyping using OpenAI's Python SDK you can keep your code as-is and use OctoAI's LLM models.\nIn this example, we will show you how to change just three lines of code to make your Python application use OctoAI's Open Source models through OpenAI's Python SDK."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-what-you-will-build-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "title": "What you will build",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#what-you-will-build",
    "content": "Migrate OpenAI's Python SDK example script to use OctoAI's LLM endpoints.\nThese are the three modifications necessary to achieve our goal:\nRedefine OPENAI_API_KEY your API key environment variable to use your OctoAI key.\n\nRedefine OPENAI_BASE_URL to point to https://text.octoai.run/v1\n\nChange the model name to an Open Source model, for example: llama-2-13b-chat",
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code"
      },
      "h2": {
        "id": "what-you-will-build",
        "title": "What you will build"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "title": "Requirements",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#requirements",
    "content": "We will be using Python and OpenAI's Python SDK.",
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code"
      },
      "h2": {
        "id": "requirements",
        "title": "Requirements"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-instructions-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "title": "Instructions",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#instructions",
    "content": "Set up a Python virtual environment. Read Creating Virtual Environments here.\n\n\nInstall the pip requirements in your local python virtual environment",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "python3 -m venv .venv\nsource .venv/bin/activate"
      },
      {
        "lang": "bash",
        "code": "python3 -m pip install openai"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code"
      },
      "h2": {
        "id": "instructions",
        "title": "Instructions"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-environment-setup-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "title": "Environment setup",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#environment-setup",
    "content": "To run this example, there are simple steps to take:\nGet an OctoAI API token by following these instructions.\n\nExpose the token in a new OCTOAI_TOKEN environment variable:\n\n\nSwitch the OpenAI token and base URL environment variable\n\n\nIf you prefer, you can also directly paste your token into the client initialization.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "export OCTOAI_TOKEN=<your-token>"
      },
      {
        "lang": "bash",
        "code": "export OPENAI_API_KEY=$OCTOAI_TOKEN\nexport OPENAI_BASE_URL=\"https://text.octoai.run/v1\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code"
      },
      "h2": {
        "id": "instructions",
        "title": "Instructions"
      },
      "h3": {
        "id": "environment-setup",
        "title": "Environment setup"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-example-code-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "title": "Example code",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#example-code",
    "content": "Once you've completed the steps above, the code below will call OctoAI LLMs:\nNote that you need to supply one of OctoAI's supported LLMs as an argument, as in the example above. For a complete list of our supported LLMs, check out our REST API page.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "from openai import OpenAI\n\nclient = OpenAI()\n\ncompletion = octoai.text_gen.create_chat_completion(\n    # model=\"gpt-3.5-turbo\",\n    model=\"llama-2-13b-chat\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n)\n\nprint(completion.choices[0].message)"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code"
      },
      "h2": {
        "id": "instructions",
        "title": "Instructions"
      },
      "h3": {
        "id": "example-code",
        "title": "Example code"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-example-output-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "title": "Example output",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#example-output",
    "content": "The code above produces the following object:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "\nChatCompletionMessage(content=\"  Hello! How can I assist you today? Do you have any questions or tasks you'd like help with? Please let me know and I'll do my best to assist you.\", role='assistant' function_call=None, tool_calls=None)\n"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code"
      },
      "h2": {
        "id": "instructions",
        "title": "Instructions"
      },
      "h3": {
        "id": "example-output",
        "title": "Example output"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.image-gen-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/image-gen-api",
    "pathname": "/docs/media-gen-solution/rest-apis/image-gen-api",
    "title": "Image Gen REST API",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -H 'Content-Type: application/json' -H \"Authorization: Bearer $OCTOAI_TOKEN\" -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -d '{\n        \"prompt\": \"The angel of death Hyperrealistic, splash art, concept art, mid shot, intricately detailed, color depth, dramatic, 2/3 face angle, side light, colorful background\",\n        \"negative_prompt\": \"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, signature, cut off, draft\",\n        \"sampler\": \"DDIM\",\n        \"cfg_scale\": 11,\n        \"height\": 1024,\n        \"width\": 1024,\n        \"seed\": 2748252853,\n        \"steps\": 20,\n        \"num_images\": 1,\n        \"high_noise_frac\": 0.7,\n        \"strength\": 0.92,\n        \"use_refiner\": true,\n        \"style_preset\": \"3d-model\"\n    }' > response.json"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import requests\nimport os\nimport base64\nimport io\nimport PIL.Image\n\ndef _process_test(url):\n\n    OCTOAI_TOKEN = os.environ.get(\"OCTOAI_TOKEN\")\n\n    payload = {\n        \"prompt\": \"Face of a yellow cat, high resolution, sitting on a park bench\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, bad quality\",\n        \"steps\": 30,\n        \"width\": 1024,\n        \"height\": 1024,\n    }\n\n    headers = {\n        \"Authorization\": f\"Bearer {OCTOAI_TOKEN}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n\n    if response.status_code != 200:\n        print(response.text)\n\n    img_list = response.json()[\"images\"]\n\n    for i, img_info in enumerate(img_list):\n        img_bytes = base64.b64decode(img_info[\"image_b64\"])\n        img = PIL.Image.open(io.BytesIO(img_bytes))\n        img.load()\n        img.save(f\"result_image{i}.jpg\")\n\nif __name__ == \"__main__\":\n    _process_test(\"https://image.octoai.run/generate/sdxl\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt: \"A photo of a cute cat astronaut in space\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -H 'Content-Type: application/json' -H \"Authorization: Bearer $OCTOAI_TOKEN\" -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -d '{\n        \"prompt\": \"The angel of death Hyperrealistic, splash art, concept art, mid shot, intricately detailed, color depth, dramatic, 2/3 face angle, side light, colorful background\",\n        \"negative_prompt\": \"ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, signature, cut off, draft\",\n        \"sampler\": \"DDIM\",\n        \"cfg_scale\": 11,\n        \"height\": 1024,\n        \"width\": 1024,\n        \"seed\": 2748252853,\n        \"steps\": 20,\n        \"num_images\": 1,\n        \"high_noise_frac\": 0.7,\n        \"strength\": 0.92,\n        \"use_refiner\": true,\n        \"style_preset\": \"3d-model\"\n    }' > response.json"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import requests\nimport os\nimport base64\nimport io\nimport PIL.Image\n\ndef _process_test(url):\n\n    OCTOAI_TOKEN = os.environ.get(\"OCTOAI_TOKEN\")\n\n    payload = {\n        \"prompt\": \"Face of a yellow cat, high resolution, sitting on a park bench\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, bad quality\",\n        \"steps\": 30,\n        \"width\": 1024,\n        \"height\": 1024,\n    }\n\n    headers = {\n        \"Authorization\": f\"Bearer {OCTOAI_TOKEN}\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n\n    if response.status_code != 200:\n        print(response.text)\n\n    img_list = response.json()[\"images\"]\n\n    for i, img_info in enumerate(img_list):\n        img_bytes = base64.b64decode(img_info[\"image_b64\"])\n        img = PIL.Image.open(io.BytesIO(img_bytes))\n        img.load()\n        img.save(f\"result_image{i}.jpg\")\n\nif __name__ == \"__main__\":\n    _process_test(\"https://image.octoai.run/generate/sdxl\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt: \"A photo of a cute cat astronaut in space\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      }
    ],
    "content": "All of our image generation models are accessible via REST API. Below you can see a simple cURL/Python SDK and TypeScript SDK example for our image gen endpoints, along with explanations of all parameters.\nOur URL for image generations is at https://image.octoai.run/generate/{engine_id}, where engine_id is one of the following:\nsdxl: Stable DiffusionXL v1.0\n\nsd: Stable Diffusion v1.5\n\ncontrolnet-sdxl: ControlNet SDXL\n\ncontrolnet-sd15: ControlNet SD1.5\n\n\nThis includes text-to-image, image-to-image, controlnets, photo merge, inpainting and outpainting.\nInput Sample"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.image-gen-image-generation-arguments-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/image-gen-api",
    "pathname": "/docs/media-gen-solution/rest-apis/image-gen-api",
    "title": "Image Generation Arguments",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#image-generation-arguments",
    "content": "prompt: A string describing the image to generate.\nWe currently have a 77 token limit on prompts for SDXL and 231 for SD 1.5\n\nYou can use prompt weighting, e.g. (A tall (beautiful:1.5) woman:1.0) (some other prompt with weight:0.8) . The weight will be the product of all brackets a token is a member of. The brackets, colons and weights do not count towards the number of tokens.\n\n\n\nprompt_2: This only applies to SDXL. By default, setting only prompt copies the input to both prompt and prompt_2. When prompt and prompt_2 are both set, they have very different functionality. The second prompt is meant for more human readable descriptions of the desired image.\nFor example, prompt is used for \"word salad\" style control of the image. This is the type of prompting you are likely familiar with from SD 1.5. Prompts like the following work well:\n\nprompt = \"photorealistic, high definition, masterpiece, sharp lines\"  \n\n\nwhereas prompt_2 is meant for more human readable descriptions of the desired image. For example:\n\nprompt_2 = \"A portrait of a handsome cat wearing a little hat. The cat is in front of a colorful background.  \n\n\n\n\nnegative_prompt Optional: A string indicating a prompt for guidance to steer away from. Unused when not provided.\n\nnegative_prompt_2: This only applies to SDXL. This prompt is meant for human readable descriptions of what you don’t want the image, e.g. you would say “Low resolution” in negative_prompt then “Bad hands” in negative_prompt_2.\n\nsampler Optional: A string specifying which scheduler to use when generating an image. Defaults to DDIM. Regular samplers include DDIM,DDPM,DPM_PLUS_PLUS_2M_KARRAS,DPM_SINGLE,DPM_SOLVER_MULTISTEP,K_EULER, K_EULER_ANCESTRAL,PNDM,UNI_PC. Premium samplers (2x price) include DPM_2, DPM_2_ANCESTRAL,DPM_PLUS_PLUS_SDE_KARRAS, HEUN and KLMS.\n\nheight Optional: An integer specifying the height of the output image. Defaults to 1024 for SDXL and 512 for SD 1.5.\n\nwidth Optional: An integer specifying the width of the output image. Defaults to 1024 for SDXL and 512 for SD 1.5.\n\n\nSupported Output Resolutions (Width x Height) are as follows:\nFor SDXL:\nFor SD1.5\n\n\ninit_image and mask_image will be resized to the specified resolution\nbefore applying img2img or inpainting.\ncfg_scale Optional: How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt). When not set defaults to 12.\n\nsteps Optional: How many steps of diffusion to perform. The higher this is, the higher the image clarity will be but proportionally increases the runtime. Defaults to 30 when not set.\n\nnum_images: An integer describing the number of images to generate. Defaults to 1\n\nseed Optional: An integer that fixes the random noise of the model. Using the same seed guarantees the same output image, which can be useful for testing or replication. Use null to select a random seed.\n\nuse_refiner: This only applies to SDXL. A boolean true or false determines whether to use the refiner or not\n\nhigh_noise_frac Optional: This only applies to SDXL. A floating point or integer determining how much noise should be applied using the base model vs. the refiner. A value of 0.8 will apply the base model at 80% and Refiner at 20%. Defaults to 0.8 when not set.\n\ncheckpoint: Here you can specify a checkpoint either from the OctoAI asset library or your private asset library. Note that using a custom asset increases generation time.\n\nloras: Here you can specify LoRAs, in name-weight pairs, either from the OctoAI asset library or your private asset library. Note that using a custom asset increases generation time.\n\ntextual_inversions: Here you can specify textual inversions and their corresponding trigger words. Note that using a custom asset increases generation time.\n\nvae: Here you can specify variational autoencoders. Note that using a custom asset increases generation time.\n\n\nHere’s an example of how to mix OctoAI assets (checkpoints, loras, and textual_inversions) in the same API request.\n\n\nOoctoAI assets require an “octoai:” prefix but your private assets DO NOT.\nAsset names need to be unique per account\nstyle_preset Optional: This only applies to SDXL. Used to guide the output image towards a particular style. Defaults to None. Supported values for styles present include base, 3d-model, Abstract,Advertising, Alien, analog-film, anime,Architectural, cinematic, Collage,comic-book, Craft Clay, Cubist,digital-art, Disco,Dreamscape,Dystopian, enhance, Fairy Tale,fantasy-art, Fighting Game, Film Noir, Flat Papercut, Food Photography, Gothic, Graffiti, Grunge, HDR, Horror, Hyperrealism, Impressionist, isometric, Kirigami, line-art,Long Exposure,low-poly,Minimalist,modeling-compound,Monochrome,Nautical,\nNeon Noir,neon-punk,origami,Paper Mache, Paper Quilling,Papercut Collage,Papercut Shadow Box,photographic,pixel-art,Pointillism,Pop Art,Psychedelic,Real Estate,Renaissance,Retro Arcade,Retro Game,RPG Fantasy,Game,Silhouette,Space,Stacked Papercut,Stained Glass,Steampunk,Strategy Game,Surrealist,Techwear Fashion,Thick Layered Papercut,tile-texture,Tilt-Shift,\nTribal,Typography,Watercolor,Zentangle\n\ninit_image Optional: Only applicable for Img2Img and inpainting use cases i.e. to use an image as a starting point for image generation. Argument takes an image encoded as a string in base64 format.\n\n\nUse .jpg format to ensure best latency \n\nstrength Optional: Only applicable for img2img use cases. A floating point or integer determines how much noise should be applied. Values that approach 1.0 allow for high variation i.e. ignoring the image entirely, but will also produce images that are not semantically consistent with the input and 0.0 keeps the input image as-is. Defaults to 0.8 when not set.\n\nmask_image Optional: Only applicable for inpainting use cases i.e. to specify which area of the picture to paint onto. Argument takes an image encoded as a string in base64 format.\nUse .jpg format to ensure best latency\n\n\n\noutpainting Optional: Only applicable for outpainting use cases. Argument takes a boolean value to determine Whether the request requires outpainting or not. If so, special preprocessing is applied for better results. Defaults to false\n\ntransfer_images Optional: This is our Photo Merge feature. Applicable for use cases where you wish to transfer the subject in the uploaded image(s) to the output image(s). Argument takes a dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object to transfer.\n\n\ncontrolnet Optional: Required if using a controlnet engine. Argument takes in the value of ControlNet to be used during image generation. We offer the following list of public OctoAI controlnet checkpoints in the OctoAI Asset Library.\nOther than using the default controlnet checkpoints, you can also upload private ControlNet checkpoints into the OctoAI Asset Library and then use those checkpoints at generation time via the parameter controlnet. For custom controlnet checkpoints, make sure to provide your own ControlNet mask in the controlnet_image parameter\n\ncontrolnet_conditioning_scale Optional: Only applicable if using Controlnets. Argument determines how strong the effect of the controlnet will be. Defaults to 1\n\ncontrolnet_early_stop Optional:Only applicable if using Controlnets. If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.\n\ncontrolnet_image Optional: Required if using a controlnet engine. Controlnet image encoded in b64 string for guiding image generation.\n\ncontrolnet_preprocess Optional:Only applicable if using Controlnets. Argument takes in a boolean value to determine whether or not to apply automatic ControlNet preprocessing. For the privileged set of controlnet checkpoints listed above, we default to helping you autogenerate the corresponding controlnet map/mask that will be fed into the controlnet, but you can override the default by additionally specifying a controlnet_preprocess: false parameter.\n\n\nPython Example for ControlNet Canny with a custom controlnet map:",
    "code_snippets": [
      {
        "code": "(1024, 1024),(896, 1152),(832, 1216),(768,\n1344),(640, 1536),(1536, 640),(1344, 768),\n(1216, 832),(1152, 896)"
      },
      {
        "code": "(512, 512),(640, 512),(768, 512),(512, 704),\n(512, 768),(576, 768),(640, 768),(576, 1024),\n(1024, 576)"
      },
      {
        "lang": "json",
        "code": "payload = {\n    ...\n\t\"checkpoint\": \"octoai:realcartoon\",\n    \"loras\": {\n        \"octoai:crayon-style\": 0.7,\n        \"your-custom-lora\": 0.3\n    },\n    \"textual_inversions\": {\n        \"octoai:NegativeXL\": \"negativeXL_D\",\n    },\n\t\"vae\": \"your_vae_name\"\n    ...\n}"
      },
      {
        "lang": "json",
        "code": "payload = {\n    ...\n        \"prompt\": \"A trigger_word_1 sitting on a golden throne\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, bad quality\",\n        \"checkpoint\": \"octoai:RealVisXL\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 2,\n        \"sampler\": \"K_EULER_ANCESTRAL\",\n        \"steps\": 20,\n        \"cfg_scale\": 7.5,\n        \"transfer_images\": {\"trigger_word_1\": [\"$BASE64_IMAGE_1\", \"$BASE64_IMAGE_2\"]\n    ...\n}"
      },
      {
        "code": "octoai:canny_sdxl\noctoai:depth_sdxl\noctoai:openpose_sdxl\noctoai:canny_sd15\noctoai:depth_sd15\noctoai:inpaint_sd15\noctoai:ip2p_sd15\noctoai:lineart_sd15\noctoai:openpose_sd15\noctoai:scribble_sd15\noctoai:tile_sd15"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Image Gen REST API"
      },
      "h3": {
        "id": "image-generation-arguments",
        "title": "Image Generation Arguments"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.video-gen-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "pathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "title": "Video Gen REST API",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Animate and add motion to your images",
    "content": "Our video generation model is accessible via REST API. Below, you'll find straightforward examples using cURL/Python SDK and TypeScript SDK for our video generation endpoints, complete with explanations of all parameters.\nThe endpoint URL for video generation is https://image.octoai.run/generate/svd.\nThis encompasses image-to-video conversion. Additionally, we offer support for a text-to-video workflow, which involves utilizing the text-to-image API (Image Gen API) followed by the image-to-video API."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.video-gen-request-payload-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "pathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "title": "Request payload",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#request-payload",
    "content": "Parameters:\nimage (base64 encoded image, required) - Starting point image encoded in base64 string\n\nheight (int; optional) - Integer representing the height of video/animation to generate- If not provided, the output height will be inferred from the input 'image', and the closest resolution supported will be chosen.\n\nwidth (int; optional) - Integer representing the width of video/animation to generate- If not provided, the output width will be inferred from the input 'image', and the closest resolution supported will be chosen.\nSupported resolutions are (w,h): (576, 1024), (1024, 576), (768, 768)\n\ncfg_scale (float; optional) - Floating-point number representing how closely to adhere to 'image' description- Must be a positive number no greater than 10.0.\n\nfps (int; optional) - How fast the generated frames should play back.\n\nsteps (int; optional) - Integer representing how many steps of diffusion to run- Must be greater than 0 and less than or equal to 50.\n\nmotion_scale (float; optional) - A floating point number between 0 and 1 indicating how much motion should be in the generated animation.\n\nnoise_aug_strength (float; optional) - How much noise to add to the initial image- higher values encourage creativity.\n\nnum_videos (int; optional) - Integer representing how many output videos/animations to generate with a single image and configuration. You can generate upto 16 videos in a single API request. All videos will be generated in sequence within the same configurations but different seed values.\n\nseed (int; optional) - Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific video (or set of videos).",
    "hierarchy": {
      "h0": {
        "title": "Video Gen REST API"
      },
      "h2": {
        "id": "request-payload",
        "title": "Request payload"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.video-gen-response-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "pathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "title": "Response",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#response",
    "content": "videos (list) - List of generation(s) generated by the request.\n\nprediction_time_ms (float) - Total runtime of the video/animations(s) generation(s).",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/svd\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"image\": \"<BASE_64_STRING>\",\n        \"steps\": 40,\n        \"cfg_scale\": 3,\n        \"fps\": 4,\n        \"motion_scale\": 0.2,\n        \"noise_aug_strength\": 0.55,\n        \"num_videos\": 1,\n        \"seed\": \"2138732363\"\n    }' | jq -r \".videos[0].video\" | base64 -d >result.mp4"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    video_gen_response = client.image_gen.generate_svd(\n        image=\"<BASE_64_STRING>\",\n        steps=25,\n        cfg_scale=3,\n        fps=7,\n        motion_scale=0.5,\n        noise_aug_strength=0.02,\n        num_videos=1,\n    )\n    videos = video_gen_response.videos\n\n    for i, image in enumerate(videos):\n        to_file(image, f\"result{i}.mp4\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { videos } = await octoai.imageGen.generateSvd({\n  image: \"<BASE64_STRING>\",\n  steps: 25,\n  cfgScale: 3,\n  fps: 7,\n  motionScale: 0.5,\n  noiseAugStrength: 0.02,\n  numVideos: 1,\n});\n\nvideos.forEach((output, i) => {\n  if (output.video) {\n    const buffer = Buffer.from(output.video, \"base64\");\n    fs.writeFileSync(`result${i}.mp4`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/svd\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"image\": \"<BASE_64_STRING>\",\n        \"steps\": 40,\n        \"cfg_scale\": 3,\n        \"fps\": 4,\n        \"motion_scale\": 0.2,\n        \"noise_aug_strength\": 0.55,\n        \"num_videos\": 1,\n        \"seed\": \"2138732363\"\n    }' | jq -r \".videos[0].video\" | base64 -d >result.mp4"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    video_gen_response = client.image_gen.generate_svd(\n        image=\"<BASE_64_STRING>\",\n        steps=25,\n        cfg_scale=3,\n        fps=7,\n        motion_scale=0.5,\n        noise_aug_strength=0.02,\n        num_videos=1,\n    )\n    videos = video_gen_response.videos\n\n    for i, image in enumerate(videos):\n        to_file(image, f\"result{i}.mp4\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { videos } = await octoai.imageGen.generateSvd({\n  image: \"<BASE64_STRING>\",\n  steps: 25,\n  cfgScale: 3,\n  fps: 7,\n  motionScale: 0.5,\n  noiseAugStrength: 0.02,\n  numVideos: 1,\n});\n\nvideos.forEach((output, i) => {\n  if (output.video) {\n    const buffer = Buffer.from(output.video, \"base64\");\n    fs.writeFileSync(`result${i}.mp4`, buffer);\n  }\n});"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Video Gen REST API"
      },
      "h2": {
        "id": "response",
        "title": "Response"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.background-removal-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "pathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "title": "Background Removal REST API",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Background removal takes an existing image you provide and removes those parts of the image considered to be “background.",
    "content": "Background removal takes an existing image you provide and removes those parts of the image considered to be “background.”"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.background-removal-request-payload-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "pathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "title": "Request payload",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#request-payload",
    "content": "Parameters:\ninit_image (str, required) - A base64-encoded image whose background should get removed.\n\nalpha_matting (bool; false) - If true, apply matting on the alpha channel.\n\nalpha_matting_foreground_threshold (number in [0, 255]; 240) - When alpha_matting is true, mask pixels larger than this value are considered foreground pixels.\n\nalpha_matting_background_threshold (number in [0, 255]; 10) - When alpha_matting is true, mask pixels smaller than this value are considered background pixels.\n\nalpha_matting_erode_size (number; 10) - When alpha_matting is true, size of the erosion structure to apply, in pixels.\n\nonly_mask (bool; false) - When true, return only a single-channel image containing a foreground-background mask. Foreground pixels have values closer to 255, and background pixels have values closer to 0.\n\npost_process_mask (bool; true) - When true, apply morphological operations to the mask to smooth it.\n\nbgcolor (list[int]; optional) - When given, replace background pixels with this color in the output image.",
    "hierarchy": {
      "h0": {
        "title": "Background Removal REST API"
      },
      "h2": {
        "id": "request-payload",
        "title": "Request payload"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.background-removal-response-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "pathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "title": "Response",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#response",
    "content": "image_b64 (str) - Base64-encoded png containing the processed image.\n\nremoved_for_safety (bool) - When true, background removal was not performed because init_image was found to have violated our terms of service.",
    "hierarchy": {
      "h0": {
        "title": "Background Removal REST API"
      },
      "h2": {
        "id": "response",
        "title": "Response"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.upscaling",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/upscaling",
    "pathname": "/docs/media-gen-solution/rest-apis/upscaling",
    "title": "Upscaling REST API",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "In addition to image generation, OctoAI can also upscale images to higher resolutions."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.upscaling-api-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/upscaling",
    "pathname": "/docs/media-gen-solution/rest-apis/upscaling",
    "title": "API",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api",
    "content": "Upscaling takes an existing image you provide and upscales it to a higher resolution.\nParameters:\ninit_image_url / init_image - The URL to an image or a base64-encoded image (respectively) to upscale. Specify only one of these.\n\nscale / output_image_height / output_image_width - determined how much to upscale the provided image. Specify only one of these.\nscale - floating point value indicating how much to scale the input resolution by (e.g., scale: 2.0 would double the input width and height)\n\noutput_image_height - height of the desired upscaled image in pixels. The corresponding width will be computed off of this value to preserve the aspect ratio.\n\noutput_image_width - width of the desired upscaled image in pixels. The corresponding height will be computed off of this value to preserve the aspect ratio.\n\n\n\nmodel (optional): The model to use for upscaling faces. Default value is real-esrgan-x4-plus. Options:\nreal-esrgan-x4-plus\n\nreal-esrgan-x4-v3\n\nreal-esrgan-x4-v3-wdn\n\nreal-esrgan-animevideo-v3\n\nreal-esrgan-x4-plus-anime\n\nreal-esrgan-x2-plus",
    "hierarchy": {
      "h0": {
        "title": "Upscaling REST API"
      },
      "h3": {
        "id": "api",
        "title": "API"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.adetailer-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "pathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "title": "Adetailer REST API",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Adetailer takes an existing image you provide, detects faces and hands and fixes them.",
    "content": "Adetailer takes an existing image you provide, detects faces and hands and fixes them."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.adetailer-request-payload-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "pathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "title": "Request payload",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#request-payload",
    "content": "Parameters:\ninit_image (str, required) - A base64-encoded image. Resolution must be supported by inpainting_base_model:\nsdxl:  640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640, 1664x2432, 2048x2048, 2432x1664\nsd15: 384x704, 448x576, 512x512, 512x704, 512x768, 512x832, 576x448, 576x768, 576x768, 576x1024, 640x512, 640x640, 640x768, 704x384, 704x1216, 768x512, 768x576, 768x1024, 832x512, 896x896, 1024x576, 1024x768, 1024x1024, 1024x1536, 1216x704, 1536x1024\n\ninit_image_url (string, required if init_image not specified) - If given, download init_image from this URL.\n\ndetector (str,required) - Detection model to use. Configures whether e.g. faces or hands or people are targeted for after-detailing.\nAvailable options: face_yolov8n, hand_yolov8n, face_full_mediapipe, face_short_mediapipe, face_mesh_mediapipe, eyes_mesh_mediapipe \n\ninpainting_base_model (str, required) -\nThe base model to be used for inpainting. Typically should match the model used to generate init_image.\nAvailable options: sdxl, sd15 \n\ncfg_scale (number,optional default: 7.5)-\nFloating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.\n\ncheckpoint(string, optional)-  Name of a checkpoint to use for inpainting.\n\nconfidence (number,optional, default: 0.3) -\nInpainted areas are determined using a detector. This setting adjusts the sensitivity of the detector (lower considers more image fragments for inpainting).\n\nimage_encoding (string,optional)-\nDefine which encoding process should be applied before returning the modified image.\nAvailable options: jpeg, png \n\nloras (object | optional) - A dict mapping the name of a LoRA to apply to its weight.\n\nmask_blur (integer, optional,default: 4) - A mask is created for each inpainted area in the image. After dilation (see mask_dilation parameter), the mask is blurred. This technique is typically used to smoothly blend the inpainted area with the original image. This option specifies the radius, in pixels, of the gaussian blur kernel. The higher the value, the wider the blur. Defaults to 4. Must be greater than or equal to 0 and recommended to be less than 64.\n\nmask_dilation (integer,optional,default: 4) - A mask is created for each inpainted area in the image. Mask Dilation allows you to expand the size of the mask while maintaining its shape. This technique is typically used to reduce artifacts near borders in the mask. This parameter is the size, in pixels, of the dilation kernel to apply. Defaults to 4. Must be greater than or equal to 0 and recommended to be less than 64.\n\nmask_padding (integer,optional,default: 32) -\\ Each inpainted area is passed to the image-to-image generator with some surrounding context. The contextual area is created by padding the area occupied by the blurred, dilated mask. This technique improves inpainting quality, and the contextual area is not modified. This parameter specifies the amount of padding, in pixels, to apply around the processed mask. When the computed padding goes off the edge of the image, the padded area is slid towards the center of the image. Must be greater than or equal to 0 and recommended to be less than 10% the size of an inpainting mask.\n\nmax_num_detections (integer,optional) - Inpaint at most this many objects, starting with the most confident matches.\n\nnegative_prompt (string,optional)\n\nprompt (string | optional)\n\nsampler (string | optional) - The schedulers available for image generation.\nAvailable options: PNDM, LMS, KLMS, DDIM, DDPM, HEUN, K_HEUN, K_EULER, K_EULER_ANCESTRAL, DPM_SOLVER_MULTISTEP, DPM_PLUS_PLUS_2M_KARRAS, DPM_SINGLE, DPM_2, DPM_2_ANCESTRAL, DPM_PLUS_PLUS_SDE_KARRAS, UNI_PC,LCM\n\nseed (integer,optional) - Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.\n\nsteps (integer, optional,default: 20)\n\nstrength (number,optional, default 0.9`)\n\nstyle_preset(string,optional,default: base)\n\nunion_masks (boolean, optional,default: false) - When true, create a single mask by unioning the mask for each detected object together, then send a single inpainting request to the backing model.\n\nuse_refiner (boolean,optional,default: false)",
    "hierarchy": {
      "h0": {
        "title": "Adetailer REST API"
      },
      "h2": {
        "id": "request-payload",
        "title": "Request payload"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.adetailer-response-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "pathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "title": "Response",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      },
      {
        "title": "Media Gen REST APIs",
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#response",
    "content": "image_b64 (string) - The modified image or None if it was removed for safety.\n\nnum_objects_detected (integer) - The number of objects that were successfully detected.\n\nnum_objects_inpainted (integer) - The number of objects that were successfully inpainted.\n\nnum_removed_for_safety (integer) - Number of inpainting requests that violated the OctoAI Terms of Service.",
    "hierarchy": {
      "h0": {
        "title": "Adetailer REST API"
      },
      "h2": {
        "id": "response",
        "title": "Response"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.inference-models",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/inference-models",
    "pathname": "/docs/getting-started/inference-models",
    "title": "Inference models",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.inference-models-serverless-endpoints-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/inference-models",
    "pathname": "/docs/getting-started/inference-models",
    "title": "Serverless Endpoints",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#serverless-endpoints",
    "content": "OctoAI currently supports the self-service models & checkpoints organized on this page, and we’ll continue to expand our models and services. Ready to run your first inference? Navigate to our Quickstart guide to get started.",
    "hierarchy": {
      "h0": {
        "title": "Inference models"
      },
      "h2": {
        "id": "serverless-endpoints",
        "title": "Serverless Endpoints"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.inference-models-text-gen-models-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/inference-models",
    "pathname": "/docs/getting-started/inference-models",
    "title": "Text Gen Models",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#text-gen-models",
    "content": "Organization Use Cases Model Name API Model String Context Length \nMeta Chat Llama2-Chat (13B) llama-2-13b-chat 4,096 \nMeta Chat Llama2-Chat (70B) llama-2-70b-chat 4,096 \nMeta Chat Llama3-Instruct (8B) meta-llama-3-8b-instruct 8,192 \nMeta Chat Llama3-Instruct (70B) meta-llama-3-70b-instruct 8,192 \nMeta Coding Codellama-Instruct (7B) codellama-7b-instruct 16,384 \nMeta Coding Codellama-Instruct (13B) codellama-13b-instruct 16,384 \nMeta Coding Codellama-Instruct (34B) codellama-34b-instruct 16,384 \nMistral Chat, Coding Mistral Instruct v0.2 (7B) mistral-7b-instruct 32,768 \nNous Research Chat, Coding Nous Hermes 2 Pro Mistral (7B) hermes-2-pro-mistral-7b 32,768 \nMistral Chat, Coding Mixtral Instruct (8x7B) mixtral-8x7b-instruct 32,768 \nNous Research Content Moderation Nous Hermes 2 Mixtral DPO (8x7B) nous-hermes-2-mixtral-8x7b-dpo 32,768 \nMistral Chat, Coding Mixtral Instruct (8x22B) mixtral-8x22b-instruct 65,536 \nMeta Content Moderation Llama Guard llamaguard-7b 4,096 \nAlibaba DAMO Embedding GTE Large thenlper/gte-large n/a \n\nCheck out our REST API, Python SDK, or TypeScript SDK docs when you’re ready to use text gen models programmatically.",
    "hierarchy": {
      "h0": {
        "title": "Inference models"
      },
      "h2": {
        "id": "serverless-endpoints",
        "title": "Serverless Endpoints"
      },
      "h3": {
        "id": "text-gen-models",
        "title": "Text Gen Models"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.inference-models-media-gen-models-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/inference-models",
    "pathname": "/docs/getting-started/inference-models",
    "title": "Media Gen Models",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#media-gen-models",
    "content": "Service Model API Model String \nImage Gen Stable Diffusion v1.5 sd \nImage Gen Stable Diffusion XL v1.0 sdxl \nImage Gen Segmind Stable Diffusion ssd \nImage Gen ControlNet SD v1.5 controlnet-sd15 \nImage Gen ControlNet SDXL controlnet-sdxl \nImage Animation Stable Video Diffusion v1.1 svd \nBackground Removal IS-Net background-removal \nUpscaling REAL-ESRGAN x4 Plus real-esrgan-x4-plus \nUpscaling REAL-ESRGAN x4 v3 real-esrgan-x4-v3 \nUpscaling REAL-ESRGAN x4 v3 WDN real-esrgan-x4-v3-wdn \nUpscaling REAL-ESRGAN Anime Video v3 real-esrgan-animevideo-v3 \nUpscaling REAL-ESRGAN x4 Plus Anime real-esrgan-x4-plus-anime \nUpscaling REAL-ESRGAN x2 Plus real-esrgan-x2-plus \nAdetailer Face YOLOv8n face_yolov8n \nAdetailer Hand YOLOv8n hand_yolov8n \nAdetailer Face Full MediaPipe face_full_mediapipe \nAdetailer Face Short MediaPipe face_short_mediapipe \nAdetailer Face Mesh MediaPipe face_mesh_mediapipe \nAdetailer Eyes Mesh MediaPipe eyes_mesh_mediapipe \n\nCheck out our Image Gen API and Video Gen API docs when you’re ready to use media gen models programmatically. You can also easily upload and run custom checkpoints and assets using OctoAI’s Asset Library.",
    "hierarchy": {
      "h0": {
        "title": "Inference models"
      },
      "h2": {
        "id": "serverless-endpoints",
        "title": "Serverless Endpoints"
      },
      "h3": {
        "id": "media-gen-models",
        "title": "Media Gen Models"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.create-an-api-token-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/getting-started/how-to-create-an-octoai-access-token",
    "pathname": "/docs/getting-started/how-to-create-an-octoai-access-token",
    "title": "How to create an OctoAI API token",
    "breadcrumb": [
      {
        "title": "Quickstart",
        "pathname": "/docs/documentation/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "export OCTOAI_TOKEN=<INSERT_HERE>"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://text.octoai.run/v1/chat/completions\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello world\"\n            }\n        ],\n        \"model\": \"mixtral-8x7b-instruct\",\n        \"max_tokens\": 512,\n        \"presence_penalty\": 0,\n        \"temperature\": 0.1,\n        \"top_p\": 0.9\n    }'"
      }
    ],
    "content": "All endpoints require authentication by default. That means you will need an access token in order to run inferences against those endpoints. To generate a token, head to your Account Settings and click Generate token:\n\nAfter generating a token, make sure to store it in your terminal and/or environment file for your app.\nNow you'll be able to run inferences! For example:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "pathname": "/docs/text-gen-solution/getting-started",
    "title": "Getting started with our Text Gen Solution",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "The OctoAI Text Gen Solution offers market-leading price and performance for a growing list open source LLMs including Llama2, CodeLlama, and Mistral (see Supported models section below). We offer a WebUI playground, API endpoints, and Python/TypeScript SDK solution for interacting with these models. All of our endpoints are callable via chat completions format currently popular in the industry (see API documentation).",
    "content": "In the coming months, we will launch additional features including efficient fine-tuning, longer-context models, JSON mode support, and other features.\nIf you have an LLM use case that our existing endpoints do not support, contact us. We offer low-latency and throughput-optimized solutions for all LLama2, CodeLlama, and Mistral checkpoints."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-self-service-models-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "pathname": "/docs/text-gen-solution/getting-started",
    "title": "Self-Service Models",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#self-service-models",
    "content": "We are always expanding our offering of models and other features. Presently, OctoAI supports the following models & checkpoints for self-service models:\nMistral-7b-Instruct-v0.2: Updated by Mistral AI in December 2023, this model has impressed the LLM community with its high-quality performance at a very low parameter count. This model is available for commercial use. Read more. We offer a single endpoint here: the 7B parameter model, which supports up to 32,768 tokens. Note that Mistral's model does not have any moderation mechanisms. For more sensitive use cases, we recommend using Llama2 and Codellama endpoints.\nMistral-8x7b-Instruct: Mistral AI's December 2023 release, Mistral-8x7b-Instruct, is a \"mixture of experts\" model utilizing conditional computing for efficient token generation, reducing computational demands while improving response quality (GPT-4 is widely believed to be an MoE model). Mistral-8x7b-Instruct brings these efficiencies to the open-source LLM realm, and it is licensed for commercial use. It supports up to 32,768 tokens. Read more. Note: As with Mistral-7B-Instruct, Mixtral lacks moderation mechanisms. For sensitive applications, consider Llama2 or Codellama endpoints.\nNous-Hermes-2-Mixtral-8x7b-DPO The flagship Nous Research model trained over the Mixtral 8x7B MoE LLM. The model was trained on over 1,000,000 entries of data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks. It supports up to 32,768 tokens.\nHermes-2-Pro-Mistral-7b An upgraded, retrained version of Nous Hermes 2 Mistral 7B, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset. It is especially good at JSON schema following. It supports up to 32,768 tokens. Read more about how to use schema following here.\nMixtral-8x22B-Instruct coming soon!\nMixtral 8x22B (fine-tune) With the recent release of Mixtral 8x22B base model, new fine tunes are emerging from the community at a rapid rate. We will be using this particular endpoint to trying ou the best community fine tunes as they become available, meaning this model will change and serve as a testing grounds for our users. Check back frequently to see which new fine tune is available. After thorough testing we will select the top performing fine tune to persistantly host on OctoAI.\nLlama2-Chat: Released by Meta in July 2023, this auto-regressive language model uses an optimized transformer architecture. This model is available for commercial use. The \"Chat\" versions that OctoAI hosts by default utilize supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Read more. OctoAI offers this model in 13- and 70-billion parameter sizes. Our quality testing has indicated that the 7 billion parameter variant does not meet competitive quality standards. All checkpoints of this model hosted by OctoAI are limited to a max token length of 4,096.\nCodellama-Instruct: Released by Meta in August 2023, this model builds upon the Llama2 architecture but offers specialized support for coding and other structured tasks. This model is available for commercial use. We host the \"Instruct\" variant, optimized for instruction following and safer deployment. Read more. We support 7-, 13-,and -34 billion parameter variants. Other variants, including the Python checkpoints and 70B variants, are available upon request. All endpoints support up to 16,384 tokens.\nLlama Guard: A 7B content moderation model released by Meta, which can classify text as safe or unsafe according to an editable set of policies. As a 7B parameter model, it is optimized for latency and can be used to moderate other LLM interactions in real time. Read more. Note: This model requires a specific prompt template to be applied, and is not compatible with the ChatCompletion API.\nGTE Large An embeddings model released by Alibaba DAMO Academy. Trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. Consistently ranked highly on Huggingface's MTEB leaderboard. In combination with a vector database, this embeddings model is especially useful for powering semantic search and Retrieval Augmented Generation (RAG) applications. Read more.\nFor pricing of all of these endpoints, please refer to our pricing page.",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Text Gen Solution"
      },
      "h2": {
        "id": "self-service-models",
        "title": "Self-Service Models"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-web-ui-playground-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "pathname": "/docs/text-gen-solution/getting-started",
    "title": "Web UI playground",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#web-ui-playground",
    "content": "You can start familiarizing yourself with our Text Gen features using the web UI, but note that we have even more features available via the API.\nFirst, click on the top navigation bar and click Text Tools. Here you will see the different model families that we offer for self-service users:\n\nClick the Demo or API selections to enter our playground, where you can:\nEasily switch between all of our models, parameter counts, and quantization settings\n\nTest each model using our chat interface\n\nAdjust common settings such as temperature\n\nSee the pricing and context limits for any selected model.\n\n\n\nSelecting the \"API\" toggle will show you code samples in Python, TypeScript, and CURL format for calling the endpoint that you've selected, as well as key input & output parameters:",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Text Gen Solution"
      },
      "h2": {
        "id": "web-ui-playground",
        "title": "Web UI playground"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-billing-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "pathname": "/docs/text-gen-solution/getting-started",
    "title": "Billing",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#billing",
    "content": "For pricing of all of these endpoints, please refer to our pricing page.\nOnce you provide billing information and generate an API key, any usage of these endpoints will be viewable under Accounts -> Billing & Usage -> Text Generation Usage. Note that these endpoints are very price competitive, so you'll generally needs to rack up tens of thousands of tokens before you can see the charges!",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Text Gen Solution"
      },
      "h2": {
        "id": "billing",
        "title": "Billing"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-api-docs-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "pathname": "/docs/text-gen-solution/getting-started",
    "title": "API Docs",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-docs",
    "content": "When you're ready to start calling the endpoint programmatically, check out our REST API, Python SDK, and TypeScript SDK docs.",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Text Gen Solution"
      },
      "h2": {
        "id": "api-docs",
        "title": "API Docs"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "title": "Text Gen Python SDK",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Use the OctoAI Chat Completion API to easily generate text.",
    "content": "The OctoAI class allows you to run inferences simply to any model that accepts JSON-formatted inputs as a dictionary, and provides you with all JSON-formatted outputs as a dictionary. The OctoAI class also supports the Chat Completions API and provides easy access to a set of highly optimized text models on OctoAI.\nThis guide will walk you through how to select your model of interest, how to call highly optimized text models on OctoAI using the Chat Completions API, and how to use the responses in both streaming and regular modes."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "title": "Requirements",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#requirements",
    "content": "Please create an OctoAI API token if you don't have one already.\n\nPlease also verify you've completed Python SDK Installation & Setup.\nIf you use the OCTOAI_TOKEN envvar for your token, you can instantiate the OctoAI client with octoai = OctoAI() after importing the octoai package.",
    "hierarchy": {
      "h0": {
        "title": "Text Gen Python SDK"
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-text-generation-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "title": "Text Generation",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#text-generation",
    "content": "The following snippet shows you how to use the Chat Completions API to generate text using Llama2.\nThe response is of type octoai.text_gen.ChatCompletionResponse. If you print the response from this call as in the example above, it looks similar to the following:\nNote that billing is based upon \"prompt tokens\" and \"completion tokens\" above. View prices on our pricing page.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import json\n\nfrom octoai.client import OctoAI\nfrom octoai.text_gen import ChatMessage\n\nclient = OctoAI()\ncompletion = client.text_gen.create_chat_completion(\n    model=\"llama-2-70b-chat\",\n    messages=[\n        ChatMessage(\n            role=\"system\",\n            content=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n        ),\n        ChatMessage(role=\"user\", content=\"Write a blog about Seattle\"),\n    ],\n    max_tokens=150,\n)\n\nprint(json.dumps(completion.dict(), indent=2))"
      },
      {
        "code": "{\n  \"id\": \"cmpl-8ea213aece0747aca6d0608b02b57196\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Founded in 1921, Seattle is the mother city of Pacific Northwest. Seattle is the densely populated second-largest city in the state of Washington along with Portland. A small city at heart, Seattle has transformed itself from a small manufacturing town to the contemporary Pacific Northwest hub to its east. The city's charm and frequent unpredictability draw tourists and residents alike. Here are my favorite things about Seattle.\\n* Seattle has a low crime rate and high quality of life.\\n* Seattle has rich history which included the building of the first Pacific Northwest harbor and the development of the Puget Sound irrigation system. Seattle is also home to legendary firm Boeing.\\n\",\n        \"function_call\": null\n      },\n      \"delta\": null,\n      \"finish_reason\": \"length\"\n    }\n  ],\n  \"created\": 5399,\n  \"model\": \"llama2-70b\",\n  \"object\": \"chat.completion\",\n  \"system_fingerprint\": null,\n  \"usage\": {\n    \"completion_tokens\": 150,\n    \"prompt_tokens\": 571,\n    \"total_tokens\": 721\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Text Gen Python SDK"
      },
      "h4": {
        "id": "text-generation",
        "title": "Text Generation"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-streaming-responses-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "title": "Streaming Responses",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#streaming-responses",
    "content": "The following snippet shows you how to obtain the model's response incrementally as it is generated using streaming (using stream=True).\nWhen using streaming mode, the response is of type Iterable[ChatCompletionChunk]. To read each incremental response from the model, you can use a for loop over the returned object. The example above prints each incremental response as it arrives, and they accumulate to form the entire response in the output as the model prediction progresses.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.client import OctoAI\nfrom octoai.text_gen import ChatMessage\n\nclient = OctoAI()\nfor completion in client.text_gen.create_chat_completion_stream(\n    model=\"llama-2-70b-chat\",\n    messages=[\n        ChatMessage(\n            role=\"system\",\n            content=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n        ),\n        ChatMessage(role=\"user\", content=\"Write a blog about Seattle\"),\n    ],\n    max_tokens=150,\n):\n    print(completion.choices[0].delta.content, end='', flush=True)\n"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Text Gen Python SDK"
      },
      "h4": {
        "id": "streaming-responses",
        "title": "Streaming Responses"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-additional-parameters-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "title": "Additional Parameters",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#additional-parameters",
    "content": "To learn about the additional parameters supported by the OctoAI().text_gen.create_chat_completion() method.",
    "hierarchy": {
      "h0": {
        "title": "Text Gen Python SDK"
      },
      "h4": {
        "id": "additional-parameters",
        "title": "Additional Parameters"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "title": "Text Gen TypeScript SDK",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "The OctoAI Text Gen TypeScript SDK supports both the Chat Completions API and the Completions API."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-at-a-glance-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "title": "At a Glance",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#at-a-glance",
    "content": "This guide will walk you through how to use the TypeScript SDK to call our Text Gen API. The TypeScript SDK supports streaming and non-streaming inferences for both the Chat Completions API and legacy Completions API. There are also additional parameters such as frequencyPenalty, maxTokens, presencePenalty, etc. that can be used for finer control.",
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK"
      },
      "h2": {
        "id": "at-a-glance",
        "title": "At a Glance"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "title": "Requirements",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#requirements",
    "content": "Please create an OctoAI API token if you don't have one already.\n\nPlease also verify you've completed TypeScript SDK Installation & Setup.\nIf you use the OCTOAI_TOKEN envvar for your token, you can instantiate the client with octoai = new OctoAIClient(), otherwise you will need to pass an API token using: octoai = new OctoAIClient({ apiKey: process.env.OCTOAI_TOKEN })",
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK"
      },
      "h2": {
        "id": "requirements",
        "title": "Requirements"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-non-streaming-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "title": "Non-Streaming Example",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#non-streaming-example",
    "content": "To make a chat completions call, you will need to provide the model you wish to call and a list of chat messages.",
    "code_snippets": [
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst result = await octoai.textGen.createChatCompletion({\n  model: \"meta-llama-3-8b-instruct\",\n  messages: [\n    {\n      role: \"system\",\n      content:\n        \"You are a helpful assistant. Keep your responses limited to one short paragraph if possible.\",\n    },\n    {\n      role: \"user\",\n      content: \"Write a blog about Seattle\",\n    },\n  ],\n});\n\nconsole.log(result.choices[0].message.content);\n// \"Seattle is a vibrant and eclectic city...\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK"
      },
      "h2": {
        "id": "chat-completions-api",
        "title": "Chat Completions API"
      },
      "h3": {
        "id": "non-streaming-example",
        "title": "Non-Streaming Example"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-streaming-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "title": "Streaming Example",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#streaming-example",
    "content": "The above example can work great in some scenarios, but if you're dealing with larger requests or are building a highly-interactive user experience, using the streaming interface may be a better choice. The available options between non-streaming and streaming inferences are identical, but there are two main code changes needed:\nYou will need to use the createChatCompletionStream() method instead of createChatCompletion().\n\nInstead of grabbing the final text message from the response, you will need to loop over the individual chunks and concatenate the tokens.",
    "code_snippets": [
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst stream = await octoai.textGen.createChatCompletionStream({\n  model: \"meta-llama-3-8b-instruct\",\n  messages: [\n    {\n      role: \"system\",\n      content:\n        \"You are a helpful assistant. Keep your responses limited to one short paragraph if possible.\",\n    },\n    {\n      role: \"user\",\n      content: \"Write a blog about Seattle\",\n    },\n  ],\n});\n\nlet result = \"\";\n\n// Loops over the returned chunks whenever they're ready\nfor await (const chunk of stream) {\n  // The content of the first chunk can be `undefined`\n  result += chunk.choices[0].delta.content ?? \"\";\n}\n\nconsole.log(result);\n// \"Seattle is a vibrant and eclectic city...\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK"
      },
      "h2": {
        "id": "chat-completions-api",
        "title": "Chat Completions API"
      },
      "h3": {
        "id": "streaming-example",
        "title": "Streaming Example"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-completions-api-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "title": "Completions API",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#completions-api",
    "content": "The TypeScript SDK also supports the legacy Completions API with the same customization options as the Chat Completions API. The key difference between the two is that you provide a prompt string instead of a list of chat message objects. Much like the Chat Completions API, you can choose between non-streaming and streaming inference.",
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK"
      },
      "h2": {
        "id": "completions-api",
        "title": "Completions API"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "pathname": "/docs/text-gen-solution/json-mode",
    "title": "Using JSON mode with Text Gen endpoints",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Ensure Text Gen outputs fit into your desired JSON schema.",
    "content": "OctoAIs Large Language Models (LLMs) can generate generate outputs that not only adhere to JSON format but also align with your unique schema specifications.\nThis is supported for all models, but works especially well with Mixtral & Mistral models, including the Hermes family of fine tunes."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-getting-started-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "pathname": "/docs/text-gen-solution/json-mode",
    "title": "Getting started",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#getting-started",
    "content": "Setup credentials:\nCurl example (Mistral-7B): Let's say that you want to ensure that your LLM responses format user feedback about cars into a usable JSON format. To do so, you provide the LLM with a reponse schema ensuring that it knows it must provide \"color\" and \"maker\" in a structured format--see \"response format below\":\nThe LLM will respond in the exact schema specified:",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "export OCTOAI_TOKEN=YOUR_TOKEN_HERE"
      },
      {
        "lang": "bash",
        "code": "curl -X POST \"https://text.octoai.run/v1/chat/completions\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n  --data-raw '{\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"the car was black and it was a toyota camry.\"\n            }\n        ],\n        \"model\": \"mistral-7b-instruct\",\n        \"max_tokens\": 512,\n        \"presence_penalty\": 0,\n        \"temperature\": 0.1,\n        \"top_p\": 0.9,\n        \"response_format\": {\n            \"type\": \"json_object\",\n            \"schema\": {\"properties\": {\"color\": {\"title\": \"Color\", \"type\": \"string\"}, \"maker\": {\"title\": \"Maker\", \"type\": \"string\"}}, \"required\": [\"color\", \"maker\"], \"title\": \"Car\", \"type\": \"object\"}\n        }\n    }'"
      },
      {
        "lang": "bash",
        "code": "{\n  \"id\": \"chatcmpl-d5d81b7c80b249ea8177f95f68a51d8e\",\n  \"object\": \"chat.completion\",\n  \"created\": 1709830931,\n  \"model\": \"mistral-7b-instruct\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"{\\\"color\\\": \\\"black\\\", \\\"maker\\\": \\\"Toyota”, \\\"}\",\n        \"function_call\": null\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 98,\n    \"completion_tokens\": 16,\n    \"total_tokens\": 114\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints"
      },
      "h2": {
        "id": "getting-started",
        "title": "Getting started"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-pydantic-and-octoais-python-sdk-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "pathname": "/docs/text-gen-solution/json-mode",
    "title": "Pydantic and OctoAI's Python SDK",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#pydantic-and-octoais-python-sdk",
    "content": "Pydantic is a popular Python library for data validation and settings management using Python type annotations. By combining Pydantic with the OctoAI SDK, you can easily define the desired JSON schema for your LLM responses and ensure that the generated content adheres to that structure.\nFirst, make sure you have the required packages installed:",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "python3 -m pip install openai pydantic==2.5.3"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints"
      },
      "h2": {
        "id": "pydantic-and-octoais-python-sdk",
        "title": "Pydantic and OctoAI's Python SDK"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-basic-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "pathname": "/docs/text-gen-solution/json-mode",
    "title": "Basic example",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#basic-example",
    "content": "Let's start with a basic example to demonstrate how Pydantic and the OctoAI SDK work together. In this example, we'll define a simple Car model with color and maker attributes, and ask the LLM to generate a response that fits this schema.\nThe key points to note here are:\nWe import the necessary classes from the OctoAI SDK: Client, TextModel, and ChatCompletionResponseFormat.\n\nWe define a Car class inheriting from BaseModel, specifying the color and maker attributes with their expected types.\n\nWhen creating the chat completion, we set the response_format using ChatCompletionResponseFormat and include the JSON schema generated from our Car model using Car.model_json_schema().\n\n\nThe output will be a JSON object adhering to the specified schema:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "from octoai.client import OctoAI\nfrom octoai.text_gen import ChatCompletionResponseFormat, ChatMessage\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclient = OctoAI()\n\nclass Car(BaseModel):\n    color: str\n    maker: str\n\ncompletion = client.text_gen.create_chat_completion(\n    model=\"mistral-7b-instruct\",\n    messages=[\n        ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n        ChatMessage(role=\"user\", content=\"the car was black and it was a toyota camry.\"),\n    ],\n    max_tokens=512,\n    presence_penalty=0,\n    temperature=0.1,\n    top_p=0.9,\n    response_format=ChatCompletionResponseFormat(\n        type=\"json_object\",\n        schema=Car.model_json_schema(),\n    ),\n)\n\nprint(completion.choices[0].message.content)"
      },
      {
        "lang": "json",
        "code": "{ \"color\": \"black\", \"maker\": \"Toyota\" }"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints"
      },
      "h2": {
        "id": "pydantic-and-octoais-python-sdk",
        "title": "Pydantic and OctoAI's Python SDK"
      },
      "h3": {
        "id": "basic-example",
        "title": "Basic example"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-array-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "pathname": "/docs/text-gen-solution/json-mode",
    "title": "Array example",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#array-example",
    "content": "Next, let's look at an example involving arrays. Suppose we want the LLM to generate a list of names based on a given prompt. We can define a Meeting model with a names attribute of type List[str].\nThe LLM will generate a response containing an array of names:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "from octoai.client import OctoAI\nfrom octoai.text_gen import ChatCompletionResponseFormat, ChatMessage\nfrom pydantic import BaseModel, Field\nfrom typing import List\n\nclient = OctoAI()\n\nclass Meeting(BaseModel):\n    names: List[str]\n\n\nchat_completion = client.text_gen.create_chat_completion(\n    model=\"<model>\",\n    messages=[\n        ChatMessage(role=\"system\", content=\"You are a helpful assistant.\"),\n        ChatMessage(role=\"user\", content=\"John and Jane meet the day after\"),\n    ],\n    temperature=0,\n    response_format=ChatCompletionResponseFormat(\n        type=\"json_object\",\n        schema=Meeting.model_json_schema()\n    ),\n)\n\nprint(chat_completion.choices[0].message.content)"
      },
      {
        "lang": "json",
        "code": "{ \"names\": [\"John\", \"Jane\"] }"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints"
      },
      "h2": {
        "id": "pydantic-and-octoais-python-sdk",
        "title": "Pydantic and OctoAI's Python SDK"
      },
      "h3": {
        "id": "array-example",
        "title": "Array example"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-nested-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "pathname": "/docs/text-gen-solution/json-mode",
    "title": "Nested example",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#nested-example",
    "content": "Finally, let's explore a more complex example involving nested models. In this case, we'll define a Person model with name and age attributes, and a Result model containing a sorted list of Person objects.\nIn this example:\nWe define a Person model with name and age attributes, along with descriptions using the Field function from Pydantic.\n\nWe define a Result model containing a sorted_list attribute of type List[Person].\n\nWhen creating the chat completion, we set the response_format using ChatCompletionResponseFormat and include the JSON schema generated from our Result model.\n\n\nThe LLM will generate a response containing a sorted list of Person objects:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "class Person(BaseModel):\n    \"\"\"The object representing a person with name and age\"\"\"\n\n    name: str = Field(description=\"Name of the person\")\n    age: int = Field(description=\"The age of the person\")\n\n\nclass Result(BaseModel):\n    \"\"\"The format of the answer.\"\"\"\n\n    sorted_list: List[Person] = Field(description=\"List of the sorted objects\")\n\n\ncompletion = octoai.text_gen.create_chat_completion(\n    model=\"mistral-7b-instruct\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant designed to output JSON.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Alice is 10 years old, Bob is 7 and carol is 2. Sort them by age in ascending order.\",\n        },\n    ],\n    max_tokens=512,\n    presence_penalty=0,\n    temperature=0.1,\n    top_p=0.9,\n    response_format=ChatCompletionResponseFormat(\n        type=\"json_object\",\n        schema=Result.model_json_schema(),\n    ),\n)\n\nprint(completion.choices[0].message.content)"
      },
      {
        "lang": "json",
        "code": "{\n  \"sorted_list\": [\n    { \"name\": \"Carol\", \"age\": 2 },\n    { \"name\": \"Bob\", \"age\": 7 },\n    { \"name\": \"Alice\", \"age\": 10 }\n  ]\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints"
      },
      "h2": {
        "id": "pydantic-and-octoais-python-sdk",
        "title": "Pydantic and OctoAI's Python SDK"
      },
      "h3": {
        "id": "nested-example",
        "title": "Nested example"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-instructor-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "pathname": "/docs/text-gen-solution/json-mode",
    "title": "Instructor",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#instructor",
    "content": "Instructor makes it easy to reliably get structured data like JSON from Large Language Models (LLMs). Read more here",
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints"
      },
      "h2": {
        "id": "instructor",
        "title": "Instructor"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "pathname": "/docs/text-gen-solution/json-mode",
    "title": "Example",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#example",
    "content": "Let's break down the code step by step:\nAfter importing the necessary modules and setting the clients, we:\nWe use the instructor.patch function to patch the ChatCompletion.create method of the OctoAI client. This allows us to use the response_model parameter directly with a Pydantic model.\n\nWe define a Pydantic model called UserExtract that represents the desired structure of the extracted user information. In this case, it has two fields: name (a string) and age (an integer).\n\nWe call the chat.completions.create method of the patched OctoAI client, specifying the model (mistral-7b-instruct), the response_model (our UserExtract model), and the user message that contains the information we want to extract.\n\nFinally, we print the extracted user information using the model_dump_json method, which serializes the Pydantic model to a JSON string with indentation for better readability.\n\n\nThe output will be a JSON object containing the extracted user information, adhering to the specified UserExtract schema:\nBy leveraging Instructor and the OctoAI SDK, you can easily define the desired output schema and ensure that the LLM generates structured data that fits your application's requirements. This simplifies the process of integrating LLM-generated content into your software systems.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import os\nimport openai\nfrom pydantic import BaseModel\nimport instructor\n\nclient = openai.OpenAI(\n    base_url=\"https://text.octoai.run/v1\",\n    api_key=os.environ[\"OCTOAI_TOKEN\"],\n)\n\n\n# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods to support the response_model parameter\nclient = instructor.patch(client, mode=instructor.Mode.JSON_SCHEMA)\n\n\n# Now, we can use the response_model parameter using only a base model\n# rather than having to use the OpenAISchema class\nclass UserExtract(BaseModel):\n    name: str\n    age: int\n\n\nuser: UserExtract = client.chat.completions.create(\n    model=\"mistral-7b-instruct\",\n    response_model=UserExtract,\n    messages=[\n        {\"role\": \"user\", \"content\": \"Extract jason is 25 years old\"},\n    ],\n)\n\nprint(user.model_dump_json(indent=2))"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"jason\",\n  \"age\": 25\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints"
      },
      "h2": {
        "id": "instructor",
        "title": "Instructor"
      },
      "h3": {
        "id": "example",
        "title": "Example"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-llama-guard-to-moderate-text",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/llama-guard",
    "pathname": "/docs/text-gen-solution/llama-guard",
    "title": "Using Llama Guard to moderate text",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "An LLM to guard your AI applications from misuse."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-llama-guard-to-moderate-text-introduction-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/llama-guard",
    "pathname": "/docs/text-gen-solution/llama-guard",
    "title": "Introduction",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#introduction",
    "content": "LlamaGuard is a 7B parameter LLM designed for moderating content in Human-AI interactions, able to focus on safety risks in both prompts and responses.\nBuilt on the Llama2-7B architecture, it utilizes a safety risk taxonomy for categorizing various types of content risks. This taxonomy aids in the classification of LLM prompts and responses, ensuring that conversations remain within safe boundaries. The model has been fine-tuned on a specially curated dataset, showing strong performance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, often outperforming existing content moderation tools.\nLlamaGuard7B operates by performing multi-class classification and generating binary decision scores, making it a versatile tool for managing content safety across various conversational AI applications. Its instruction fine-tuning feature allows for task customization and adaptation of output formats, making it adaptable to a range of use cases by adjusting taxonomy categories as needed.",
    "hierarchy": {
      "h0": {
        "title": "Using Llama Guard to moderate text"
      },
      "h1": {
        "id": "introduction",
        "title": "Introduction"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-llama-guard-to-moderate-text-prompt-template-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/llama-guard",
    "pathname": "/docs/text-gen-solution/llama-guard",
    "title": "Prompt Template",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#prompt-template",
    "content": "LlamaGuard requires a very specific prompt template to function properly. Effective use/customization of LLamaGuard requires understanding this template. Here is a helpful illustration from Meta's paper on the subject:\n\nLet's go ahead and try this out on OctoAI.  First, let's configure our OctoAI API token:\nNow, let's set up the prompt template:\nFinally, let's call the model with one normal prompt and one toxic prompt:\nBelow, we can see LLama Guard's response from the two prompts submitted:\nThe prompt about crystal meth is marked by Llama Guard as unsafe/no06, indicating that it is unsafe under policy 06: Regulated or Controlled Substances.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "token = 'foo'"
      },
      {
        "lang": "python",
        "code": "for prompt in [\n    \"User: Can you recommend a savory pasta recipe?\",\n    \"User: How can I make crystal meth?\",\n]:\n    response = requests.post(\n        \"https://text.octoai.run/v1/completions\",\n        headers={\"Authorization\": f\"Bearer {token}\"},\n        json={\n            \"model\": \"llamaguard-7b\",\n            \"prompt\": format_prompt(\"User\", prompt), #Submit the prompt and specify the role as \"user\" for this exercise\n            \"max_tokens\": 100,\n            \"top_p\": 0.9,\n            \"temperature\": 0,\n        },\n    )\n    json = response.json()\n    print(json['choices'])"
      },
      {
        "code": "[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' safe'}]\n[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' unsafe\\nO6'}]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using Llama Guard to moderate text"
      },
      "h1": {
        "id": "prompt-template",
        "title": "Prompt Template"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-llama-guard-to-moderate-text-policy-adjustment-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/llama-guard",
    "pathname": "/docs/text-gen-solution/llama-guard",
    "title": "Policy Adjustment",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#policy-adjustment",
    "content": "Now, let's try deleting policy #6 and seeing and re-submitting the unsafe prompt:\nWith the controllled substances policy removed, the model deems a question about the creation of crystal meth to be \"safe\".  This might not be a great policy, but it does demonstrate the flexibiliy of LlamaGuard!\nYou can test this yourself on the OctoAI platform, adding new policies, or editing the policies to tweak the line between allowable/disallowable for a given category. Try out safe/unsafe prompts and see how flexible Llama Guard can be!",
    "code_snippets": [
      {
        "lang": "python",
        "code": "for prompt in [\n    \"User: Can you recommend a savory pasta recipe?\",\n    \"Agent: How can I make crystal meth?\",\n]:\n    response = requests.post(\n        \"https://text.octoai.run/v1/completions\",\n        headers={\"Authorization\": f\"Bearer {token}\"},\n        json={\n            \"model\": \"llamaguard-7b\",\n            \"prompt\": format_prompt(\"User\", prompt),\n            \"max_tokens\": 100,\n            \"top_p\": 0.9,\n            \"temperature\": 0,\n        },\n    )\n    json = response.json()\n    print(json['choices'])"
      },
      {
        "code": "[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' safe'}]\n[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' safe'}]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using Llama Guard to moderate text"
      },
      "h1": {
        "id": "policy-adjustment",
        "title": "Policy Adjustment"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-unstructured-io-for-embedding-documents",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "pathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "title": "Using Unstructured.io for embedding documents",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Fast and easy document parsing and embedding using Unstuctured.io and OctoAI."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-unstructured-io-for-embedding-documents-introduction-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "pathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "title": "Introduction",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#introduction",
    "content": "Unstructured is both an open-source library and an API service. The library provides components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more.\nIt also provides components to very easily embed these documents. In Unstructured's jargon this component is called an EmbeddingEncoder. The OctoAIEmbedingEncoder is available, so documents parsed with Unstructured can easily be embedded with the OctoAI embeddings endpoint.",
    "hierarchy": {
      "h0": {
        "title": "Using Unstructured.io for embedding documents"
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-unstructured-io-for-embedding-documents-using-the-octoaiembeddingencocer-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "pathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "title": "Using the OctoAIEmbeddingEncocer",
    "breadcrumb": [
      {
        "title": "Text Gen Solution",
        "pathname": "/docs/documentation/text-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-the-octoaiembeddingencocer",
    "content": "The OctoAIEmbeddingEncoder class connects to the OctoAI Text&Embedding API to obtain embeddings for pieces of text.\nembed_documents will receive a list of Elements, and return an updated list which includes the embeddings attribute for each Element.\nembed_query will receive a query as a string, and return a list of floats which is the embedding vector for the given query string.\nnum_of_dimensions is a metadata property that denotes the number of dimensions in any embedding vector obtained via this class.\nis_unit_vector is a metadata property that denotes if embedding vectors obtained via this class are unit vectors.\nThe following code block shows an example of how to use OctoAIEmbeddingEncoder.\nYou will see the updated elements list (with the embeddings attribute included for each element),\nthe embedding vector for the query string, and some metadata properties about the embedding model.\nYou will need to set an environment variable named OCTOAI_API_KEY to be able to run this example.\nTo obtain an API key, visit: How to create an OctoAI API token.",
    "code_snippets": [
      {
        "lang": "Python",
        "code": "import os\n\nfrom unstructured.documents.elements import Text\nfrom unstructured.embed.octoai import OctoAiEmbeddingConfig, OctoAIEmbeddingEncoder\n\nembedding_encoder = OctoAIEmbeddingEncoder(\n    config=OctoAiEmbeddingConfig(api_key=os.environ[\"OCTOAI_API_KEY\"])\n)\nelements = embedding_encoder.embed_documents(\n    elements=[Text(\"This is sentence 1\"), Text(\"This is sentence 2\")],\n)\n\nquery = \"This is the query\"\nquery_embedding = embedding_encoder.embed_query(query=query)\n\n[print(e.embeddings, e) for e in elements]\nprint(query_embedding, query)\nprint(embedding_encoder.is_unit_vector(), embedding_encoder.num_of_dimensions())"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using Unstructured.io for embedding documents"
      },
      "h2": {
        "id": "using-the-octoaiembeddingencocer",
        "title": "Using the OctoAIEmbeddingEncocer"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.getting-started-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/getting-started",
    "pathname": "/docs/media-gen-solution/getting-started",
    "title": "Getting started with our Media Gen Solution",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "The OctoAI Media Gen Solution offers access to the fastest and most customizable Stable Diffusion models including Stable Video Diffusion 1.1, Stable Diffusion XL and 1.5 for image-to-video, text-to-image, image-to-image use cases and more. We offer a WebUI playground, API endpoints, and Python/TypeScript SDKs for interacting with these models.",
    "content": "The OctoAI Media Gen Solution empowers users with unparalleled access to cutting-edge Stable Diffusion models, delivering lightning-fast performance and unmatched customization options. With our platform, users can effortlessly create high-quality media content for a wide range of applications, from image-to-video to text-to-image, and beyond."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.getting-started-key-features-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/getting-started",
    "pathname": "/docs/media-gen-solution/getting-started",
    "title": "Key Features",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#key-features",
    "content": "Fastest Inference Speed: OctoAI boasts the fastest inference speed in the market, ensuring swift generation of media content. Our latency-optimized Stable Video Diffusion (SVD) endpoint achieves an impressive average latency of ~30 seconds for default parameters to generate 3-4 second-long videos, and the Stable Diffusion XL (SDXL) endpoint achieves an average latency of ~3.1 seconds for default parameters. The cost-optimized SDXL maintains an average latency of under 7 seconds.\n\nExtensive Range of Features: The OctoAI Media Gen solution offers a comprehensive suite of capabilities, supporting a diverse array of models including SVD, SDXL, and SD 1.5. These models cater to a wide range of use cases, spanning from text-to-image, image-to-image, and image-to-video functionalities, to advanced features like upscaling, image editing with controlnets, inpainting, outpainting, background removal, and photo merge. Additionally, advanced functionalities such as Adetailer and Background replacement are accessible through private preview, allowing users to finely customize their media generation processes according to their unique requirements.\n\nAdvanced Customization Options: Users can customize their media generation process by adjusting various parameters such as image dimensions, samplers, number of diffusion steps, and prompt weighting. Additionally, the OctoAI Media Gen Solution allows you to mix and match different Stable Diffusion assets, including checkpoints, Low Rank Adaptations (LoRAs), and textual inversions. It offers the flexibility to fine-tune Stable Diffusion with your own custom tuning image datasets to tailor AI-generated images for your business needs. Fine-tuning is supported for Stable Diffusion 1.5 (SD 1.5) and SDXL. Our proprietary Asset Orchestrator technology enables efficient caching and loading of assets, ensuring optimized performance even with highly customized configurations.\n\nComprehensive Toolkit: The OctoAI Media Gen Solution provides a comprehensive toolkit for interacting with our models, including Stable Diffusion API endpoints, a user-friendly web UI, and Python/TypeScript SDKs. This allows seamless integration into existing workflows and facilitates easy experimentation with model parameters.\n\n\nBy combining state-of-the-art technology with unparalleled flexibility, the OctoAI Media Gen Solution empowers users to unlock new possibilities in media generation, revolutionizing content creation across industries.",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Media Gen Solution"
      },
      "h1": {
        "id": "key-features",
        "title": "Key Features"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.getting-started-web-ui-playground-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/getting-started",
    "pathname": "/docs/media-gen-solution/getting-started",
    "title": "Web UI playground",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#web-ui-playground",
    "content": "You can start familiarizing yourself with our Media Gen features using the web UI, but note that we have even more features available via the API.\nFirst, click on the top navigation bar and click Media Tools. Here you will see that Image Generation, Image Animation are available to use via Demo and API. Curently, for other image utilities such as background removal, photo merge, inpainting, outpainting and upscaling, only APIs are available.\n\nWhen you navigate to the Image Gen Demo, you will see this page where you can play with the different settings and click the Generate button to start generating images!\nDefault settings for SDXL run at about 3.1 seconds of latency.\n\nYou can customize images by selecting different checkpoints, LoRAs, and Textual Inversions. This increases E2E latency slightly, but is still blazing fast thanks to OctoAI's proprietary Asset Orchestrator technology, which enables fast loading and smart caching of assets.\n\n\nIf you want to see a list of all public assets in the OctoAI library as well as your own private assets, you can navigate to the Asset Library page via the top nav bar.\n\n\n\nAdditionally, when you navigate to the Image Animation Demo, you will see this page where you can play with the different settings and click the Generate button to start generating videos!\nDefault settings for SVD1.1 run at about 30 seconds of latency.\n\nYou can leverage advanced video settings such as motion scale, cfg scale, frames per secs, steps and tailor the output of your 3 secs image animation.",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Media Gen Solution"
      },
      "h1": {
        "id": "key-features",
        "title": "Key Features"
      },
      "h2": {
        "id": "web-ui-playground",
        "title": "Web UI playground"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.getting-started-api-docs-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/getting-started",
    "pathname": "/docs/media-gen-solution/getting-started",
    "title": "API Docs",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-docs",
    "content": "When you're ready to start calling the endpoint programmatically, check out Image Gen API and Video Gen API docs.",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Media Gen Solution"
      },
      "h1": {
        "id": "key-features",
        "title": "Key Features"
      },
      "h2": {
        "id": "api-docs",
        "title": "API Docs"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.overview-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/customizations/overview",
    "pathname": "/docs/media-gen-solution/customizations/overview",
    "title": "Overview",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Customizations",
        "pathname": "/docs/documentation/media-gen-solution/customizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "You can tweak your images using various customizations available within OctoAI Media Gen solution including checkpoints, LoRAs, textual inversions and ControlNets",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"Commercial photography,(snowy:0.8) ,luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n        \"checkpoint\": \"octoai:RealVisXL\",\n        \"loras\": {\n            \"octoai:add-detail\": 1\n        },\n        \"textual_inversions\": {\n            \"octoai:NegativeXL\": \"“negativeXL_D”\"\n        },\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_sdxl(\n        prompt=\"Commercial photography,(snowy:0.8) ,luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n        checkpoint=\"octoai:RealVisXL\",\n        loras={\"octoai:add-detail\":1},\n        textual_inversions={\"octoai:NegativeXL\":\"“negativeXL_D”\"},\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n        use_refiner=True,\n        high_noise_frac=0.8,\n        style_preset=\"base\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt:\n    \"Commercial photography, (snowy:0.8), luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n  negativePrompt:\n    \"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n  checkpoint: \"octoai:RealVisXL\",\n  loras: {\n    \"octoai:add-detail\": 1,\n  },\n  textualInversions: {\n    \"octoai:NegativeXL\": \"negativeXL_D\",\n  },\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"Commercial photography,(snowy:0.8) ,luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n        \"checkpoint\": \"octoai:RealVisXL\",\n        \"loras\": {\n            \"octoai:add-detail\": 1\n        },\n        \"textual_inversions\": {\n            \"octoai:NegativeXL\": \"“negativeXL_D”\"\n        },\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_sdxl(\n        prompt=\"Commercial photography,(snowy:0.8) ,luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n        checkpoint=\"octoai:RealVisXL\",\n        loras={\"octoai:add-detail\":1},\n        textual_inversions={\"octoai:NegativeXL\":\"“negativeXL_D”\"},\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n        use_refiner=True,\n        high_noise_frac=0.8,\n        style_preset=\"base\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt:\n    \"Commercial photography, (snowy:0.8), luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n  negativePrompt:\n    \"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n  checkpoint: \"octoai:RealVisXL\",\n  loras: {\n    \"octoai:add-detail\": 1,\n  },\n  textualInversions: {\n    \"octoai:NegativeXL\": \"negativeXL_D\",\n  },\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      }
    ],
    "content": "With OctoAI Media Gen solution, you can effortlessly integrate Stable Diffusion’s customizable image generation features into your application. While standard pre-trained image generation assets from repositories like HuggingFace may suffice for simple tasks, customization becomes crucial for commercial uses. Customization allows precise control over generating specific subjects and environments, which is essential for most commercial needs. Stable Diffusion, offered by OctoAI, provides both basic and advanced customization options. These include adjusting prompt weights, applying style presets, and employing advanced techniques like LoRAs, checkpoints, textual inversions, and ControlNets. Additionally, with OctoAI, you can create your own LoRA asset using custom image datasets and leverage it to meet your business requirements. To learn more, review Fine-tuning on OctoAI\n\n\n Pro or Enterprise account is required to access fine-tuning. OctoAI web UI provides an easy way to experiment by combining assets. An equivalent API call is displayed in the example below.\nExample Code:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.overview-start-creating-images-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/customizations/overview",
    "pathname": "/docs/media-gen-solution/customizations/overview",
    "title": "Start creating images",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Customizations",
        "pathname": "/docs/documentation/media-gen-solution/customizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#start-creating-images",
    "content": "Ready to start creating images? Get started with our Image Gen API.",
    "hierarchy": {
      "h0": {
        "title": "Overview"
      },
      "h3": {
        "id": "start-creating-images",
        "title": "Start creating images"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.checkpoints-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/customizations/checkpoints",
    "pathname": "/docs/media-gen-solution/customizations/checkpoints",
    "title": "Checkpoints",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Customizations",
        "pathname": "/docs/documentation/media-gen-solution/customizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Custom checkpoints are fine-tuned versions of the original model and allow users to refine customizations while creating images or videos.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"A photo of an Australian cattle dog running through a park\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"checkpoint\": \"octoai:samaritan\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_sdxl(\n        prompt=\"A photo of an Australian cattle dog running through a park\",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality\",\n        checkpoint=\"octoai:samaritan\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n        use_refiner=True,\n        high_noise_frac=0.8,\n        style_preset=\"base\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt: \"A photo of an Australian cattle dog running through a park\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  checkpoint: \"octoai:samaritan\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"A photo of an Australian cattle dog running through a park\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"checkpoint\": \"octoai:samaritan\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_sdxl(\n        prompt=\"A photo of an Australian cattle dog running through a park\",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality\",\n        checkpoint=\"octoai:samaritan\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n        use_refiner=True,\n        high_noise_frac=0.8,\n        style_preset=\"base\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt: \"A photo of an Australian cattle dog running through a park\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  checkpoint: \"octoai:samaritan\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      }
    ],
    "content": "Custom Stable Diffusion checkpoints are fine-tuned versions of the original model, trained to capture particular styles, subjects, or objects. They are designed to provide users with more control and customization options when generating images. These checkpoints can be tailored to produce images in various styles, such as realistic photography, artwork, or even specific themes like landscapes or portraits.\nWhile checkpoints represent a significant investment in terms of storage and computational resources, they excel in maintaining the desired customizations consistently. OctoAI's Asset Library boasts a rich collection of pre-loaded custom checkpoints, offering a diverse array of styles to enhance your images. Additionally, users have the flexibility to import bespoke checkpoints from external sources, integrating them seamlessly into OctoAI's Asset Library as personalized assets.\nThe image results with different checkpoints, even using the same prompt, can be significantly different. Using the simple prompt A photo of an Australian cattle dog running through a park, you can see see the results from the SDXL base model (left) and samaritan model (right). The samaritan model represents a 3D-cartoon image style.\n\n\n\n\n\n\n\n\nExample Code:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.lo-r-as-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/customizations/loras",
    "pathname": "/docs/media-gen-solution/customizations/loras",
    "title": "LoRAs",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Customizations",
        "pathname": "/docs/documentation/media-gen-solution/customizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "LoRAs for image or video AI models are custom weights applied to a base checkpoint. LoRAs are a way to make highly customized AI images or videos.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"loras\": {\n            \"octoai:add-detail\": 0.3,\n            \"octoai:more_art\": 1\n        },\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.util import to_file\nfrom octoai.client import OctoAI\n\nclient = OctoAI()\n\nimage_gen_response = client.image_gen.generate_sdxl(\n    prompt=\"Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\",\n    negative_prompt=\"Blurry photo, distortion, low-res, poor quality\",\n    loras={\"octoai:add-detail\":0.3,\"octoai:more_art\":1},\n    width=1024,\n    height=1024,\n    num_images=1,\n    sampler=\"DDIM\",\n    steps=30,\n    cfg_scale=12,\n    use_refiner=True,\n    high_noise_frac=0.8,\n    style_preset=\"base\",\n)\nimages = image_gen_response.images\n\nfor i, image in enumerate(images):\n    to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt:\n    \"Commercial photography, snowy, luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  loras: {\n    \"octoai:add-detail\": 0.3,\n    \"octoai:more_art\": 1,\n  },\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"loras\": {\n            \"octoai:add-detail\": 0.3,\n            \"octoai:more_art\": 1\n        },\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.util import to_file\nfrom octoai.client import OctoAI\n\nclient = OctoAI()\n\nimage_gen_response = client.image_gen.generate_sdxl(\n    prompt=\"Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\",\n    negative_prompt=\"Blurry photo, distortion, low-res, poor quality\",\n    loras={\"octoai:add-detail\":0.3,\"octoai:more_art\":1},\n    width=1024,\n    height=1024,\n    num_images=1,\n    sampler=\"DDIM\",\n    steps=30,\n    cfg_scale=12,\n    use_refiner=True,\n    high_noise_frac=0.8,\n    style_preset=\"base\",\n)\nimages = image_gen_response.images\n\nfor i, image in enumerate(images):\n    to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt:\n    \"Commercial photography, snowy, luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  loras: {\n    \"octoai:add-detail\": 0.3,\n    \"octoai:more_art\": 1,\n  },\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      }
    ],
    "content": "LoRAs are additional custom weights applied to a base checkpoint. Similar to checkpoints, LoRAs can represent a specific style or custom subject, but they are much smaller in size and more economical to use. You can include multiple LoRAs in a single image generation, and provide a weight for each LoRA. A greater weight value will have more influence on the generated image. Similar to checkpoints, users have the flexibility to import LoRAs from external sources, and integrate them seamlessly into OctoAI's Asset Library as personalized assets.\nBelow is an example of using a LoRA along with a simple prompt Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details. You can see the results from the SDXL base model (top left) and subsequent results with add-detail LoRA - varying weights (top right and bottom left). Add-details LoRA adds intricate details to the output image. The image generated on the botton right is a result of two LoRAs, add-details (weight:0.3) and more-art (weight:1.0). More-art LoRA adds artistic details to the output image. You can clearly see the impact of more-art LoRA over add-detail LoRA in the resulting image.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample Code:\n\n\nLoRAs can further customize your images, by including custom objects or styles."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.textual-inversions-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/customizations/textual-inversions",
    "pathname": "/docs/media-gen-solution/customizations/textual-inversions",
    "title": "Textual Inversions",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Customizations",
        "pathname": "/docs/documentation/media-gen-solution/customizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Customize your images on OctoAI using Textual inversions, which are embeddings that represent custom subjects.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n        \"textual_inversions\": {\n            \"octoai:NegativeXL\": \"“negativeXL_D”\"\n        },\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nclient = OctoAI()\n\nimage_gen_response = client.image_gen.generate_sdxl(\n    prompt=\"Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\",\n    negative_prompt=\"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n    textual_inversions={\"octoai:NegativeXL\":\"“negativeXL_D”\"},\n    width=1024,\n    height=1024,\n    num_images=1,\n    sampler=\"DDIM\",\n    steps=30,\n    cfg_scale=12,\n    use_refiner=True,\n    high_noise_frac=0.8,\n    style_preset=\"base\",\n)\nimages = image_gen_response.images\n\nfor i, image in enumerate(images):\n    to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt:\n    \"Commercial photography, snowy, luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n  negativePrompt:\n    \"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n  textualInversions: {\n    \"octoai:NegativeXL\": \"negativeXL_D\",\n  },\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n        \"textual_inversions\": {\n            \"octoai:NegativeXL\": \"“negativeXL_D”\"\n        },\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nclient = OctoAI()\n\nimage_gen_response = client.image_gen.generate_sdxl(\n    prompt=\"Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\",\n    negative_prompt=\"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n    textual_inversions={\"octoai:NegativeXL\":\"“negativeXL_D”\"},\n    width=1024,\n    height=1024,\n    num_images=1,\n    sampler=\"DDIM\",\n    steps=30,\n    cfg_scale=12,\n    use_refiner=True,\n    high_noise_frac=0.8,\n    style_preset=\"base\",\n)\nimages = image_gen_response.images\n\nfor i, image in enumerate(images):\n    to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt:\n    \"Commercial photography, snowy, luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details\",\n  negativePrompt:\n    \"Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\",\n  textualInversions: {\n    \"octoai:NegativeXL\": \"negativeXL_D\",\n  },\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      }
    ],
    "content": "Textual inversions are embeddings that represent custom subjects.They can also represent negative embeddings, which are trained on undesirable content like bad quality hands or lighting. You can use these in your negative prompt to improve your images, such as avoiding bad quality hands. These are the smallest and cheapest assets we currently support.\nThe name of the textual inversion acts as a specific trigger word, which must be included in the prompt. Similar to prompt weighting, you can increase the weight of textual inversion using the format (textual-inversion:weight).\nBelow is an example of using a NegativeXL textual inversion (trigger word: negativeXL_D).\nPrompt:  Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details\nNegative prompt: Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)\nYou can see the results from the SDXL base model (left) generating an image with snowflake or flower shaped designs on the bottle and subsequent results with textual inversion (right) which ensures that the negative prompt is followed and no flower, snowflake design appears on the bottle in the output image.\n\n\n\n\n\n\n\n\nExample Code:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.prompt-weighting-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/customizations/prompt-weighting",
    "pathname": "/docs/media-gen-solution/customizations/prompt-weighting",
    "title": "Prompt Weighting",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Customizations",
        "pathname": "/docs/documentation/media-gen-solution/customizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI allows for prompt weighting, the emphasis or de-emphasis of certain words or phrases, in prompts to create customized images.",
    "content": "You can emphasize, or de-emphasize, specific words or phrases of the image generation prompt using weighting. To use prompt weighting, format your prompt using parentheses: prompt = \"A cat with (long whiskers)\"\nThis emphasizes the phrase “long whiskers” with a weight of 1.1. Adding additional parentheses such as \"\\(\\(\\(long whiskers)))\" performs additional multiples of 1.1, so for 3 sets of parentheses, the weight would be 1.33. More specific weights can also be specified in the form: prompt = \"A cat with (long whiskers: 0.8)\"\nThis will weigh all words in the parentheses by a factor of 0.8. Notably, weights do not have to be greater than one. Using a weight of less than 1 will de-emphasize the contained words.\nUsing weights in negative prompts can also be helpful. For example, you can avoid distorted hands: negative_prompt = \"(distorted hands: 1.5)\""
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.control-nets-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/customizations/controlnets",
    "pathname": "/docs/media-gen-solution/customizations/controlnets",
    "title": "ControlNets",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Customizations",
        "pathname": "/docs/documentation/media-gen-solution/customizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI's asset library is pre-populated with the most popular available ControlNets which allow added image input to influence and customize the image generation.",
    "code_snippets": [
      {
        "code": "octoai:canny_sdxl\noctoai:depth_sdxl\noctoai:openpose_sdxl\noctoai:canny_sd15\noctoai:depth_sd15\noctoai:inpaint_sd15\noctoai:ip2p_sd15\noctoai:lineart_sd15\noctoai:openpose_sd15\noctoai:scribble_sd15\noctoai:tile_sd15"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/controlnet-sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"A photo of woman wearing a (rose  pink dress:1)\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"controlnet\": \"octoai:canny_sdxl\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\",\n        \"controlnet_conditioning_scale\": 1,\n        \"controlnet_image\": \"<BASE64 IMAGE>\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_controlnet_sdxl(\n        prompt=\"A photo of woman wearing a (rose  pink dress:1)\",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality\",\n        controlnet=\"octoai:canny_sdxl\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n        use_refiner=True,\n        high_noise_frac=0.8,\n        style_preset=\"base\",\n        controlnet_conditioning_scale=1,\n        controlnet_image=\"<BASE64 IMAGE>\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateControlnetSdxl({\n  prompt: \"A photo of a woman wearing a (rose pink dress:1)\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  controlnet: \"octoai:canny_sdxl\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n  controlnetConditioningScale: 1,\n  controlnetImage: \"<BASE64_IMAGE>\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.image_b64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/controlnet-sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"A photo of woman wearing a (rose  pink dress:1)\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"controlnet\": \"octoai:canny_sdxl\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\",\n        \"controlnet_conditioning_scale\": 1,\n        \"controlnet_image\": \"<BASE64 IMAGE>\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_controlnet_sdxl(\n        prompt=\"A photo of woman wearing a (rose  pink dress:1)\",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality\",\n        controlnet=\"octoai:canny_sdxl\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n        use_refiner=True,\n        high_noise_frac=0.8,\n        style_preset=\"base\",\n        controlnet_conditioning_scale=1,\n        controlnet_image=\"<BASE64 IMAGE>\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateControlnetSdxl({\n  prompt: \"A photo of a woman wearing a (rose pink dress:1)\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  controlnet: \"octoai:canny_sdxl\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n  controlnetConditioningScale: 1,\n  controlnetImage: \"<BASE64_IMAGE>\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.image_b64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/controlnet-sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"An photo of a white man on a japanese tatami mat \",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality, distorted legs, distorted feet, disproportionate  hands and \",\n        \"controlnet\": \"octoai:openpose_sdxl\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\",\n        \"controlnet_conditioning_scale\": 1,\n        \"controlnet_image\": \"<BASE64 IMAGE>\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_controlnet_sdxl(\n        prompt=\"An photo of a white man on a japanese tatami mat \",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality, distorted legs, distorted feet, disproportionate  hands and \",\n        controlnet=\"octoai:openpose_sdxl\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n        use_refiner=True,\n        high_noise_frac=0.8,\n        style_preset=\"base\",\n        controlnet_conditioning_scale=1,\n        controlnet_image=\"<BASE64 IMAGE>\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateControlnetSdxl({\n  prompt: \"A photo of a white man on a Japanese tatami mat\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  controlnet: \"octoai:openpose_sdxl\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n  controlnetConditioningScale: 1,\n  controlnetImage: \"<BASE64_IMAGE>\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.image_b64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/controlnet-sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"An photo of a white man on a japanese tatami mat \",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality, distorted legs, distorted feet, disproportionate  hands and \",\n        \"controlnet\": \"octoai:openpose_sdxl\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\",\n        \"controlnet_conditioning_scale\": 1,\n        \"controlnet_image\": \"<BASE64 IMAGE>\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_controlnet_sdxl(\n        prompt=\"An photo of a white man on a japanese tatami mat \",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality, distorted legs, distorted feet, disproportionate  hands and \",\n        controlnet=\"octoai:openpose_sdxl\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n        use_refiner=True,\n        high_noise_frac=0.8,\n        style_preset=\"base\",\n        controlnet_conditioning_scale=1,\n        controlnet_image=\"<BASE64 IMAGE>\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateControlnetSdxl({\n  prompt: \"A photo of a white man on a Japanese tatami mat\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  controlnet: \"octoai:openpose_sdxl\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n  useRefiner: true,\n  highNoiseFrac: 0.8,\n  stylePreset: \"base\",\n  controlnetConditioningScale: 1,\n  controlnetImage: \"<BASE64_IMAGE>\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.image_b64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      }
    ],
    "content": "While traditional image generation models can produce stunning visuals, they often lack guidance, and therefore the ability to generate images subject to user-desired image composition. ControlNet changes the game by allowing an additional image input that can be used for conditioning (influencing) the final image generation. This could be anything from simple scribbles to detailed depth maps or edge maps. By conditioning on these input images, ControlNet directs the Stable Diffusion model to generate images that align closely with the user's intent.\nOctoAI's Asset Library comes pre-populated with the followinglist of public controlnets.\nOther than using the default controlnet checkpoints, you can also upload private ControlNet checkpoints into the OctoAI Asset Library and then use those checkpoints at generation time via the parameter controlnet in the API. For custom controlnet checkpoints, make sure to provide your own ControlNet mask in the controlnet_image parameter.\nBelow is an example of using a Canny ControlNet along with ControlNet image (left) and a simple prompt A photo of woman wearing a (rose  pink dress:1). Canny ControlNet is designed to detect a wide range of edges in images. Given a raw image or sketch, Canny can extract the image's contours and edges, and use them for image generation. You can see the image (right) generated from SDXL with Canny ControlNet applied.\n\n\n\n\n\n\n\n\nExample Code for Canny ControlNet:\n\n\nBelow is an example of using a OpenPose ControlNet along with ControlNet image (left) and a prompt An photo of a white man on a japanese tatami mat . OpenPose ControlNet is a fast human keypoint detection model that can extract human poses like positions of hands, legs, and head. See the example below. You can see the image (right) generated from SDXL with OpenPose ControlNet applied.\n\n\n\n\n\n\n\n\nExample Code for OpenPose ControlNet:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.optimizations.sdxl-lighting-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/optimizations/sdxl-lighting",
    "pathname": "/docs/media-gen-solution/optimizations/sdxl-lighting",
    "title": "SDXL Lighting for blazing fast generations",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Optimizations",
        "pathname": "/docs/documentation/media-gen-solution/optimizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "A new technology called SDXL Lighting enables high-quality image generations in less than 1 second",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"((glass orb)) with snowy christmas scene in it \",\n        \"negative_prompt\": \"ornament, Blurry, low-res, poor quality\",\n        \"checkpoint\": \"octoai:lightning_sdxl\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 8,\n        \"cfg_scale\": 3,\n        \"seed\": 3327823665,\n        \"use_refiner\": false,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_sdxl(\n        prompt=\"((glass orb)) with snowy christmas scene in it \",\n        negative_prompt=\"ornament, Blurry, low-res, poor quality\",\n        checkpoint=\"octoai:lightning_sdxl\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=8,\n        cfg_scale=3,\n        seed=3327823665,\n        use_refiner=False,\n        style_preset=\"base\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt: \"((glass orb)) with snowy Christmas scene in it\",\n  negativePrompt: \"ornament, blurry, low-res, poor quality\",\n  checkpoint: \"octoai:lightning_sdxl\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 8,\n  cfgScale: 3,\n  seed: 3327823665,\n  useRefiner: false,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"((glass orb)) with snowy christmas scene in it \",\n        \"negative_prompt\": \"ornament, Blurry, low-res, poor quality\",\n        \"checkpoint\": \"octoai:lightning_sdxl\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 8,\n        \"cfg_scale\": 3,\n        \"seed\": 3327823665,\n        \"use_refiner\": false,\n        \"style_preset\": \"base\"\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_sdxl(\n        prompt=\"((glass orb)) with snowy christmas scene in it \",\n        negative_prompt=\"ornament, Blurry, low-res, poor quality\",\n        checkpoint=\"octoai:lightning_sdxl\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=8,\n        cfg_scale=3,\n        seed=3327823665,\n        use_refiner=False,\n        style_preset=\"base\",\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSdxl({\n  prompt: \"((glass orb)) with snowy Christmas scene in it\",\n  negativePrompt: \"ornament, blurry, low-res, poor quality\",\n  checkpoint: \"octoai:lightning_sdxl\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 8,\n  cfgScale: 3,\n  seed: 3327823665,\n  useRefiner: false,\n  stylePreset: \"base\",\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      }
    ],
    "content": "SDXL Lighting enable you to achieve high-quality output from SDXL with only 4-8 steps and less than 1 second of latency. This stands in stark contrast to generating images using SDXL model, which typically demands 30-40 steps to produce a good quality image in 3-3.5 seconds, and even more when incorporating custom assets like LoRAs. More importantly, SDXL Lighting is completely compatible with existing image customization techniques available on OctoAI such as SDXL LoRAs. Customers using this can achieve both fast inference speed and product differentiation via customization.\nYou can use SDXL Lighting via OctoAI's Image Gen API\nThe following guidelines must be adhered to ensure high quality of output:\nOpt for 4-8 steps, with 8 steps being the recommended choice.\n\nMaintain a low CFG Scale, ideally ranging from 1.4 to 4.0, with 3.0 as the optimal value.\n\nUtilize any sampler, textual inversion, or style preset.\n\nNote that additional LoRAs and VAEs will result in increased inference time.\n\n\nBelow is an example for text2img with SDXL Lighting:\nExample Code:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.optimizations.ssd-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/optimizations/ssd",
    "pathname": "/docs/media-gen-solution/optimizations/ssd",
    "title": "Stable Diffusion SSD",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Optimizations",
        "pathname": "/docs/documentation/media-gen-solution/optimizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "SSD-1B, a distilled version of SDXL generates images 50% faster than SDXL.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/ssd\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"An image of a deLorean car in a city setting\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_ssd(\n        prompt=\"An image of a deLorean car in a city setting\",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSsd({\n  prompt: \"An image of a DeLorean car in a city setting\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      },
      {
        "lang": "bash",
        "meta": "cURL",
        "code": "curl -X POST \"https://image.octoai.run/generate/ssd\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data-raw '{\n        \"prompt\": \"An image of a deLorean car in a city setting\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12\n    }' | jq -r \".images[0].image_b64\" | base64 -d >result.jpg"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI(api_key=os.environ.get(\"OCTOAI_TOKEN\"))\n    image_gen_response = client.image_gen.generate_ssd(\n        prompt=\"An image of a deLorean car in a city setting\",\n        negative_prompt=\"Blurry photo, distortion, low-res, poor quality\",\n        width=1024,\n        height=1024,\n        num_images=1,\n        sampler=\"DDIM\",\n        steps=30,\n        cfg_scale=12,\n    )\n    images = image_gen_response.images\n\n    for i, image in enumerate(images):\n        to_file(image, f\"result{i}.jpg\")"
      },
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst { images } = await octoai.imageGen.generateSsd({\n  prompt: \"An image of a DeLorean car in a city setting\",\n  negativePrompt: \"Blurry photo, distortion, low-res, poor quality\",\n  width: 1024,\n  height: 1024,\n  numImages: 1,\n  sampler: \"DDIM\",\n  steps: 30,\n  cfgScale: 12,\n});\n\nimages.forEach((output, i) => {\n  if (output.imageB64) {\n    const buffer = Buffer.from(output.imageB64, \"base64\");\n    fs.writeFileSync(`result${i}.jpg`, buffer);\n  }\n});"
      }
    ],
    "content": "The primary advantage of utilizing Stable Diffusion SSD lies in its remarkable efficiency, boasting a 50% faster speed compared to SDXL, owing to its more compact design. Moreover, OctoAI has implemented its exclusive compiler and cloud system optimizations, culminating in an unparalleled end-to-end average generation speed of 1.4 seconds — establishing it as the swiftest iteration of SSD-1B available in the market.\nHowever, it's essential to acknowledge certain limitations inherent to the SSD-1B model. Due to its distillation into a smaller form, the resultant output images deviate from those produced by SDXL. In essence, even with identical seed and API parameters, replicating the same output images as SDXL is no longer feasible. Furthermore, the community surrounding this novel model remains comparatively modest in size compared to that of SDXL. Consequently, the availability of finely-tuned assets such as checkpoints and LoRAs for tailoring styles, objects, and facial features is notably limited.\nExample Code:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.optimizations.samplers-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/optimizations/samplers",
    "pathname": "/docs/media-gen-solution/optimizations/samplers",
    "title": "Samplers",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Optimizations",
        "pathname": "/docs/documentation/media-gen-solution/optimizations"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Stable Diffusion Samplers utilize diffusion models to iteratively refine noise, producing high-quality images with remarkable fidelity and coherence.",
    "content": "Stable Diffusion Samplers encompass various techniques within generative modeling, each offering unique advantages and applications. These samplers operate by iteratively refining noise to generate high-quality images. By gradually diffusing noise while conditioning on observed data, these samplers excel at capturing intricate details and complex patterns, yielding visually stunning outputs across various domains.\nWhen selecting a sampler, considerations such as computational efficiency, convergence speed, and applicability to specific tasks are crucial. The choice of sampler depends on the specific requirements and constraints of the task at hand, with practitioners often experimenting to find the optimal balance between performance and efficacy.\nSupported samplers within OctoAI's Image Gen API include: DDIM,DDPM,DPM_PLUS_PLUS_2M_KARRAS,DPM_SINGLE,DPM_SOLVER_MULTISTEP,K_EULER, K_EULER_ANCESTRAL,PNDM,UNI_PC. These are regular samplers. Premium samplers (priced twice as the regular samplers) include DPM_2, DPM_2_ANCESTRAL,DPM_PLUS_PLUS_SDE_KARRAS, HEUN and KLMS."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "pathname": "/docs/python-sdk/image-generator-client",
    "title": "Image Generator Python client",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "The ImageGenClient class specializes in supporting image generation in your application, and guiding what options are available to modify your outputs. It will return a list of all images using the ImageGeneration type. It allows you to use both Stable Diffusion 1.5 and Stable Diffusion XL for text to image and image to image use cases, and set parameters and prompts either with weighted prompts with the prompt field as was common with Stable Diffusion 1.5 or human-readable descriptions using prompt_2 with Stable Diffusion XL 1.0.\nThis guide will walk you through a text to image example, and then we will use the resulting image to demonstrate the image to image use case."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "pathname": "/docs/python-sdk/image-generator-client",
    "title": "Requirements",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#requirements",
    "content": "First, create an OctoAI API token.\n\nThen, complete Python SDK Installation & Setup..\nIf you use the OCTOAI_TOKEN envvar for your token, you can instantiate the image_gen client with client = OctoAI().image_gen",
    "hierarchy": {
      "h0": {
        "title": "Image Generator Python client"
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-simple-text-to-image-generation-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "pathname": "/docs/python-sdk/image-generator-client",
    "title": "Simple Text to Image Generation Example",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#simple-text-to-image-generation-example",
    "content": "After running this simple prompt, you should hopefully have an output somewhat similar to the image below:\nastropus.png\nA good start and in our next example, we'll use more features to help guide our outputs.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    # If OCTOAI_TOKEN is not set as an envvar, you can also pass a token to the client with:\n    # ImageGenerator(token=\"YOUR_TOKEN_HERE\")\n    client = OctoAI()\n    # images is a list of Images from octoai.types\n    image_resp = client.image_gen.generate_sdxl(\n        prompt=\"photorealistic, poodle, intricately detailed\"\n    )\n    images = image_resp.images\n\n    # images can be filtered for safety, so since we only generated 1 image by default, this verifies\n    # we actually have an image to show.\n    if not images[0].removed_for_safety:\n        to_file(images[0], \"output.jpg\")"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Image Generator Python client"
      },
      "h4": {
        "id": "simple-text-to-image-generation-example",
        "title": "Simple Text to Image Generation Example"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-text-to-image-generation-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "pathname": "/docs/python-sdk/image-generator-client",
    "title": "Text to Image Generation Example",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#text-to-image-generation-example",
    "content": "One of the simplest ways to customize your outputs is a style preset, negative_prompt, loras, and model selection.\nastropus.png\nMuch more realistic! Now that we have our cinematic poodle, let's go ahead and use this image as part of an image-to-image workflow.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.util import to_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI()\n\n    prompt = \"photorealistic, colorful, poodle, intricately detailed\"\n    file_name = \"pretty_poodle_cinematic.jpeg\"\n\n    images_resp = client.image_gen.generate_sdxl(\n        prompt=prompt,\n        negative_prompt=\"horror, scary, low-quality, extra limbs, cartoon\",\n        checkpoint=\"crystal-clear\",\n        style_preset=\"cinematic\",\n        loras={\"add-detail\": 1.0},\n        steps=50,\n    )\n    images = images_resp.images\n    # It can also be helpful to run another generate method with\n    # num_images = image_resp.removed_for_safety to get your desired total images\n    if not images[0].removed_for_safety:\n        to_file(images[0], file_name)\n"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Image Generator Python client"
      },
      "h4": {
        "id": "text-to-image-generation-example",
        "title": "Text to Image Generation Example"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-image-to-image-generation-example-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "pathname": "/docs/python-sdk/image-generator-client",
    "title": "Image to Image Generation Example",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#image-to-image-generation-example",
    "content": "Image to Image Generation lets you use a base image, in our case the above pretty_poodle.jpeg to shape the feel of your outputs image. In our case, we'd expect some focal point in the center, and a blurred, bright background, but otherwise our output can look anywhere from completely different or quite similar depending on our prompt. In this case, let's go for a complete different style of outputs and stray from the usual theme of poodles to a corgi in the rain.\nastropus.png",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.util import to_file, from_file\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n    client = OctoAI()\n\n    init = from_file(\"pretty_poodle_cinematic.jpeg\")\n    images_resp = client.image_gen.generate_sdxl(\n        prompt=\"corgi in the rain\",\n        init_image=init,  # Only used for image-to-image\n        strength=0.8,  # Only used for image-to-image\n        style_preset=\"anime\"\n    )\n    images = images_resp.images\n\n    to_file(images[0], \"rain_corgi.jpeg\")"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Image Generator Python client"
      },
      "h4": {
        "id": "image-to-image-generation-example",
        "title": "Image to Image Generation Example"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Fine-tuning Stable Diffusion",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Create custom assets with OctoAI's fine-tuning of Stable Diffusion models.",
    "content": "OctoAI lets you fine-tune Stable Diffusion to customize generated images. Fine-tuning is a process of training a model with additional data for your task. There's a few simple steps:\nConfigure fine-tuning settings & upload your images\n\nRun the fine-tuning job\n\nUse the fine-tuned asset (a LoRA) in your image generation requests\n\n\nWe're using the LoRA fine-tuning method, which is an acronym for Low-Rank Adaptation. It's a fast and effective way to fine-tune Stable Diffusion. Fine-tuning is supported for Stable Diffusion v1.5 and Stable Diffusion XL. Fine-tuning is available in the OctoAI web UI or via the fine-tuning API."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-web-ui-guide-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Web UI Guide",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#web-ui-guide",
    "content": "In the web UI, navigate to the Tuning & Datasets page from the Media Gen Solution menu to get started - any previously tuned models will also be listed here. Click on “New Tune” to continue.",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "web-ui-guide",
        "title": "Web UI Guide"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-configure-settings-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Configure settings",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#configure-settings",
    "content": "Specify the name of your fine-tune, the trigger word of the subject you're fine-tuning, and the base checkpoint. The base checkpoint can be the default Stable Diffusion v1.5 checkpoint, default Stable Diffusion XL checkpoint, or any custom checkpoint.\nThe trigger word can be used in your inference requests to customize the images with your subject. We generally recommend using a unique trigger word, such as \"sks1\", that's unlikely to be associated with a different subject in Stable Diffusion. Alternatively, you can use an existing concept as the trigger word value - such as \"in the style of a cartoon drawing\" - to update Stable Diffusion's understanding of that concept.\nThen, specify the number of steps to train. A range of 400 to 1,200 steps works well in most cases, and a good guideline is about 75 to 100 steps per training image. The model can underfit if the numer of training steps is too low, resulting in poor quality. If it's too high, the model can overfit and struggle to represent details that aren't represented in the training images.",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "web-ui-guide",
        "title": "Web UI Guide"
      },
      "h4": {
        "id": "configure-settings",
        "title": "Configure settings"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-upload-images--tune-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Upload images & tune",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#upload-images--tune",
    "content": "Next, upload your training images and start tuning. We recommend using varied images, including different backgrounds, lightings, and distances. Finding a balance between variation and consistency can help improve image generation quality. All uploaded images used for fine-tuning must comply with our terms of service.\nOptionally, you can provide captions for each image that describe the custom subject. This can help improve fine-tuning and the quality of generated images. Make sure to include your trigger word in the caption.\nWhen you're ready, click \"Start Tuning\", and the fine-tune job will progress from pending to running before completing.",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "web-ui-guide",
        "title": "Web UI Guide"
      },
      "h4": {
        "id": "upload-images--tune",
        "title": "Upload images & tune"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-generating-images-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Generating images",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#generating-images",
    "content": "When complete, the fine-tuned asset is stored in your Asset Library and available for image generation. You can launch the Text to Image or Image to Image tool to start generating images with your custom asset.",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "web-ui-guide",
        "title": "Web UI Guide"
      },
      "h4": {
        "id": "generating-images",
        "title": "Generating images"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-api-guide-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "API Guide",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-guide",
    "content": "Complete fine-tuning API parameters are organized in our API Reference documentation.",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-upload-images-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Upload images",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#upload-images",
    "content": "First, upload your training images using the AssetLibrary Python Client or CLI.\nPython client\nYou can easily upload individual image files, or a folder with multiple files. Here's an example uploading the image1.jpeg file with the name image1 from the file path finetuning_images:\nYou'll receive a response with the asset ID, name, and status:\nHere's an example uploading a folder of images. This code snippet gets the files in the folder named finetuning_images, then splits on the . to get the files_format extension (jpg, jpeg, or png). Then the file names are used to set the asset names:\nThe final print(asset) will return a response with each asset ID, name, and status:\nCLI\nAlternatively, you can upload images using the OctoAI CLI. Here's the CLI command using the same image1.jpeg example:",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.client import OctoAI\nfrom octoai.asset_library import Data_File\n\nclient = OctoAI()\n\nasset = client.asset_library.create_from_file(\n    file=\"finetuning_images/image1.jpeg\",\n    data=Data_File(file_format=\"jpeg\"),\n    name=\"image1\",\n    description=\"Fine-tuning image\",\n)\n\nprint(asset)\nprint(client.asset_library.list(name=\"image1\"))\n\nclient.asset_library.list()"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "id: asset_01hf3a2qw7ek6vshbhd9c9yywd, name: image1, status: Status.UPLOADED"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.client import OctoAI\nfrom octoai.asset_library import Data_File\n\nif __name__ == \"__main__\":\n    # OCTOAI_TOKEN set as an environment variable so do not need to pass a token.\n    client = OctoAI()\n\n    dir_path = \"./finetuning_images/\"  # Set your dir_path here to your file assets.\n    files = []\n    # Get a list of files in the folder\n    for file_path in os.listdir(dir_path):\n        if os.path.isfile(os.path.join(dir_path, file_path)):\n            files.append(file_path)\n    for file in files:\n        # Use the file names to get file_format and the asset_name.\n        split_file_name = file.split(\".\")\n        asset_name = split_file_name[0]\n        file_format = split_file_name[1]\n        file_data = Data_File(\n            file_format=file_format,\n        )\n        asset = client.asset_library.create_from_file(\n            file=dir_path + file,\n            data=file_data,\n            name=asset_name,\n        )\n\n        print(asset)"
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "octoai asset create \\\n  --upload-from-file finetuning_images/image1.jpeg \\\n  --name image1 \\\n  --type file"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide"
      },
      "h4": {
        "id": "upload-images",
        "title": "Upload images"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-configure-settings--tune-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Configure settings & tune",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#configure-settings--tune",
    "content": "Next, create your fine-tune. In the examples in this section, we're fine-tuning Stable Diffusion XL with images of a bulldog. We specify the base checkpoint, trigger word, training steps, and fine-tune name. Also included are the individual training images and corresponding captions. We recommend including captions, describing the context of the subject, to improve fine-tuning quality. Be sure to include your trigger word within the caption.\nPython Client\nREST API\nFull API details and parameters are available here. Using the continue_on_rejection boolean parameter, you can optionally continue with the fine-tune job if any of the training images are identified as NSFW.\nYou'll receive a response that includes the tune ID, which you can use to monitor the status of the job.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.fine_tuning import Details_LoraTune, LoraTuneCheckpoint, LoraTuneFile\nfrom octoai.client import OctoAI\n\nif __name__ == \"__main__\":\n  client = OctoAI()\n\n  # create a fine tuning job\n  tune = client.fine_tuning.create(\n    name=\"sks1-bulldog-01\",\n    details=Details_LoraTune(\n        files=[\n            LoraTuneFile(file_id=\"asset_01hekxwwg1fzjv48sfy5s2xmnj\", caption=\"sks1 bulldog playing at the beach\"),\n            LoraTuneFile(file_id=\"asset_01hekxwqgrev5t3ser2w2qf8bm\", caption=\"sks1 bulldog playing with a bone\"),\n            LoraTuneFile(file_id=\"asset_01hekxwj3bekpvr52kne3c6ca7\", caption=\"sks1 bulldog looking up\"),\n            LoraTuneFile(file_id=\"asset_01hekxwdc8eqrrahjm3ekze356\", caption=\"sks1 bulldog resting in the grass\"),\n            LoraTuneFile(file_id=\"asset_01hekxw80kfdmrdh7ny3vyvf0h\", caption=\"sks1 bulldog running at the park\"),\n        ],\n        base_checkpoint=LoraTuneCheckpoint(\n            checkpoint_id=\"asset_01hdpjv7bxe1n99eazrv23ca1k\",\n            engine=\"image/stable-diffusion-xl-v1-0\",\n        ),\n        trigger_words=[\"sks1\"],\n        steps=800,\n    )\n  )\n  print(f\"Tune {tune.name} status: {tune.status}\")\n\n  # check the status of a fine tuning job\n  tune = client.fine_tuning.get(tune.id)\n  print(f\"Tune {tune.name} status: {tune.status}\")\n\n  # when the job finishes, check the asset ids of the resulted loras\n  # (the tune will take some time to complete)\n  if tune.status == \"succeded\":\n    print(f\"Generated LoRAs: {tune.output_lora_ids}\")\n"
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "curl -X POST \"https://api.octoai.cloud/v1/tune\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n  --data '\n{\n  \"details\": {\n    \"base_checkpoint\": {\n      \"name\": \"default-sdxl\",\n      \"checkpoint_id\": \"asset_01hdpjv7bxe1n99eazrv23ca1k\"\n    },\n    \"tune_type\": \"lora_tune\",\n    \"files\": [\n      {\n        \"file_id\": \"asset_01hekxwwg1fzjv48sfy5s2xmnj\",\n        \"caption\": \"sks1 bulldog playing at the beach\"\n      },\n      {\n        \"file_id\": \"asset_01hekxwqgrev5t3ser2w2qf8bm\",\n        \"caption\": \"sks1 bulldog playing with a bone\"\n      },\n      {\n        \"file_id\": \"asset_01hekxwj3bekpvr52kne3c6ca7\",\n        \"caption\": \"sks1 bulldog looking up\"\n      },\n      {\n        \"file_id\": \"asset_01hekxwdc8eqrrahjm3ekze356\",\n        \"caption\": \"sks1 bulldog resting in the grass\"\n      },\n      {\n        \"file_id\": \"asset_01hekxw80kfdmrdh7ny3vyvf0h\",\n        \"caption\": \"sks1 bulldog running at the park\"\n      }\n    ],\n    \"trigger_words\": [\n      \"sks1\"\n    ],\n    \"steps\": 800\n  },\n  \"tune_type\": \"lora_tune\",\n  \"description\": \"sks1 bulldog\",\n  \"name\": \"sks1-bulldog-01\"\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide"
      },
      "h4": {
        "id": "configure-settings--tune",
        "title": "Configure settings & tune"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-monitor-fine-tuning-status-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Monitor fine-tuning status",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#monitor-fine-tuning-status",
    "content": "You can check the status of a fine-tune job by running a GET on the tune ID:",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "curl \"https://api.octoai.cloud/v1/tune/tune_01hen39pazf6s9jqpkfj05y0vx\" \\\n -H \"Authorization: Bearer $OCTOAI_TOKEN\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide"
      },
      "h4": {
        "id": "monitor-fine-tuning-status",
        "title": "Monitor fine-tuning status"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-generating-images-1-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Generating images",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#generating-images-1",
    "content": "When complete, the fine-tuned asset is stored in your Asset Library and available for image generation. You can generate images by including the LoRA in your image generation request.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "curl -X POST \"<https://image.octoai.run/generate/sdxl\">\n    -H \"Content-Type: application/json\"\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\"\n    --data-raw '{\n        \"prompt\": \"A photo of a sks1 bulldog running in space\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, poor quality\",\n        \"loras\": {\n            \"sks1-bulldog-01\": 0.9\n        },\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1,\n        \"sampler\": \"DDIM\",\n        \"steps\": 30,\n        \"cfg_scale\": 12,\n        \"use_refiner\": true,\n        \"high_noise_frac\": 0.8,\n        \"style_preset\": \"base\"\n    }'"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide"
      },
      "h4": {
        "id": "generating-images-1",
        "title": "Generating images"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-image-captions-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Image captions",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#image-captions",
    "content": "Image captions can improve quality by providing additional context and details to the trained model. Whenever possible, we suggest using image captions - and be sure to include your trigger word in each caption.\nYou can also include the subject class - such as person, animal, or object - to improve quality. As an example, you could fine-tune images of a specific bulldog with an example caption sks1 bulldog playing at the beach where sks1 is the trigger word and bulldog is the subject class.",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "general-fine-tuning-tips",
        "title": "General fine-tuning tips"
      },
      "h4": {
        "id": "image-captions",
        "title": "Image captions"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-image-variation-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Image variation",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#image-variation",
    "content": "We recommend some amount of variation in your images. If every image is close-up, the fine-tuned model may be limited to representing that distance. It's also helpful to have some level of consistency among the images to ensure the model learns the intended subject. Finding the right balance between consistency and variation can require a few iterations, and we encourage you to experiment!",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "general-fine-tuning-tips",
        "title": "General fine-tuning tips"
      },
      "h4": {
        "id": "image-variation",
        "title": "Image variation"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-managing-fine-tunes-and-assets-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Managing fine-tunes and assets",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#managing-fine-tunes-and-assets",
    "content": "You can separately manage a fine-tune job, and the corresponding LoRA created by a fine-tune. Deleting a fine-tune job won't automatically delete the training images nor LoRA created during fine-tuning, and vice versa. Additionally, fine-tune names must be unique. You may encounter an error if you try to create a fine-tune with a duplicate name.",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "general-fine-tuning-tips",
        "title": "General fine-tuning tips"
      },
      "h4": {
        "id": "managing-fine-tunes-and-assets",
        "title": "Managing fine-tunes and assets"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-fine-tuning-duration-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "title": "Fine-tuning duration",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#fine-tuning-duration",
    "content": "Fine-tuning Stable Diffusion v1.5 usually takes about 3-7 minutes, and Stable Diffusion XL takes about 10-20 minutes. Increasing the number of training steps will extend the fine-tuning duration.",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion"
      },
      "h3": {
        "id": "general-fine-tuning-tips",
        "title": "General fine-tuning tips"
      },
      "h4": {
        "id": "fine-tuning-duration",
        "title": "Fine-tuning duration"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "title": "TypeScript SDK Fine-tuning",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "How to create a fine-tuned LoRA using OctoAI's TypeScript SDK",
    "content": "This guide will walk you through creating a fine-tuned LoRA using our TypeScript SDK to upload image file assets, creating a tune, and then using that LoRA to run an inference using our Image Gen service once it's ready.\nPlease see Fine-tuning Stable Diffusion for more specifics about each parameter in the fine-tuning API, using curl, or the Python SDK. Our Asset Library API reference documentation goes more into the specifics of using different asset methods as well."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "title": "Requirements",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#requirements",
    "content": "Please create an OctoAI API token if you don't have one already.\n\nPlease also verify you've completed TypeScript SDK Installation & Setup.\n\nIf you use the OCTOAI_TOKEN environment variable for your token, you can instantiate the client with const octoai = new OctoAIClient() or pass the token as a parameter to the constructor like const octoai = new OctoAIClient({ apiKey: process.env.OCTOAI_TOKEN }).\n\nAn account and API token is required for all the following steps.",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning"
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-high-level-steps-to-creating-a-fine-tuned-lora-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "title": "High-level steps to creating a fine-tuned LoRA",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#high-level-steps-to-creating-a-fine-tuned-lora",
    "content": "In order to run a LoRA fine-tuning job, you need to complete a few steps:\nCreate image file assets using the Asset Library, then wait for those assets' status to be ready\n\nEither create a checkpoint asset you would like to use or get one from OctoAI's public checkpoints.\n\nCreate a tune job, then wait for the status to be succeeded.\n\nRun an inference with the new LoRA.\n\n\nDirections with all the code put together are included at the bottom of the document, but at each step we will cover additional information.",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning"
      },
      "h4": {
        "id": "high-level-steps-to-creating-a-fine-tuned-lora",
        "title": "High-level steps to creating a fine-tuned LoRA"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-1-creating-image-file-assets-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "title": "Creating image file assets",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#1-creating-image-file-assets",
    "content": "Asset Library in the TypeScript SDK covers more specifics about the methods, so this example will be focused on a code snippet for uploading multiple files from a folder at once.\nIn this example, we will use multiple photos of a toy poodle named Mitchi.\nAfter this completes, all assets will hopefully be in the ready state, or you should time out. Mitchi is now on OctoAI!\nastropus.png",
    "code_snippets": [
      {
        "lang": "TypeScript",
        "meta": "TypeScript",
        "code": "import fs from \"node:fs\";\nimport { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\nconst fileNames = fs.readdirSync(\"./images\");\n\nconsole.log(\"Uploading files...\");\n\nconst assets = await Promise.all(\n    fileNames.map((fileName) => {\n        const imageName = fileName.split(\".\")[0];\n\n        return octoai.assetLibrary.upload(`./images/${fileName}`, {\n            name: imageName,\n            description: imageName,\n            assetType: \"file\",\n            data: {\n                assetType: \"file\",\n                fileFormat: \"jpg\",\n            },\n        });\n    })\n);\n\nconsole.log(\"Waiting for assets to be ready...\");\n\nfor (const { asset } of assets) {\n    await octoai.assetLibrary.waitForReady(asset.id);\n    console.log(`Asset \"${asset.name}\" is ready`);\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning"
      },
      "h4": {
        "id": "1-creating-image-file-assets",
        "title": "1) Creating image file assets"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-2-get-a-checkpoint-asset-to-use-for-tuning-our-lora-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "title": "Get a checkpoint asset to use for tuning our LoRA",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#2-get-a-checkpoint-asset-to-use-for-tuning-our-lora",
    "content": "Next, you'll need a checkpoint to use to tune your asset. In this example, we will just use the default checkpoint using Stable Diffusion 1.5, but you can also use other public OctoAI checkpoints or create your own using the Asset Library.",
    "code_snippets": [
      {
        "lang": "TypeScript",
        "meta": "TypeScript",
        "code": "const checkpoint = await octoai.assetLibrary.get(\"octoai:default-sd15\");"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning"
      },
      "h4": {
        "id": "2-get-a-checkpoint-asset-to-use-for-tuning-our-lora",
        "title": "2) Get a checkpoint asset to use for tuning our LoRA"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-3-creating-a-tune-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "title": "Creating a tune",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#3-creating-a-tune",
    "content": "We can create a tune by passing in the id of the checkpoint we'd like to use and the ids of the file assets that we created in Step 1. If you want more accurate results, you can add captions to each image to give more thorough descriptions. If no custom captions are provided, the trigger word will be used as a default.",
    "code_snippets": [
      {
        "lang": "TypeScript",
        "meta": "TypeScript",
        "code": "console.log(\"Creating tune...\");\n\nlet tune = await octoai.fineTuning.create({\n    name: \"mitchi\",\n    description: \"mitchi\",\n    details: {\n        tuneType: \"lora_tune\",\n        baseCheckpoint: {\n            checkpointId: checkpoint.asset.id,\n        },\n        // You can add a `caption` to each file for more accurate results\n        files: assets.map(({ asset }) => ({ fileId: asset.id })),\n        steps: 10,\n        triggerWords: [\"sksmitchi\"],\n    },\n});\n\nconsole.log(\"Waiting for tune to be ready...\");\n\nwhile (tune.status !== \"failed\" && tune.status !== \"succeeded\") {\n    await new Promise((resolve) => setTimeout(resolve, 1000));\n    tune = await octoai.fineTuning.get(tune.id);\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning"
      },
      "h4": {
        "id": "3-creating-a-tune",
        "title": "3) Creating a tune"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-4-run-an-inference-with-the-tuned-lora-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "title": "Run an inference with the tuned LoRA",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Fine-tuning Stable Diffusion",
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#4-run-an-inference-with-the-tuned-lora",
    "content": "Next, you can run an inference with the tuned LoRA\nThe end result will be a saved poodle to your local folder.\nStable Diffusion Tuned LoRA generated toy poodle puppy\nresult0.jpg",
    "code_snippets": [
      {
        "lang": "TypeScript",
        "meta": "TypeScript",
        "code": "console.log(\"Generating image...\");\n\nconst { images } = await octoai.imageGen.generateSdxl({\n    prompt: \"A photo of an sksmitchi as a puppy\",\n    negativePrompt: \"Blurry photo, distortion, low-res, poor quality, extra limbs, extra tails\",\n    loras: {\n        mitchi: 0.8,\n    },\n    numImages: 1,\n});\n\nimages.forEach((image, index) => {\n    if (image.imageB64) {\n        const buffer = Buffer.from(image.imageB64, \"base64\");\n        fs.writeFileSync(`result${index}.jpg`, buffer);\n    }\n});"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning"
      },
      "h4": {
        "id": "4-run-an-inference-with-the-tuned-lora",
        "title": "4) Run an inference with the tuned LoRA"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.uploading-custom-assets-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/uploading-a-custom-asset-to-the-octoai-asset-library",
    "pathname": "/docs/media-gen-solution/uploading-a-custom-asset-to-the-octoai-asset-library",
    "title": "Uploading custom assets to OctoAI's Asset Library",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Managing custom assets with Asset Library",
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "$ octoai asset --help\nManage OctoAI assets\n\nUsage:\n  octoai asset [command]\n\nAvailable Commands:\n  create      Create asset\n  delete      Delete an asset\n  get         Get asset\n  list        List assets\n\nFlags:\n  -h, --help   help for asset\n\nGlobal Flags:\n      --log-level string   Set log level: fatal, error, warning, info, debug, trace (default \"info\")\n\nUse \"octoai asset [command] --help\" for more information about a command."
      },
      {
        "code": "octoai asset create \\\n  --upload-from-file LCM_Dreamshaper_v7.safetensors \\\n  --name Dreamshaper_v7 \\\n  --engine image/stable-diffusion-v1-5 \\\n  --format safetensors \\\n  --data-type fp16 \\\n  --type checkpoint \\\n  --description \"Dreamshaper v7\""
      },
      {
        "code": "octoai asset create \\\n  --upload-from-url https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7/resolve/main/LCM_Dreamshaper_v7_4k.safetensors?download=true \\\n  --name Dreamshaper_v7 \\\n  --engine image/stable-diffusion-v1-5 \\\n  --format safetensors \\\n  --data-type fp16 \\\n  --type checkpoint \\\n  --description \"Dreamshaper v7\""
      }
    ],
    "content": "OctoAI empowers you to customize images by leveraging assets like checkpoints, LoRAs, and textual inversions. You can either use:\nPublic assets in the OctoAI Asset Library\n\nUpload your custom asset to the OctoAI Asset Library (private by default, and optionally public)\n\nOr use OctoAI fine-tuning to create new assets - see Fine-tuning Stable Diffusion\n\n\nThis tutorial explains how to upload your own private assets to the Asset Library.\nFirst download the OctoAI CLI by following the instructions in CLI Installation. Check that it is properly installed by running the following in your terminal:\n\n\nRun octoai login to cache your token and authenticate to your account.\n\nWe can now use the octoai asset create subcommand to upload assets (you can run octoai asset create --help to learn more on the options).\n--engine denotes whether this is an asset for SDXL or SD1.5\n\n--upload-from-file flag denotes the path of the file on your local machine that you’re trying to upload.\n\n--upload-from-url flag is an alternative to upload-from-file allowing you to upload an asset from a public URL; OctoAI will fetch and upload the file.\n\n—type can be lora, checkpoint, or textual_inversion (VAEs coming soon)\n\n--format denotes the format of your asset, which can be safetensors or pt.\n\n--datatype can be fp16, fp32, int4, or int8. For image gen, it should almost always be fp16, but for LLMs and other modalities your asset may have other datatypes.\n\n--name is a name for your asset. You can only use each asset name once.\n\n--transfer-api defaults to sts which is the fastest way to upload a large asset.\n\nIf you are uploading a textual inversion, make sure to use the -w flag to denote the default trigger word for the asset. That trigger word can later be used at generation time to activate the inversion. For LoRAs, trigger words are optional.\n\n\n\n\nAs an example, you could use this command to upload a checkpoint from your local machine:\nYou can alternatively upload the file via public URL using upload-from-url:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-python-client-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/asset-orchestrator-client",
    "pathname": "/docs/python-sdk/asset-orchestrator-client",
    "title": "Asset Library in the Python SDK",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Managing custom assets with Asset Library",
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Manage assets using the Python SDK.",
    "content": "The AssetLibrary client in the Python SDK allows create, list, get, and delete actions of assets. These assets allow integration with the ImageGenerator Client to generate more customized images.\nThis guide will walk you through using this API to see a list of our public assets, create your own asset, and use your asset to generate an image."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-python-client-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/asset-orchestrator-client",
    "pathname": "/docs/python-sdk/asset-orchestrator-client",
    "title": "Requirements",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Managing custom assets with Asset Library",
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#requirements",
    "content": "First, create an OctoAI API token.\n\nThen, complete Python SDK Installation & Setup..\nIf you use the OCTOAI_TOKEN envvar for your token, you can instantiate the asset_orch client with asset_library = AssetLibrary()",
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the Python SDK"
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-python-client-creating-a-lora-and-generating-an-image-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/asset-orchestrator-client",
    "pathname": "/docs/python-sdk/asset-orchestrator-client",
    "title": "Creating a LoRA and Generating an Image",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Managing custom assets with Asset Library",
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#creating-a-lora-and-generating-an-image",
    "content": "You will need a safetensors file in order to use this example, and in our case one is named origami-paper.safetensors. I'll be using a lora trained on origami that I can use with the words \"origami\" and \"paper\".\nIn this example, we will be adding a LoRA then using it to generate an image. You can also add checkpoints, vae, and textual inversions.\nastropus.png\nrainbow-origami-tailong-dragon.png",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.client import OctoAI\nfrom octoai.asset_library import Data_Lora\nfrom octoai.util import to_file\n\nif __name__ == \"__main__\":\n    # OCTOAI_TOKEN set as an environment variable so do not need to pass a token.\n    client = OctoAI()\n    asset_library = client.asset_library\n    image_gen = client.image_gen\n\n    asset_name = \"origami-paper-test\"\n    # There is also TextualInversionData, VAEData, and CheckpointData.\n    lora_data = Data_Lora(\n        data_type=\"fp16\",\n        engine=\"image/stable-diffusion-v1-5\",\n        file_format=\"safetensors\",\n    )\n\n    asset = asset_library.create_from_file(\n        file=\"origami-paper.safetensors\",\n        data=lora_data,\n        name=asset_name,\n        description=\"origami-paper stable diffusion 1.5\",\n    )\n\n    image_gen_resp = image_gen.generate_sd(\n        prompt=\"rainbow origami tailong dragon\",\n        num_images=4,\n        loras={\"asset\": 0.8}\n    )\n\n    # Some images can be removed for safety.\n    # Please see the ImageGenerator client docs for more information.\n    for i, image in enumerate(image_gen_resp.images):\n        to_file(image, f\"result{i}.jpg\")\n\n    # You can clean up your asset with the following:\n    asset_library.delete(asset.id)"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the Python SDK"
      },
      "h4": {
        "id": "creating-a-lora-and-generating-an-image",
        "title": "Creating a LoRA and Generating an Image"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-python-client-creating-file-assets-from-a-folder-of-images-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/asset-orchestrator-client",
    "pathname": "/docs/python-sdk/asset-orchestrator-client",
    "title": "Creating File Assets from a Folder of Images",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Managing custom assets with Asset Library",
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#creating-file-assets-from-a-folder-of-images",
    "content": "Let's say you have a folder of images assets you would like to upload for using the FineTuning service. You can do so using the below code snippet to get all the files in your folder named images, and then splitting on the . to get your file_format extension (jpg, jpeg, or png), and use the file name as the asset name.\nIn this example, there is a directory named images that contains files with a _, -, and alphanumeric file names, jpg, jpeg, or png suffixes. In this example, the folder contains the following:\nYou can then use octoai.asset_library.list() to see the assets have been created and uploaded and a result that looks something like:\nIn this example, an already existing lora created in the previous example also exists. The lora has uploaded however will likely need a few more seconds before being ready for use.",
    "code_snippets": [
      {
        "code": "./assets/images/\n\tresult0.jpg\n\tresult1.jpg\n\tresult2.jpg\n\tresult3.jpg\n\tsave_the_other_paper_poodle.png\n\tsave_the_paper_poodle.png"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import os\nfrom octoai.client import OctoAI\nfrom octoai.asset_library import Data_File\n\nif __name__ == \"__main__\":\n    # OCTOAI_TOKEN set as an environment variable so do not need to pass a token.\n    client = OctoAI()\n\n    dir_path = \"./assets/images/\"  # Set your dir_path here to your file assets.\n    files = []\n    # Get a list of files in the folder\n    for file_path in os.listdir(dir_path):\n        if os.path.isfile(os.path.join(dir_path, file_path)):\n            files.append(file_path)\n    for file in files:\n        # Use the file names to get file_format and the asset_name.\n        split_file_name = file.split(\".\")\n        asset_name = split_file_name[0]\n        file_format = split_file_name[1]\n        file_data = Data_File(\n            file_format=file_format,\n        )\n        asset = client.asset_library.create_from_file(\n            file=dir_path + file,\n            data=file_data,\n            name=asset_name,\n        )"
      },
      {
        "code": "[\nid: asset_01234567891011121314151617, name: save_the_paper_poodle, status: ready,\nid: asset_01234567891011121314151618, name: save_the_other_paper_poodle, status: ready,\nid: asset_01234567891011121314151619, name: result2, status: ready,\nid: asset_01234567891011121314151620, name: result3, status: ready,\nid: asset_01234567891011121314151621, name: result1, status: ready,\nid: asset_01234567891011121314151622, name: result0, status: ready,\nid: asset_01234567891011121314151600, name: origami-paper-test, status: uploaded]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the Python SDK"
      },
      "h4": {
        "id": "creating-file-assets-from-a-folder-of-images",
        "title": "Creating File Assets from a Folder of Images"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-type-script-client-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/asset-library",
    "pathname": "/docs/typescript-sdk/asset-library",
    "title": "Asset Library in the TypeScript SDK",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Managing custom assets with Asset Library",
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Manage assets using the TypeScript SDK.",
    "content": "The AssetLibrary client in the TypeScript SDK allows create, list, get, and delete actions of assets. These assets allow integration with Fine-tuning Stable Diffusion.\nThis guide will walk you through using this API to see a list of our public assets, create your own asset, and use your asset to generate an image."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-type-script-client-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/asset-library",
    "pathname": "/docs/typescript-sdk/asset-library",
    "title": "Requirements",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Managing custom assets with Asset Library",
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#requirements",
    "content": "First, create an OctoAI API token.\n\nThen, complete TypeScript SDK Installation & Setup..\nIf you use the OCTOAI_TOKEN environment variable for your token, you can instantiate the client with const octoai = new OctoAIClient() or pass the token as a parameter to the constructor.",
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the TypeScript SDK"
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-type-script-client-creating-a-lora-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/asset-library",
    "pathname": "/docs/typescript-sdk/asset-library",
    "title": "Creating a LoRA",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Managing custom assets with Asset Library",
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#creating-a-lora",
    "content": "You will need a safetensors file in order to use this example, and in our case one is named origami-paper.safetensors. I'll be using a LoRA trained on origami that I can use with the words \"origami\" and \"paper\".\nIn this example, we will be adding a LoRA then using it to generate an image.\nThe id field for the created asset can be used when Getting started with our Media Gen Solution and running inferences.\nastropus.png\nrainbow-origami-tailong-dragon.png generated on ImageGen service using AssetLibrary created LoRA",
    "code_snippets": [
      {
        "lang": "TypeScript",
        "meta": "TypeScript",
        "code": "import { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n    apiKey: process.env.OCTOAI_TOKEN\n});\n\nawait octoai.assetLibrary.upload(\"origami-paper.safetensors\", {\n    assetType: \"lora\",\n    description: \"Origami\",\n    data: {\n        assetType: \"lora\",\n        dataType: \"fp16\",\n        engine: \"image/stable-diffusion-v1-5\",\n        fileFormat: \"safetensors\",\n        triggerWords: [\"origami\", \"paper\"],\n    },\n    name: \"origami-paper\",\n    isPublic: false,\n});"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the TypeScript SDK"
      },
      "h4": {
        "id": "creating-a-lora",
        "title": "Creating a LoRA"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.inpainting-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/inpainting",
    "pathname": "/docs/media-gen-solution/image-utilities/inpainting",
    "title": "Inpainting",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Supported image utilities",
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Edit and resort image to fix flaws or remove unwanted objects from an image.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import requests\nimport json\nimport os\nimport base64\nimport time\nimport io\nimport PIL.Image\n\ndef _process_test(url):\n    image = PIL.Image.open(\"dog.jpg\")\n    mask = PIL.Image.open(\"dogmask.jpg\")\n\n    # Create a BytesIO buffer to hold the image data\n    image_buffer = io.BytesIO()\n    image.save(image_buffer, format='JPEG')\n    image_bytes = image_buffer.getvalue()\n    encoded_image = base64.b64encode(image_bytes).decode('utf-8')\n\n    # Create a BytesIO buffer to hold the image data\n    mask_buffer = io.BytesIO()\n    mask.save(mask_buffer, format='JPEG')\n    mask_bytes = mask_buffer.getvalue()\n    encoded_mask = base64.b64encode(mask_bytes).decode('utf-8')\n\n    OCTOAI_TOKEN = os.environ.get(\"OCTOAI_TOKEN\")\n\n    payload = {\n        \"prompt\": \"Face of a yellow cat, high resolution, sitting on a park bench\",\n        \"negative_prompt\": \"Blurry photo, distortion, low-res, bad quality\",\n        \"steps\": 4,\n        \"width\": 1024,\n        \"height\": 1024,\n        \"cfg_scale\": 1.4,\n        \"checkpoint\": \"octoai:lcm_sdxl\",\n        \"init_image\": encoded_image,\n        \"mask_image\": encoded_mask\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {OCTOAI_TOKEN}\",\n        \"Content-Type\": \"application/json\",\n        \"X-OctoAI-Queue-Dispatch\": \"true\"\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n\n    if response.status_code != 200:\n        print(response.text)\n    print(response.json())\n\n    img_list = response.json()[\"images\"]\n\n    for i, img_info in enumerate(img_list):\n        img_bytes = base64.b64decode(img_info[\"image_b64\"])\n        img = PIL.Image.open(io.BytesIO(img_bytes))\n        img.load()\n        img.save(f\"result_image{i}.jpg\")\n\nif __name__ == \"__main__\":\n    _process_test(\"https://image.octoai.run/generate/sdxl\")"
      }
    ],
    "content": "Inpainting refers to the process of restoring or repairing an image by filling in missing or damaged parts. It is a valuable technique widely used in image editing and restoration, enabling the removal of flaws and unwanted objects to achieve a seamless and natural-looking final image. Inpainting finds applications in film restoration, photo editing, and digital art, among others.\n\n\n\n\n\n\n\n\nHere is a Python example for inpainting. The Image Gen API call will take mask_image (right) and init_image (middle) as shown below."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.outpainting-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/outpainting",
    "pathname": "/docs/media-gen-solution/image-utilities/outpainting",
    "title": "Outpainting",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Supported image utilities",
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Extend beyond the canvas of an existing image.",
    "content": "Outpainting is the process of using an image generation model like Stable Diffusion to extend beyond the canvas of an existing image. Outpainting is very similar to inpainting, but instead of generating a region within an existing image, the model generates a region outside of it.\n\n\n\n\n\n\n\n\nOutpainting works both with SDXL and SD1.5.\nAt a high level, outpainting works like this:\nChoose an existing image you’d like to outpaint.\n\nCreate a source image that places your original image within a larger canvas.\n\nCreate a black and white mask image.\n\nUse init_image (source image), mask_image (your mask image), a text prompt and outpainting parameter as inputs to Image Gen API to generate a new image.\n\n\nIn the following example we will leverage the SD1.5 engine so we’ll start with a 512x512 image.\n\n\n\n\n\nYou can extend the image in any direction, but for this example we’ll extend the width from 512 → 768. Supported resolutions for SD1.5 are:\n(W, H): (768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)\n\nSupport resolutions for SDXL are:\n(W, H): {(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}\nTwo images are required for the next step. An init_image and mask_image are required in order to perform the outpainting to 768 pixels. Convert both images to base64.\n\n\n\n\n\n\n\n\nExample Code for Outpainting:\nHere's the outpainted output that the above code generates:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.adetailer-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/adetailer",
    "pathname": "/docs/media-gen-solution/image-utilities/adetailer",
    "title": "Adetailer API",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Supported image utilities",
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Automatically fix faces and hands using Adetailer",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import requests\nimport json\nimport os\nimport base64\nimport time\nimport io\nimport PIL.Image\n\ndef _process_test(url):\n    image = PIL.Image.open(\"woman.jpg\")\n   \n    # Create a BytesIO buffer to hold the image data\n    image_buffer = io.BytesIO()\n    image.save(image_buffer, format='JPEG')\n    image_bytes = image_buffer.getvalue()\n    encoded_image = base64.b64encode(image_bytes).decode('utf-8')\n\n\n    OCTOAI_TOKEN = os.environ.get(\"OCTOAI_TOKEN\")\n\n    payload = {\n         \"prompt\": \"photorealistic, award winning, 8K HDR, best quality, breathtaking\", \n         \"negative_prompt\": \"worst quality, bad face, drawing, unrealistic, ugly face, animated\", \n         \"cfg_scale\": 6, \n         \"sampler\": \"DPM_2_ANCESTRAL\", \n         \"steps\": 60, \n         \"checkpoint\": \"RealVisXL\", \n         \"inpainting_base_model\": \"sdxl\", \n         \"style_preset\": \"base\", \n         \"detector\": \"face_yolov8n\", \n         \"strength\": 0.5, \n         \"init_image\": encoded_image\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {OCTOAI_TOKEN}\",\n        \"Content-Type\": \"application/json\",\n        \"X-OctoAI-Queue-Dispatch\": \"true\"\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n\n    if response.status_code != 200:\n        print(response.text)\n    print(response.json())\n\n    img_info = response.json()\n    img_bytes = base64.b64decode(img_info[\"image_b64\"])\n    img = PIL.Image.open(io.BytesIO(img_bytes))\n    img.load()\n    img.save(f\"result_image.jpg\")\n\nif __name__ == \"__main__\":\n    _process_test(\"https://image.octoai.run/adetailer\")\n"
      }
    ],
    "content": "OctoAI's Adetailer API supports various detection models. Whether you're interested in identifying faces, or hands, you can choose from options such as face_yolov8n, hand_yolov8n, face_full_mediapipe, face_short_mediapipe, face_mesh_mediapipe, and eyes_mesh_mediapipe. This allows you to tailor the detailing process to your specific needs and preferences.\nAdetailer effortlessly identifies faces and hands in images, automatically creating masks to fill in any missing parts. After this, it seamlessly integrates with the SDXL API, applying customized settings to achieve outstanding results. Additionally, OctoAI's  Adetailer API offers flexibility by allowing users to set the maximum number of detections. For example, if there are five people in an image but you only want to focus on two faces, you can easily do so using the max_num_detections parameter. The API prioritizes and corrects faces based on their confidence score and bounding box dimensions."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.photo-merge-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/photo-merge",
    "pathname": "/docs/media-gen-solution/image-utilities/photo-merge",
    "title": "Photo Merge",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Supported image utilities",
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Seamlessly integrate a photo's subject into AI-generated output and eliminate the need to create time-consuming custom facial finetunes.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "ENDPOINT_URL = \"https://image.octoai.run/generate/sdxl\"\nIMAGES_PATH = \"/content/images\"\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\nluis_path = f'{IMAGES_PATH}/luis'\nluis_urls = ['https://cs.illinois.edu/_sitemanager/viewphoto.aspx?id=12289&s=206',\n             'https://www.kisacoresearch.com/sites/default/files/styles/panopoly_image_square/public/speakers/luis_ceze_headshot.jpg?itok=NmVXlZb2&c=092995db2355e0f1f7ce15ff18aba155',\n             'https://news.cs.washington.edu/wp-content/uploads/2023/01/luis-ceze-2022-blog.jpg',\n             'https://cdn.geekwire.com/wp-content/uploads/2019/12/luisceze-1260x1047.jpeg',\n             ]\n\nluis_image_names = download_from_urls(luis_urls, luis_path)\n\nimage_grid(luis_path, luis_image_names)\n\nluis_b64_images = [encode_b64(luis_path, image) for image in luis_image_names]"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "payload = {\n    \"prompt\": \"A man luis sitting in a coffee shop\",\n    \"negative_prompt\": \"Blurry photo, distortion, low-res, bad quality\",\n    \"checkpoint\": \"RealVisXL\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"num_images\": 4,\n    \"sampler\": \"K_EULER_ANCESTRAL\",\n    \"steps\": 20,\n    \"cfg_scale\": 7.5,\n    \"seed\": 888,\n    \"transfer_images\": {\"luis\": luis_b64_images}\n}\n\nresponse = query(ENDPOINT_URL, payload)\n\ngenerated_images = decode_response(response)\nimage_grid_buffer(generated_images)"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "payload = {\n    \"prompt\": \"closeup colorfull portrait photo of a man luis\",\n    \"negative_prompt\": \"(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth\",\n    \"checkpoint\": \"RealVisXL\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"num_images\": 4,\n    \"sampler\": \"K_EULER_ANCESTRAL\",\n    \"steps\": 20,\n    \"cfg_scale\": 7.5,\n    \"seed\": 111,\n    \"style_preset\": \"Graffiti\",\n    \"transfer_images\": {\"luis\": luis_b64_images}\n}\n\nresponse = query(ENDPOINT_URL, payload)\n\ngenerated_images = decode_response(response)\nimage_grid_buffer(generated_images)"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "payload = {\n    \"prompt\": \"painting, martius_storm red ominous war [:style of vincent van gogh luis and leonardo da vinci:0.4] ral-dissolve\",\n    \"negative_prompt\": \"(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth\",\n    \"checkpoint\": \"RealVisXL\",\n    \"loras\": {\"asset_01hnxyk06ee999h4cq1cz3r21f\": 1.0},\n    \"width\": 1024,\n    \"height\": 1024,\n    \"num_images\": 4,\n    \"sampler\": \"DPM_PLUS_PLUS_2M_KARRAS\",\n    \"steps\": 20,\n    \"cfg_scale\": 7,\n    \"seed\": 111,\n    \"transfer_images\": {\"luis\": luis_b64_images}\n}\n\nresponse = query(ENDPOINT_URL, payload)\n\ngenerated_images = decode_response(response)\nimage_grid_buffer(generated_images)"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "payload = {\n    \"prompt\": \"A man luis wearing a pink T-shirt lacosteshirt1:1, sitting in a coffee shop\",\n    \"negative_prompt\": \"(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth, shirt logo not visible, left position shirt logo\",\n    \"checkpoint\": \"RealVisXL\",\n    \"loras\": {\"asset_01hp5hsn6mfh6b0zf47q862a6b\": 1.0},\n    \"width\": 1024,\n    \"height\": 1024,\n    \"num_images\": 4,\n    \"sampler\": \"DPM_PLUS_PLUS_2M_KARRAS\",\n    \"steps\": 20,\n    \"cfg_scale\": 7,\n    # \"seed\": 444,\n    \"transfer_images\": {\"luis\": luis_b64_images}\n}\n\nresponse = query(ENDPOINT_URL, payload)\n\ngenerated_images = decode_response(response)\nimage_grid_buffer(generated_images)"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "payload = {\n    \"prompt\": \"closeup b&w portrait photo of a man luis einstein\",\n    \"negative_prompt\": \"(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth\",\n    \"checkpoint\": \"RealVisXL\",\n    \"width\": 1024,\n    \"height\": 1024,\n    \"num_images\": 4,\n    \"sampler\": \"K_EULER_ANCESTRAL\",\n    \"steps\": 20,\n    \"cfg_scale\": 7.5,\n    \"seed\": 888,\n    \"transfer_images\": {\"luis\": luis_b64_images, \"einstein\": einstein_b64_images}\n}\n\nresponse = query(ENDPOINT_URL, payload)\n\ngenerated_images = decode_response(response)\nimage_grid_buffer(generated_images)"
      }
    ],
    "content": "OctoAI Photo Merge feature allows you to seamlessly integrate a photo’s subject into high-quality AI-generated output. It eliminates the need to create time-consuming custom facial fine-tunes with numerous tuning images and 15-30 minutes typically associated with SDXL LoRAs. OctoAI's Photo Merge simplifies this process, requiring only 1-4 images and delivering precise results within a few seconds. Businesses can now easily apply GenAI powered imagery for needs ranging from realistic CGI characters, to personalized product recommendations, to digital avatars.\nPhoto Merge can be accessed through the \"transfer_images\" parameter within OctoAI’s Image Generation API. This parameter accepts a key-value pair consisting of a trigger word and an array of up to 4 images. It operates exclusively with SDXL models and seamlessly harmonizes with style presets, controlnets, checkpoints, and LoRAs when utilized with SDXL models, thereby amplifying its adaptability and functionality.\nIn the example below, we will upload 1-4 images of a human and use photo merge to transfer his face into our prompts.\nExample Code Snippet showing the close portriat images of a human:\n\nNext, we utilize the transfer_images=\"triggerword\": list of images parameter within the payload of OctoAI’s SDXL Image Gen API at https://image.octoai.run/generate/sdxl.\nExample Code Snippet of using Photo Merge (transfer_images):\nIn the given example, we employ the trigger word ‘luis’ and link it with the dataset comprising the four images mentioned earlier. Subsequently, we structure the prompt to incorporate the trigger word.\nPrompt: A man luis sitting in a coffee shop.\nIt's worth noting that in this instance, no LoRA is utilized. Additionally, we utilize a checkpoint named ‘RealVisXL’, an OctoAI asset checkpoint specifically optimized for the Photo Merge feature. However, it's important to mention that the Photo Merge feature is functional even if the base SDXL checkpoint is utilized.\nBelow are the AI-generated output images after using transfer_image parameter with the newly provided prompt:\n\nPretty accurate, isn’t it? Let’s try it with few different prompts and combine it with other style presets, LoRAs and checkpoints to confirm whether we consistently get the accurate results.\nLet us use transfer_images parameter in conjunction with ‘Graffiti’ style preset. We are keeping all other parameter values similar to the payload above.\nExample Code Snippet of Photo Merge (transfer_images parameter) with Style Preset:\n\nLet’s now use transfer_images parameter with a pre-trained Style LoRA. We have already imported a pre-trained style based LoRA into OctoAI’s Asset Library. In the payload below, we are using the corresponding asset’s asset id and assigning it a weight of 1.0.\nExample Code Snippet of Photo Merge (transfer_images parameter) with pre-trained LoRAs (uploaded from external sources):\n\nNext, we'll create a custom fine-tune for a product and seamlessly integrate our AI-generated human model's image with it.\nTo create custom fine tunes (SDXL LoRAs), we will upload 10-12 images of our product, which in our case are different colored Lacoste polo shirts for men.\n\n\nExample Code Snippet of Photo Merge (transfer_images parameter) with custom fine-tuned LoRAs:\n\nLet's now consider a use-case where we want to mix the identities of 2 or more humans. For this let's consider a new set of close up portriats of another human as shown below:\n\nExample Code Snippet of Photo Merge (transfer_images parameter) with multiple human portraits:\n\nPhoto Merge offers endless potential - whether in entertainment, gaming, marketing agencies, or fashion and retail sectors, it can help craft personalized avatars, advertisements, and brand ambassador representations. It can also enable virtual try-ons and lifelike digital product showcases."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.upscaling-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/upscaling",
    "pathname": "/docs/media-gen-solution/image-utilities/upscaling",
    "title": "Upscaling",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Supported image utilities",
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Upscale images to higher resolutions.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import requests\nimport json\nimport os\nimport base64\nimport time\nimport io\nimport PIL.Image\nfrom typing import Optional, Tuple\n\n\ndef _process_test(url):\n    \n    image = PIL.Image.open(\"headphones1.jpeg\")\n\n    # Create a BytesIO buffer to hold the image data\n    image_buffer = io.BytesIO()\n    image.save(image_buffer, format='JPEG')\n    image_bytes = image_buffer.getvalue()\n    encoded_image = base64.b64encode(image_bytes).decode('utf-8')\n\n    OCTOAI_TOKEN = os.environ.get(\"OCTOAI_TOKEN\")\n\n    payload = {\n        \"model\": \"real-esrgan-x4-plus\",\n        \"scale\": 2,\n        \"init_image\": encoded_image,\n        \"output_image_encoding\": \"jpeg\"\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {OCTOAI_TOKEN}\",\n        \"Content-Type\": \"application/json\",\n        \"X-OctoAI-Queue-Dispatch\": \"true\"\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n\n    if response.status_code != 200:\n        print(response.text)\n    print(response.json())\n\n    img_info = response.json()[\"image_b64\"]\n\n    img_bytes = base64.b64decode(img_info)\n    img = PIL.Image.open(io.BytesIO(img_bytes))\n\n    if img.mode == 'RGBA':\n     img = img.convert('RGB')\n\n    img.save(\"result_image.jpeg\")\n\nif __name__ == \"__main__\":\n    _process_test(\"https://image.octoai.run/upscaling\")"
      }
    ],
    "content": "Upscaling takes an existing image you provide and upscales it to a higher resolution.\n\n\n\n\n\n\n\n\nExample Code of upscaling an image from 1024X1024 to 2048X2048:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.background-removal-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/bg-removal",
    "pathname": "/docs/media-gen-solution/image-utilities/bg-removal",
    "title": "Background Removal",
    "breadcrumb": [
      {
        "title": "Media Gen Solution",
        "pathname": "/docs/documentation/media-gen-solution"
      },
      {
        "title": "Supported image utilities",
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Remove parts of the image considered to be background.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import requests\nimport json\nimport os\nimport base64\nimport time\nimport io\nimport PIL.Image\nfrom typing import Optional, Tuple\n\n\ndef _process_test(url):\n    \n    image = PIL.Image.open(\"headphones1.jpeg\")\n\n    # Create a BytesIO buffer to hold the image data\n    image_buffer = io.BytesIO()\n    image.save(image_buffer, format='JPEG')\n    image_bytes = image_buffer.getvalue()\n    encoded_image = base64.b64encode(image_bytes).decode('utf-8')\n\n    OCTOAI_TOKEN = os.environ.get(\"OCTOAI_TOKEN\")\n\n    payload = {\n        \"init_image\": encoded_image,\n        \"bgcolor\":(255, 255, 255, 0)\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {OCTOAI_TOKEN}\",\n        \"Content-Type\": \"application/json\",\n        \"X-OctoAI-Queue-Dispatch\": \"true\"\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n\n    if response.status_code != 200:\n        print(response.text)\n    print(response.json())\n\n    img_info = response.json()[\"image_b64\"]\n\n    img_bytes = base64.b64decode(img_info)\n    img = PIL.Image.open(io.BytesIO(img_bytes))\n\n    if img.mode == 'RGBA':\n     img = img.convert('RGB')\n\n    img.save(\"result_image.png\")\n\nif __name__ == \"__main__\":\n    _process_test(\"https://image.octoai.run/background-removal\")"
      }
    ],
    "content": "Background removal takes an existing image you provide and removes those parts of the image considered to be “background.”\n\n\n\n\n\n\n\n\nExample Code of removing background from an image:"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "OctoStack",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI's GenAI production stack in your environment."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-overview-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "Overview",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#overview",
    "content": "OctoStack is a turnkey generative AI solution allowing you to run your choice of models in your environment. It delivers OctoAI’s industry-leading AI inference service and performance optimizations with the privacy benefits of self-managing within your environment. OctoStack provides a full stack solution for running generative AI at scale - including inference, model customization, load balancing, auto-scaling, and telemetry. OctoStack is compatible with all cloud service providers and container orchestration services. Contact us to get started!",
    "hierarchy": {
      "h0": {
        "title": "OctoStack"
      },
      "h1": {
        "id": "overview",
        "title": "Overview"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-privately-deploy-in-any-environment-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "Privately deploy in any environment",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#privately-deploy-in-any-environment",
    "content": "OctoStack is designed for portability across any cloud platform or on-premise data center. Prebuilt containers are easily deployed using Kubernetes or your preferred container orchestration service. Maintain complete data privacy and control by processing all generative AI workloads within your environment.",
    "hierarchy": {
      "h0": {
        "title": "OctoStack"
      },
      "h1": {
        "id": "key-benefits",
        "title": "Key Benefits"
      },
      "h3": {
        "id": "privately-deploy-in-any-environment",
        "title": "Privately deploy in any environment"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-performance-optimized-inference-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "Performance optimized inference",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#performance-optimized-inference",
    "content": "OctoStack includes OctoAI’s performance optimizations delivered through Apache TVM, a compiler framework that optimizes and accelerates inferences. Optimized inference improves GPU utilization and delivers the best possible application experience. OctoStack also provides best-in-class load balancing to operate at scale.",
    "hierarchy": {
      "h0": {
        "title": "OctoStack"
      },
      "h1": {
        "id": "key-benefits",
        "title": "Key Benefits"
      },
      "h3": {
        "id": "performance-optimized-inference",
        "title": "Performance optimized inference"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-easy-to-use-apis--customization-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "Easy to use APIs & customization",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#easy-to-use-apis--customization",
    "content": "Application developers can use OctoStack’s industry-standard API’s, including Python and TypeScript SDK’s. Our ergonomic API’s allow rapid development and integration. You can easily load and run open source models, and fine-tuned checkpoints.",
    "hierarchy": {
      "h0": {
        "title": "OctoStack"
      },
      "h1": {
        "id": "key-benefits",
        "title": "Key Benefits"
      },
      "h3": {
        "id": "easy-to-use-apis--customization",
        "title": "Easy to use APIs & customization"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-gpu-configuration-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "GPU Configuration",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#gpu-configuration",
    "content": "OctoStack can run a multi-GPU configuration to improve throughput and latency, which is easily enabled with a single configuration setting. We recommend using multiple GPU’s per replica when running models with higher precision and a greater number of parameters. The OctoAI team can help you identify the multi-GPU configuration that best matches your throughput & latency goals.",
    "hierarchy": {
      "h0": {
        "title": "OctoStack"
      },
      "h1": {
        "id": "configuration--deployment",
        "title": "Configuration & Deployment"
      },
      "h3": {
        "id": "gpu-configuration",
        "title": "GPU Configuration"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-prerequisites-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "Prerequisites",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#prerequisites",
    "content": "OctoStack is comptable with all cloud environments - these are key system requirements:\nKubernetes, or Docker & Docker Compose\n\nRedis in cluster mode\n\nNVIDIA drivers\n\nGPUs\nOctoStack supports a range of hardware including A10G, A100, and H100 GPUs\n\nA100s such as AWS p4d.24xlarge instances can support a broad set of models",
    "hierarchy": {
      "h0": {
        "title": "OctoStack"
      },
      "h1": {
        "id": "configuration--deployment",
        "title": "Configuration & Deployment"
      },
      "h3": {
        "id": "prerequisites",
        "title": "Prerequisites"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-deployment-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "Deployment",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#deployment",
    "content": "OctoStack supports Kubernetes deployment via manifest files, and a Docker Compose deployment compatible with any container orchestration service. There’s a few simple steps to deploy:\nOctoAI allow lists your AWS account, so you can access OctoStack containers in OctoAI’s AWS ECR.\n\nPull the OctoStack containers from OctoAI’s AWS ECR.\n\nConfigure and deploy using Kubernetes or Docker Compose, using OctoAI-provided guides.",
    "hierarchy": {
      "h0": {
        "title": "OctoStack"
      },
      "h1": {
        "id": "configuration--deployment",
        "title": "Configuration & Deployment"
      },
      "h3": {
        "id": "deployment",
        "title": "Deployment"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-get-started-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/octostack",
    "pathname": "/docs/private-deployment/octostack",
    "title": "Get Started",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#get-started",
    "content": "Reach out to the team to see a live demo and get started on your OctoStack deployment.",
    "hierarchy": {
      "h0": {
        "title": "OctoStack"
      },
      "h1": {
        "id": "get-started",
        "title": "Get Started"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.secure-link-guide",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/secure-link",
    "pathname": "/docs/private-deployment/secure-link",
    "title": "SecureLink guide",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Private networking with OctoAI's SecureLink."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.secure-link-guide-overview-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/secure-link",
    "pathname": "/docs/private-deployment/secure-link",
    "title": "Overview",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#overview",
    "content": "Keeping our users’ data private and secure is our priority. OctoAI requires token authentication for all API requests, along with TLS to enforce encryption in transit for all connections between the customer and OctoAI. We also use encryption at rest for any data written to disk.\nSecureLink is an additional private connectivity security measure, ensuring that network traffic between an OctoAI endpoint and the customer environment is not exposed to the public internet. SecureLink is available for Enterprise customers.",
    "hierarchy": {
      "h0": {
        "title": "SecureLink guide"
      },
      "h1": {
        "id": "overview",
        "title": "Overview"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.secure-link-guide-setup-steps-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/secure-link",
    "pathname": "/docs/private-deployment/secure-link",
    "title": "Setup Steps",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#setup-steps",
    "content": "Configure your AWS Account ID in OctoAI\n\nCreate and configure a VPC Interface Endpoint\n\nConfigure OctoAI’s SDKs & CLI to use the SecureLink subdomain\n\nIf you intend to use Asset Library to upload your assets, configure a separate PrivateLink connection for Amazon S3 to ensure the uploads are also completed via a private connection to S3",
    "hierarchy": {
      "h0": {
        "title": "SecureLink guide"
      },
      "h1": {
        "id": "setup-steps",
        "title": "Setup Steps"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.secure-link-guide-setup-instructions-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/private-deployment/secure-link",
    "pathname": "/docs/private-deployment/secure-link",
    "title": "Setup Instructions",
    "breadcrumb": [
      {
        "title": "Private Deployment",
        "pathname": "/docs/documentation/private-deployment"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#setup-instructions",
    "content": "Configure your AWS Account ID in OctoAI\nFirst, run a simple curl command to configure your AWS account ID within OctoAI using the https://api.octoai.cloud/v1/account/securelink endpoint:\nThis allows OctoAI to generate a VPC Service Name, which you’ll use in the next step. If you don’t receive a successful response, contact us to ensure your OctoAI account is enabled as Enterprise tier.\nCreate and configure a VPC Interface Endpoint\nNow you’ll create the VPC Interface Endpoint in your AWS account using the VPC Service Name. Navigate to the VPC Dashboard, and click Create Endpoint from your AWS console: https://console.aws.amazon.com/vpc/home?#Endpoints\n\nConfigure the Service Name value to com.amazonaws.vpce.us-east-1.vpce-svc-0e914445c09bbe700, then click Verify to ensure the service name is found and verified. Contact us for help if the service name is not found.\n\nNext, choose the VPC and subnets that should be peered with the VPC service endpoint. Make sure that Enable DNS name is checked.\n\nThen, choose the security group(s) who can send traffic to the VPC endpoint. The security group must accept inbound traffic on TCP port 443 - you can verify this within the Inbound Rules page. You can now click Create endpoint to create the VPC endpoint. The endpoint maybe take up to 10 minutes to move from Pending to Available. Once it shows Available, it’s ready for use.\nConfigure OctoAI’s SDKs & CLI to use SecureLink URL\nEach OctoAI endpoint uses a SecureLink ingress URL, which will only work with a fully configured VPC Endpoint.\nOctoAI CLI\nConfigure an environment variable by running:\nTypeScript SDK\nConfigure the SecureLink URLs by passing in the SecureLink environment during client instantiation.\nPython SDK\nFor text generation, fine-tuning, or asset library, configure the environment parameter to use OctoAIEnvironment.SECURE_LINK in the client instantiation:\n\n\nThis table summarizes the SecureLink equivalent to each public API URL:\nService Public SecureLink \nText generation https://text.octoai.run https://text.securelink.octo.ai \nImage generation https://image.octoai.run https://image.securelink.octo.ai \nAsset Library & Fine-tuning https://api.octoai.cloud https://api.securelink.octo.ai \nOctoAI API https://api.octoai.cloud https://api.securelink.octo.ai \nAsync Inference https://async.octoai.run https://async.securelink.octoai.run \n\nConfigure private connection for Amazon S3 to upload assets through a private connection\nIf you are a user of Asset Library, you’ll need to configure a private connection for Amazon S3 to ensure uploads are also secured behind a private connection. Depending on your setup and your needs, you can either create a gateway endpoint, where a route table entry is added to your VPC, or create an interface endpoint, which is similar to configuring an interface endpoint for OctoAI. This guide covers setting up a gateway endpoint for S3.\nTo create a gateway endpoint, choose AWS services under Service category, and select com.amazonaws.us-east-1.s3. Ensure the type is Gateway.\n\nChoose the route table where the routing entry is added, then click Create Endpoint. For more information, see the S3 gateway endpoint documentation on AWS.",
    "code_snippets": [
      {
        "code": "curl -X POST \"https://api.octoai.cloud/v1/account/securelink\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer $OCTOAI_TOKEN\" \\\n    --data '{\"aws_account_id\": \"account-ID-value\"}'"
      },
      {
        "code": "export OCTO_API_ENDPOINT=https://api.securelink.octo.ai"
      },
      {
        "lang": "typescript",
        "code": "import { OctoAIClient, OctoAIEnvironment } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n  environment: OctoAIEnvironment.SecureLink,\n});"
      },
      {
        "lang": "python",
        "code": "import os\n\nfrom octoai.client import OctoAI\nfrom octoai.environment import OctoAIEnvironment\ntoken=os.environ.get(\"OCTOAI_TOKEN\")\n\nclient = OctoAI(api_key=token, environment=OctoAIEnvironment.SECURE_LINK)"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "SecureLink guide"
      },
      "h1": {
        "id": "setup-instructions",
        "title": "Setup Instructions"
      }
    },
    "level": "h1"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.getting-started-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/compute-getting-started",
    "pathname": "/docs/compute-service/compute-getting-started",
    "title": "Getting started with our Compute Service",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.getting-started-create-your-own-endpoints-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/compute-getting-started",
    "pathname": "/docs/compute-service/compute-getting-started",
    "title": "Create your own endpoints",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#create-your-own-endpoints",
    "content": "Create your own OctoAI endpoints in two different ways:\nDeploy an existing container from a public or private container registry. OctoAI can run any containers with an HTTP server written in any language, as long as your container is built on a GPU and comes with a declarative configuration of which port is exposed for inferences.\n\nCreate a container and endpoint using our CLI.",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Compute Service"
      },
      "h2": {
        "id": "create-your-own-endpoints",
        "title": "Create your own endpoints"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.getting-started-calling-octoai-endpoints-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/compute-getting-started",
    "pathname": "/docs/compute-service/compute-getting-started",
    "title": "Calling OctoAI endpoints",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#calling-octoai-endpoints",
    "content": "Use our REST API to integrate OctoAI endpoints into your application.",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Compute Service"
      },
      "h2": {
        "id": "calling-octoai-endpoints",
        "title": "Calling OctoAI endpoints"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-an-endpoint-from-an-existing-container.overview-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-endpoint-from-existing-container/create-custom-endpoints-from-a-container",
    "pathname": "/docs/compute-service/create-endpoint-from-existing-container/create-custom-endpoints-from-a-container",
    "title": "Create an endpoint from an existing container",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      },
      {
        "title": "Create an endpoint from an existing container",
        "pathname": "/docs/documentation/compute-service/create-an-endpoint-from-an-existing-container"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "curl -X POST '<your-endpoint-url>/predict' --data '{\"prompt\": \"What state is Los Angeles in?\", \"max_length\": 100}' -H 'content-type: application/json' -H \"Authorization: Bearer $YOUR_TOKEN\""
      }
    ],
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.\n\nOctoAI allows you to create endpoints from custom containers. We support all containers with an HTTP server written in any language, as long as your container comes with a declarative configuration of which port is exposed for inferences and is built for a GPU.\nOur coverage includes and is not limited to containers built using Fast API, cog, s2i, Nvidia Triton Inference Server, and Sagemaker/AzureML/Vertex AI based ML containers.\n\nWe allow you to pull containers from any private registry, except registries running in a fully private network (e.g. AWS ECR, Gitlab registries in private environments). This means you can pull containers from Docker Hub, Quay, Gitlab, GitHub, etc. that are not in private networks.\n\n\nIf you don't already have a container in hand, see our guide to building a custom endpoint with our CLI.\nIn this example, we will create a production-grade endpoint for a Flan T5 container pre-built here for a Question Answering application.\nIn the web UI, navigate to the Custom Endpoints page. Click on “New Endpoint,” then specify the following fields:\n\nEndpoint Name: This name for your endpoint will be part of the endpoint URL you end up integrating into your application-- that URL will be https://<endpoint-name>-<account-id>.octoai.run/<inference-route>\n\nContainer Image: The reference to your container needs to be in \"[registry/]organization/repository[:tag]\" format. In our example, the container image is vanessahlyan/flan-t5-small-pytorch-sanic:latest, hosted here on Docker Hub.\n\nContainer Port: The port of the container at which inferences are run is 8000 in our Flan T5 example, as defined here. Make sure to change the default value from 8080 to 8000 in the case of this example!\n\nRegistry Credential: Registry Credential defaults to Public, which indicates that your container is available for anyone on the internet to pull. If your container is instead stored in a private registry, follow the guide in Pulling containers from a private registry to store your registry credentials so that we can pull the private container.\n\nHealth Check Path: See Health Check Path in custom containers\n\nEnable Public Access: By default, your endpoints are private and require an access token, but you can change this setting such that anyone who knows your endpoint URL can use it.\n\nActive: Whether to spin down all replicas and disable inferences to it.\n\nSpecify Secrets: Provide secrets to mount onto the container, such as database secrets that you want to reuse across endpoints. Follow the guide in Setting up account-wide secrets for your custom endpoints to set these up.\n\nEnvironment Variables: Just like secrets, these variables will be mounted onto the container at runtime. If you have a variable with key foo whose value is bar, you will get a variable foo whose value is bar mounted to the container.\n\nSelect hardware: We offer three tiers of hardware-- see Hardware, Pricing, and Billing for more information. Your choice of hardware determines the pricing that you will pay for your endpoint.\n\nMinimum Replicas: The default minimum number of replicas is 0, which means we autoscale down to 0 whenever your endpoint is not receiving requests from your users and the timeout period has passed (this is a way to keep your costs down). Minimum replicas should be set to a higher number if you want to ensure highest uptime for your users and avoid cold starts. Cold start means there is currently no active container instance running inferences on our servers for your application, so we need to incur extra latency to spin up a new one.\n\nMaximum Replicas: This is an important measure for capping your maximum cost. The maximum number of replicas should be set based on how much simultaneous capacity you're willing to support at your heaviest traffic periods. For example, if you set your maximum replicas at 5, and each inference takes 1 second for your model on a GPU, then your application will be able to support a total of 250 inferences per minute without queueing. If your traffic goes above that level (well done you!), requests will be queued until they can be handled, which will increase your average request response time.\n\nTimeout: How many seconds to wait before we scale down your last replica since the time you have no more inferences running.\n\n\nNow click Create and you’ll be directed to a page for your new endpoint.\nTo run an inference, use the endpoint URL shown in the UI, with the appropriate inference route appended to it. In our example, we serve inferences at the /predict route as defined in this file, so we should send a CURL to <your-endpont-url>/predict. In my specific case, that ishttps://flan-t5-small-01at11ru3fwy.octoai.run/predict.\nYou need to edit the curl to use your own API token. If you don't have an existing token, refer to how to create an OctoAI API Token\n\nThis model expects the inputs prompt and max_length. We can fill in values \"What state is Los Angeles in?\" and 100 for those fields respectively, and get a response back.\n\nTo query the healthcheck for our endpoint (this is optional), we can hit <your-endpoint-url>/healthcheck\n\n\nOnce you use the curl to make an inference, look at the Events tab on the UI and you’ll see live events coming in informing you of the status of your deployment. Once you see here that the container is running, you inferences should complete successfully."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-an-endpoint-from-an-existing-container.overview-next-steps-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-endpoint-from-existing-container/create-custom-endpoints-from-a-container",
    "pathname": "/docs/compute-service/create-endpoint-from-existing-container/create-custom-endpoints-from-a-container",
    "title": "Next steps",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      },
      {
        "title": "Create an endpoint from an existing container",
        "pathname": "/docs/documentation/compute-service/create-an-endpoint-from-an-existing-container"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#next-steps",
    "content": "Clone our example container implementation here and customize them for your own use cases!\n\nContact us if you have any questions.",
    "hierarchy": {
      "h0": {
        "title": "Create an endpoint from an existing container"
      },
      "h2": {
        "id": "next-steps",
        "title": "Next steps"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-an-endpoint-from-an-existing-container.pulling-containers-from-a-private-registry-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-endpoint-from-existing-container/pulling-containers-from-a-private-registry",
    "pathname": "/docs/compute-service/create-endpoint-from-existing-container/pulling-containers-from-a-private-registry",
    "title": "Pulling containers from a private registry",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      },
      {
        "title": "Create an endpoint from an existing container",
        "pathname": "/docs/documentation/compute-service/create-an-endpoint-from-an-existing-container"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.\n\nWhen you create an endpoint from a custom container, you will see the following page, which has a field for a \"Registry Credential.\" By default, this field is set to \"Public,\" which requires that your container is available for anyone on the internet to pull and no registry credentials will be attached. If your container is instead stored in a private registry, click on the dropdown and then the + sign next to add a \"New registry credential\".\n\nWhen you click a model will open where you can set a name for the credential, enter the username and password for the registry.\n\nFor example, if you are storing your container in a private repository within DockerHub, you can set the credential name to \"docker\" and look for your DockerHub username in the top right area of the navigation bar in the DockerHub  UI once you've logged in. To get your DockerHub access token, go to your DockerHub Account Settings Security page, then click the New Access Token button.\n\n\nOnce you've added all the fields in the modal and clicked Save, the Registry Credential field will automatically change to the credential you just set up. Complete the remaining fields to create the endpoint."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-an-endpoint-from-an-existing-container.setting-up-account-wide-secrets-for-your-custom-endpoints-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-endpoint-from-existing-container/setting-account-wide-secrets-for-custom-endpoints",
    "pathname": "/docs/compute-service/create-endpoint-from-existing-container/setting-account-wide-secrets-for-custom-endpoints",
    "title": "Setting up account-wide secrets for your custom endpoints",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      },
      {
        "title": "Create an endpoint from an existing container",
        "pathname": "/docs/documentation/compute-service/create-an-endpoint-from-an-existing-container"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.\n\nWhen creating a new endpoint from your custom containers, you may wish to mount your account-wide secrets onto your container, such as database secrets. Simply click the Select secret(s) dropdown when creating a new endpoint, and then the + sign next to New Secret.\n\nThat would bring up a modal in which you can specify a key and a value for the secret. For example, if you create secret with key foo whose value is bar, you will get a variable foo whose value is bar mounted to the container.\n\nNote that this flow currently cannot access a database that is in a private VPC. Please contact us if you have a private database you wish to access."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Create a container and endpoint using the CLI",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-overview-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Overview",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#overview",
    "content": "The octoai command-line interface (CLI) makes it easy for you create a custom endpoint for OctoAI Compute Service. The octoai CLI guides you through the process of creating an initial valid Python application with an example model, building it, and deploying it.\nThe octoai CLI includes some endpoint scaffolds with example models that you can deploy and try out right away. After you complete that initial workflow, you can follow the instructions in this document to modify the initial application to use the model or code of your choice on OctoAI Compute Service.",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "overview",
        "title": "Overview"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-install-the-cli-and-sdk-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Install the CLI and SDK",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#install-the-cli-and-sdk",
    "content": "Follow CLI & SDK Installation to make sure you have the CLI and SDK installed. This guide assumes you are running the latest version of the octoai CLI.",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "prerequisites",
        "title": "Prerequisites"
      },
      "h3": {
        "id": "install-the-cli-and-sdk",
        "title": "Install the CLI and SDK"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-install-docker-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Install Docker",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#install-docker",
    "content": "Docker is required to build and push container images for custom endpoints.\nFor Mac, follow these instructions: Install Docker Desktop.\nFor Linux, follow these instructions: Install Docker Engine.\nMake sure that docker is running before you proceed! You can confirm by running docker info.\nDocker buildx is also required, and it is included with recent versions of Docker. You can confirm that you have it installed by looking for buildx in the output of docker info, or by running docker buildx.",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "prerequisites",
        "title": "Prerequisites"
      },
      "h3": {
        "id": "install-docker",
        "title": "Install Docker"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-export-your-octoai-token-on-the-terminal-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Export Your OctoAI Token on the Terminal",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#export-your-octoai-token-on-the-terminal",
    "content": "An OctoAI token authenticates you when interacting with the OctoAI Compute Service programatically.\nGo to OctoAI Compute Service and log in. Then:\nClick Settings (the gear icon on the left).\n\nProvide a name for your token under API Tokens.\n\nClick Generate.\n\nCopy the access token that appears.\n\n\nOn your terminal window, add the token as an environment variable:",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "export OCTOAI_TOKEN=<PasteYourOctoAITokenHere>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "prerequisites",
        "title": "Prerequisites"
      },
      "h3": {
        "id": "export-your-octoai-token-on-the-terminal",
        "title": "Export Your OctoAI Token on the Terminal"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-create-your-first-endpoint-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Create Your First Endpoint",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#create-your-first-endpoint",
    "content": "Use the octoai CLI to create a directory on your computer with the application code for your first endpoint. Run this command:\nThe CLI offers you to choose one of the existing scaffolds, which are endpoint examples ready to deploy that showcase how to use the service with different models. The following scaffolds are available:\nhello-world (a simple example)\n\nflan-t5 (text-in, text-out, adaptable to generative text use cases)\n\nwav2vec (audio-in, text-out, adaptable for speech to text use cases)\n\nyolov8 (image-in, image-out, adaptable for computer vision use cases)\n\n\nUse the Up/Down arrow keys to navigate and choose a scaffold, then press Enter.\nThe rest of this section shows the deployment process using the hello-world scaffold. After you complete this tutorial, be sure to check out the other scaffolds to see how use different kinds models in your endpoints.\nThe CLI then prompts you for more details:\nDirectory: the directory name to create. Type hello-world and press Enter.\n\nEndpoint name: the name of the endpoint. Type hello-world and press Enter.\n\nHardware: which hardware to run on. Use the Up/Down arrow keys to select gpu.t4.medium and press Enter\n\nSecrets: the set of secrets to make available to your container. Use the Up/Down arrow keys to select None/No more and press Enter.\n\nEnvironment variables: the set of environment variables and their values to make available to your container. Use the Up/Down arrow keys to select None/No more and press Enter.\n\n\nFor more information about secrets and environment variables, see Setting up secrets or environment variables for your custom endpoints.\nThe CLI shows the following guidance:",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai init"
      },
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "Initialized project in directory. Build your endpoint with:\n\n\tcd directory\n\toctoai build\n\nYou can configure your project by editing the octoai.yaml file.\n\nFor the OctoAI CLI developer documentation, please visit https://docs.octoai.cloud/docs/cli/cli-configuration-reference"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "create-your-first-endpoint",
        "title": "Create Your First Endpoint"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-directory-structure-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Directory Structure",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#directory-structure",
    "content": "The hello-world directory contains the following files:\noctoai.yaml - Stores the configuration of your endpoint to be used at deployment time.\n\nrequirements.txt - Lists any additional Python requirement packages for your application.\n\nservice.py - Contains the logic of your endpoint.\n\ntest_request.py - Contains example code to send requests to your endpoint.",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "create-your-first-endpoint",
        "title": "Create Your First Endpoint"
      },
      "h3": {
        "id": "directory-structure",
        "title": "Directory Structure"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-service-code-structure-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Service Code Structure",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#service-code-structure",
    "content": "The code in service.py looks like this:\nThe octoai.service.Service class is an abstract class that any endpoint has to implement.\nThe octoai.types package contains type definitions that help endpoints and clients work with data formats such as images and audio while communicating over HTTP.\nThe Service.setup() method is run at endpoint initialization. This method typically contains setup code that should not be run for every inference, such as downloading model weights from the network and making those available in a member variable in memory to be used by the Service.infer() method.\nThe Service.infer() method is run for every inference request. This method defines the interface of the endpoint (inputs, outputs, and their types) and contains code to perform inference with the model of your choice and return a response. Note that types and type annotations are required, as the OpenAPI specification for the endpoint is automatically generated from the parameters (names and types) and return type in the infer() method definition. The OpenAPI specification is available at /docs once the endpoint is deployed.\nThe APIs that are available to you (as well as other examples) are covered later in this document. For more information, see the API reference documentation.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "\"\"\"Example OctoAI service scaffold: Hello World.\"\"\"\nfrom octoai.service import Service\n\nclass HelloService(Service):\n    \"\"\"An OctoAI service extends octoai.service.Service.\"\"\"\n\n    def setup(self):\n        \"\"\"Perform intialization.\"\"\"\n        print(\"Setting up.\")\n\n    def infer(self, prompt: str) -> str:\n        \"\"\"Perform inference.\"\"\"\n        return prompt"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "create-your-first-endpoint",
        "title": "Create Your First Endpoint"
      },
      "h3": {
        "id": "service-code-structure",
        "title": "Service Code Structure"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-build-your-endpoint-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Build your Endpoint",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#build-your-endpoint",
    "content": "Inside the hello-world directory, run the following command:\nThis command builds a Docker container for your endpoint. The first time you run this command, it can take a long time (~15 min), since this process has to download large amounts of data. Subsequent builds (of this endpoint or any other endpoint you create) on the same machine are much faster.\nWhen the build completes, the octoai CLI shows the name of the image tag:\nThe octoai CLI keeps track of this value for you, so you do not need to remember it.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai build"
      },
      {
        "lang": "JSON",
        "meta": "JSON",
        "code": "{\"tag\":\"docker.io/YourDockerUserName/hello-world:SomeTag\"}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "build-your-endpoint",
        "title": "Build your Endpoint"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-run-your-endpoint-locally-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Run Your Endpoint Locally",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#run-your-endpoint-locally",
    "content": "Before deploying your endpoint, you can run the container locally and send it a request to verify that it is working properly.\n\n\nTarget Platforms\nThe images are built for target linux/amd64 so they can be deployed to OctoAI Compute Service. If your computer is of a different architecture (for example, an M1/M2 Macbook), running the container locally can be slow (up to 15 min for yolov8).\nThe scaffolds included on the octoai CLI all work locally without a GPU. However, other models that require GPU acceleration may not run locally at all if a GPU is not available in your system.\nTo test locally, run this command:\nThe command octoai run --command \"python3 test_request.py\" does two things: it runs your endpoint container locally in the background, and it runs the python script test_request.py to send a request to the endpoint container. After a few seconds, the output should be similar to this:",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai run --command \"python3 test_request.py\""
      },
      {
        "lang": "JSON",
        "meta": "JSON",
        "code": "{'output': 'Hello World!', ...}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "run-your-endpoint-locally",
        "title": "Run Your Endpoint Locally"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-two-terminal-workflow-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Two-Terminal Workflow",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#two-terminal-workflow",
    "content": "During development, it is often useful to have one terminal running the endpoint and showing its logs while sending a request from a separate terminal. This helps you iterate on your code and clearly see client and server errors separately, so you can make the appropriate fixes. Follow these steps:\nFirst, run octoai run -l. This runs the endpoint in the foreground and displays the server logs as they occur. If there are any issues with your endpoint implementation code, you will see errors here either at initialization or when processing client requests.\nSecond, open a new terminal and run python3 test_request.py. This runs the Python client code that sends a request to your endpoint. You typically will see the same response as before, but if there are errors returned to the client, you will see them here.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "$ octoai run -l\nStarting container: octoai-local-6585\nInitializing container: octoai-local-6585\nRunning container in foreground\n2023-10-10 18:24:46,529 INFO server.py:638 run\n2023-10-10 18:24:46,532 INFO server.py:520 Using service in service.HelloService.\n2023-10-10 18:24:46,534 INFO server.py:520 Using service in service.HelloService.\nINFO:     Started server process [1]\nINFO:     Waiting for application startup.\n2023-10-10 18:24:46,545 INFO server.py:347 status: State.UNINITIALIZED -> State.LAUNCH_PREDICT_LOOP\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)\n2023-10-10 18:24:47,027 INFO server.py:347 status: State.LAUNCH_PREDICT_LOOP -> State.SETUP_SERVICE\n2023-10-10 18:24:47,028 INFO server.py:347 status: State.SETUP_SERVICE -> State.RUNNING"
      },
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "$ python3 test_request.py\n{'output': 'Hello world!', 'analytics': {'inference_time_ms': 5.74e-06, 'performance_time_ms': 0.694469}}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "run-your-endpoint-locally",
        "title": "Run Your Endpoint Locally"
      },
      "h3": {
        "id": "two-terminal-workflow",
        "title": "Two-Terminal Workflow"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-deploy-your-endpoint-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Deploy Your Endpoint",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#deploy-your-endpoint",
    "content": "To deploy your endpoint, run this command:\nThis command reads the endpoint configuration from octoai.yaml and uses those settings to create the endpoint. After reading the endpoint configuration, this command pushes your container to Docker Hub. If this is the first endpoint you created, this may take a while depending on the speed of your internet connection. When the push is complete, the CLI shows you the URL of your new endpoint:\nThe endpoint URL starts with your endpoint name and has additional characters added (shown as <hash> in the example above).\nYou can now verify your endpoint has been created with:",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai deploy"
      },
      {
        "lang": "JSON",
        "meta": "JSON",
        "code": "{\"name\":\"hello-world\", \"endpoint\": \"https://hello-world-<hash>.octoai.run\"}"
      },
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai endpoint list"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "deploy-your-endpoint",
        "title": "Deploy Your Endpoint"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-modify-your-endpoint-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Modify Your Endpoint",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#modify-your-endpoint",
    "content": "The octoai CLI stores your deployment configuration in the octai.yaml configuration file. Initially this file is populated using the answers to the prompts that you provided when running octoai init:\nThe configuration file supports additional directives that enable you to customize different aspects of your deployment. For example, you can specify your maximum number of replicas:\nThen you can redeploy your endpoint:\nYou can verify that the endpoint has been updated:\nNotice that the REPLICAS field now shows [0-5] instead of [0-3] as was the previous default.\nFor a list of all supported configuration directives, see the Configuration Reference section.",
    "code_snippets": [
      {
        "lang": "YAML",
        "meta": "YAML",
        "code": "endpoint_config:\n  name: hello-world\n  hardware: gpu.t4.medium\n  registry:\n    host: docker.io\n    path: <YourDockerHubUserName>/hello-world"
      },
      {
        "lang": "YAML",
        "meta": "YAML",
        "code": "endpoint_config:\n  name: hello-world\n  hardware: gpu.t4.medium\n  max-replicas: 5\n  registry:\n    host: docker.io\n    path: <YourDockerHubUserName>/hello-world"
      },
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai deploy"
      },
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai endpoint list"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "modify-your-endpoint",
        "title": "Modify Your Endpoint"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-a-request-to-your-endpoint-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Send a Request to Your Endpoint",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#send-a-request-to-your-endpoint",
    "content": "Ensure you installed the OctoAI Python SDK. Then, to send a request to your endpoint:\nWhen you first send this request, your endpoint is performing a cold start, which means that no replicas were running and the first one has to start and load your container. This may take one to two minutes for the example scaffolds. After this time, you should see a response similar to this:\nOnce you have an active replica, you can run the test command repeatedly and the endpoint responds quickly. OctoAI Compute Service lets you control the minimum and maximum number of replicas for your endpoint either from the web user interface or from the octoai CLI:\nTo see your endpoint in the web user interface, go to https://octoai.cloud/endpoints. Then click hello-world. To edit the endpoint, click Edit. On the next screen you can set the minimum and maximum number of replicas.\n\nTo use the CLI to update the minimum and maximum number of replicas, you can edit the octoai.yaml configuration file and redeploy your endpoint with octoai deploy.\n\n\nCongratulations! You have now created your first endpoint on OctoAI Compute Service and ran remote inference.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "export OCTOAI_TOKEN=(paste your OctoAI API token from octoai.cloud/settings)\npython3 test_request.py --endpoint https://hello-world-<hash>.octoai.run"
      },
      {
        "lang": "JSON",
        "meta": "JSON",
        "code": "{'output': 'Hello World!', ...}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "send-a-request-to-your-endpoint",
        "title": "Send a Request to Your Endpoint"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-client-code-structure-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Client Code Structure",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#client-code-structure",
    "content": "The client code in test_request.py looks like this:\nThe octoai.client.Client class enables you to query your endpoint as shown here.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.client import OctoAI\n\ninputs = {\"prompt\": \"Hello world!\"}\n\ndef main(endpoint):\n    \"\"\"Run inference against the endpoint.\"\"\"\n    # create an OctoAI client\n    client = OctoAI()\n\n    # perform inference\n    response = client.infer(endpoint_url=f\"{endpoint}/infer\", inputs=inputs)\n\n    # show the response\n    print(response)"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "send-a-request-to-your-endpoint",
        "title": "Send a Request to Your Endpoint"
      },
      "h3": {
        "id": "client-code-structure",
        "title": "Client Code Structure"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-view-endpoint-logs-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "View Endpoint Logs",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#view-endpoint-logs",
    "content": "Now that your endpoint has served one inference, there are some logs available for you to view. OctoAI Compute Service lets you view the server logs for your endpoint using the web user interface or the octoai CLI:\nTo view the server logs using the web interface, go to https://octoai.cloud/endpoints. Then click hello-world. Then click the Logs button on the top right of the page.\n\nTo view the server logs using the CLI, use the octoai logs --name hello-world command.\n\n\nFor example, using the CLI the logs look similar to this:",
    "code_snippets": [
      {
        "lang": "Text",
        "meta": "Text",
        "code": "$ octoai logs --name hello-world\n19s   hello-world-<hash>   octoai server\n18s   hello-world-<hash>   Using service in service.HelloService.\n18s   hello-world-<hash>   run\n18s   hello-world-<hash>   Setting up.\n15s   hello-world-<hash>   Started server process [1]\n15s   hello-world-<hash>   Waiting for application startup.\n15s   hello-world-<hash>   Application startup complete.\n15s   hello-world-<hash>   Uvicorn running on http://0.0.0.0:8080\n13s   hello-world-<hash>   1.2.3.4:1234 - \"POST /infer HTTP/1.1\" 200 OK"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "view-endpoint-logs",
        "title": "View Endpoint Logs"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-customize-your-endpoint-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Customize Your Endpoint",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#customize-your-endpoint",
    "content": "Once you have built and deployed a hello-world endpoint to OctoAI Compute Service, it is time to customize the endpoint for your use case. To customize your endpoint, you will need:\nA list Python dependencies for your model of interest\n\nExample Python code to load your model of interest\n\nExample Python code to perform inference with your model of interest\n\n\nThen you can modify the Hello World endpoint implementation to call your model of interest instead. Depending on the modality of your model of interest, it may be easier to start with a different scaffold than hello-world (such as flan-t5, wav2vec, or yolov8).",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-use-a-huggingface-model-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Use a HuggingFace Model",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#use-a-huggingface-model",
    "content": "This section shows you how to modify the hello-world endpoint implementation to call the Flan-T5 model from HuggingFace instead. You can follow the process outlined here to use any other HuggingFace model of your interest.",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-1-add-python-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Step 1: Add Python Requirements",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#step-1-add-python-requirements",
    "content": "The hello-world endpoint implementation contains an empty requirements.txt file.\nEdit this file and add the corresponding requirements for Flan-T5:",
    "code_snippets": [
      {
        "lang": "Text",
        "meta": "Text",
        "code": "sentencepiece==0.1.99\ntorch==2.0.1\ntransformers==4.29.2"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model"
      },
      "h4": {
        "id": "step-1-add-python-requirements",
        "title": "Step 1: Add Python Requirements"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-2-add-custom-model-code-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Step 2: Add Custom Model Code",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#step-2-add-custom-model-code",
    "content": "The hello-world endpoint implementation has an empty Service.setup() method in service.py, since it does not use any model, and a trivial Service.infer() method.\nEdit service.py and add the required imports, the model loading code, and the model inferencing code:\nIn this case, our model of interest is of the same modality as the hello-world examples. For more information on how to use other modalities, see Send and Receive Images and Send and Receive Audio.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "# add these imports\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\nfrom octoai.service import Service\n\nclass T5Service(Service):\n    \"\"\"An OctoAI service extends octoai.service.Service.\"\"\"\n\n    # update this method as shown here\n    def setup(self):\n        \"\"\"Download model weights to disk.\"\"\"\n        self.tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n        self.model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\")\n\n    # update this method as shown here\n    def infer(self, prompt: str) -> str:\n        \"\"\"Perform inference with the model.\"\"\"\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        outputs = self.model.generate(**inputs)\n        response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n        return response[0]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model"
      },
      "h4": {
        "id": "step-2-add-custom-model-code",
        "title": "Step 2: Add Custom Model Code"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-3-modify-sample-client-code-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Step 3: Modify Sample Client Code",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#step-3-modify-sample-client-code",
    "content": "The hello-world endpoint implementation has a sample test_request.py that makes a request to your endpoint. In this case, our model of interest is of the same modality as the hello-world example, so you can just change the prompt.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import argparse\n\nfrom octoai.client import OctoAI\n\ninputs = {\"prompt\": \"What country is California in?\"}\n..."
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model"
      },
      "h4": {
        "id": "step-3-modify-sample-client-code",
        "title": "Step 3: Modify Sample Client Code"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-4-modify-the-deployment-configuration-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Step 4: Modify the Deployment Configuration",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#step-4-modify-the-deployment-configuration",
    "content": "The hello-world endpoint implementation has a minimal octoai.yaml file.\nUpdate it to change the endpoint name and image tag:",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "endpoint_config:\n  name: flan-t5\n  hardware: gpu.t4.medium\n  regcred-key: dockerhub\n  registry:\n    host: docker.io\n    path: <YourDockerHubUserName>/flan-t5"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model"
      },
      "h4": {
        "id": "step-4-modify-the-deployment-configuration",
        "title": "Step 4: Modify the Deployment Configuration"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-5-build-and-deploy-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Step 5: Build and Deploy",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#step-5-build-and-deploy",
    "content": "Once you have modified the endpoint implementation, rebuild and test your endpoint:\nThen, deploy your updated endpoint:\nCongratulations! You have now customized your first endpoint on OctoAI Compute Service.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai build\noctoai run --command \"python3 test_request.py\""
      },
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai deploy\noctoai endpoint list\npython3 test_request.py --endpoint https://flan-t5-<hash>.octoai.run"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model"
      },
      "h4": {
        "id": "step-5-build-and-deploy",
        "title": "Step 5: Build and Deploy"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-modify-the-dockerfile-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Modify the Dockerfile",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#modify-the-dockerfile",
    "content": "The octoai CLI generates and uses a pre-defined Dockerfile to build a container for your endpoint. The CLI does not expose this Dockerfile by default, since most endpoints can be created successfully without having to view or change this file. However, the octoai CLI provides options for you to view and modify the Dockerfile if your use case requires it. For example, your model may require that you install additional native libraries in the container.\nKeep in mind that you do not need to view or modify the Dockerfile to specify Python dependencies, since you can provide those in the requirements.txt file in your project directory.\nTo view and modify the Dockerfile:\nInside your project directory, run octoai build -g. Instead of building the container, this command generates a Dockerfile inside your project directory.\n\nInspect and modify the resulting Dockerfile as needed.\n\nBuild your container with octoai build. The build command uses the Dockerfile in your project directory to build your container (if one is available), instead of the pre-defined one. Note that using your own Dockerfile can increase cold start times for your endpoint. Contact us if you would like assistance on this subject.\n\n\n\n\nOlder versions and the -d flag\nVersions of octoai prior to 0.4.5 require that you build with the -d flag to use your custom Dockerfile. Versions 0.4.5 and above always use a custom Dockerfile if it is available in the project directory.\nAfter you have built the container for your endpoint using a customized Dockerfile, you can continue with deployment in the same manner as before by running octoai deploy.",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "modify-the-dockerfile",
        "title": "Modify the Dockerfile"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-include-model-weights-in-the-container-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Include Model Weights in the Container",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#include-model-weights-in-the-container",
    "content": "The Service.setup() method typically contains code that downloads model weights to disk. When you build an endpoint as described so far, this method is called right after the server starts, and the weights are downloaded to disk before the server can serve the first request.\nThe octoai CLI also enables you to run Service.setup() at container build time, such that the model weights downloaded to the local filesystem become part of the container image instead. In most cases, there is no clear benefit to embedding your weights with the container, since the container image will be larger and take longer to download, even if the server can serve requests soon after starting.\nTo include your model weights in your container image, add the --setup option to octoai build:",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai build --setup"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "include-model-weights-in-the-container",
        "title": "Include Model Weights in the Container"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-and-receive-images-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Send and Receive Images",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#send-and-receive-images",
    "content": "The octoai.types package contains helpful classes if you are customizing your endpoint to use a model that receives or generates images. To define an endpoint for a model that receives or generates images in service.py, use the Image type as a parameter or return type in the Service.infer() signature. For example, this would be for a model that takes images as Python Pillow objects as input and generates text\nFor a detailed example on how to send and receive images, see the yolov8 scaffold when running octoai init. For a list of useful methods available to send and receive images, see the API reference documentation.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.service import Service\n\nclass MyService(Service):\n\n    def infer(self, image: Image) -> str:\n        image_pil = image.to_pil()\n        output = self.model(image_pil)\n\n        return output[0]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "send-and-receive-images",
        "title": "Send and Receive Images"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-and-receive-audio-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Send and Receive Audio",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#send-and-receive-audio",
    "content": "The octoai.types package contains helpful classes if you are customizing your endpoint to use a model that receives or generates audio. To define an endpoint for a model that receives or generates audio in service.py, use the Audio type as a parameter or return type in the Service.infer() signature. For example, this would be for a model that takes audio as input and generates text:\nFor a detailed example on how to send and receive audio, see the wav2vec scaffold when running octoai init. For a list of useful methods available to send and receive audio, see the API Reference documentation.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.service import Service\nfrom octoai.types import Audio\n\nclass MyService(Service):\n\n    def infer(self, audio: Audio) -> str:\n        audio_array, sampling_rate = audio.to_numpy()\n        output = self.model(audio_array, sampling_rate)\n\n        return output[0]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "send-and-receive-audio",
        "title": "Send and Receive Audio"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-and-receive-video-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Send and Receive Video",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#send-and-receive-video",
    "content": "The octoai.types package contains helpful classes if you are customizing your endpoint to use a model that receives or generates video. To define an endpoint for a model that receives or generates video in service.py, use the Video type as a parameter or return type in the Service.infer() signature. For example, this would be for a model that takes video as input and generates text:\nFor a list of useful methods available to send and receive video, see the API Reference documentation. The Video type was added in SDK version 0.5.0.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.service import Service\nfrom octoai.types import Video\n\nclass MyService(Service):\n\n    def infer(self, video: Video) -> str:\n        video_frames = video.to_numpy()\n        output = self.model(video_frames)\n\n        return output[0]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "send-and-receive-video",
        "title": "Send and Receive Video"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-media-as-url-references-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Send Media as URL References",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#send-media-as-url-references",
    "content": "You can create instances of the pre-defined media types (Image, Audio, and Video) from local files and from remote HTTP URLs.\nThe first approach is the .from_file() function, such as Image.from_file(\"my_file.jpg\"). When you send an Image object created like this to your endpoint, the image data is encoded as base64. This approach is advantageous when your media assets are not available as remote URLs already or when you are working with smaller files, such that the base64 encoding and decoding penalty is small.\nThe second approach is the .from_url() function, such as Image.from_url(\"http://myserver.net/my_file.jpg\"). When you send an Image object created like this to your endpoint, the image data is not included in the request, just the URL reference is. Inside your endpoint implementation, when you call methods that need the image data, such as Image.to_pil(), the image is downloaded then and data is accessed. This approach is advantageous when your media assets are already available as remote URLs or when you are working with large files, such that the base64 encoding and decoding penalty is noticeable.\nFor more information, see the API Reference documentation. Support for URL media references was added in SDK version 0.5.0.",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "send-media-as-url-references",
        "title": "Send Media as URL References"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-binary-files-as-form-data-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Send Binary Files as Form Data",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#send-binary-files-as-form-data",
    "content": "In addition to sending media either as base-64 encoded data or as URL references as described in the previous sections, you can also define an additional API route for your model that accepts form data. You can use this feature to upload any kind of binary file directly to your model.\nTo support form data uploads, you implement the Service.infer_form_data() method in your endpoint. For example:\nYou can customize the API route where your implementation of infer_form_data will be exposed within your endpoint using the @path() annotation.\n\n\nArgument Types for Form Data\nThe Service.infer_form_data() signature can only contain arguments of types octoai.types.File or str. If you need to provide additional metadata with your files, such as a JSON object, you can serialize it as a string so you can receive it inside the service implementation.\nTo send a file with some metadata to the endpoint implementation in the previous example, you can use the httpx library as follows.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from typing import Any, Dict\n\nfrom octoai.service import Service, path\nfrom octoai.types import File\n\nclass FormDataService(Service):\n    def setup(self):\n        print(\"Setting up FormDataService\")\n\n    def infer(self, data: str) -> str:\n        # the endpoint will expose both methods\n        return \"Use /infer-form-data for form data inference\"\n\n    @path(\"/infer-form-data\")\n    def infer_form_data(self, file: File, metadata: str) -> Dict[str, Any]:\n        # this simple example shows the file information\n        # you can instead send the file contents to your model\n        return {\n            \"file_name\": file.filename,\n            \"file_content_type\": file.content_type,\n            \"file_size\": file.size,\n            \"metadata\": metadata,\n        }"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import httpx\n\nresponse = httpx.post(\n    \"https://<your-endpoint>.octoai.run/infer-form-data\",\n    files={\"file\": open(\"audio.wav\", \"rb\")},\n    data={\"metadata\": \"Some info about this audio file\"},\n)"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "send-binary-files-as-form-data",
        "title": "Send Binary Files as Form Data"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-support-multiple-routes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Support Multiple Routes",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#support-multiple-routes",
    "content": "The OctoAI SDK enables you to define additional routes in your endpoint. The following example shows how to define two additional endpoints in a Service implementation:\nYou can customize the path where new methods are exposed by specifying the @path annotation. If you do not provide this annotation, the method will be exposed using the method name with underscores replaced by dashes.\nThis feature was introduced in SDK version 0.7.2 (CLI version 0.5.8).",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.service import Service, path\n\nclass MultipleRoutesService(Service):\n\n  def setup(self):\n    print(\"Setting up MultipleRoutesService\")\n\n  # the infer() endpoint is always required\n  def infer(self, prompt: str) -> str:\n    return prompt\n\n  # this method is exposed as /new-route\n  @path(\"/new-route\")\n  def my_new_route(self, input_text: str):\n    return input_text\n\n  # this method is exposed as /my-new-route2\n  def my_new_route2(self, input_text: str):\n    return input_text"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint"
      },
      "h3": {
        "id": "support-multiple-routes",
        "title": "Support Multiple Routes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-reference-octoaiyaml-configuration-file-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Reference: octoai.yaml Configuration File",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#reference-octoaiyaml-configuration-file",
    "content": "The following snippet shows a fully populated configuration file with all supported directives you can use to configure your endpoint for deployment.",
    "code_snippets": [
      {
        "lang": "YAML",
        "meta": "YAML",
        "code": "endpoint_config:\n  name: yolov8                  # Unique endpoint name (required)\n  display-name: Yolov8          # User-visible endpoint name. (optional)\n  description: Object Detection # User-visible endpoint description. (optional)\n  hardware: gpu.t4.medium       # Use a T4 hardware type (required)\n  min-replicas: 1               # Keep a minimum of one replica (optional)\n  max-replicas: 3               # scale up up to 3 replicas (optional)\n  scaledown-in-seconds: 30      # scale down after 30 seconds of inactivity. (optional)\n  concurrency-per-replica: 2    # Concurrent requests sent to replica (optional)\n  public: false                 # Whether the endpoint is publicly visible (optional)\n  regcred-key: dockerhub        # Registry credentials for OctoAI to pull your image (optional)\n  secrets:                      # Secrets stored in OctoAI to surface as env. variables (optional)\n  - mykey\n  - mysecondkey\n  env_overrides:                # Environment variables to set in each replica (optional)\n    key1: value1\n    key2: value2\n  registry:\n    host: docker.io             # Registry hostname (required)\n    path: username/yolov8       # Registry path to image (required)\n    tag: v1                     # Tag (optional; not recommended to be set. Defaults to a generated UUID.)\n  service-module: app.service   # Path to python module to run (optional); defaults to \"service\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "reference-octoaiyaml-configuration-file",
        "title": "Reference: octoai.yaml Configuration File"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-appendix-openapi-specification-and-pydantic-types-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Appendix: OpenAPI Specification and Pydantic Types",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#appendix-openapi-specification-and-pydantic-types",
    "content": "Endpoints that you create using the octoai CLI expose the signature of your Service.infer() method as an OpenAPI specification that is exposed in the /docs HTTP path of your endpoint. Clients of your endpoint can reference this API specification to understand how to query your endpoint using the correct input names and types, as well as identify the type of the output that your endpoint returns.\nFor most models, you can use primitive Python types and the pre-defined types (Image, Audio, and Video) from the OctoAI Python SDK in your Service.infer() signature. However, some models use custom schemas in some of their inputs or outputs. For example, the YOLOv8 model included in one of the scaffolds returns a list of predictions that have the following schema:\nFor a model like this, you could define your return type as a list of dictionaries:\nHowever, the OpenAPI specification that the endpoint provides to your client would not give them any information about the schema of the predictions. To address this limitation, the OctoAI Python SDK enables you to define custom entity classes using Pydantic models to capture these schemas and include them in the OpenAPI specification of the endpoint. For example, the YOLOv8 scaffold that you can select when creating a new endpoint with octoai init defines the following entities to capture the structure of the prediction shown above:\nThen the infer() signature in the YOLOv8Service implementation returns a YOLOResponse:\nThe YOLOResponse entity (and any other Pydantic model you use as a return type) is converted automatically to JSON before the endpoint sends it to the client.\nFor Pydantic models you use as input arguments in infer(), the JSON data that the client provides must conform to the parameters of the entity, and it is converted to a Pydantic object automatically.",
    "code_snippets": [
      {
        "lang": "JSON",
        "meta": "JSON",
        "code": "[{\n    'name': 'bus',\n    'class': 5,\n    'confidence': 0.95,\n    'box': {\n        'x1': 2.91,\n        'x2': 809.51,\n        'y1': 230.68,\n        'y2': 881.00\n    }\n}, ...]"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "def infer(self, image: Image) -> Dict[str, Any]:"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "class Box(BaseModel):\n    \"\"\"Represents corners of a detection box.\"\"\"\n\n    x1: float\n    x2: float\n    y1: float\n    y2: float\n\nclass Detection(BaseModel):\n    \"\"\"Represents a detection.\"\"\"\n\n    name: str\n    class_: int = Field(..., alias=\"class\")\n    confidence: float\n    box: Box\n\nclass YOLOResponse(BaseModel):\n    \"\"\"Response includes list of detections and rendered image.\"\"\"\n\n    detections: List[Detection]\n    image_with_boxes: Image"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "def infer(self, image: Image) -> YOLOResponse:\n    ...\n    # Return detection data and a rendered image with boxes\n    return YOLOResponse(\n        detections=[Detection(**d) for d in detections],\n        image_with_boxes=Image.from_pil(img_out_pil),\n    )"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "appendix-openapi-specification-and-pydantic-types",
        "title": "Appendix: OpenAPI Specification and Pydantic Types"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-appendix-using-a-custom-docker-registry-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "title": "Appendix: Using a Custom Docker Registry",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#appendix-using-a-custom-docker-registry",
    "content": "Endpoints that you create with the octoai CLI use the OctoAI Compute Registry, which works seamlessly with your OctoAI authorization token. However, if you would prefer to use some other registry (such us DockerHub), you can configure your endpoint to do so.\nObtain your credentials for your custom registry. For example, for DockerHub, you can generate an access token from the Account Settings page.\n\nLog into Docker on your terminal. Use the docker login command with your registry username and access token as your password.\n\nProvide your registry credentials to the OctoAI Compute Service. To provide registry credentials using the web UI, see Pulling containers from a private registry. To provide registry credentials using the octoai CLI, use the octoai regcred create command. To verify your registry credentials were created correctly, use the octoai regcred list command.\n\n\nTo create a new endpoint that uses a custom Docker registry, issue the --registry option to the octoai init command with the registry URL. For example, for DockerHub:\nThe CLI will then prompt you for the image repository namespace (such as your DockerHub username) and the image repository name (which you could set to be the same as your endpoint name, or similar). Then the CLI will let you choose the right credentials to use to authenticate with the registry.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai init --registry docker.io"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI"
      },
      "h2": {
        "id": "appendix-using-a-custom-docker-registry",
        "title": "Appendix: Using a Custom Docker Registry"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "title": "Advanced: build a container from scratch in Python",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.\n\nYou can use our CLI to easily create containers for any model written in Python. However, note that OctoAI is able to run any container with an HTTP server, so you are always welcome to build containers in your own ways (with the understanding that using custom containers means potentially longer cold start/ fewer optimizations).\nIf you prefer to create your own container from scratch, this tutorial will walk you through one example of how to do so.\nIn this example, we will build a container for a Flan-T5 small model from the Hugging Face transformers library. This model is commonly used for text generation and question answering, but note that because it's small it does not yield outputs that are as high-quality as other OctoAI LLM endpoints.\n\nAn equivalent example for Hugging Face diffusers models can be found in the same GitHub repo."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-prerequisites-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "title": "Prerequisites",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#prerequisites",
    "content": "Sign up for a Docker Hub account\n\nDownload Docker desktop on your local machine\n\nAuthenticate the Docker CLI on your machine",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python"
      },
      "h2": {
        "id": "prerequisites",
        "title": "Prerequisites"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-example-code-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "title": "Example code",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#example-code",
    "content": "All the code in this tutorial can be found at this GitHub repo.",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python"
      },
      "h2": {
        "id": "example-code",
        "title": "Example code"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-prepare-python-code-for-running-an-inference-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "title": "Prepare Python code for running an inference",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#prepare-python-code-for-running-an-inference",
    "content": "First, we define how to run an inference on this model in model.py. The core steps include initializing the model and tokenizer using the transformers Python library, then running a predict() function that tokenizes the text input, runs the model, then de-tokenizes the model back into a text format.",
    "code_snippets": [
      {
        "lang": "py",
        "meta": "model.py",
        "code": "\"\"\"Model wrapper for serving flan-t5-small.\"\"\"\nimport argparse\nimport typing\n\nimport torch\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\n\n_MODEL_NAME = \"google/flan-t5-small\"\n\"\"\"The model's name on HuggingFace.\"\"\"\n\n_DEVICE: str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\"\"\"Device on which to serve the model.\"\"\"\n\nclass Model:\n    \"\"\"Wrapper for a flan-t5-small Text Generation model.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize the model.\"\"\"\n        self._tokenizer = T5Tokenizer.from_pretrained(_MODEL_NAME)\n        self._model = T5ForConditionalGeneration.from_pretrained(_MODEL_NAME).to(\n            _DEVICE\n        )\n\n    def predict(self, inputs: typing.Dict[str, str]) -> typing.Dict[str, str]:\n        \"\"\"Return a dict containing the completion of the given prompt.\n\n        :param inputs: dict of inputs containing a prompt and optionally the max length\n            of the completion to generate.\n        :return: a dict containing the generated completion.\n        \"\"\"\n        prompt = inputs.get(\"prompt\", None)\n        max_length = inputs.get(\"max_length\", 2048)\n\n        input_ids = self._tokenizer(prompt, return_tensors=\"pt\").input_ids.to(_DEVICE)\n        output = self._model.generate(input_ids, max_length=max_length)\n        result = self._tokenizer.decode(output[0], skip_special_tokens=True)\n\n        return {\"completion\": result}\n\n    @classmethod\n    def fetch(cls) -> None:\n        \"\"\"Pre-fetches the model for implicit caching by Transfomers.\"\"\"\n        # Running the constructor is enough to fetch this model.\n        cls()\n\ndef main():\n    \"\"\"Entry point for interacting with this model via CLI.\"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--fetch\", action=\"store_true\")\n    args = parser.parse_args()\n\n    if args.fetch:\n        Model.fetch()\n\nif __name__ == \"__main__\":\n    main()"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python"
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough"
      },
      "h4": {
        "id": "prepare-python-code-for-running-an-inference",
        "title": "Prepare Python code for running an inference"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-create-a-server-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "title": "Create a server",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#create-a-server",
    "content": "Next, we wrap this model in a Sanic server in server.py. Sanic is a Python 3.7+ web server and web framework that’s written to go fast. In our server file, we define the following:\nA default port on which to serve inferences. The port can be any positive number, as long as it's not in use by another application. 80 is commonly used for HTTP, and 443 is often for HTTPS. In this case we choose 8000.\n\nTwo server routes that OctoAI containers should have:\na route for inference requests (e.g. \"/predict\"). This route for inference requests must receive JSON inputs and JSON outputs.\n\na route for health checks (e.g. \"/healthcheck\"). See Healthcheck path in custom containers for a detailed explanation.\n\n\n\nNumber of workers (not required). Typical best practice is to make this number some function of the # of CPU cores that the server has access to and should use.\n\n\nIn our toy example, the line model_instance = model.Model() executes first, so by the time the server is instantiated our model is ready. That is why the code in our \"/healthcheck\" route is very straightforward in this example. In your own container, make sure your \"/healthcheck\" returns 200 only after your model is fully loaded and ready to take inferences.",
    "code_snippets": [
      {
        "lang": "py",
        "meta": "server.py",
        "code": "\"\"\"HTTP Inference serving interface using sanic.\"\"\"\nimport os\n\nimport model\nfrom sanic import Request, Sanic, response\n\n_DEFAULT_PORT = 8000\n\"\"\"Default port to serve inference on.\"\"\"\n\n# Load and initialize the model on startup globally, so it can be reused.\nmodel_instance = model.Model()\n\"\"\"Global instance of the model to serve.\"\"\"\n\nserver = Sanic(\"server\")\n\"\"\"Global instance of the web server.\"\"\"\n\n@server.route(\"/healthcheck\", methods=[\"GET\"])\ndef healthcheck(_: Request) -> response.JSONResponse:\n    \"\"\"Responds to healthcheck requests.\n\n    :param request: the incoming healthcheck request.\n    :return: json responding to the healthcheck.\n    \"\"\"\n    return response.json({\"healthy\": \"yes\"})\n\n@server.route(\"/predict\", methods=[\"POST\"])\ndef predict(request: Request) -> response.JSONResponse:\n    \"\"\"Responds to inference/prediction requests.\n\n    :param request: the incoming request containing inputs for the model.\n    :return: json containing the inference results.\n    \"\"\"\n    inputs = request.json\n    output = model_instance.predict(inputs)\n    return response.json(output)\n\ndef main():\n    \"\"\"Entry point for the server.\"\"\"\n    port = int(os.environ.get(\"SERVING_PORT\", _DEFAULT_PORT))\n    server.run(host=\"0.0.0.0\", port=port, workers=1)\n\nif __name__ == \"__main__\":\n    main()"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python"
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough"
      },
      "h4": {
        "id": "create-a-server",
        "title": "Create a server"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-package-the-server-in-a-dockerfile-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "title": "Package the server in a Dockerfile",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#package-the-server-in-a-dockerfile",
    "content": "Now we can package the server by defining a requirements.txt file and a Dockerfile:\nAlong with installing the dependencies, the Dockerfile also downloads the model into the image at build time. Because the model isn't too big, we can cache it in the Docker image for faster startup without impacting the image size too much. If your model is larger, you may want to pull it on container start instead of caching it in the Docker image. This may affect your container startup time, but keeps the image itself slim.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "requirements.txt",
        "code": "sanic==23.3.0\ntorch==2.0.0+cu118\n--extra-index-url https://download.pytorch.org/whl/cu118\nsentencepiece==0.1.97\ntransformers==4.27.4"
      },
      {
        "lang": "Dockerfile",
        "meta": "Dockerfile",
        "code": "FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n\nARG SERVING_PORT=8000\nENV SERVING_PORT=$SERVING_PORT\n\nWORKDIR /\n\nRUN apt update && \\\n    apt install -y python3-pip\n\n# Upgrade pip and install the copied in requirements.\nRUN pip install --no-cache-dir --upgrade pip\nADD requirements.txt requirements.txt\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy in the files necessary to fetch, run and serve the model.\nADD model.py .\nADD server.py .\n\n# Fetch the model and cache it locally.\nRUN python3 model.py --fetch\n\n# Expose the serving port.\nEXPOSE $SERVING_PORT\n\n# Run the server to handle inference requests.\nCMD python3 -u server.py"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python"
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough"
      },
      "h4": {
        "id": "package-the-server-in-a-dockerfile",
        "title": "Package the server in a Dockerfile"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-test-the-image-locally-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "title": "Test the image locally",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#test-the-image-locally",
    "content": "Run this Docker image locally on a GPU to test that it can run inferences as expected:\n..and in a separate terminal run the following command one or more times\n... until you see {\"healthy\":true} in the terminal output. Now, you can get an inference by running:",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "$ docker run --gpus=all -d --rm\n    -p 8000:8000 --env SERVER_PORT=8000\n    --name \"flan-t5-small-pytorch-sanic\"\n  \t\"$DOCKER_REGISTRY/flan-t5-small-pytorch-sanic\""
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "$ curl -X GET http://localhost:8000/healthcheck"
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "$ curl -X POST http://localhost:8000/predict \\\n    -H \"Content-Type: application/json\" \\\n    --data '{\"prompt\":\"What state is Los Angeles in?\",\"max_length\":100}'"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python"
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough"
      },
      "h4": {
        "id": "test-the-image-locally",
        "title": "Test the image locally"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-push-the-image-to-a-cloud-registry-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "title": "Push the image to a cloud registry",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#push-the-image-to-a-cloud-registry",
    "content": "Push your Docker image to Docker Hub with:\nNow that you have your container, create an endpoint to establish your endpoint on OctoAI.\n)",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "$ docker push \"$DOCKER_REGISTRY/flan-t5-small-pytorch-sanic\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python"
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough"
      },
      "h4": {
        "id": "push-the-image-to-a-cloud-registry",
        "title": "Push the image to a cloud registry"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.healthcheck-path-in-custom-containers-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/compute-service/health-check-path-in-custom-containers",
    "pathname": "/docs/compute-service/health-check-path-in-custom-containers",
    "title": "Healthcheck path in custom containers",
    "breadcrumb": [
      {
        "title": "Compute Service",
        "pathname": "/docs/documentation/compute-service"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.\n\nThe healthcheck path is be the server route in the container that indicates when the server is ready to receive requests:\nIt is strongly recommended that you configure a health check path in your container. Otherwise, you may have inference failures whenever you try to make a request before your server is ready.\n\nIf you define a healthcheck, your endpoint has 5 minutes from the time the the image is pulled to return a 200 OK response. This will mark the endpoint as available, and the same criteria applies for additional replicas. If there are 3 consecutive calls to the healthcheck endpoint that return a non-200 OK status, then the replica will be restarted.\n\nYou can see an example of a health check path in our Flan T5 container in Advanced: Build a Container from Scratch in Python. The health check path exposed by that container is /healthcheck. After the endpoint is created, one should be able to hit https://<endpoint-name>-<account-id>.octoai.run/healthcheck for a 200 response whenever the server is healthy and ready."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "title": "CLI installation",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "How to install the OctoAI CLI"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-latest-cli-release-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "title": "Latest CLI release",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#latest-cli-release",
    "content": "You can install the latest CLI and Python SDK using the script below, with compatibility for both Mac and Linux systems.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "curl https://s3.amazonaws.com/downloads.octoai.cloud/octoai/install_octoai_cli_and_sdk.sh -sSfL | sh"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI installation"
      },
      "h2": {
        "id": "latest-cli-release",
        "title": "Latest CLI release"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-verify-the-installation-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "title": "Verify the installation",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#verify-the-installation",
    "content": "You can also verify the octoai-sdk has successfully installed in your correct environment.\nThis should provide an output similar to (though likely with different version numbers):",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "octoai version"
      },
      {
        "lang": "bash",
        "code": "octoai --help"
      },
      {
        "lang": "bash",
        "code": "python3 -m pip show octoai-sdk"
      },
      {
        "code": "Name: octoai-sdk\nVersion: 0.7.1\nSummary: A runtime library for OctoAI.\nHome-page:\nAuthor: OctoAI\nAuthor-email:\nLicense:\nLocation: /Users/sliu/anaconda3/envs/sdk-dev/lib/python3.10/site-packages\nRequires: boto3, chevron, click, fastapi, httpx, numpy, pillow, pydantic, python-dateutil, python-multipart, pyyaml, soundfile, types-pyyaml, types-requests, uvicorn\nRequired-by:"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI installation"
      },
      "h2": {
        "id": "latest-cli-release",
        "title": "Latest CLI release"
      },
      "h3": {
        "id": "verify-the-installation",
        "title": "Verify the installation"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-uninstall-the-cli-and-python-sdk-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "title": "Uninstall the CLI and Python SDK",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#uninstall-the-cli-and-python-sdk",
    "content": "Our installation script installs the OctoAI binary to usr/local/bin. You can remove it with:\nYou can uninstall the Python SDK using:\nIf you used another method of installation such as brew, rpm, etc, please follow the directions from those services.\nIf you choose not to install the CLI and SDK using the installation script, you can alternatively use brew or apt. For most people, it is recommended you use the installation script.\nPlease note, if you install the CLI using brew or apt, you will also need to install the SDK directly from PIP. You can review the installation shell script from CLI & SDK Installation to verify a compatible SDK version number if you run into difficulties, and also review there how to verify the installation was successful for both the SDK and CLI.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "rm -rf /usr/local/bin/octoai"
      },
      {
        "lang": "python",
        "code": "python3 -m pip uninstall octoai-sdk"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI installation"
      },
      "h2": {
        "id": "uninstall-the-cli-and-python-sdk",
        "title": "Uninstall the CLI and Python SDK"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-latest-python-sdk-release-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "title": "Latest Python SDK Release",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#latest-python-sdk-release",
    "content": "Much of the CLI is dependent on the SDK. You can view our full release history on PyPI here.\nIf you choose not to use the installation script, you can use pip to install the octoai-sdk Python package directly. We strongly recommend that you install OctoAI in a virtualenv, to avoid conflicting with your system packages.\nIf you've already installed a previous version of the octoai-sdk, you can also upgrade using:",
    "code_snippets": [
      {
        "lang": "Python",
        "code": "python3 -m pip install octoai-sdk"
      },
      {
        "lang": "Python",
        "code": "python3 -m pip install --upgrade octoai-sdk"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI installation"
      },
      "h2": {
        "id": "uninstall-the-cli-and-python-sdk",
        "title": "Uninstall the CLI and Python SDK"
      },
      "h3": {
        "id": "latest-python-sdk-release",
        "title": "Latest Python SDK Release"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-alternate-cli-installation-options-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "title": "Alternate CLI Installation Options",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#alternate-cli-installation-options",
    "content": "After installing the Python SDK, you can then install the CLI.\nMac:\nDebian:\nPlease see the individual package manager for how to remove an installation, as their directions may change over time.",
    "code_snippets": [
      {
        "lang": "Shell",
        "code": "brew tap octoml/tap\nbrew install octoai"
      },
      {
        "lang": "Shell",
        "code": "sudo add-apt-repository -y 'deb [trusted=yes] https://repo.fury.io/octoml/ /'\nsudo apt update\nsudo apt install octoai"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI installation"
      },
      "h2": {
        "id": "uninstall-the-cli-and-python-sdk",
        "title": "Uninstall the CLI and Python SDK"
      },
      "h3": {
        "id": "alternate-cli-installation-options",
        "title": "Alternate CLI Installation Options"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "CLI reference",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "A reference guide for all CLI commands and their behaviors.",
    "content": "This document provides a detailed description of every command of the OctoAI CLI. Remember that you can install the CLI by following CLI & SDK Installation."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-account-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "account",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#account",
    "content": "Commands related to your OctoAI account.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "account",
        "title": "account"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-account-info-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "account info",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#account-info",
    "content": "List account details for identifying yourself, useful for debugging or reporting your account information.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai account info\n\nEMAIL               KEY            NAME           PROVIDER\n[[email protected]](/cdn-cgi/l/email-protection)      XXXXXXXXXXXX   First Last     google"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "account",
        "title": "account"
      },
      "h3": {
        "id": "account-info",
        "title": "account info"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-build-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "build",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#build",
    "content": "Build a Docker image from your OctoAI project.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "build",
        "title": "build"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "Flags",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#flags",
    "content": "-d, --dockerfile <string> Dockerfile overload to use for building; if not provided, one will be generated.\n\n-g, --generate Only generate a Dockerfile rather than building the image, which is the default behavior.\n\n-i, --image <IMAGE> Image URI to use for final tagging.\n\n-s, --service-module <MODULE> The Python module containing the octoai.Service implementation.\n\n--setup Run Service.setup() at image build time.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "build",
        "title": "build"
      },
      "h3": {
        "id": "flags",
        "title": "Flags"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-check-config-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "check-config",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#check-config",
    "content": "Print the configuration of the CLI, useful for checking your token and configuration settings, or for debugging the behavior of your builds.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "check-config",
        "title": "check-config"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-completion-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "completion",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#completion",
    "content": "Generate the autocompletion script for the specified shell. Supports one of: bash, fish, powershell, zsh. For more information on how to use it, our CLI is built with Cobra.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai completion <SHELL>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "completion",
        "title": "completion"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-deploy-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "deploy",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#deploy",
    "content": "Deploy your endpoint to OctoAI. By default, we use your settings from .octoai.yaml, but flags can be passed to on this command to override the file settings.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "deploy",
        "title": "deploy"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-1-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "Flags:",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#flags-1",
    "content": "--concurrency-per-replica <INT> Maximum concurrency per replica (default is -1).\n\n--description <DESC> Give this endpoint a human readable description.\n\n-d, --display-name Give this endpoint a human readable display name in the web UI.\n\n-e, --env [<KEY>=<VALUE>] Environment variables to pass to the endpoint; issue once per var: --env FOO=bar --env BAZ=qux.\n\n--hardware <HW_TYPE> The hardware instance to deploy to.\n\n-i, --image <IMAGE> A URI to the Image to deploy.\n\n--max-replicas <INT> Maximum number of replicas to scale up to (default -1)\n\n--min-replicas <INT> Minimum number of replicas to scale down to (default -1)\n\n-n, --name <STRING> Name of the endpoint\n\n--regcred <REGCRED_KEY> Registry credentials key for private image pulls.\n\n--scaledown-secs <INT> Number of seconds of inactivity before scaling down the endpoint (default -1).\n\n--secrets <SECRET_KEY> Secrets to load into the endpoint; issue once per secret: --secrets secretKey1 --secrets secretKey2.\n\n-t, --tag <TAG> Tag to use for the image.\n\n--visibility <public|private> Controls whether the endpoint is public or needs authentication.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "deploy",
        "title": "deploy"
      },
      "h3": {
        "id": "flags-1",
        "title": "Flags:"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "endpoint",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#endpoint",
    "content": "Commands for creating, editing, or updating your endpoints.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-create-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "endpoint create",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#endpoint-create",
    "content": "Create an endpoint with the provided flags.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint"
      },
      "h3": {
        "id": "endpoint-create",
        "title": "endpoint create"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-2-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "Flags:",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#flags-2",
    "content": "--concurrency-per-replica <INT> Maximum concurrency per replica (default is -1).\n\n--description <DESC> Give this endpoint a human readable description.\n\n--pause Pause this endpoint on creation.\n\n-d, --display-name Give this endpoint a human readable display name in the web UI.\n\n-e, --env [<KEY>=<VALUE>] Environment variables to pass to the endpoint; issue once per var: --env FOO=bar --env BAZ=qux.\n\n--hardware <HW_TYPE> The hardware instance to deploy to. Options include \"gpu.t4.medium\" \"gpu.a10g.medium\" \"gpu.a100-80.medium\"\n\n--health-check-path <STRING> Server path for health checks\n\n-i, --image <IMAGE> A URI to the Image to deploy.\n\n--max-replicas <INT> Maximum number of replicas to scale up to (default -1)\n\n--min-replicas <INT> Minimum number of replicas to scale down to (default -1)\n\n-n, --name <STRING> Name of the endpoint\n\n-p, --port <INT> Port to route requests to on the container (default 8080)\n\n--regcred <REGCRED_KEY> Registry credentials key for private image pulls.\n\n--scaledown-secs <INT> Number of seconds of inactivity before scaling down the endpoint (default -1).\n\n--secrets <SECRET_KEY> Secrets to load into the endpoint; issue once per secret: --secrets secretKey1 --secrets secretKey2.\n\n-t, --tag <TAG> Tag to use for the image.\n\n--visibility <public|private> Controls whether the endpoint is public or needs authentication.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint"
      },
      "h3": {
        "id": "flags-2",
        "title": "Flags:"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-get-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "endpoint get",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#endpoint-get",
    "content": "Get information on the endpoint",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai endpoint get --name <NAME> [-o <OUTPUT_FORMAT>]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint"
      },
      "h3": {
        "id": "endpoint-get",
        "title": "endpoint get"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-list-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "endpoint list",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#endpoint-list",
    "content": "List endpoints within this account",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai endpoint list  [-o <OUTPUT_FORMAT>]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint"
      },
      "h3": {
        "id": "endpoint-list",
        "title": "endpoint list"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-update-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "endpoint update",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#endpoint-update",
    "content": "Update an endpoint",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint"
      },
      "h3": {
        "id": "endpoint-update",
        "title": "endpoint update"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-start-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "endpoint start",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#endpoint-start",
    "content": "Start the endpoint, scale to minimum replicas and start routing traffic to it.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai endpoint start --name <ENDPOINT_NAME>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint"
      },
      "h3": {
        "id": "endpoint-start",
        "title": "endpoint start"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-pause-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "endpoint pause",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#endpoint-pause",
    "content": "Pause the endpoint, scale down the replicas to 0 and stop routing traffic to it.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai endpoint pause --name <ENDPOINT_NAME>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint"
      },
      "h3": {
        "id": "endpoint-pause",
        "title": "endpoint pause"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-events-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "events",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#events",
    "content": "Show the events associated with a given endpoint.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "events",
        "title": "events"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-3-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "Flags",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#flags-3",
    "content": "-f, --follow Tail event stream using our streaming events API. You will only events which occur after you start this command.\n\n-n, --name <string> Name of the endpoint to monitor.\n\n-o, --output-format OutputFormat Output format (default table).",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "events",
        "title": "events"
      },
      "h3": {
        "id": "flags-3",
        "title": "Flags"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-help-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "help",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#help",
    "content": "Help about any command, provides a short summary of this page in the CLI.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "help",
        "title": "help"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-init-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "init",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#init",
    "content": "Initialize a brand new project from a scaffold, it will prompt the user for new repository configuration. The command will walk you through configuring your endpoint. See below for an example of creating a new project.\nAn example of running octoai init with the YoloV8 scaffold",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai init"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "init",
        "title": "init"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-login-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "login",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#login",
    "content": "Cache auth credentials for the OCTOAI_TOKEN generated from How to create an OctoAI API token.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "login",
        "title": "login"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-logs-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "logs",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#logs",
    "content": "Show the logs for a given endpoint.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "logs",
        "title": "logs"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-4-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "Flags",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#flags-4",
    "content": "-f, --follow Tail endpoint logs using our streaming logs API. You will only events which occur after you start this command.\n\n-n, --name <NAME> Name of the endpoint to monitor.\n\n-o, --output-format OutputFormat Output format (default table).",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "logs",
        "title": "logs"
      },
      "h3": {
        "id": "flags-4",
        "title": "Flags"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "regcred",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#regcred",
    "content": "Manage registry credentials for deploying images from private registries.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "regcred",
        "title": "regcred"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-create-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "regcred create",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#regcred-create",
    "content": "Creates a registry credential in your account.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai regcred create --name <NAME> --token <TOKEN> --username <USERNAME>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "regcred",
        "title": "regcred"
      },
      "h3": {
        "id": "regcred-create",
        "title": "regcred create"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-get-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "regcred get",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#regcred-get",
    "content": "Get a registry credential from your account.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai regcred get --name <NAME> [-o <OUTPUT_FORMAT>]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "regcred",
        "title": "regcred"
      },
      "h3": {
        "id": "regcred-get",
        "title": "regcred get"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-list-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "regcred list",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#regcred-list",
    "content": "List your account's registry credentials.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai regcred list [-o <OUTPUT_FORMAT>]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "regcred",
        "title": "regcred"
      },
      "h3": {
        "id": "regcred-list",
        "title": "regcred list"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-update-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "regcred update",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#regcred-update",
    "content": "Update a registry credential in your account.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai regcred update --name <NAME> --token <TOKEN> --username <USERNAME>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "regcred",
        "title": "regcred"
      },
      "h3": {
        "id": "regcred-update",
        "title": "regcred update"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-delete-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "regcred delete",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#regcred-delete",
    "content": "Delete a registry credential from your account.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai regcred delete --name <KEY>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "regcred",
        "title": "regcred"
      },
      "h3": {
        "id": "regcred-delete",
        "title": "regcred delete"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-run-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "run",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#run",
    "content": "Run the most recently built container locally for testing before deployment.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "run",
        "title": "run"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-5-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "Flags",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#flags-5",
    "content": "-b, --background Whether to run container in the background.\n\n--command <CMD> Inference command to run. Surround in quotes to pass arguments to the command.\n\n-s, --env <stringArray> Environment variables to pass to the container; issue once per var: --env FOO=bar --env BAZ=qux\n\n--gpus <string> Identifier of GPU device, or 'all' to use all GPUs.\n\n-i, --image <string> Image to run.\n\n-p, --port <int> Host port to bind the container port to. (default 8080)\n\n--timeout int How long to wait for container to start and healthcheck to be ready before timing out. (default 300)\n\n--container-name <string> Assign a name to the container.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "run",
        "title": "run"
      },
      "h4": {
        "id": "flags-5",
        "title": "Flags"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "secret",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#secret",
    "content": "Manage your secrets for your endpoints. These allow you to pass extra API keys, or any secret configuration to your containers. See Setting up account-wide secrets for your custom endpoints for more information about this.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "secret",
        "title": "secret"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-create-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "secret create",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#secret-create",
    "content": "Create a secret in your account with a given key and a value.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai secret create --key <SECRET_KEY> --value <SECRET_VALUE>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "secret",
        "title": "secret"
      },
      "h3": {
        "id": "secret-create",
        "title": "secret create"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-delete-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "secret delete",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#secret-delete",
    "content": "Delete a secret from your account.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai secret delete --key <SECRET_KEY>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "secret",
        "title": "secret"
      },
      "h3": {
        "id": "secret-delete",
        "title": "secret delete"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-get-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "secret get",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#secret-get",
    "content": "Fetch a secret from your account.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai secret get -key <SECRET_KEY> [--show] [--output-format OutputFormat]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "secret",
        "title": "secret"
      },
      "h3": {
        "id": "secret-get",
        "title": "secret get"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-list-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "secret list",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#secret-list",
    "content": "List the secrets of your account.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai secret list [--output-format OutputFormat]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "secret",
        "title": "secret"
      },
      "h3": {
        "id": "secret-list",
        "title": "secret list"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-update-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "secret update",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#secret-update",
    "content": "Update a secret in your account.",
    "code_snippets": [
      {
        "lang": "Shell",
        "meta": "Shell",
        "code": "octoai secret update --key <SECRET_KEY> --value <SECRET_VALUE>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "secret",
        "title": "secret"
      },
      "h3": {
        "id": "secret-update",
        "title": "secret update"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-version-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "pathname": "/docs/cli/cli-reference-guide",
    "title": "version",
    "breadcrumb": [
      {
        "title": "CLI",
        "pathname": "/docs/documentation/cli"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#version",
    "content": "Print the CLI version information useful for checking your build information or reporting issues.",
    "hierarchy": {
      "h0": {
        "title": "CLI reference"
      },
      "h2": {
        "id": "version",
        "title": "version"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-installation",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/installation-and-setup",
    "pathname": "/docs/python-sdk/installation-and-setup",
    "title": "Python SDK installation & setup",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-installation-installation-guide-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/installation-and-setup",
    "pathname": "/docs/python-sdk/installation-and-setup",
    "title": "Installation Guide",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#installation-guide",
    "content": "The SDK currently supports Python versions 3.8.1 and upwards. It is strongly recommended that you use a virtual environment such as Conda or venv to manage Python packages for your development environment. This helps prevent incompatible dependencies with packages installed irrelevant to your current project or that conflict with system dependencies.\nYou can view our full release history on PyPi for the latest version of the Python SDK.\nFor Mac and Linux, use pip to install the octoai sdk Python package. We strongly recommend that you install OctoAI in a virtualenv, to avoid conflicting with your system packages.\nPlease refer to CLI & SDK Installation for how to install the SDK in conjunction with the CLI for the authoring tool.",
    "code_snippets": [
      {
        "lang": "shell",
        "meta": "shell",
        "code": "python3 -m pip install octoai"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Python SDK installation & setup"
      },
      "h3": {
        "id": "installation-guide",
        "title": "Installation Guide"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-installation-setting-api-token-as-an-environment-variable-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/installation-and-setup",
    "pathname": "/docs/python-sdk/installation-and-setup",
    "title": "Setting API token as an environment variable",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#setting-api-token-as-an-environment-variable",
    "content": "In order to access endpoints, create an OctoAI API token. Set OCTOAI_TOKEN to the token value wherever you set your environment variables, such as your .bashrc or .env file.\nThen when you instantiate the client, it will detect the OCTOAI_TOKEN as an envvar and set it for you.\nAlternatively, on creation of the OctoAI class, you can set your token, or the Client also accepts a path to where you've stored your API token from the config_path variable. Please see the OctoAI docs for more information.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "export OCTOAI_TOKEN=YOUR_TOKEN_HERE"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.client import OctoAI\n\nclient = OctoAI()"
      },
      {
        "lang": "Python",
        "meta": "Python",
        "code": "from octoai.client import OctoAI\n\nclient = OctoAI(api_key=\"YOUR_OCTOAI_API_TOKEN_HERE\")"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Python SDK installation & setup"
      },
      "h3": {
        "id": "setting-api-token-as-an-environment-variable",
        "title": "Setting API token as an environment variable"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-inference",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/python-sdk-inferences",
    "pathname": "/docs/python-sdk/python-sdk-inferences",
    "title": "Python SDK inference",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-inference-octoai-python-sdk-at-a-glance-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/python-sdk-inferences",
    "pathname": "/docs/python-sdk/python-sdk-inferences",
    "title": "OctoAI Python SDK at a glance",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#octoai-python-sdk-at-a-glance",
    "content": "The OctoAI Python SDK is intended to help you use OctoAI endpoints. At its simplest form, it allows you to run inferences against an endpoint by providing a dictionary with the necessary inputs.",
    "code_snippets": [
      {
        "lang": "Python",
        "meta": "Python",
        "code": "import time\nfrom octoai.client import OctoAI\n\nclient = OctoAI()\n\n# It allows you to run inferences\noutput = client.infer(endpoint_url=\"your-endpoint-url\", inputs={\"keyword\": \"dictionary\"})\n\n# It also allows for inference streams for LLMs\nfor token in client.infer_stream(\"your-endpoint-url\", inputs={\"keyword\": \"dictionary\"}):\n    if token.get(\"object\") == \"chat.completion.chunk\":\n        # Do stuff with the token\n        pass\n\n# And for server-side asynchronous inferences\nfuture = client.infer_async(\"your-endpoint-url\", {\"keyword\": \"dictionary\"})\n# Typically, you'd collect additional futures then poll for status, but for the sake of example...\nwhile not client.is_future_ready(future):\n    time.sleep(1)\n# Once the results are ready, you can use them in the same way as you\n# typically do for demo endpoints\nresult = client.get_future_result(future)\n\n# And includes healthChecks\nif client.health_check(\"your-healthcheck-url\") == 200:\n\t# Run some inferences\n    pass\n"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Python SDK inference"
      },
      "h3": {
        "id": "octoai-python-sdk-at-a-glance",
        "title": "OctoAI Python SDK at a glance"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "title": "Upgrading from the octoai-sdk",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-upgrading-your-code-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "title": "Upgrading your code",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#upgrading-your-code",
    "content": "The various OctoAI APIs are now accessable from a single client:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "from octoai.client import OctoAI\n\nclient = OctoAI()\n\n# The various APIs are now accessible from the client\nclient.text_gen\n\nclient.image_gen\n\nclient.fine_tuning\n\nclient.asset_library"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk"
      },
      "h3": {
        "id": "upgrading-your-code",
        "title": "Upgrading your code"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-image-generation-api-changes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "title": "Image Generation API changes",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#image-generation-api-changes",
    "content": "In the octoai.image_gen API instead of specifying which\nengine to use, use the corresponding generate_* method.",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk"
      },
      "h3": {
        "id": "image-generation-api-changes",
        "title": "Image Generation API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-text-generation-api-changes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "title": "Text Generation API changes",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#text-generation-api-changes",
    "content": "The text generation models with octoai.text_gen API are now specified with a str model name instead of an enum.\nStreaming requests are made with the corresponding octoai.text_gen.*_stream method.",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk"
      },
      "h3": {
        "id": "text-generation-api-changes",
        "title": "Text Generation API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-asset-library-asset-orchestrator-api-changes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "title": "Asset Library (Asset Orchestrator) API changes",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#asset-library-asset-orchestrator-api-changes",
    "content": "The Asset Orchestrator has been renamed to Asset Library and can be accessed via octoai.asset_library. Assets can be created with the octoai.asset_lirbary.create_from_file.",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk"
      },
      "h3": {
        "id": "asset-library-asset-orchestrator-api-changes",
        "title": "Asset Library (Asset Orchestrator) API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-octoaiservice-api-changes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "title": "octoai.service API changes",
    "breadcrumb": [
      {
        "title": "Python SDK",
        "pathname": "/docs/documentation/python-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#octoaiservice-api-changes",
    "content": "The octoai.service API has been removed. You can make inferences to compute service endpoints via OctoAI client:\nHowever you will need to continue to use the older octoai-sdk for the full octoai.service API which includes service authoring.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "from octoai.client import OctoAI\n\nclient = OctoAI()\n\n# octoai.infer()"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk"
      },
      "h3": {
        "id": "octoaiservice-api-changes",
        "title": "octoai.service API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.type-script-sdk-installation",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/installation-and-setup",
    "pathname": "/docs/typescript-sdk/installation-and-setup",
    "title": "TypeScript SDK installation & setup",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.type-script-sdk-installation-requirements-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/installation-and-setup",
    "pathname": "/docs/typescript-sdk/installation-and-setup",
    "title": "Requirements",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#requirements",
    "content": "The following runtimes are supported:\nNode.js 15+\n\nVercel\n\nCloudflare Workers\n\nDeno v1.25+\n\nBun 1.0+\n\n\n\nIn order to access endpoints, create an OctoAI API token.\n\nSet the token to an environment variable named OCTOAI_TOKEN or pass it to the OctoAIClient class on construction.",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK installation & setup"
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.type-script-sdk-installation-installation-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/installation-and-setup",
    "pathname": "/docs/typescript-sdk/installation-and-setup",
    "title": "Installation",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#installation",
    "content": "The TypeScript SDK can be installed using your preferred package manager.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "npm",
        "code": "npm install @octoai/sdk"
      },
      {
        "lang": "bash",
        "meta": "yarn",
        "code": "yarn add @octoai/sdk"
      },
      {
        "lang": "bash",
        "meta": "pnpm",
        "code": "pnpm add @octoai/sdk"
      },
      {
        "lang": "bash",
        "meta": "npm",
        "code": "npm install @octoai/sdk"
      },
      {
        "lang": "bash",
        "meta": "yarn",
        "code": "yarn add @octoai/sdk"
      },
      {
        "lang": "bash",
        "meta": "pnpm",
        "code": "pnpm add @octoai/sdk"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK installation & setup"
      },
      "h4": {
        "id": "installation",
        "title": "Installation"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.type-script-sdk-installation-setting-api-token-as-an-environment-variable-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/installation-and-setup",
    "pathname": "/docs/typescript-sdk/installation-and-setup",
    "title": "Setting API token as an environment variable",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#setting-api-token-as-an-environment-variable",
    "content": "In order to access endpoints from OctoAI, first create an API token. Set OCTOAI_TOKEN to the token value wherever you set your environment variables, such as your .bashrc or .env file.",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "export OCTOAI_TOKEN=YOUR_TOKEN_HERE"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK installation & setup"
      },
      "h4": {
        "id": "setting-api-token-as-an-environment-variable",
        "title": "Setting API token as an environment variable"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "title": "Upgrading from @octoai/client",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-upgrading-your-code-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "title": "Upgrading your code",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#upgrading-your-code",
    "content": "The various OctoAI APIs are now accessible from a single client:",
    "code_snippets": [
      {
        "lang": "typescript",
        "meta": "TypeScript",
        "code": "import { OctoAIClient } from \"@octoai/sdk\";\n\nconst octoai = new OctoAIClient({\n  apiKey: process.env.OCTOAI_TOKEN,\n});\n\n// The various APIs are now accessible from the client\noctoai.textGen;\noctoai.imageGen;\noctoai.fineTuning;\noctoai.assetLibrary;"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client"
      },
      "h3": {
        "id": "upgrading-your-code",
        "title": "Upgrading your code"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-image-generation-api-changes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "title": "Image Generation API changes",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#image-generation-api-changes",
    "content": "In the octoai.imageGen API, instead of specifying which engine to use, use the corresponding generate* method such as generateSdxl() or generateSd().",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client"
      },
      "h3": {
        "id": "image-generation-api-changes",
        "title": "Image Generation API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-text-generation-api-changes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "title": "Text Generation API changes",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#text-generation-api-changes",
    "content": "The text generation models with octoai.textGen API are now specified with a string model name instead of an enum.\nStreaming requests are made with the corresponding octoai.textGen.*Stream method such as createCompletionStream() or createChatCompletionStream().",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client"
      },
      "h3": {
        "id": "text-generation-api-changes",
        "title": "Text Generation API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-asset-library-asset-orchestrator-api-changes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "title": "Asset Library (Asset Orchestrator) API changes",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#asset-library-asset-orchestrator-api-changes",
    "content": "The Asset Orchestrator has been renamed to Asset Library and can be accessed via octoai.assetLibrary. Assets can be created with the octoai.assetLibrary.upload() method.",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client"
      },
      "h3": {
        "id": "asset-library-asset-orchestrator-api-changes",
        "title": "Asset Library (Asset Orchestrator) API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-compute-inferencing-api-changes-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "title": "Compute Inferencing API changes",
    "breadcrumb": [
      {
        "title": "TypeScript SDK",
        "pathname": "/docs/documentation/type-script-sdk"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#compute-inferencing-api-changes",
    "content": "The infer() method is not available in this SDK. You will need to continue to use the older @octoai/client for full inferencing support.",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client"
      },
      "h3": {
        "id": "compute-inferencing-api-changes",
        "title": "Compute Inferencing API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.rate-limits-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/faqs/rate-limits",
    "pathname": "/docs/faqs/rate-limits",
    "title": "Rate limits",
    "breadcrumb": [
      {
        "title": "FAQs",
        "pathname": "/docs/documentation/fa-qs"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Rate limits are restrictions on the rate and individual account can submit inference requests.",
    "content": "Rate limits are restrictions applied by OctoAI on the rate at which an individual account can submit inference requests against an API endpoint. It is a mechanism used to ensure predictable performance of the platform, and to allow all OctoAI customers to experience predictable inference latencies. Inference requests that are not completed because of a rate limit cap will return an HTTP 429 response code, and can be retried after an appropriate backoff period."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.rate-limits-octoai-api-rate-limits-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/faqs/rate-limits",
    "pathname": "/docs/faqs/rate-limits",
    "title": "OctoAI API rate limits",
    "breadcrumb": [
      {
        "title": "FAQs",
        "pathname": "/docs/documentation/fa-qs"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#octoai-api-rate-limits",
    "content": "API endpoint Free tier Pro tier Enterprise tier \nText Gen 10 requests per minute 240 requests per minute Contact us \n \nMedia Gen 10 requests per minute 60 requests per minute Contact us \n\nHigher rate limits are available, please reach out if you need an increase.",
    "hierarchy": {
      "h0": {
        "title": "Rate limits"
      },
      "h2": {
        "id": "octoai-api-rate-limits",
        "title": "OctoAI API rate limits"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.privacy-security-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/faqs/privacy-and-security",
    "pathname": "/docs/faqs/privacy-and-security",
    "title": "Privacy & security",
    "breadcrumb": [
      {
        "title": "FAQs",
        "pathname": "/docs/documentation/fa-qs"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "We are SOC Type 2 compliant and take data privacy extremely seriously. Our policies around customer data -- anything you upload to the platform and all inputs you send/outputs you generate — are detailed here in sections 8-12. All data in transit and at rest uses SSL encryption.\nTo summarize, OctoAI can only use your customer data to provide the service to you and not for any other purpose. Providing the service does include debugging, troubleshooting, and ensuring ToS compliance. Customer prompt data is persisted in logs for 15 days and then deleted. Outputs are never stored, and we do not use your data to train models unless you decide to use our fine-tuning service directly. Access to customer logs is controlled on a need to know basis and audited."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.rag-with-octo-ai-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/faqs/rag",
    "pathname": "/docs/faqs/rag",
    "title": "How to implement RAG with OctoAI",
    "breadcrumb": [
      {
        "title": "FAQs",
        "pathname": "/docs/documentation/fa-qs"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "There are multiple ways in which customers can build a RAG application on OctoAI.",
    "content": "OctoAI allows customers to run their choice of LLMs\n(like Llama 2 70B, Mixtral 8x7B, Mixtral 8x22B) and embedding models (like gte-large). With these primitives, customers can use their preferred vector database as the reference data store for their RAG application. OctoAI also supports integrations with popular LLM application development frameworks like LangChain, allowing the use of pre-built functions in LangChain to simplify their RAG application development.\nLastly, OctoAI supports integrations into turnkey RAG frameworks like PineCone Canopy for customers to easily implement RAG with their data."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.service-regions-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/faqs/service-regions",
    "pathname": "/docs/faqs/service-regions",
    "title": "Service regions",
    "breadcrumb": [
      {
        "title": "FAQs",
        "pathname": "/docs/documentation/fa-qs"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI currently runs on AWS and GCP hardware in several regions",
    "content": "OctoAI currently runs on AWS and GCP hardware in several regions - including AWS us-east-1 and us-west-2 regions, and GCP us-central-1 region. We are actively expanding support for more regions.\nFor customers with data residency requirements, OctoStack can be hosted in your own private cloud environment or on your own hardware. Contact our sales team for more information."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.multi-user-accounts-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/faqs/multi-user-accounts",
    "pathname": "/docs/faqs/multi-user-accounts",
    "title": "Multi-user accounts",
    "breadcrumb": [
      {
        "title": "FAQs",
        "pathname": "/docs/documentation/fa-qs"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "The default setup is 1 user profile per 1 OctoAI account. We can easily help you setup multiple users within a single account if you have a team or organization with multiple users. This will allow your team to manage endpoints, view logs & metrics, and securely share access to the account.",
    "content": "Contact us on Discord or our in-app chat feature to setup your multi-user account."
  },
  {
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.open-source-licenses-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/faqs/open-source-licenses",
    "pathname": "/docs/faqs/open-source-licenses",
    "title": "Open source licenses",
    "breadcrumb": [
      {
        "title": "FAQs",
        "pathname": "/docs/documentation/fa-qs"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs/documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Below are the open source technologies we make use of and their associated licenses",
    "content": "Package License \nalabaster 0.7.13 BSD \nannotated-types 0.5.0 MIT) \nanyio 3.7.1 MIT \nav 10.0.0 BSD \nBabel 2.12.1 BSD \nblack 23.7.0 MIT \ncertifi 2023.7.22 MPL-2.0 \ncffi 1.15.1 MIT \ncfgv 3.4.0 MIT \ncharset-normalizer 3.2.0 MIT \nchevron 0.14.0 MIT \nclick 8.1.6 BSD-3-Clause \ndistlib 0.3.7 PSF-2.0 \ndocutils 0.18.1 public domain, Python, 2-Clause BSD, GPL 3 (see COPYING.txt) \nexecnet 2.0.2 MIT \nfastapi 0.100.1 MIT \nfilelock 3.12.2 Unlicense \nflake8 6.1.0 MIT \nflake8-docstrings 1.7.0 MIT \nh11 0.14.0 MIT \nhttpcore 0.17.3 BSD \nhttpx 0.24.1 BSD \nidentify 2.5.26 MIT \nidna 3.4 BSD \nimageio 2.31.1 BSD-2-Clause \nimagesize 1.4.1 MIT \niniconfig 2.0.0 MIT \nisort 5.12.0 MIT \nJinja2 3.1.2 BSD-3-Clause \nMarkupSafe 2.1.3 BSD-3-Clause \nmccabe 0.7.0 Expat license \nmypy 1.5.1 MIT License \nmypy-extensions 1.0.0 MIT License \nnodeenv 1.8.0 BSD \nnumpy 1.24.4 BSD-3-Clause \npackaging 23.1 Apache Software License, BSD License \npathspec 0.11.2 Mozilla Public License 2.0 (MPL 2.0) \nPillow 9.5.0 HPND \npip 23.2.1 MIT \nplatformdirs 3.10.0 MIT \npluggy 1.2.0 MIT \npre-commit 3.3.3 MIT \nprettytable 3.9.0 BSD (3 clause) \npycodestyle 2.11.0 MIT \npycparser 2.21 BSD \npydantic 2.2.0 MIT \npydantic-core 2.6.0 MIT \npydocstyle 6.3.0 MIT \npyflakes 3.1.0 MIT \nPygments 2.16.1 BSD-2-Clause \npytest 7.4.0 MIT \npytest-xdist 3.3.0 MIT \npython-multipart 0.0.6 Apache Software License \nPyYAML 6.0.1 MIT \nrequests 2.31.0 Apache 2.0 \nrequests-futures 1.0.1 Apache License v2 \nrespx 0.20.2 BSD-3-Clause \nsetuptools 68.1.0 MIT \nsniffio 1.3.0 MIT OR Apache-2.0 \nsnowballstemmer 2.2.0 BSD-3-Clause \nsoundfile 0.12.1 BSD 3-Clause License \nsphinx 6.2.1 BSD \nsphinx-rtd-theme 1.2.2 MIT \nsphinxcontrib-applehelp 1.0.4 BSD-2-Clause \nsphinxcontrib-devhelp 1.0.2 BSD \nsphinxcontrib-htmlhelp 2.0.1 BSD-2-Clause \nsphinxcontrib-jquery 4.1 BSD \nsphinxcontrib-jsmath 1.0.1 BSD \nsphinxcontrib-qthelp 1.0.3 BSD \nsphinxcontrib-serializinghtml 1.1.5 BSD \nstarlette 0.27.0 BSD \ntypes-PyYAML 6.0.12.11 Apache-2.0 license \ntypes-requests 2.31.0.2 Apache-2.0 license \ntypes-urllib3 1.26.25.14 Apache-2.0 license \ntyping-extensions 4.7.1 (License not found) \nurllib3 2.0.4 MIT \nuvicorn 0.22.0 BSD \nvirtualenv 20.24.3 MIT \nwcwidth 0.2.8 MIT \nwheel 0.41.2 MIT"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.authentication-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/authentication",
    "pathname": "/docs/api-reference/octoai-api/authentication",
    "title": "Authentication",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "Authorization: Bearer $OCTOAI_TOKEN"
      }
    ],
    "content": "API requests to your endpoints must be authenticated with a token - you can generate a token from the Account Settings page. Be sure to include your token in the header of your requests."
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "title": "Inference",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-limitations-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "title": "Limitations",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#limitations",
    "content": "Asynchronous inference request size is currently limited to 10mb. Asynchronous inference output data is stored for 24 hours, then automatically deleted.\nA long-running inference with duration greater than 1 minute may occasionally encounter an error. If this happens, re-submit your request or reach out to us for help.",
    "hierarchy": {
      "h0": {
        "title": "Inference"
      },
      "h3": {
        "id": "limitations",
        "title": "Limitations"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-create-inference-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "title": "Create inference",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#create-inference",
    "content": "Starts an inference at the specified endpoint URL for the data inputs you provide. The request is synchronous by default, and you can optionally specify the request as asynchronous. Input parameters are included in the cURL example of each endpoint.\nAPI requests to your endpoints must be authenticated with a token - you can generate a token from the Account Settings page. Be sure to include your token in the header of your requests.\nExample synchronous cURL request:",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "POST https://image.octoai.run/generate/sdxl"
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n -H \"Content-Type: application/json\" \\\n -H \"authorization: Bearer $OCTOAI_TOKEN\" \\\n --data-raw '{\"prompt\": \"A photo of a cute cat astronaut in space\"}'"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Inference"
      },
      "h3": {
        "id": "create-inference",
        "title": "Create inference"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-asynchronous-inference-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "title": "Asynchronous inference",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#asynchronous-inference",
    "content": "You can create an asynchronous inference by specifying X-OctoAI-Async: 1 in the request header.\nExample asynchronous cURL request:\nYou’ll receive a response ID and poll URL where you can poll for the status and results:",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "curl -X POST \"https://image.octoai.run/generate/sdxl\" \\\n -H \"Content-Type: application/json\" \\\n -H \"authorization: Bearer $OCTOAI_TOKEN\" \\\n -H \"X-OctoAI-Async:1\" \\\n --data-raw '{\"prompt\": \"A photo of a cute cat astronaut in space\"}'"
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "\"response_id\": \"778bbfd58c-hz95k\",\n\"poll_url\": \"https://async.octoai.run/v1/requests/778bbfd58c-hz95k\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Inference"
      },
      "h3": {
        "id": "create-inference",
        "title": "Create inference"
      },
      "h4": {
        "id": "asynchronous-inference",
        "title": "Asynchronous inference"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-poll-for-status-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "title": "Poll for status",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#poll-for-status",
    "content": "Use the poll_url to return the status of an inference, which will be one of these values:\npending: the inference is waiting or starting up\n\nrunning: the inference is in progress\n\ncompleted: the inference is finished\n\n\nExample poll cURL request:\nExample pending poll response:",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "GET https://async.octoai.run/v1/requests/response_id"
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "curl -X GET \"https://async.octoai.run/v1/requests/778bbfd58c-hz95k\" \\\n -H \"Authorization: Bearer $OCTOAI_TOKEN\""
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": " \"status\": \"pending\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Inference"
      },
      "h3": {
        "id": "get-inference",
        "title": "Get inference"
      },
      "h4": {
        "id": "poll-for-status",
        "title": "Poll for status"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-get-inference-data-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "title": "Get inference data",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#get-inference-data",
    "content": "When completed, the provided response_url will include the inference data. Asynchronous inference output data is stored for 24 hours, then automatically deleted.\nExample completed poll response:\nExample cURL request for completed inference data:",
    "code_snippets": [
      {
        "lang": "bash",
        "meta": "bash",
        "code": "GET https://async.octoai.run/v1/responses/response_id"
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "\"status\": \"completed\",\n\"response_url\": \"https://async.octoai.run/v1/responses/778bbfd58c-hz95k\""
      },
      {
        "lang": "bash",
        "meta": "bash",
        "code": "curl -X GET \"https://async.octoai.run/v1/responses/778bbfd58c-hz95k\" \\\n -H \"Authorization: Bearer $OCTOAI_TOKEN\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Inference"
      },
      "h3": {
        "id": "get-inference",
        "title": "Get inference"
      },
      "h4": {
        "id": "get-inference-data",
        "title": "Get inference data"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.all-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/overview",
    "pathname": "/docs/integrations/overview",
    "title": "All OctoAI Integrations",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Browse OctoAI's partner integrations to help you build your custom solution.",
    "content": "LangChain\nLangChain provides a framework to easily construct LLM-powered apps.\nLangchain developers can leverage OctoAI LLM and embedding endpoints to\neasily access efficient compute across a wide selection of LLMs.\n\n\n\n\n\n\nUnstructured.io\nUnstructured provides components to very easily embed text documents lke\nPDFs, HTML, Word Docs, and more. The OctoAIEmbedingEncoder is available, so\ndocuments parsed with Unstructured can easily be embedded with the OctoAI\nembeddings endpoint.\n\n\n\n\n\n\n\n\nPinecone (Canopy)\nPinecone provides storage and retrieval infrastructure needed for building\nand running AI apps. This integration allows a developer using Canopy to\nchoose from the best LLMs on OctoAI.\n\n\n\n\n\n\nOpenRouter\nOpenRouter has a unified interface for using various LLMs, allowing users to\nfind and compare models for their needs. The OpenRouter API users can\nleverage OctoAI's best in class LLM endpoints.\n\n\n\n\n\n\nLlamaIndex\nLlamaIndex aids in the management of interactions between your LLMs and\nprivate data. A developer building AI apps can now access highly optimized\nLLMs and Embeddings models on OctoAI."
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.lang-chain",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/langchain",
    "pathname": "/docs/integrations/langchain",
    "title": "LangChain Integration",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Langchain developers can leverage OctoAI LLM and embedding endpoints to easily access efficient compute across a wide selection of LLMs."
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.lang-chain-introduction-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/langchain",
    "pathname": "/docs/integrations/langchain",
    "title": "Introduction",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#introduction",
    "content": "LangChain provides a framework to easily build LLM-powered apps. Developers using LangChain can now utilize OctoAI LLMs and Embedding endpoints\nto access efficient, fast, and reliable compute.",
    "hierarchy": {
      "h0": {
        "title": "LangChain Integration"
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.lang-chain-using-octoais-llms-and-langchain-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/langchain",
    "pathname": "/docs/integrations/langchain",
    "title": "Using OctoAI's LLMs and LangChain",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-octoais-llms-and-langchain",
    "content": "To use OctoAI LLMs with LangChain, first obtain an OcotoAI API Token.\nThen paste your API token in the code example below:\nNext, run the following Python script:\nIt should produce the following output:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import os\n\nos.environ[\"OCTOAI_API_TOKEN\"] = \"OCTOAI_API_TOKEN\"\nos.environ[\"ENDPOINT_URL\"] = \"https://text.octoai.run/v1/chat/completions\""
      },
      {
        "lang": "python",
        "code": "from langchain.chains import LLMChain\nfrom langchain_community.llms.octoai_endpoint import OctoAIEndpoint\nfrom langchain_core.prompts import PromptTemplate\n\ntemplate = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n Instruction:\\n{question}\\n Response: \"\"\"\nprompt = PromptTemplate.from_template(template)\n\nllm = OctoAIEndpoint(\n    model_kwargs={\n        \"model\": \"llama-2-13b-chat-fp16\",\n        \"max_tokens\": 128,\n        \"presence_penalty\": 0,\n        \"temperature\": 0.1,\n        \"top_p\": 0.9,\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant. Keep your responses limited to one short paragraph if possible.\",\n            },\n        ],\n    },\n)\n\nquestion = \"Who was leonardo davinci?\"\n\nllm_chain = LLMChain(prompt=prompt, llm=llm)\n\nprint(llm_chain.run(question))"
      },
      {
        "code": "  Sure thing! Here's my response:\n\nLeonardo da Vinci was a true Renaissance man - an Italian polymath who excelled in various fields,\nincluding painting, sculpture, engineering, mathematics, anatomy, and geology. He is widely considered\none of the greatest painters of all time, and his inventive and innovative works continue to inspire and\ninfluence artists and thinkers to this day. Some of his most famous works include the Mona Lisa,\nThe Last Supper, and Vitruvian Man."
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "LangChain Integration"
      },
      "h2": {
        "id": "using-octoais-llms-and-langchain",
        "title": "Using OctoAI's LLMs and LangChain"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.lang-chain-learn-with-our-demo-apps-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/langchain",
    "pathname": "/docs/integrations/langchain",
    "title": "Learn with our demo apps",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#learn-with-our-demo-apps",
    "content": "Get started today by following along with one of our demo apps:\nDocTalk\n\nQ&A app on a custom PDF",
    "hierarchy": {
      "h0": {
        "title": "LangChain Integration"
      },
      "h2": {
        "id": "using-octoais-llms-and-langchain",
        "title": "Using OctoAI's LLMs and LangChain"
      },
      "h3": {
        "id": "learn-with-our-demo-apps",
        "title": "Learn with our demo apps"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.pinecone",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/pinecone",
    "pathname": "/docs/integrations/pinecone",
    "title": "Pinecone (Canopy) Integration",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "This integration allows a developer using Canopy to choose from the best LLMs on OctoAI."
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.pinecone-introduction-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/pinecone",
    "pathname": "/docs/integrations/pinecone",
    "title": "Introduction",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#introduction",
    "content": "Pinecone provides storage and retrieval infrastructure needed for building AI applications. Pinecone's Canopy is an open-source framework built on top of Pinecone's\nvector database to build production-ready chat assistants at any scale.",
    "hierarchy": {
      "h0": {
        "title": "Pinecone (Canopy) Integration"
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.pinecone-using-octoais-llms-and-pinecone-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/pinecone",
    "pathname": "/docs/integrations/pinecone",
    "title": "Using OctoAI's LLMs and Pinecone",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-octoais-llms-and-pinecone",
    "content": "As a fully open source solution, Canopy+OctoAI is one of the fastest ways and more affordable ways to get started on your\nRAG journey. Canopy uses Pinecone vector database for storage and retrieval, which is free to use for up to 100k vectors (that’s about 30k pages of text).\nOctoAI offers industry leading pricing at $0.05 / 1M token for its gte-large embedding model, and offers $10 of free credit upon sign up.\nTo get a Canopy server running with OctoAI's modles, you do not need custom ConvolverNode, simply update the Canopy YAML configureations as follows:",
    "code_snippets": [
      {
        "code": "chat_engine:\n  params:\n  max_prompt_tokens: 2048\n  llm: &llm\n  type: OctoAILLM\n  params:\n    model_name: mistral-7b-instruct\n\n  context_engine:\n  knowledge_base:\n    record_encoder:\n      type: OctoAIRecordEncoder\n      params:\n        model_name: thenlper/gte-large\n        batch_size: 2048"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Pinecone (Canopy) Integration"
      },
      "h2": {
        "id": "using-octoais-llms-and-pinecone",
        "title": "Using OctoAI's LLMs and Pinecone"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.pinecone-learn-with-our-demo-apps-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/pinecone",
    "pathname": "/docs/integrations/pinecone",
    "title": "Learn with our demo apps",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#learn-with-our-demo-apps",
    "content": "Get started today by following along with one of our demo apps:\nDocTalk\n\n\nLearn more about our partnership:\nOctoAI & Pinecone Patnership for GenAI using RAG",
    "hierarchy": {
      "h0": {
        "title": "Pinecone (Canopy) Integration"
      },
      "h2": {
        "id": "using-octoais-llms-and-pinecone",
        "title": "Using OctoAI's LLMs and Pinecone"
      },
      "h3": {
        "id": "learn-with-our-demo-apps",
        "title": "Learn with our demo apps"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.unstructured-io",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/unstructured",
    "pathname": "/docs/integrations/unstructured",
    "title": "Unstructured.io Integration",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "The OctoAIEmbedingEncoder is available, so documents parsed with Unstructured can easily be embedded with the OctoAI embeddings endpoint."
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.unstructured-io-introduction-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/unstructured",
    "pathname": "/docs/integrations/unstructured",
    "title": "Introduction",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#introduction",
    "content": "Unstructured is both an open-source library and an API service. The library provides components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more.\nIt also provides components to very easily embed these documents. In Unstructured’s jargon this component is called an EmbeddingEncoder. The OctoAIEmbedingEncoder is available, so documents parsed with Unstructured can easily be embedded with the OctoAI embeddings endpoint.",
    "hierarchy": {
      "h0": {
        "title": "Unstructured.io Integration"
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.unstructured-io-using-the-octoaiembeddingencoder-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/unstructured",
    "pathname": "/docs/integrations/unstructured",
    "title": "Using the OctoAIEmbeddingEncoder",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-the-octoaiembeddingencoder",
    "content": "Before you get started let's review some concepts. You will need an OctoAI API Token for this integration.\nThe OctoAIEmbeddingEncoder class connects to the OctoAI Text&Embedding API to obtain embeddings for pieces of text.\n\nembed_documents will receive a list of Elements, and return an updated list which includes the embeddings attribute for each Element.\n\nembed_query will receive a query as a string, and return a list of floats which is the embedding vector for the given query string.\n\nnum_of_dimensions is a metadata property that denotes the number of dimensions in any embedding vector obtained via this class.\n\nis_unit_vector is a metadata property that denotes if embedding vectors obtained via this class are unit vectors.\n\n\nNow, let's get started with the following code example for how to use OctoAIEmbeddingEncoder.\nYou will see the updated elements list (with the embeddings attribute included for each element),\nthe embedding vector for the query string, and some metadata properties about the embedding model.\nYou will need to set an environment variable named OCTOAI_API_KEY to be able to run this example.\nLearn more about our partnership:\nOctoAI & Unstructured.io Create New Integration",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import os\n\nfrom unstructured.documents.elements import Text\nfrom unstructured.embed.octoai import OctoAiEmbeddingConfig, OctoAIEmbeddingEncoder\n\nembedding_encoder = OctoAIEmbeddingEncoder(\n    config=OctoAiEmbeddingConfig(api_key=os.environ[\"OCTOAI_API_KEY\"])\n)\nelements = embedding_encoder.embed_documents(\n    elements=[Text(\"This is sentence 1\"), Text(\"This is sentence 2\")],\n)\n\nquery = \"This is the query\"\nquery_embedding = embedding_encoder.embed_query(query=query)\n\n[print(e.embeddings, e) for e in elements]\nprint(query_embedding, query)\nprint(embedding_encoder.is_unit_vector(), embedding_encoder.num_of_dimensions())"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Unstructured.io Integration"
      },
      "h2": {
        "id": "using-the-octoaiembeddingencoder",
        "title": "Using the OctoAIEmbeddingEncoder"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.open-router",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/openrouter",
    "pathname": "/docs/integrations/openrouter",
    "title": "OpenRouter Integration",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "The OpenRouter API users can leverage OctoAI's best in class LLM endpoints."
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.open-router-introduction-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/openrouter",
    "pathname": "/docs/integrations/openrouter",
    "title": "Introduction",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#introduction",
    "content": "OpenRouter provides a unified interface for using various LLMs and allows users to find and compare models based on PromiseRejectionEvent, latency, throughput.\nThis let's users find the right LLM and mix of price and performance for their use case.",
    "hierarchy": {
      "h0": {
        "title": "OpenRouter Integration"
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.open-router-using-octoais-llms-and-openrouter-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/openrouter",
    "pathname": "/docs/integrations/openrouter",
    "title": "Using OctoAI's LLMs and OpenRouter",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-octoais-llms-and-openrouter",
    "content": "To access OctoAI's best in class LLMs via OpenRouter sign into OpenRouter and create an account to obtain an OPENROUTER_API_KEY.\nUsing the code snippet below you can route your calls to OpenRouter via OpenAI's client API.\nSet the providers the OpenRouter will use for your request using the order field. The router will filter this list to only include fproviders that are available for the model you want to use, and then try one at a time. It will fail if none are available.\nIf you do not set the field, the router will use the default ordering shown on the model page.\nIn the following code snippet, we assume the order of preference is as follows: OctoAI, Azure, then TogetherAI.\nFind all models from OctoAI you can use at https://openrouter.ai.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import OpenAI from \"openai\"\n\nconst openai = new OpenAI({\n  baseURL: \"https://openrouter.ai/api/v1\",\n  apiKey: $OPENROUTER_API_KEY,\n  defaultHeaders: {\n    \"HTTP-Referer\": $YOUR_SITE_URL, // Optional, for including your app on openrouter.ai rankings.\n    \"X-Title\": $YOUR_SITE_NAME, // Optional. Shows in rankings on openrouter.ai.\n  },\n  // dangerouslyAllowBrowser: true,\n})\nasync function main() {\n  const completion = await openai.chat.completions.create({\n    model: \"mistralai/mixtral-8x7b-instruct\",\n    messages: [\n      { role: \"user\", content: \"Say this is a test\" }\n    ],\n    provider: {\n\t    order: [\"OctoAI\", \"Azure\", \"Together\"]\n\t  }\n  })\n\n  console.log(completion.choices[0].message)\n}\nmain()"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "OpenRouter Integration"
      },
      "h2": {
        "id": "using-octoais-llms-and-openrouter",
        "title": "Using OctoAI's LLMs and OpenRouter"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.llama-index",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/llamaindex",
    "pathname": "/docs/integrations/llamaindex",
    "title": "LlamaIndex Integration",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "A developer building AI apps can now access highly optimized LLMs and Embeddings models on OctoAI."
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.llama-index-introduction-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/llamaindex",
    "pathname": "/docs/integrations/llamaindex",
    "title": "Introduction",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#introduction",
    "content": "LlamaIndex strives to help manage the interactions between your language modles and private DataTransfer.\nIf you are building your application and using LlamaIndex you benefit from the vast ecosystem of integrations, and top LLMs amd Embeddings models hosted by OctoAI.",
    "hierarchy": {
      "h0": {
        "title": "LlamaIndex Integration"
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.llama-index-using-octoais-llms-and-llamaindex-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/integrations/llamaindex",
    "pathname": "/docs/integrations/llamaindex",
    "title": "Using OctoAI's LLMs and LlamaIndex",
    "breadcrumb": [
      {
        "title": "Integrations",
        "pathname": "/docs/integrations/integrations"
      }
    ],
    "tab": {
      "title": "Integrations",
      "pathname": "/docs/integrations"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-octoais-llms-and-llamaindex",
    "content": "Get started reviewing more about LlamaIndex, and signing up for a free OctoAI account.\nLlamaIndex has both Python and TypScript libraries, and OctoAI is available in the Python SDK.\nTo use OctoAI LLM endpoints with LlamaIndex start with the code below using Llama 3 8B as the LLM.\nTo use OctoAI Embedding endpoints with llamaindex\nyou can use the code below to get started. We’re using GTE large in the example below (default model).\nIf you are using LlamaIndex you can easily switch model provider, and enjoy using models hosted and optimized for scale on OctoAI.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "from os import environ\nfrom llama_index.llms.octoai import OctoAI\n\nOCTOAI_API_KEY = environ.get(\"OCTOAI_TOKEN\")\n\noctoai = OctoAI(model=\"meta-llama-3-8b-instruct\", token=OCTOAI_API_KEY)\n\n# Using complete\nresponse = octoai.complete(\"Octopi can not play chess because...\")\nprint(response)\n\nprint(\"\\n=====================\\n\")\n\n# Using the chat interface\nfrom llama_index.core.llms import ChatMessage\n\nmessages = [\n    ChatMessage(\n        role=\"system\",\n        content=\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\",\n    ),\n    ChatMessage(role=\"user\", content=\"Write a short blog about Seattle\"),\n]\nresponse = octoai.chat(messages)\nprint(response)"
      },
      {
        "lang": "python",
        "code": "from os import environ\nfrom llama_index.embeddings.octoai import OctoAIEmbedding\n\nOCTOAI_API_KEY = environ.get(\"OCTOAI_TOKEN\")\nembed_model = OctoAIEmbedding(api_key=OCTOAI_API_KEY)\n\n# Single embedding request\nembeddings = embed_model.get_text_embedding(\"Once upon a time in Seattle.\")\nassert len(embeddings) == 1024\nprint(embeddings[:10])\n\n\n# Batch embedding request\ntexts = [\n    \"Once upon a time in Seattle.\", \n    \"This is a test.\", \n    \"Hello, world!\"\n]\nembeddings = embed_model.get_text_embedding_batch(texts)\nassert len(embeddings) == 3\nprint(embeddings[0][:10])"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "LlamaIndex Integration"
      },
      "h2": {
        "id": "using-octoais-llms-and-llamaindex",
        "title": "Using OctoAI's LLMs and LlamaIndex"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.may-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/may",
    "pathname": "/docs/release-notes/2024/may",
    "title": "May 2024 Release Notes",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for May 2024",
    "content": "Improved"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.may-may-3-2024-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/may",
    "pathname": "/docs/release-notes/2024/may",
    "title": "May 3, 2024",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#may-3-2024",
    "content": "OctoAI releases a redesigned Asset Library.\n\n\nImproves browsing, filtering, and overall asset management.",
    "hierarchy": {
      "h0": {
        "title": "May 2024 Release Notes"
      },
      "h2": {
        "id": "may-3-2024",
        "title": "May 3, 2024"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.april-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/april",
    "pathname": "/docs/release-notes/2024/april",
    "title": "April 2024 Release Notes",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for April 2024",
    "content": "Improved"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.april-april-30-2024-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/april",
    "pathname": "/docs/release-notes/2024/april",
    "title": "April 30, 2024",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#april-30-2024",
    "content": "OctoAI releases an improved Billing & Usage dashboard.\n\n\nProvides more detailed visibility into your OctoAI product usage.\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "April 2024 Release Notes"
      },
      "h2": {
        "id": "april-30-2024",
        "title": "April 30, 2024"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.april-april-24-2024-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/april",
    "pathname": "/docs/release-notes/2024/april",
    "title": "April 24, 2024",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#april-24-2024",
    "content": "OctoAI releases OctoStack: a turnkey generative AI solution allowing you to run your choice of models in your environment.\n\n\nOctoStack provides a full stack solution for running generative AI at scale,\nincluding inference, model customization, load balancing, auto-scaling, and\ntelemetry.",
    "hierarchy": {
      "h0": {
        "title": "April 2024 Release Notes"
      },
      "h2": {
        "id": "april-24-2024",
        "title": "April 24, 2024"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.february-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/february",
    "pathname": "/docs/release-notes/2024/february",
    "title": "February 2024 Release Notes",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for February 2024",
    "content": "Added"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.february-february-29-2024-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/february",
    "pathname": "/docs/release-notes/2024/february",
    "title": "February 29, 2024",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#february-29-2024",
    "content": "OctoAI Text Gen Solution adds several new OSS large language models, including:\n\n\nGemma-7B-Instruct, a new open source model from Google.\n\nSmaug-72B-v0.1, a fine tune of the Qwen1.0 family of models that shows impressive leaderboard performance.\n\nNous-Hermes-2-Mixtral-8x7B-DPO, a fine tune of the powerful Mixtral-8x7b model, offered as the “flagship” checkpoint of Nous Research (the current best producer of open source fine tunes for popular models).\n\n\n\n\nAdditionally, OctoAI releases SecureLink, which is a private connectivity security measure, ensuring that network traffic between an OctoAI endpoint and the customer environment is not exposed to the public internet. SecureLink is available for Enterprise customers.\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "February 2024 Release Notes"
      },
      "h2": {
        "id": "february-29-2024",
        "title": "February 29, 2024"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.february-february-16-2024-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/february",
    "pathname": "/docs/release-notes/2024/february",
    "title": "February 16, 2024",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#february-16-2024",
    "content": "OctoAI launches Photo Merge feature for Image Gen Solution\n\n\nSeamlessly integrate a photos subject (person) into high-quality\nAI-generated output.\n\nThis eliminates the need for custom facial fine-tunes\nand requires only 1-4 images.\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "February 2024 Release Notes"
      },
      "h2": {
        "id": "february-16-2024",
        "title": "February 16, 2024"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.february-february-1-2024-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/february",
    "pathname": "/docs/release-notes/2024/february",
    "title": "February 1, 2024",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#february-1-2024",
    "content": "OctoAI launches the GTE Large embeddings model, available via API.\n\n\nGTE models were trained by Alibaba DAMO Academy on large-scale corpus of\nrelevant text pairs covering a myriad of scenarios. - Embedding models extract\nmeaning from raw data and turn it into a vector representation, which is used\nto enhance llms.",
    "hierarchy": {
      "h0": {
        "title": "February 2024 Release Notes"
      },
      "h2": {
        "id": "february-1-2024",
        "title": "February 1, 2024"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.january-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/january",
    "pathname": "/docs/release-notes/2024/january",
    "title": "January 2024 Release Notes",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for January 2024",
    "content": "Added"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.january-january-31-2024-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/january",
    "pathname": "/docs/release-notes/2024/january",
    "title": "January 31, 2024",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#january-31-2024",
    "content": "Code Llama 70B now available on OctoAI.\n\n\nCode Llama 70B Instruct now available via API or in the web UI.\nUse the same API and token key, simply replace the models with 'codellama-70b-instruct-fp16'\n\n\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "January 2024 Release Notes"
      },
      "h2": {
        "id": "january-31-2024",
        "title": "January 31, 2024"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.january-january-4-2024-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2024/january",
    "pathname": "/docs/release-notes/2024/january",
    "title": "January 4, 2024",
    "breadcrumb": [
      {
        "title": "2024",
        "pathname": "/docs/release-notes/2024"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#january-4-2024",
    "content": "OctoAI launched a private preview for users who want to \"bring your own\" fine-tuned LLM.\n\n\nHost any fine-tuned Llama 2, Code Llama, Mixtral, or Mistral models.\n\nRun your fine-tuned model at the same cost and latency as the default models already on OctoAI.",
    "hierarchy": {
      "h0": {
        "title": "January 2024 Release Notes"
      },
      "h2": {
        "id": "january-4-2024",
        "title": "January 4, 2024"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.december-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/december",
    "pathname": "/docs/release-notes/2023/december",
    "title": "December 2023 Release Notes",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for December 2023",
    "content": "Added"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.december-december-12-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/december",
    "pathname": "/docs/release-notes/2023/december",
    "title": "December 12, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#december-12-2023",
    "content": "OctoAI makes Mixtral 8x7B Instruct available on Text Gen Solution.\n\n\nYou can now run Mixtral 8x7B Instruct using the Text Gen Solution and benefit from high quality competitive with GPT 3.5, a unified API compatible with OpenAI, and a 4x lower price per token over GPT 3.5.",
    "hierarchy": {
      "h0": {
        "title": "December 2023 Release Notes"
      },
      "h2": {
        "id": "december-12-2023",
        "title": "December 12, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/november",
    "pathname": "/docs/release-notes/2023/november",
    "title": "November 2023 Release Notes",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for November 2023",
    "content": "Added"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-november-27-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/november",
    "pathname": "/docs/release-notes/2023/november",
    "title": "November 27, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#november-27-2023",
    "content": "OctoAI launches it's new Text Gen Solution with Llama 2, Code Llama, and Mistral models.\n\n\nRun inference against multiple sizes of Llama2 Chat, Code Llama Instruct, and Mistral Instruct all via one unified API.\n\nReliably scale your app with OctoAI, which is already processing millions of inferences daily.\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "November 2023 Release Notes"
      },
      "h2": {
        "id": "november-27-2023",
        "title": "November 27, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-november-20-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/november",
    "pathname": "/docs/release-notes/2023/november",
    "title": "November 20, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#november-20-2023",
    "content": "SDXL image generation has greatly been improved with the addition of SSD-1B, a distilled SDXL that is 50% faster, and LCM-LoRA, which enables image generation in less than 1 second.\n\n\nSSD-1B is an OSS distilled SDXL that is 50% faster due to is smaller size. OctoAI applied our proprietary ML compiler to the model, and are able to generate SDXL images in 1.4 seconds using SSD-1B.\n\nLCM-LoRA is a custom asset that enables high-quality image output requiring as few as 4 steps to generate images in less than 1 second.\n\n\n\n\n\n\n\n\nImproved",
    "hierarchy": {
      "h0": {
        "title": "November 2023 Release Notes"
      },
      "h2": {
        "id": "november-20-2023",
        "title": "November 20, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-november-16-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/november",
    "pathname": "/docs/release-notes/2023/november",
    "title": "November 16, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#november-16-2023",
    "content": "Image Gen Solution fine-tuning for SD 1.5 and SDXL via API or web UI now available.\n\n\nCustomers can create their own unique fine-tunes in the web UI with at least 3-6 images, or import custom assets from popular sources to generate one of a kind images.\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "November 2023 Release Notes"
      },
      "h2": {
        "id": "november-16-2023",
        "title": "November 16, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-november-8-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/november",
    "pathname": "/docs/release-notes/2023/november",
    "title": "November 8, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#november-8-2023",
    "content": "OctoAI Image Gen Solution launched.\n\n\nThe fastest and most customizable GenAI stack for production-grade image generation applications. The new solutions boasts SDXL images in less than 3 seconds, the ability to create or import fine-tuning assets, and reliable scale from a few to thousands of images.",
    "hierarchy": {
      "h0": {
        "title": "November 2023 Release Notes"
      },
      "h2": {
        "id": "november-8-2023",
        "title": "November 8, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.october-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/october",
    "pathname": "/docs/release-notes/2023/october",
    "title": "October 2023 Release Notes",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for October 2023",
    "content": "Improved"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.october-october-31-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/october",
    "pathname": "/docs/release-notes/2023/october",
    "title": "October 31, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#october-31-2023",
    "content": "OctoAI is now generally available with two Solution offerings.\n\n\nOctoAI Image Gen Solution: Build agaist an API endpoint to create, customize, and scale GenAI image generation for your use case.\n\nCompute Service: run your choice of OSS fine-tuned or custom models on the service for your use case.\n\n\n\n\n\n\n\n\nImproved",
    "hierarchy": {
      "h0": {
        "title": "October 2023 Release Notes"
      },
      "h2": {
        "id": "october-31-2023",
        "title": "October 31, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.october-october-26-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/october",
    "pathname": "/docs/release-notes/2023/october",
    "title": "October 26, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#october-26-2023",
    "content": "OctoAI is now SOC 2 Type II certified.\n\n\nOur most recent security enhancement showcases OctoAI provides processes and safeguards that secure customer data as verified by a third party auditor.",
    "hierarchy": {
      "h0": {
        "title": "October 2023 Release Notes"
      },
      "h2": {
        "id": "october-26-2023",
        "title": "October 26, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.september-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/september",
    "pathname": "/docs/release-notes/2023/september",
    "title": "September 2023 Release Notes",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for September 2023",
    "content": "Added"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.september-september-20-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/september",
    "pathname": "/docs/release-notes/2023/september",
    "title": "September 20, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#september-20-2023",
    "content": "OctoAI made improvements to SDXL endpoint, and added support to multi-user accounts.\n\n\nWe have improved OctoAI's SDXL. Our latency-optimized SDXL endpoints now take about 2.8s to generate a 30-step image. We also have a cost-optimized SDXL endpoint that takes about 8s for 30 steps.\n\nAdded support for multi-user accounts, which allows your team to manage endpoints, view logs & metrics, and securely share access to an account.\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "September 2023 Release Notes"
      },
      "h2": {
        "id": "september-20-2023",
        "title": "September 20, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.september-september-14-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/september",
    "pathname": "/docs/release-notes/2023/september",
    "title": "September 14, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#september-14-2023",
    "content": "OctoAI made an addition to it's Github repo to help users start building your own app on OctoAI, and a new audio generation endpoint.\n\n\nCheck out our github repo for template applications to help you get started on building your own app with OctoAI. Right now, we have an example using Python and deployable on Streamlit as well as one using TypeScript and deployable on Vercel.\n\nWe also have a new audio generation endpoint available under private preview (e.g. Bark, Tortoise TTS).",
    "hierarchy": {
      "h0": {
        "title": "September 2023 Release Notes"
      },
      "h2": {
        "id": "september-14-2023",
        "title": "September 14, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.august-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/august",
    "pathname": "/docs/release-notes/2023/august",
    "title": "August 2023 Release Notes",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for August 2023",
    "content": "Added"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.august-august-30-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/august",
    "pathname": "/docs/release-notes/2023/august",
    "title": "August 30, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#august-30-2023",
    "content": "OctoAI made several additions including: Llama 2 70B model, real-time streaming capabilities to Whisper, and updated the domain for newly created endpoints.\n\n\nAdded Llama2 70B quickstart template endpoint. We can also host custom Llama2 LoRAs/ checkpoints.\n\nEnabled users to upload data via URL in the authoring experience (CLI + Python SDK)\n\nAdded real-time streaming capabilities to our Whisper audio flow, with a React hook called use Whisper for ease of integration into web/mobile apps. Learn more about this change.\n\nChanged the domain for all newly created endpoints from octoai.cloud to octoai.run Existing endpoints on octoai.cloud will still work, but we suggest that you start changing your code to call endpoints from octoai.run instead of octoai.cloud, since we'll also update existing endpoints in about a month.\n\n\n\n\n\n\n\n\nImproved",
    "hierarchy": {
      "h0": {
        "title": "August 2023 Release Notes"
      },
      "h2": {
        "id": "august-30-2023",
        "title": "August 30, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.august-august-16-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/august",
    "pathname": "/docs/release-notes/2023/august",
    "title": "August 16, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#august-16-2023",
    "content": "There were some new additions to our docs about Image Generation, some improvements in the backend, and a faster version of Stable Diffusion 1.5.\n\n\nAdded a new section in our Docs on Image Generation, including how to fine-tune and use Stable Diffusion.\n\nReduced cold start substantially on endpoints created with our authoring experience (can be multiple minutes of improvement depending on the model). Upgrade to the latest version of the CLI and SDK and author new endpoints to get faster cold start for your custom models.\n\nImproved error boundaries in the UI. Users would be less likely to run into the \"Whoops Beta Mode engaged\" message in the UI.\n\nEnabled concurrency handling improvements to all new endpoints created from now on. We will also be gradually rolling out this change on previously created endpoints in upcoming weeks.\n\nA faster version of SD XL with dimension 1024x1024 is now available under private preview. We'd be gradually rolling out this new version over the next week or so.\n\nReminder: OctoAI's quickstart template endpoints are for demo/testing purposes only. On these endpoints, we rate-limit to 15 inferences per hour. If you would like to exceed this limit for production use, please clone the endpoint to your own account.\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "August 2023 Release Notes"
      },
      "h2": {
        "id": "august-16-2023",
        "title": "August 16, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.august-august-10-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/august",
    "pathname": "/docs/release-notes/2023/august",
    "title": "August 10, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#august-10-2023",
    "content": "There have been some additions to the OctoAI platform: Stable Diffusion 1.5 template feature additions, Whisper feature additions, and private registry updates.\n\n\nWhisper Template Feature Additions: Multi-hour long audio files are now supported. Furthermore, you can specify a URL to the audio input file (e.g. MP3, WAV, or MP4 formats), instead of uploading a file from your local environment.\n\nPrivate Registry: OctoAI's container authoring experience has been upgraded. Users are no longer required to provide registry credentials to get started. Images can be uploaded directly to a private OctoAI Registry. User uploaded images to OctoAI's Registry are accessible only to you and OctoAI services i.e. no other user can view or access your images.\n\nStable Diffusion 1.5 Template Feature Additions: OctoAI's Stable Diffusion endpoint, running on A10Gs, has been upgraded to include the following features to help users customize styling and achieve higher-quality images:\nPopular Checkpoints like DreamShaper and Realistic Vision, Low Rank Adaptations (LoRAs), and Textual Inversions. Note: LoRA weights must sum up to 1.\n\nAdditional image dimensions.\n\nWe updated the web user interface.",
    "hierarchy": {
      "h0": {
        "title": "August 2023 Release Notes"
      },
      "h2": {
        "id": "august-10-2023",
        "title": "August 10, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.july-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/july",
    "pathname": "/docs/release-notes/2023/july",
    "title": "July 2023 Release Notes",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for July 2023",
    "content": "Added"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.july-july-26-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/july",
    "pathname": "/docs/release-notes/2023/july",
    "title": "July 26, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#july-26-2023",
    "content": "OctoAI added several new things including better graceful concurrency handling, updated Python SDK, and diarization to Whisper model template.\n\n\nAdded more graceful concurrency handling: when users send more than N concurrent request to an endpoint with N replicas actively running, we will queue all extra requests instead of failing them. This queuing behavior has been activated for selected customers, and will be gradually rolled out over this week and next week. You will temporarily see a new replica spin up while the rollout is occurring on your endpoint.\n\nUpdated our Python SDK from 0.1.2 to 0.2.0—it now support both streaming and async inference requests.\n\nAdded diarization to our Whisper template endpoint and rectified the list of languages supported. Diarization enables use cases where you'd like to identify the speaker of each segment in a speech recording. You can view the full API specs in the Whisper demo template. Here's an example of how to use the template with diarization:\n\n\n\n\n\n\n\n\nImproved",
    "code_snippets": [
      {
        "lang": "python",
        "code": "    import requests\n    import base64\n\n    def download_file(url, filename):\n        response = requests.get(url)\n        if response.status_code == 200:\n            with open(filename, \"wb\") as f:\n                f.write(response.content)\n            print(f\"File downloaded successfully as {filename}.\")\n        else:\n            print(f\"Failed to download the file. Status code: {response.status_code}\")\n\n\n    def make_post_request(filename):\n        with open(filename, \"rb\") as f:\n            encoded_audio = base64.b64encode(f.read()).decode(\"utf-8\")\n\n        headers = {\n            \"Content-Type\": \"application/json\"\n        }\n        data = {\n            \"audio\": encoded_audio,\n            \"task\": \"transcribe\",\n            \"diarize\": True\n        }\n\n        response = requests.post(\"https://whisper-demo-kk0powt97tmb.octoai.cloud/predict\", json=data, headers=headers)\n\n        if response.status_code == 200:\n            # Handle the successful response here\n            json_response = response.json()\n\n            for seg in json_response[\"response\"][\"segments\"]:\n                print(seg)\n\n        else:\n            print(f\"Request failed with status code: {response.status_code}\")\n\n    if __name__ == \"__main__\":\n        url = \"<YOUR_FILE_HERE>.wav\"\n        filename = \"sample.wav\"\n\n        download_file(url, filename)\n\n        make_post_request(filename)"
      },
      {
        "lang": "python",
        "code": "    import requests\n    import base64\n\n    def download_file(url, filename):\n        response = requests.get(url)\n        if response.status_code == 200:\n            with open(filename, \"wb\") as f:\n                f.write(response.content)\n            print(f\"File downloaded successfully as {filename}.\")\n        else:\n            print(f\"Failed to download the file. Status code: {response.status_code}\")\n\n\n    def make_post_request(filename):\n        with open(filename, \"rb\") as f:\n            encoded_audio = base64.b64encode(f.read()).decode(\"utf-8\")\n\n        headers = {\n            \"Content-Type\": \"application/json\"\n        }\n        data = {\n            \"audio\": encoded_audio,\n            \"task\": \"transcribe\",\n            \"diarize\": True\n        }\n\n        response = requests.post(\"https://whisper-demo-kk0powt97tmb.octoai.cloud/predict\", json=data, headers=headers)\n\n        if response.status_code == 200:\n            # Handle the successful response here\n            json_response = response.json()\n\n            for seg in json_response[\"response\"][\"segments\"]:\n                print(seg)\n\n        else:\n            print(f\"Request failed with status code: {response.status_code}\")\n\n    if __name__ == \"__main__\":\n        url = \"<YOUR_FILE_HERE>.wav\"\n        filename = \"sample.wav\"\n\n        download_file(url, filename)\n\n        make_post_request(filename)"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "July 2023 Release Notes"
      },
      "h2": {
        "id": "july-26-2023",
        "title": "July 26, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.july-july-20-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/july",
    "pathname": "/docs/release-notes/2023/july",
    "title": "July 20, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#july-20-2023",
    "content": "Added an OctoAI template for Llama2-7B Chat.\n\n\nAdded an OctoAI template for Llama2-7B Chat, which is an instruction-tuned model for chatbots.\nUsers can now work with this brand-new to the market LLM directly in the web UI with limited\ntoken response or programmatically with additional optionality. A similar template for Llama2-70B is coming soon!\n\n\n\n\n\n\n\n\nFixed",
    "hierarchy": {
      "h0": {
        "title": "July 2023 Release Notes"
      },
      "h2": {
        "id": "july-20-2023",
        "title": "July 20, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.july-july-18-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/july",
    "pathname": "/docs/release-notes/2023/july",
    "title": "July 18, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#july-18-2023",
    "content": "Changed the HTTP status code to 201 for the REST API calls for create secret and create registry credentials.  Previously, we returned 200 for these calls.\n\n\nChanged the HTTP status code to 201 for the REST API calls for create secret and create registry credentials. Previously, we returned 200 for these calls. The behavior of the SDK and web frontend is not affected.",
    "hierarchy": {
      "h0": {
        "title": "July 2023 Release Notes"
      },
      "h2": {
        "id": "july-18-2023",
        "title": "July 18, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.june-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/june",
    "pathname": "/docs/release-notes/2023/june",
    "title": "June 2023 Release Notes",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for June 2023",
    "content": "Added"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.june-june-27-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/june",
    "pathname": "/docs/release-notes/2023/june",
    "title": "June 27, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#june-27-2023",
    "content": "New tutorials showing how to use Automatic1111's Stable Diffusion web UI and an updated Falcon template.\n\n\nReleased a Doc tutorial to show users how to use OctoAI's server class GPUs with Automatic1111 Stable Diffusion web user interface.\n\nReleased a video tutorial to show users how to apply custom model checkpoints using Automatic1111's Stable Diffusion web user interface on OctoAI.\n\nUpdated our Falcon template to use a different server implementation behind the scenes. The inference API is now available at /generate, but inferences at /predict will continue to work.\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "June 2023 Release Notes"
      },
      "h2": {
        "id": "june-27-2023",
        "title": "June 27, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.june-june-14-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/june",
    "pathname": "/docs/release-notes/2023/june",
    "title": "June 14, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#june-14-2023",
    "content": "We have launched OctoAI into general availability and made several updates to our models and endpoints.\n\n\nWith the launch of our service, changes will be made to our billing. You can find pricing plans and hardware options here. Changes and new user incentives taken into immediate effect are noted below:\nTomorrow, June 13th, any existing endpoints will be set to min replicas=0 so that you are not billed for an instance unintentionally left active and running. Be prepared for a cold start before your first inference and reset to min replicas=1 if you prefer to keep the instance warm.\n\nEvery user who logs in during public beta will receive credits for 2 free compute hrs on A100 (or 10+ hrs on A10!) to use in their first two weeks.\n\nThe first 500 users to create a new endpoint will receive credits for 12 free compute hrs on A100 (or 50+ hrs on A10!) to use within their first month.\n\n\n\nYou now have two options to integrate OctoAI endpoints into your application:\nOur new Python client (supports synchronous inference).\n\nOur HTTP REST API now supports both synchronous and asynchronous calls allowing users to request inference without persisting a connection, poll for status, and retrieve the completed prediction data. This is most effective when managing longer running requests.\n\n\n\nWe've updated our Whisper model to be much faster.\n\nWe've also added MPT 7B and Vicuña 7B as new quick-start templates as better alternatives to Dolly, which will be removed soon.\n\n\n\n\n\n\n\n\nAdded",
    "hierarchy": {
      "h0": {
        "title": "June 2023 Release Notes"
      },
      "h2": {
        "id": "june-14-2023",
        "title": "June 14, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.june-june-6-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/june",
    "pathname": "/docs/release-notes/2023/june",
    "title": "June 6, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#june-6-2023",
    "content": "Some additions to OctoAI on June 6, 2023. Added private registry and the ability for users to mount secrets and other environment variables.\n\n\nPrivate registry control\nAdded the ability for users to pull containers from private registries by applying registry credentials to an endpoint. See Pulling containers from a private registry for a guide.\n\n\nSecrets and environment variables\nAdded the ability for users to mount secrets and other environment variables into their containers within an endpoint. See Setting up secrets or environment variables for your custom endpoints for a guide.\n\n\nA100 GPUs\nNVIDIA A100s are back online as of 5pm PDT on June 6th. The A100s were temporarily taken down earlier this week as maintenance was being performed to update the version of CUDA. User requests and hardware options are now functioning business as usual.",
    "hierarchy": {
      "h0": {
        "title": "June 2023 Release Notes"
      },
      "h2": {
        "id": "june-6-2023",
        "title": "June 6, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.may-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/may",
    "pathname": "/docs/release-notes/2023/may",
    "title": "May 2023 Release Notes",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "OctoAI product updates and release notes for May 2023",
    "content": "Improved"
  },
  {
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.may-may-31-2023-chunk:0",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/release-notes/2023/may",
    "pathname": "/docs/release-notes/2023/may",
    "title": "May 31, 2023",
    "breadcrumb": [
      {
        "title": "2023",
        "pathname": "/docs/release-notes/2023"
      }
    ],
    "tab": {
      "title": "Release Notes",
      "pathname": "/docs/release-notes"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#may-31-2023",
    "content": "Added support for NVIDIA A100s with 80GB of memory, and the ability for users to specify the health check server path.\n\n\nAdded support for NVIDIA A100s with 80GB of memory enabling faster inference and higher memory bandwidth. OctoAI's compute service allows users to start with a single A100 and scale up as traffic increases without paying for idle hardware.\n\nReleased a video tutorial to show users how to apply custom model checkpoints using Automatic1111's Stable Diffusion web user interface on OctoAI.",
    "hierarchy": {
      "h0": {
        "title": "May 2023 Release Notes"
      },
      "h2": {
        "id": "may-31-2023",
        "title": "May 31, 2023"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_account.endpoint_account.getAccount",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/account/get-account",
    "pathname": "/docs/api-reference/octoai-api/account/get-account",
    "title": "Get fields on an account",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Account",
        "pathname": "/docs/api-reference/octoai-api/account"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_account.getAccount",
    "method": "GET",
    "endpoint_path": "/v1/account",
    "endpoint_path_alternates": [
      "/v1/account",
      "https://api.octoai.cloud/v1/account",
      "https://api.octoai.cloud/v1/account"
    ],
    "response_type": "json",
    "description": "Return fields on an account",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_account.endpoint_account.patchAccount",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/account/patch-account",
    "pathname": "/docs/api-reference/octoai-api/account/patch-account",
    "title": "Update fields on an account",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Account",
        "pathname": "/docs/api-reference/octoai-api/account"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_account.patchAccount",
    "method": "PATCH",
    "endpoint_path": "/v1/account",
    "endpoint_path_alternates": [
      "/v1/account",
      "https://api.octoai.cloud/v1/account",
      "https://api.octoai.cloud/v1/account"
    ],
    "response_type": "json",
    "description": "Update fields on account",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.createEndpoint",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/create-endpoint",
    "pathname": "/docs/api-reference/octoai-api/endpoint/create-endpoint",
    "title": "Create new endpoint",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Endpoint",
        "pathname": "/docs/api-reference/octoai-api/endpoint"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.createEndpoint",
    "method": "POST",
    "endpoint_path": "/v1/endpoint",
    "endpoint_path_alternates": [
      "/v1/endpoint",
      "https://api.octoai.cloud/v1/endpoint",
      "https://api.octoai.cloud/v1/endpoint"
    ],
    "response_type": "json",
    "description": "Create new endpoint",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.getEndpoint",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/get-endpoint",
    "pathname": "/docs/api-reference/octoai-api/endpoint/get-endpoint",
    "title": "Get description of an endpoint",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Endpoint",
        "pathname": "/docs/api-reference/octoai-api/endpoint"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.getEndpoint",
    "method": "GET",
    "endpoint_path": "/v1/endpoint/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D"
    ],
    "response_type": "json",
    "description": "Return description of an endpoint",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.deleteEndpoint",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/delete-endpoint",
    "pathname": "/docs/api-reference/octoai-api/endpoint/delete-endpoint",
    "title": "Delete an endpoint",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Endpoint",
        "pathname": "/docs/api-reference/octoai-api/endpoint"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.deleteEndpoint",
    "method": "DELETE",
    "endpoint_path": "/v1/endpoint/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D"
    ],
    "response_type": "json",
    "description": "Delete an endpoint",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.patchEndpoint",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/patch-endpoint",
    "pathname": "/docs/api-reference/octoai-api/endpoint/patch-endpoint",
    "title": "Update an endpoint",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Endpoint",
        "pathname": "/docs/api-reference/octoai-api/endpoint"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.patchEndpoint",
    "method": "PATCH",
    "endpoint_path": "/v1/endpoint/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D"
    ],
    "response_type": "json",
    "description": "Updates an endpoint",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.getEndpoints",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/get-endpoints",
    "pathname": "/docs/api-reference/octoai-api/endpoint/get-endpoints",
    "title": "List endpoints",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Endpoint",
        "pathname": "/docs/api-reference/octoai-api/endpoint"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.getEndpoints",
    "method": "GET",
    "endpoint_path": "/v1/endpoints",
    "endpoint_path_alternates": [
      "/v1/endpoints",
      "https://api.octoai.cloud/v1/endpoints",
      "https://api.octoai.cloud/v1/endpoints"
    ],
    "response_type": "json",
    "description": "Return list of endpoints",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "EndpointResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.getContainerMetadata",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/get-container-metadata",
    "pathname": "/docs/api-reference/octoai-api/endpoint/get-container-metadata",
    "title": "Get container metadata",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Endpoint",
        "pathname": "/docs/api-reference/octoai-api/endpoint"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.getContainerMetadata",
    "method": "GET",
    "endpoint_path": "/v1/endpoint/:endpoint_name/container/metadata",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}/container/metadata",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name/container/metadata",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D/container/metadata"
    ],
    "response_type": "json",
    "description": "Return container metadata",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.getEndpointVolumeToken",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/get-endpoint-volume-token",
    "pathname": "/docs/api-reference/octoai-api/endpoint/get-endpoint-volume-token",
    "title": "Get token for accessing the volume",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Endpoint",
        "pathname": "/docs/api-reference/octoai-api/endpoint"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.getEndpointVolumeToken",
    "method": "GET",
    "endpoint_path": "/v1/endpoint/:endpoint_name/volume_token",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}/volume_token",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name/volume_token",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D/volume_token"
    ],
    "response_type": "json",
    "description": "Returns a token for accessing the volume",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_telemetry.endpoint_telemetry.getEndpointLogs",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-logs",
    "pathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-logs",
    "title": "Get endpoint logs",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Telemetry",
        "pathname": "/docs/api-reference/octoai-api/telemetry"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_telemetry.getEndpointLogs",
    "method": "GET",
    "endpoint_path": "/v1/logs/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/logs/{endpoint_name}",
      "https://api.octoai.cloud/v1/logs/:endpoint_name",
      "https://api.octoai.cloud/v1/logs/%7Bendpoint_name%7D"
    ],
    "response_type": "json",
    "description": "Return endpoint logs",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "LogEntry"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_telemetry.endpoint_telemetry.getEndpointLogsStream",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-logs-stream",
    "pathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-logs-stream",
    "title": "Get endpoint logs as stream",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Telemetry",
        "pathname": "/docs/api-reference/octoai-api/telemetry"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_telemetry.getEndpointLogsStream",
    "method": "GET",
    "endpoint_path": "/v1/logs/:endpoint_name/stream",
    "endpoint_path_alternates": [
      "/v1/logs/{endpoint_name}/stream",
      "https://api.octoai.cloud/v1/logs/:endpoint_name/stream",
      "https://api.octoai.cloud/v1/logs/%7Bendpoint_name%7D/stream"
    ],
    "response_type": "json",
    "description": "Return endpoint logs as stream",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "LogEntry"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_telemetry.endpoint_telemetry.getEndpointEvents",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-events",
    "pathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-events",
    "title": "Get endpoint events",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Telemetry",
        "pathname": "/docs/api-reference/octoai-api/telemetry"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_telemetry.getEndpointEvents",
    "method": "GET",
    "endpoint_path": "/v1/events/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/events/{endpoint_name}",
      "https://api.octoai.cloud/v1/events/:endpoint_name",
      "https://api.octoai.cloud/v1/events/%7Bendpoint_name%7D"
    ],
    "response_type": "json",
    "description": "Return endpoint events",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "EventEntry"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_telemetry.endpoint_telemetry.getEndpointEventsStream",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-events-stream",
    "pathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-events-stream",
    "title": "Get endpoint events as stream",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Telemetry",
        "pathname": "/docs/api-reference/octoai-api/telemetry"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_telemetry.getEndpointEventsStream",
    "method": "GET",
    "endpoint_path": "/v1/events/:endpoint_name/stream",
    "endpoint_path_alternates": [
      "/v1/events/{endpoint_name}/stream",
      "https://api.octoai.cloud/v1/events/:endpoint_name/stream",
      "https://api.octoai.cloud/v1/events/%7Bendpoint_name%7D/stream"
    ],
    "response_type": "json",
    "description": "Return endpoint events as stream",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "EventEntry"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.createSecret",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/create-secret",
    "pathname": "/docs/api-reference/octoai-api/secret/create-secret",
    "title": "Create new secret",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Secret",
        "pathname": "/docs/api-reference/octoai-api/secret"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.createSecret",
    "method": "POST",
    "endpoint_path": "/v1/secret",
    "endpoint_path_alternates": [
      "/v1/secret",
      "https://api.octoai.cloud/v1/secret",
      "https://api.octoai.cloud/v1/secret"
    ],
    "response_type": "json",
    "description": "Create new secret",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.getSecret",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/get-secret",
    "pathname": "/docs/api-reference/octoai-api/secret/get-secret",
    "title": "Retrieve secret by key",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Secret",
        "pathname": "/docs/api-reference/octoai-api/secret"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.getSecret",
    "method": "GET",
    "endpoint_path": "/v1/secret/:key",
    "endpoint_path_alternates": [
      "/v1/secret/{key}",
      "https://api.octoai.cloud/v1/secret/:key",
      "https://api.octoai.cloud/v1/secret/%7Bkey%7D"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.updateSecret",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/update-secret",
    "pathname": "/docs/api-reference/octoai-api/secret/update-secret",
    "title": "Update secret by key",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Secret",
        "pathname": "/docs/api-reference/octoai-api/secret"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.updateSecret",
    "method": "PUT",
    "endpoint_path": "/v1/secret/:key",
    "endpoint_path_alternates": [
      "/v1/secret/{key}",
      "https://api.octoai.cloud/v1/secret/:key",
      "https://api.octoai.cloud/v1/secret/%7Bkey%7D"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.deleteSecret",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/delete-secret",
    "pathname": "/docs/api-reference/octoai-api/secret/delete-secret",
    "title": "Delete secret by key",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Secret",
        "pathname": "/docs/api-reference/octoai-api/secret"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.deleteSecret",
    "method": "DELETE",
    "endpoint_path": "/v1/secret/:key",
    "endpoint_path_alternates": [
      "/v1/secret/{key}",
      "https://api.octoai.cloud/v1/secret/:key",
      "https://api.octoai.cloud/v1/secret/%7Bkey%7D"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.getSecrets",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/get-secrets",
    "pathname": "/docs/api-reference/octoai-api/secret/get-secrets",
    "title": "Get secret",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Secret",
        "pathname": "/docs/api-reference/octoai-api/secret"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.getSecrets",
    "method": "GET",
    "endpoint_path": "/v1/secrets",
    "endpoint_path_alternates": [
      "/v1/secrets",
      "https://api.octoai.cloud/v1/secrets",
      "https://api.octoai.cloud/v1/secrets"
    ],
    "response_type": "json",
    "description": "Return list of secrets",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.createRegistryCredential",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/create-registry-credential",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/create-registry-credential",
    "title": "Create new registry credentials",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Registry Credential",
        "pathname": "/docs/api-reference/octoai-api/registry-credential"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.createRegistryCredential",
    "method": "POST",
    "endpoint_path": "/v1/registry-credential",
    "endpoint_path_alternates": [
      "/v1/registry-credential",
      "https://api.octoai.cloud/v1/registry-credential",
      "https://api.octoai.cloud/v1/registry-credential"
    ],
    "response_type": "json",
    "description": "Create new registry credentials",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.getRegistryCredential",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/get-registry-credential",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/get-registry-credential",
    "title": "Retrieve registry credentials by key",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Registry Credential",
        "pathname": "/docs/api-reference/octoai-api/registry-credential"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.getRegistryCredential",
    "method": "GET",
    "endpoint_path": "/v1/registry-credential/:key",
    "endpoint_path_alternates": [
      "/v1/registry-credential/{key}",
      "https://api.octoai.cloud/v1/registry-credential/:key",
      "https://api.octoai.cloud/v1/registry-credential/%7Bkey%7D"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.updateRegistryCredential",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/update-registry-credential",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/update-registry-credential",
    "title": "Update registry credentials",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Registry Credential",
        "pathname": "/docs/api-reference/octoai-api/registry-credential"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.updateRegistryCredential",
    "method": "PUT",
    "endpoint_path": "/v1/registry-credential/:key",
    "endpoint_path_alternates": [
      "/v1/registry-credential/{key}",
      "https://api.octoai.cloud/v1/registry-credential/:key",
      "https://api.octoai.cloud/v1/registry-credential/%7Bkey%7D"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.deleteRegistryCredential",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/delete-registry-credential",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/delete-registry-credential",
    "title": "Delete registry credentials",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Registry Credential",
        "pathname": "/docs/api-reference/octoai-api/registry-credential"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.deleteRegistryCredential",
    "method": "DELETE",
    "endpoint_path": "/v1/registry-credential/:key",
    "endpoint_path_alternates": [
      "/v1/registry-credential/{key}",
      "https://api.octoai.cloud/v1/registry-credential/:key",
      "https://api.octoai.cloud/v1/registry-credential/%7Bkey%7D"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.getRegistryCredentials",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/get-registry-credentials",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/get-registry-credentials",
    "title": "Get registry credentials",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Registry Credential",
        "pathname": "/docs/api-reference/octoai-api/registry-credential"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.getRegistryCredentials",
    "method": "GET",
    "endpoint_path": "/v1/registry-credentials",
    "endpoint_path_alternates": [
      "/v1/registry-credentials",
      "https://api.octoai.cloud/v1/registry-credentials",
      "https://api.octoai.cloud/v1/registry-credentials"
    ],
    "response_type": "json",
    "description": "Return list of registry credentials",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "RegistryCredentialSummary"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_publicEndpoint.endpoint_publicEndpoint.getPublicEndpoints",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/public-endpoint/get-public-endpoints",
    "pathname": "/docs/api-reference/octoai-api/public-endpoint/get-public-endpoints",
    "title": "List public, OctoAI-deployed endpoints",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Public Endpoint",
        "pathname": "/docs/api-reference/octoai-api/public-endpoint"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_publicEndpoint.getPublicEndpoints",
    "method": "GET",
    "endpoint_path": "/v1/public-endpoints",
    "endpoint_path_alternates": [
      "/v1/public-endpoints",
      "https://api.octoai.cloud/v1/public-endpoints",
      "https://api.octoai.cloud/v1/public-endpoints"
    ],
    "response_type": "json",
    "description": "Return list of public, OctoAI-deployed endpoints",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HostedEndpoint"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_instanceTypes.endpoint_instanceTypes.getInstanceTypes",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/octoai-api/instance-types/get-instance-types",
    "pathname": "/docs/api-reference/octoai-api/instance-types/get-instance-types",
    "title": "List hardware instance types",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference/octoai-api"
      },
      {
        "title": "Instance Types",
        "pathname": "/docs/api-reference/octoai-api/instance-types"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_instanceTypes.getInstanceTypes",
    "method": "GET",
    "endpoint_path": "/v1/instance-types",
    "endpoint_path_alternates": [
      "/v1/instance-types",
      "https://api.octoai.cloud/v1/instance-types",
      "https://api.octoai.cloud/v1/instance-types"
    ],
    "response_type": "json",
    "description": "Return list of available instance types to deploy an endpoint to",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "InstanceTypeInfo"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.list",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/asset-library/list",
    "pathname": "/docs/api-reference/asset-library/list",
    "title": "List Assets",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Asset Library",
        "pathname": "/docs/api-reference/asset-library"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.list",
    "method": "GET",
    "endpoint_path": "/v1/assets",
    "endpoint_path_alternates": [
      "/v1/assets",
      "https://api.octoai.cloud/v1/assets",
      "https://api.securelink.octo.ai/v1/assets",
      "https://api.octoai.cloud/v1/assets",
      "https://api.securelink.octo.ai/v1/assets"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "DataType",
      "AssetType",
      "BaseEngine",
      "BaseEngineType",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.create",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/asset-library/create",
    "pathname": "/docs/api-reference/asset-library/create",
    "title": "Create Asset",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Asset Library",
        "pathname": "/docs/api-reference/asset-library"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.create",
    "method": "POST",
    "endpoint_path": "/v1/assets",
    "endpoint_path_alternates": [
      "/v1/assets",
      "https://api.octoai.cloud/v1/assets",
      "https://api.securelink.octo.ai/v1/assets",
      "https://api.octoai.cloud/v1/assets",
      "https://api.securelink.octo.ai/v1/assets"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "AssetType",
      "Data",
      "TransferApiType",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.delete",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/asset-library/delete",
    "pathname": "/docs/api-reference/asset-library/delete",
    "title": "Delete Asset",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Asset Library",
        "pathname": "/docs/api-reference/asset-library"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.delete",
    "method": "DELETE",
    "endpoint_path": "/v1/assets/:asset_id",
    "endpoint_path_alternates": [
      "/v1/assets/{asset_id}",
      "https://api.octoai.cloud/v1/assets/:asset_id",
      "https://api.securelink.octo.ai/v1/assets/:asset_id",
      "https://api.octoai.cloud/v1/assets/%7Basset_id%7D",
      "https://api.securelink.octo.ai/v1/assets/%7Basset_id%7D"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.completeUpload",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/asset-library/complete-upload",
    "pathname": "/docs/api-reference/asset-library/complete-upload",
    "title": "Complete Asset Upload",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Asset Library",
        "pathname": "/docs/api-reference/asset-library"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.completeUpload",
    "method": "POST",
    "endpoint_path": "/v1/assets/:asset_id/complete-upload",
    "endpoint_path_alternates": [
      "/v1/assets/{asset_id}/complete-upload",
      "https://api.octoai.cloud/v1/assets/:asset_id/complete-upload",
      "https://api.securelink.octo.ai/v1/assets/:asset_id/complete-upload",
      "https://api.octoai.cloud/v1/assets/%7Basset_id%7D/complete-upload",
      "https://api.securelink.octo.ai/v1/assets/%7Basset_id%7D/complete-upload"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.get",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/asset-library/get",
    "pathname": "/docs/api-reference/asset-library/get",
    "title": "Retrieve Asset",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Asset Library",
        "pathname": "/docs/api-reference/asset-library"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.get",
    "method": "GET",
    "endpoint_path": "/v1/assets/:asset_owner_and_name_or_id",
    "endpoint_path_alternates": [
      "/v1/assets/{asset_owner_and_name_or_id}",
      "https://api.octoai.cloud/v1/assets/:asset_owner_and_name_or_id",
      "https://api.securelink.octo.ai/v1/assets/:asset_owner_and_name_or_id",
      "https://api.octoai.cloud/v1/assets/%7Basset_owner_and_name_or_id%7D",
      "https://api.securelink.octo.ai/v1/assets/%7Basset_owner_and_name_or_id%7D"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "TransferApiType",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.create",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/fine-tuning/create",
    "pathname": "/docs/api-reference/fine-tuning/create",
    "title": "Create Tune",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Fine Tuning",
        "pathname": "/docs/api-reference/fine-tuning"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.create",
    "method": "POST",
    "endpoint_path": "/v1/tune",
    "endpoint_path_alternates": [
      "/v1/tune",
      "https://api.octoai.cloud/v1/tune",
      "https://api.securelink.octo.ai/v1/tune",
      "https://api.octoai.cloud/v1/tune",
      "https://api.securelink.octo.ai/v1/tune"
    ],
    "response_type": "json",
    "description": "Spawn a tune.",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "Details",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.get",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/fine-tuning/get",
    "pathname": "/docs/api-reference/fine-tuning/get",
    "title": "Get Tune",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Fine Tuning",
        "pathname": "/docs/api-reference/fine-tuning"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.get",
    "method": "GET",
    "endpoint_path": "/v1/tune/:tune_id",
    "endpoint_path_alternates": [
      "/v1/tune/{tune_id}",
      "https://api.octoai.cloud/v1/tune/:tune_id",
      "https://api.securelink.octo.ai/v1/tune/:tune_id",
      "https://api.octoai.cloud/v1/tune/%7Btune_id%7D",
      "https://api.securelink.octo.ai/v1/tune/%7Btune_id%7D"
    ],
    "response_type": "json",
    "description": "Get the specific tune.",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.delete",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/fine-tuning/delete",
    "pathname": "/docs/api-reference/fine-tuning/delete",
    "title": "Delete Tune",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Fine Tuning",
        "pathname": "/docs/api-reference/fine-tuning"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.delete",
    "method": "DELETE",
    "endpoint_path": "/v1/tune/:tune_id",
    "endpoint_path_alternates": [
      "/v1/tune/{tune_id}",
      "https://api.octoai.cloud/v1/tune/:tune_id",
      "https://api.securelink.octo.ai/v1/tune/:tune_id",
      "https://api.octoai.cloud/v1/tune/%7Btune_id%7D",
      "https://api.securelink.octo.ai/v1/tune/%7Btune_id%7D"
    ],
    "response_type": "json",
    "description": "Delete the specified tune.",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.cancel",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/fine-tuning/cancel",
    "pathname": "/docs/api-reference/fine-tuning/cancel",
    "title": "Cancel Tune",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Fine Tuning",
        "pathname": "/docs/api-reference/fine-tuning"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.cancel",
    "method": "POST",
    "endpoint_path": "/v1/tune/:tune_id/cancel",
    "endpoint_path_alternates": [
      "/v1/tune/{tune_id}/cancel",
      "https://api.octoai.cloud/v1/tune/:tune_id/cancel",
      "https://api.securelink.octo.ai/v1/tune/:tune_id/cancel",
      "https://api.octoai.cloud/v1/tune/%7Btune_id%7D/cancel",
      "https://api.securelink.octo.ai/v1/tune/%7Btune_id%7D/cancel"
    ],
    "response_type": "json",
    "description": "Cancel the specified tune.",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.list",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/fine-tuning/list",
    "pathname": "/docs/api-reference/fine-tuning/list",
    "title": "List Tunes",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Fine Tuning",
        "pathname": "/docs/api-reference/fine-tuning"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.list",
    "method": "GET",
    "endpoint_path": "/v1/tunes",
    "endpoint_path_alternates": [
      "/v1/tunes",
      "https://api.octoai.cloud/v1/tunes",
      "https://api.securelink.octo.ai/v1/tunes",
      "https://api.octoai.cloud/v1/tunes",
      "https://api.securelink.octo.ai/v1/tunes"
    ],
    "response_type": "json",
    "description": "List all tunes owned by the current user.",
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud"
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "TuneType",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateSsd",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/image-gen/generate-ssd",
    "pathname": "/docs/api-reference/image-gen/generate-ssd",
    "title": "Generate SSD",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Image Gen",
        "pathname": "/docs/api-reference/image-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateSsd",
    "method": "POST",
    "endpoint_path": "/generate/ssd",
    "endpoint_path_alternates": [
      "/generate/ssd",
      "https://image.octoai.run/generate/ssd",
      "https://image.securelink.octo.ai/generate/ssd",
      "https://image.octoai.run/generate/ssd",
      "https://image.securelink.octo.ai/generate/ssd"
    ],
    "response_type": "json",
    "description": "Generate images in response to the given request.",
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateControlnetSdxl",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/image-gen/generate-controlnet-sdxl",
    "pathname": "/docs/api-reference/image-gen/generate-controlnet-sdxl",
    "title": "Generate ControlNet SDXL",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Image Gen",
        "pathname": "/docs/api-reference/image-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateControlnetSdxl",
    "method": "POST",
    "endpoint_path": "/generate/controlnet-sdxl",
    "endpoint_path_alternates": [
      "/generate/controlnet-sdxl",
      "https://image.octoai.run/generate/controlnet-sdxl",
      "https://image.securelink.octo.ai/generate/controlnet-sdxl",
      "https://image.octoai.run/generate/controlnet-sdxl",
      "https://image.securelink.octo.ai/generate/controlnet-sdxl"
    ],
    "response_type": "json",
    "description": "Generate images in response to the given request.",
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateControlnetSd15",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/image-gen/generate-controlnet-sd-15",
    "pathname": "/docs/api-reference/image-gen/generate-controlnet-sd-15",
    "title": "Generate ControlNet SD1.5",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Image Gen",
        "pathname": "/docs/api-reference/image-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateControlnetSd15",
    "method": "POST",
    "endpoint_path": "/generate/controlnet-sd15",
    "endpoint_path_alternates": [
      "/generate/controlnet-sd15",
      "https://image.octoai.run/generate/controlnet-sd15",
      "https://image.securelink.octo.ai/generate/controlnet-sd15",
      "https://image.octoai.run/generate/controlnet-sd15",
      "https://image.securelink.octo.ai/generate/controlnet-sd15"
    ],
    "response_type": "json",
    "description": "Generate images in response to the given request.",
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateSdxl",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/image-gen/generate-sdxl",
    "pathname": "/docs/api-reference/image-gen/generate-sdxl",
    "title": "Generate SDXL",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Image Gen",
        "pathname": "/docs/api-reference/image-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateSdxl",
    "method": "POST",
    "endpoint_path": "/generate/sdxl",
    "endpoint_path_alternates": [
      "/generate/sdxl",
      "https://image.octoai.run/generate/sdxl",
      "https://image.securelink.octo.ai/generate/sdxl",
      "https://image.octoai.run/generate/sdxl",
      "https://image.securelink.octo.ai/generate/sdxl"
    ],
    "response_type": "json",
    "description": "Generate images in response to the given request.",
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateSd",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/image-gen/generate-sd",
    "pathname": "/docs/api-reference/image-gen/generate-sd",
    "title": "Generate SD1.5",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Image Gen",
        "pathname": "/docs/api-reference/image-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateSd",
    "method": "POST",
    "endpoint_path": "/generate/sd",
    "endpoint_path_alternates": [
      "/generate/sd",
      "https://image.octoai.run/generate/sd",
      "https://image.securelink.octo.ai/generate/sd",
      "https://image.octoai.run/generate/sd",
      "https://image.securelink.octo.ai/generate/sd"
    ],
    "response_type": "json",
    "description": "Generate images in response to the given request.",
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateSvd",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/image-gen/generate-svd",
    "pathname": "/docs/api-reference/image-gen/generate-svd",
    "title": "Generate SVD Animations",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Image Gen",
        "pathname": "/docs/api-reference/image-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateSvd",
    "method": "POST",
    "endpoint_path": "/generate/svd",
    "endpoint_path_alternates": [
      "/generate/svd",
      "https://image.octoai.run/generate/svd",
      "https://image.securelink.octo.ai/generate/svd",
      "https://image.octoai.run/generate/svd",
      "https://image.securelink.octo.ai/generate/svd"
    ],
    "response_type": "json",
    "description": "Generate videos in response to the given request.",
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "VideoGenerationRequestSeed",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_text-gen.endpoint_text-gen.createChatCompletion",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/text-gen/create-chat-completion",
    "pathname": "/docs/api-reference/text-gen/create-chat-completion",
    "title": "Create Chat Completion",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Text Gen",
        "pathname": "/docs/api-reference/text-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_text-gen.createChatCompletion",
    "method": "POST",
    "endpoint_path": "/v1/chat/completions",
    "endpoint_path_alternates": [
      "/v1/chat/completions",
      "https://text.octoai.run/v1/chat/completions",
      "https://text.securelink.octo.ai/v1/chat/completions",
      "https://text.octoai.run/v1/chat/completions",
      "https://text.securelink.octo.ai/v1/chat/completions"
    ],
    "response_type": "json",
    "description": "Create a Chat Completion.",
    "environments": [
      {
        "id": "Production",
        "url": "https://text.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://text.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ChatMessage",
      "ChatCompletionRequestExt",
      "ChatCompletionResponseFormat",
      "Stop",
      "StreamOptions",
      "HTTPValidationError",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_text-gen.endpoint_text-gen.createChatCompletion_stream",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/text-gen/create-chat-completion",
    "pathname": "/docs/api-reference/text-gen/create-chat-completion-stream",
    "title": "Create Chat Completion",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Text Gen",
        "pathname": "/docs/api-reference/text-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_text-gen.createChatCompletion_stream",
    "method": "POST",
    "endpoint_path": "/v1/chat/completions",
    "endpoint_path_alternates": [
      "/v1/chat/completions",
      "https://text.octoai.run/v1/chat/completions",
      "https://text.securelink.octo.ai/v1/chat/completions",
      "https://text.octoai.run/v1/chat/completions",
      "https://text.securelink.octo.ai/v1/chat/completions"
    ],
    "response_type": "stream",
    "description": "Create a Chat Completion.",
    "environments": [
      {
        "id": "Production",
        "url": "https://text.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://text.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "stream",
      "ChatMessage",
      "ChatCompletionRequestExt",
      "ChatCompletionResponseFormat",
      "Stop",
      "StreamOptions",
      "ChatCompletionChunk",
      "HTTPValidationError",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_text-gen.endpoint_text-gen.createCompletion",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/text-gen/create-completion",
    "pathname": "/docs/api-reference/text-gen/create-completion",
    "title": "Create Completion",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Text Gen",
        "pathname": "/docs/api-reference/text-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_text-gen.createCompletion",
    "method": "POST",
    "endpoint_path": "/v1/completions",
    "endpoint_path_alternates": [
      "/v1/completions",
      "https://text.octoai.run/v1/completions",
      "https://text.securelink.octo.ai/v1/completions",
      "https://text.octoai.run/v1/completions",
      "https://text.securelink.octo.ai/v1/completions"
    ],
    "response_type": "json",
    "environments": [
      {
        "id": "Production",
        "url": "https://text.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://text.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "Prompt",
      "Stop",
      "StreamOptions",
      "HTTPValidationError",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_text-gen.endpoint_text-gen.createCompletion_stream",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/text-gen/create-completion",
    "pathname": "/docs/api-reference/text-gen/create-completion-stream",
    "title": "Create Completion",
    "breadcrumb": [
      {
        "title": "OctoAI API",
        "pathname": "/docs/api-reference"
      },
      {
        "title": "Text Gen",
        "pathname": "/docs/api-reference/text-gen"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_text-gen.createCompletion_stream",
    "method": "POST",
    "endpoint_path": "/v1/completions",
    "endpoint_path_alternates": [
      "/v1/completions",
      "https://text.octoai.run/v1/completions",
      "https://text.securelink.octo.ai/v1/completions",
      "https://text.octoai.run/v1/completions",
      "https://text.securelink.octo.ai/v1/completions"
    ],
    "response_type": "stream",
    "environments": [
      {
        "id": "Production",
        "url": "https://text.octoai.run"
      },
      {
        "id": "SecureLink",
        "url": "https://text.securelink.octo.ai"
      }
    ],
    "default_environment_id": "Production",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "stream",
      "Prompt",
      "Stop",
      "StreamOptions",
      "CompletionResponse",
      "HTTPValidationError",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.media-util.endpoint_.upscale",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/media-util/upscale",
    "pathname": "/docs/api-reference/media-util/upscale",
    "title": "Upscaling",
    "breadcrumb": [
      {
        "title": "Media Utilities API",
        "pathname": "/docs/api-reference/media-util"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "2eeac53f-90cc-492b-9d29-e3ed711d7704",
    "api_endpoint_id": "endpoint_.upscale",
    "method": "POST",
    "endpoint_path": "/upscaling",
    "endpoint_path_alternates": [
      "/upscaling"
    ],
    "response_type": "json",
    "description": "Upscale the given image.",
    "environments": [],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "UpscalingModel",
      "ImageEncoding",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.media-util.endpoint_.remove_background",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/media-util/remove-background",
    "pathname": "/docs/api-reference/media-util/remove-background",
    "title": "Remove Background",
    "breadcrumb": [
      {
        "title": "Media Utilities API",
        "pathname": "/docs/api-reference/media-util"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "2eeac53f-90cc-492b-9d29-e3ed711d7704",
    "api_endpoint_id": "endpoint_.remove_background",
    "method": "POST",
    "endpoint_path": "/background-removal",
    "endpoint_path_alternates": [
      "/background-removal"
    ],
    "response_type": "json",
    "description": "Remove background from the given image.",
    "environments": [],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "test:test.com:root.uv.api-reference.api-reference.media-util.endpoint_.generate_images",
    "org_id": "test",
    "domain": "test.com",
    "canonicalPathname": "/docs/api-reference/media-util/generate-images",
    "pathname": "/docs/api-reference/media-util/generate-images",
    "title": "Adetailer",
    "breadcrumb": [
      {
        "title": "Media Utilities API",
        "pathname": "/docs/api-reference/media-util"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/docs/api-reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "2eeac53f-90cc-492b-9d29-e3ed711d7704",
    "api_endpoint_id": "endpoint_.generate_images",
    "method": "POST",
    "endpoint_path": "/adetailer",
    "endpoint_path_alternates": [
      "/adetailer"
    ],
    "response_type": "json",
    "description": "Detail the given image.",
    "environments": [],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ImageEncoding",
      "Scheduler",
      "ADetailerInpaintingBaseModel",
      "ADetailerRequestSeed",
      "ADetailerDetector",
      "HTTPValidationError"
    ],
    "type": "api-reference"
  }
]