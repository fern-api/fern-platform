[
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/intro",
    "pathname": "/intro",
    "title": "Welcome to Hume AI",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Hume AI builds AI models that enable technology to communicate with empathy and learn to make people happy.",
    "content": "EVI 2 is now available! Visit platform.hume.ai to chat with Hume's new voice-language foundation model and craft a custom empathic voice for your application.\nHume AI builds AI models that enable technology to communicate with empathy and learn to make people happy.\nSo much of human communication—in-person, text, audio, or video—is shaped by emotional expression. These cues allow us to attend to each other's well-being. Our platform provides the APIs needed to ensure that technology, too, is guided by empathy and the pursuit of human well-being."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-empathic-voice-interface-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/intro",
    "pathname": "/intro",
    "title": "Empathic Voice Interface",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#empathic-voice-interface",
    "content": "Hume's Empathic Voice Interface (EVI) is the world's first emotionally intelligent voice AI. It is the only API that measures nuanced vocal modulations and responds to them using an empathic large language model (eLLM), which guides language and speech generation. Trained on millions of human interactions, our eLLM unites language modeling and text-to-speech with better EQ, prosody, end-of-turn detection, interruptibility, and alignment.",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h3": {
        "id": "empathic-voice-interface",
        "title": "Empathic Voice Interface"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-expression-measurement-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/intro",
    "pathname": "/intro",
    "title": "Expression Measurement",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#expression-measurement",
    "content": "Hume's state-of-the-art expression measurement models for the voice, face, and language are built on 10+ years of research and advances in semantic space theory pioneered by Alan Cowen. Our expression measurement models are able to capture hundreds of dimensions of human expression in audio, video, and images.",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h3": {
        "id": "expression-measurement",
        "title": "Expression Measurement"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-api-reference-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/intro",
    "pathname": "/intro",
    "title": "API Reference",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-reference",
    "content": "Alongside our documentation, we provide a detailed API reference to help you integrate and use our products. It includes descriptions of all our REST and WebSocket endpoints, as well as request and response formats and usage examples.\n\n\n\n\nAPI that measures nuanced vocal modulations and responds to them using an\nempathic large language model\n\n\nMeasure facial, vocal, and linguistic expressions",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h2": {
        "id": "api-reference",
        "title": "API Reference"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-example-code-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/intro",
    "pathname": "/intro",
    "title": "Example Code",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#example-code",
    "content": "Explore our step-by-step guides for integrating Hume APIs. Our GitHub repositories include straightforward projects to help you get started quickly, with code snippets for specific functionalities. Additionally, you'll find open-source SDKs for popular languages and frameworks to support your development process across various environments.\n\n\n\n\nDiscover sample code and projects to help you integrate our products quickly and efficiently\n\n\nHome to all our public-facing code, including Hume's open-source SDKs and examples",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h2": {
        "id": "example-code",
        "title": "Example Code"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-get-support-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/intro",
    "pathname": "/intro",
    "title": "Get Support",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#get-support",
    "content": "If you have questions or run into challenges, we're here to help!\n\n\n\n\nJoin our Discord for answers to any technical questions",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h2": {
        "id": "get-support",
        "title": "Get Support"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.api-key",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/introduction/api-key",
    "pathname": "/docs/introduction/api-key",
    "title": "Getting your API keys",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Learn how to obtain your API keys and understand the supported authentication strategies for securely accessing Hume APIs."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.api-key-api-keys-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/introduction/api-key",
    "pathname": "/docs/introduction/api-key",
    "title": "API keys",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-keys",
    "content": "Each Hume account is provisioned with an API key and Secret key. These keys are accessible from the Hume Portal.\nSign in: Visit the Hume Portal and log in, or create an account.\n\nView your API keys: Navigate to the API keys page to view your keys.",
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys"
      },
      "h2": {
        "id": "api-keys",
        "title": "API keys"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.api-key-authentication-strategies-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/introduction/api-key",
    "pathname": "/docs/introduction/api-key",
    "title": "Authentication strategies",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#authentication-strategies",
    "content": "Hume APIs support two authentication strategies:\nAPI key strategy: Use API key authentication for making server-side requests. API key authentication allows you to make authenticated requests by supplying a single secret using the X-Hume-Api-Key header. Do not expose your API key in client-side code. All Hume APIs support this authentication strategy.\n\nToken strategy:  Use Token authentication for making client-side requests. With Token authentication you first obtain a temporary access token by making a server-side request first, and use the access token when making client-side requests. This allows you to avoid exposing the API key to the client. Access tokens expire after 30 minutes, and you must obtain a new one. Today, only our Empathic Voice Interface (EVI) supports this authentication strategy.",
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys"
      },
      "h2": {
        "id": "authentication-strategies",
        "title": "Authentication strategies"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.api-key-api-key-authentication-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/introduction/api-key",
    "pathname": "/docs/introduction/api-key",
    "title": "API key authentication",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-key-authentication",
    "content": "To use API key authentication on REST API endpoints, include the API key in the X-Hume-Api-Key request header.\n\n\n\n\n\n\nFor WebSocket endpoints, include the API key as a query parameter in the URL.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/evi/{path} \\\n  --header 'Accept: application/json; charset=utf-8' \\\n  --header \"X-Hume-Api-Key: <YOUR API KEY>\""
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/evi/{path} \\\n  --header 'Accept: application/json; charset=utf-8' \\\n  --header \"X-Hume-Api-Key: <YOUR API KEY>\""
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs/{path} \\\n  --header 'Accept: application/json; charset=utf-8' \\\n  --header \"X-Hume-Api-Key: <YOUR API KEY>\""
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs/{path} \\\n  --header 'Accept: application/json; charset=utf-8' \\\n  --header \"X-Hume-Api-Key: <YOUR API KEY>\""
      },
      {
        "lang": "TypeScript",
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?api_key=${apiKey}`);"
      },
      {
        "lang": "TypeScript",
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?api_key=${apiKey}`);"
      },
      {
        "lang": "TypeScript",
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/stream/models?api_key=${apiKey}`);"
      },
      {
        "lang": "TypeScript",
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/stream/models?api_key=${apiKey}`);"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys"
      },
      "h2": {
        "id": "authentication-strategies",
        "title": "Authentication strategies"
      },
      "h3": {
        "id": "api-key-authentication",
        "title": "API key authentication"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.api-key-token-authentication-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/introduction/api-key",
    "pathname": "/docs/introduction/api-key",
    "title": "Token authentication",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#token-authentication",
    "content": "To use Token authentication you must first obtain an Access Token from the POST /oauth2-cc/token endpoint.\nThis is a unique endpoint that uses the \"Basic\" authentication scheme, with your API key as the username and the Secret key as the password. This means you must concatenate your API key and Secret key, separated by a colon (:), base64 encode this value, and then put the result in the Authorization header of the request, prefixed with Basic .\nYou must also supply the grant_type=client_credentials parameter in the request body.\n\n\n\n\n\n\n\n\nOn the client side, open an authenticated websocket by including the access token as a query parameter in the URL.\n\n\nOr, make a REST request by including the access token in the Authorization header.\n\n\n\n\nThe Hume Python and TypeScript SDKs will use the API key authentication strategy if you provide only the API key, but will use the Token authentication strategy if you provide both the API key and Secret key.",
    "code_snippets": [
      {
        "lang": "sh",
        "code": "# Assumes `HUME_API_KEY` and `HUME_SECRET_KEY` are defined as environment variables\nresponse=$(curl -s 'https://api.hume.ai/oauth2-cc/token' \\\n  -u \"${HUME_API_KEY}:${HUME_SECRET_KEY}\" \\\n  -d 'grant_type=client_credentials')\n\n# Uses `jq` to extract the access token from the JSON response body\naccessToken=$(echo $response | jq -r '.access_token')"
      },
      {
        "lang": "sh",
        "code": "# Assumes `HUME_API_KEY` and `HUME_SECRET_KEY` are defined as environment variables\nresponse=$(curl -s 'https://api.hume.ai/oauth2-cc/token' \\\n  -u \"${HUME_API_KEY}:${HUME_SECRET_KEY}\" \\\n  -d 'grant_type=client_credentials')\n\n# Uses `jq` to extract the access token from the JSON response body\naccessToken=$(echo $response | jq -r '.access_token')"
      },
      {
        "lang": "typescript",
        "code": "import {fetchAccessToken} from 'hume';\n\n// Reads `HUME_API_KEY` and `HUME_SECRET_KEY` from environment variables\nconst HUME_API_KEY = process.env.HUME_API_KEY;\nconst HUME_SECRET_KEY = process.env.HUME_SECRET_KEY;\n\nconst accessToken = await fetchAccessToken({\n  apiKey: HUME_API_KEY,\n  secretKey: HUME_SECRET_KEY\n});"
      },
      {
        "lang": "typescript",
        "code": "import {fetchAccessToken} from 'hume';\n\n// Reads `HUME_API_KEY` and `HUME_SECRET_KEY` from environment variables\nconst HUME_API_KEY = process.env.HUME_API_KEY;\nconst HUME_SECRET_KEY = process.env.HUME_SECRET_KEY;\n\nconst accessToken = await fetchAccessToken({\n  apiKey: HUME_API_KEY,\n  secretKey: HUME_SECRET_KEY\n});"
      },
      {
        "lang": "python",
        "code": "import os\nimport httpx\nimport base64\n\n# Reads `HUME_API_KEY` and `HUME_SECRET_KEY` from environment variables\nHUME_API_KEY = os.getenv('HUME_API_KEY')\nHUME_SECRET_KEY = os.getenv('HUME_SECRET_KEY');\n\nauth = f\"{HUME_API_KEY}:{HUME_SECRET_KEY}\"\nencoded_auth = base64.b64encode(auth.encode()).decode()\nresp = httpx.request(\n    method=\"POST\",\n    url=\"https://api.hume.ai/oauth2-cc/token\",\n    headers={\"Authorization\": f\"Basic {encoded_auth}\"},\n    data={\"grant_type\": \"client_credentials\"},\n)\n\naccess_token = resp.json()['access_token']"
      },
      {
        "lang": "python",
        "code": "import os\nimport httpx\nimport base64\n\n# Reads `HUME_API_KEY` and `HUME_SECRET_KEY` from environment variables\nHUME_API_KEY = os.getenv('HUME_API_KEY')\nHUME_SECRET_KEY = os.getenv('HUME_SECRET_KEY');\n\nauth = f\"{HUME_API_KEY}:{HUME_SECRET_KEY}\"\nencoded_auth = base64.b64encode(auth.encode()).decode()\nresp = httpx.request(\n    method=\"POST\",\n    url=\"https://api.hume.ai/oauth2-cc/token\",\n    headers={\"Authorization\": f\"Basic {encoded_auth}\"},\n    data={\"grant_type\": \"client_credentials\"},\n)\n\naccess_token = resp.json()['access_token']"
      },
      {
        "lang": "typescript",
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?access_token=${accessToken}`);"
      },
      {
        "lang": "typescript",
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?access_token=${accessToken}`);"
      },
      {
        "lang": "typescript",
        "code": "fetch('https://api.hume.ai/v0/evi/chats', {\n  headers: {\n    Authorization: `Bearer ${accessToken}`,\n  },\n});"
      },
      {
        "lang": "typescript",
        "code": "fetch('https://api.hume.ai/v0/evi/chats', {\n  headers: {\n    Authorization: `Bearer ${accessToken}`,\n  },\n});"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys"
      },
      "h2": {
        "id": "authentication-strategies",
        "title": "Authentication strategies"
      },
      "h3": {
        "id": "token-authentication",
        "title": "Token authentication"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.api-key-regenerating-api-keys-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/introduction/api-key",
    "pathname": "/docs/introduction/api-key",
    "title": "Regenerating API keys",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#regenerating-api-keys",
    "content": "API keys can be regenerated by clicking the Regenerate keys button on the API keys page. This permanently invalidates the current keys, requiring you to update any applications using them.",
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys"
      },
      "h2": {
        "id": "authentication-strategies",
        "title": "Authentication strategies"
      },
      "h3": {
        "id": "regenerating-api-keys",
        "title": "Regenerating API keys"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/support",
    "pathname": "/support",
    "title": "Support",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Get help from the team at Hume"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support-discord-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/support",
    "pathname": "/support",
    "title": "Discord",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#discord",
    "content": "Join our Discord for answers to any technical questions.",
    "hierarchy": {
      "h0": {
        "title": "Support"
      },
      "h2": {
        "id": "discord",
        "title": "Discord"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support-legal-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/support",
    "pathname": "/support",
    "title": "Legal",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#legal",
    "content": "Contact legal@hume.ai for legal and data privacy inquires.",
    "hierarchy": {
      "h0": {
        "title": "Support"
      },
      "h2": {
        "id": "legal",
        "title": "Legal"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support-billing-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/support",
    "pathname": "/support",
    "title": "Billing",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#billing",
    "content": "Email billing@hume.ai for any questions or concerns about billing.",
    "hierarchy": {
      "h0": {
        "title": "Support"
      },
      "h2": {
        "id": "billing",
        "title": "Billing"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support-contact-us-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/support",
    "pathname": "/support",
    "title": "Contact us",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#contact-us",
    "content": "For all other inquires, see hume.ai/contact.",
    "hierarchy": {
      "h0": {
        "title": "Support"
      },
      "h2": {
        "id": "contact-us",
        "title": "Contact us"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/overview",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "Empathic Voice Interface (EVI)",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Hume's Empathic Voice Interface (EVI) is the world’s first emotionally intelligent voice AI.",
    "content": "EVI 2 is now available! See the EVI 2 page to learn more and visit platform.hume.ai to start building with the latest features today.\nHume's Empathic Voice Interface (EVI) is the world’s first emotionally intelligent voice AI. It accepts live audio input and returns both generated audio and transcripts augmented with measures of vocal expression. By processing the tune, rhythm, and timbre of speech, EVI unlocks a variety of new capabilities, like knowing when to speak and generating more empathic language with the right tone of voice. These features enable smoother and more satisfying voice-based interactions between humans and AI, opening new possibilities for personal AI, customer service, accessibility, robotics, immersive gaming, VR experiences, and much more.\nWe provide a suite of tools to integrate and customize EVI for your application, including a WebSocket API that handles audio and text transport, a REST API, and SDKs for TypeScript and Python to simplify integration into web and Python-based projects. Additionally, we provide open-source examples and a web widget as practical starting points for developers to explore and implement EVI's capabilities within their own projects."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview-building-with-evi-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/overview",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "Building with EVI",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#building-with-evi",
    "content": "The main way to work with EVI is through a WebSocket connection that sends audio and receives responses in real-time. This enables fluid, bidirectional dialogue where users speak, EVI listens and analyzes their expressions, and EVI generates emotionally intelligent responses.\nEVI supports two authentication strategies. Learn more about them at the links below:\nAPI key authentication\n\nToken authentication\n\n\n\n\nBoth methods require specifying the chosen authentication strategy and providing the corresponding key in the request parameters of the EVI WebSocket endpoint.\nLearn more about Hume's authentication strategies here.\nYou start a conversation by connecting to the WebSocket and streaming the user’s voice input to EVI. You can also send EVI text, and it will speak that text aloud.\nEVI will respond with:\nThe text of EVI’s reply\n\nEVI’s expressive audio response\n\nA transcript of the user's message along with their vocal expression measures\n\nMessages if the user interrupts EVI\n\nA message to let you know if EVI has finished responding\n\nError messages if issues arise",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)"
      },
      "h2": {
        "id": "building-with-evi",
        "title": "Building with EVI"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview-overview-of-evi-features-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/overview",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "Overview of EVI features",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#overview-of-evi-features",
    "content": "Basic capabilities\n\nTranscribes speech (ASR)\n\nFast and accurate ASR in partnership with Deepgram returns a full transcript of the conversation, with Hume’s\nexpression measures tied to each sentence.\n\n\n\n\nGenerates language responses (LLM)\n\nRapid language generation with our eLLM, blended seamlessly with configurable partner APIs (OpenAI, Anthropic,\nFireworks).\n\n\n\n\nGenerates voice responses (TTS)\n\nStreaming speech generation via our proprietary expressive text-to-speech model.\n\n\n\nResponds with low latency\n\nImmediate response provided by the fastest models running together on one service.\n\n\n\n Empathic AI (eLLM) features\n\nResponds at the right time\n\nUses your tone of voice for state-of-the-art end-of-turn detection — the true bottleneck to responding rapidly\nwithout interrupting you.\n\n\n\n\nUnderstands users’ prosody\n\nProvides streaming measurements of the tune, rhythm, and timbre of the user’s speech using Hume’s\n\n\nprosody model, integrated with our eLLM.\n\n\n\n\nForms its own natural tone of voice\n\nGuided by the users’ prosody and language, our model responds with an empathic, naturalistic tone of voice,\nmatching the users’ nuanced “vibe” (calmness, interest, excitement, etc.). It responds to frustration with an\napologetic tone, to sadness with sympathy, and more.\n\n\n\n\nResponds to expression\n\nPowered by our empathic large language model (eLLM), EVI crafts responses that are not just intelligent but\nattuned to what the user is expressing with their voice.\n\n\n\n\nAlways interruptible\n\nStops rapidly whenever users interject, listens, and responds with the right context based on where it left off.\n\n\n\n\nAligned with well-being\n\nTrained on human reactions to optimize for positive expressions like happiness and satisfaction. EVI will\ncontinue to learn from users’ reactions using our upcoming fine-tuning endpoint.\n\n\n\n\n Developer tools\n\nWebSocket API\n\nPrimary interface for real-time bidirectional interaction with EVI, handles audio and text transport.\n\n\n\nREST API \n\nA configuration API that allows developers to customize their EVI - the system prompt, speaking rate, voice,\nLLM, tools the EVI can use, and other options. The system prompt shapes an EVI’s behavior and its responses.\n\n\n\n\nTypeScript SDK\n\nEncapsulates complexities of audio and WebSockets for seamless integration into web applications.\n\n\n\nPython SDK\n\nSimplifies the process of integrating EVI into any Python-based project.\n\n\n\nOpen source examples\n\nExample repositories provide a starting point for developers and demonstrate EVI's capabilities.\n\n\n\nWeb widget \n\nAn iframe widget that any developer can easily embed in their website, allowing users to speak to a\nconversational AI voice about your content.",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)"
      },
      "h2": {
        "id": "overview-of-evi-features",
        "title": "Overview of EVI features"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview-api-limits-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/overview",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "API limits",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-limits",
    "content": "WebSocket connections limit: limited to up to five (5) concurrent connections.\n\nWebSocket duration limit: connections are subject to a default timeout after thirty (30) minutes, or after ten (10) minutes of user inactivity. Duration limits may be adjusted by specifying the max_duration and inactivity fields in your EVI configuration.\n\nWebSocket message payload size limit: messages cannot exceed 16MB in size.\n\nRequest rate limit: HTTP requests (e.g. configs endpoints) are limited to fifty (50) requests per second.\n\n\n\n\nTo request an increase in your concurrent connection limit, please submit the \"Application to Increase EVI Concurrent Connections\" found in the EVI section of the Profile Tab.",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)"
      },
      "h2": {
        "id": "api-limits",
        "title": "API limits"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.quickstart.typescript-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/quickstart/typescript",
    "pathname": "/docs/empathic-voice-interface-evi/quickstart/typescript",
    "title": "EVI TypeScript Quickstart Guide",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      },
      {
        "title": "Quickstart",
        "pathname": "/docs/empathic-voice-interface-evi/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "A quickstart guide for implementing the Empathic Voice Interface (EVI) with TypeScript.",
    "code_snippets": [
      {
        "lang": "tsx",
        "code": "// ./app/page.tsx\nimport ClientComponent from \"@/components/ClientComponent\";\nimport { fetchAccessToken } from \"hume\";\n\nexport default async function Page() {\n  const accessToken = await fetchAccessToken({\n    apiKey: String(process.env.HUME_API_KEY),\n    secretKey: String(process.env.HUME_SECRET_KEY),\n  });\n\n  if (!accessToken) {\n    throw new Error();\n  }\n\n  return <ClientComponent accessToken={accessToken} />;\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./app/page.tsx\nimport ClientComponent from \"@/components/ClientComponent\";\nimport { fetchAccessToken } from \"hume\";\n\nexport default async function Page() {\n  const accessToken = await fetchAccessToken({\n    apiKey: String(process.env.HUME_API_KEY),\n    secretKey: String(process.env.HUME_SECRET_KEY),\n  });\n\n  if (!accessToken) {\n    throw new Error();\n  }\n\n  return <ClientComponent accessToken={accessToken} />;\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/ClientComponent.tsx\n\"use client\";\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport Messages from \"./Messages\";\nimport Controls from \"./Controls\";\n\nexport default function ClientComponent({\n  accessToken,\n}: {\n  accessToken: string;\n}) {\n  return (\n    <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n      <Messages />\n      <Controls />\n    </VoiceProvider>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/ClientComponent.tsx\n\"use client\";\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport Messages from \"./Messages\";\nimport Controls from \"./Controls\";\n\nexport default function ClientComponent({\n  accessToken,\n}: {\n  accessToken: string;\n}) {\n  return (\n    <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n      <Messages />\n      <Controls />\n    </VoiceProvider>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/Controls.tsx\n\"use client\";\nimport { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\nexport default function Controls() {\n  const { connect, disconnect, readyState } = useVoice();\n\n  if (readyState === VoiceReadyState.OPEN) {\n    return (\n      <button\n        onClick={() => {\n          disconnect();\n        }}\n      >\n        End Session\n      </button>\n    );\n  }\n\n  return (\n    <button\n      onClick={() => {\n        connect()\n          .then(() => {\n            /* handle success */\n          })\n          .catch(() => {\n            /* handle error */\n          });\n      }}\n    >\n      Start Session\n    </button>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/Controls.tsx\n\"use client\";\nimport { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\nexport default function Controls() {\n  const { connect, disconnect, readyState } = useVoice();\n\n  if (readyState === VoiceReadyState.OPEN) {\n    return (\n      <button\n        onClick={() => {\n          disconnect();\n        }}\n      >\n        End Session\n      </button>\n    );\n  }\n\n  return (\n    <button\n      onClick={() => {\n        connect()\n          .then(() => {\n            /* handle success */\n          })\n          .catch(() => {\n            /* handle error */\n          });\n      }}\n    >\n      Start Session\n    </button>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/Messages.tsx\n\"use client\";\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    <div>\n      {messages.map((msg, index) => {\n        if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n          return (\n            <div key={msg.type + index}>\n              <div>{msg.message.role}</div>\n              <div>{msg.message.content}</div>\n            </div>\n          );\n        }\n\n        return null;\n      })}\n    </div>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/Messages.tsx\n\"use client\";\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    <div>\n      {messages.map((msg, index) => {\n        if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n          return (\n            <div key={msg.type + index}>\n              <div>{msg.message.role}</div>\n              <div>{msg.message.content}</div>\n            </div>\n          );\n        }\n\n        return null;\n      })}\n    </div>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./pages/index.tsx\nimport Controls from \"@/components/Controls\";\nimport Messages from \"@/components/Messages\";\nimport { fetchAccessToken } from \"hume\";\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport { InferGetServerSidePropsType } from \"next\";\n\nexport const getServerSideProps = async () => {\n  const accessToken = await fetchAccessToken({\n    apiKey: String(process.env.HUME_API_KEY),\n    secretKey: String(process.env.HUME_SECRET_KEY),\n  });\n\n  if (!accessToken) {\n    return {\n      redirect: {\n        destination: \"/error\",\n        permanent: false,\n      },\n    };\n  }\n\n  return {\n    props: {\n      accessToken,\n    },\n  };\n};\n\ntype PageProps = InferGetServerSidePropsType<typeof getServerSideProps>;\n\nexport default function Page({ accessToken }: PageProps) {\n  return (\n    <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n      <Messages />\n      <Controls />\n    </VoiceProvider>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./pages/index.tsx\nimport Controls from \"@/components/Controls\";\nimport Messages from \"@/components/Messages\";\nimport { fetchAccessToken } from \"hume\";\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport { InferGetServerSidePropsType } from \"next\";\n\nexport const getServerSideProps = async () => {\n  const accessToken = await fetchAccessToken({\n    apiKey: String(process.env.HUME_API_KEY),\n    secretKey: String(process.env.HUME_SECRET_KEY),\n  });\n\n  if (!accessToken) {\n    return {\n      redirect: {\n        destination: \"/error\",\n        permanent: false,\n      },\n    };\n  }\n\n  return {\n    props: {\n      accessToken,\n    },\n  };\n};\n\ntype PageProps = InferGetServerSidePropsType<typeof getServerSideProps>;\n\nexport default function Page({ accessToken }: PageProps) {\n  return (\n    <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n      <Messages />\n      <Controls />\n    </VoiceProvider>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/Controls.tsx\nimport { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\nexport default function Controls() {\n  const { connect, disconnect, readyState } = useVoice();\n\n  if (readyState === VoiceReadyState.OPEN) {\n    return (\n      <button\n        onClick={() => {\n          disconnect();\n        }}\n      >\n        End Session\n      </button>\n    );\n  }\n\n  return (\n    <button\n      onClick={() => {\n        connect()\n          .then(() => {\n            /* handle success */\n          })\n          .catch(() => {\n            /* handle error */\n          });\n      }}\n    >\n      Start Session\n    </button>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/Controls.tsx\nimport { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\nexport default function Controls() {\n  const { connect, disconnect, readyState } = useVoice();\n\n  if (readyState === VoiceReadyState.OPEN) {\n    return (\n      <button\n        onClick={() => {\n          disconnect();\n        }}\n      >\n        End Session\n      </button>\n    );\n  }\n\n  return (\n    <button\n      onClick={() => {\n        connect()\n          .then(() => {\n            /* handle success */\n          })\n          .catch(() => {\n            /* handle error */\n          });\n      }}\n    >\n      Start Session\n    </button>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/Messages.tsx\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    <div>\n      {messages.map((msg, index) => {\n        if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n          return (\n            <div key={msg.type + index}>\n              <div>{msg.message.role}</div>\n              <div>{msg.message.content}</div>\n            </div>\n          );\n        }\n\n        return null;\n      })}\n    </div>\n  );\n}"
      },
      {
        "lang": "tsx",
        "code": "// ./components/Messages.tsx\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    <div>\n      {messages.map((msg, index) => {\n        if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n          return (\n            <div key={msg.type + index}>\n              <div>{msg.message.role}</div>\n              <div>{msg.message.content}</div>\n            </div>\n          );\n        }\n\n        return null;\n      })}\n    </div>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n\n// instantiate the Hume client and authenticate\nconst client = new HumeClient({\n  apiKey: import.meta.env.HUME_API_KEY || '',\n  secretKey: import.meta.env.HUME_SECRET_KEY || '',\n});\n"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n\n// instantiate the Hume client and authenticate\nconst client = new HumeClient({\n  apiKey: import.meta.env.HUME_API_KEY || '',\n  secretKey: import.meta.env.HUME_SECRET_KEY || '',\n});\n"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n\n// instantiate the Hume client and authenticate\nconst client = new HumeClient({\n  apiKey: import.meta.env.HUME_API_KEY || '',\n  secretKey: import.meta.env.HUME_SECRET_KEY || '',\n});\n\n// instantiates WebSocket and establishes an authenticated connection\nconst socket = await client.empathicVoice.chat.connect({\n  configId: import.meta.env.HUME_CONFIG_ID || null,\n});\n\n// define handler functions and assign them to the corresponding WebSocket event handlers\nsocket.on('open', handleWebSocketOpenEvent);\nsocket.on('message', handleWebSocketMessageEvent);\nsocket.on('error', handleWebSocketErrorEvent);\nsocket.on('close', handleWebSocketCloseEvent);"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n\n// instantiate the Hume client and authenticate\nconst client = new HumeClient({\n  apiKey: import.meta.env.HUME_API_KEY || '',\n  secretKey: import.meta.env.HUME_SECRET_KEY || '',\n});\n\n// instantiates WebSocket and establishes an authenticated connection\nconst socket = await client.empathicVoice.chat.connect({\n  configId: import.meta.env.HUME_CONFIG_ID || null,\n});\n\n// define handler functions and assign them to the corresponding WebSocket event handlers\nsocket.on('open', handleWebSocketOpenEvent);\nsocket.on('message', handleWebSocketMessageEvent);\nsocket.on('error', handleWebSocketErrorEvent);\nsocket.on('close', handleWebSocketCloseEvent);"
      },
      {
        "lang": "typescript",
        "code": "import {\n  convertBlobToBase64,\n  ensureSingleValidAudioTrack,\n  getAudioStream,\n  getBrowserSupportedMimeType,\n} from 'hume';\n\n// the recorder responsible for recording the audio stream to be prepared as the audio input\nlet recorder: MediaRecorder | null = null;\n\n// the stream of audio captured from the user's microphone\nlet audioStream: MediaStream | null = null;\n\n// mime type supported by the browser the application is running in\nconst mimeType: MimeType = (() => {\n  const result = getBrowserSupportedMimeType();\n  return result.success ? result.mimeType : MimeType.WEBM;\n})();\n\n// define function for capturing audio\nasync function captureAudio(): Promise<void> {\n  // prompts user for permission to capture audio, obtains media stream upon approval\n  audioStream = await getAudioStream();\n\n  // ensure there is only one audio track in the stream\n  ensureSingleValidAudioTrack(audioStream);\n\n  // instantiate the media recorder\n  recorder = new MediaRecorder(audioStream, { mimeType });\n\n  // callback for when recorded chunk is available to be processed\n  recorder.ondataavailable = async ({ data }) => {\n    // IF size of data is smaller than 1 byte then do nothing\n    if (data.size < 1) return;\n\n    // base64 encode audio data\n    const encodedAudioData = await convertBlobToBase64(data);\n\n    // define the audio_input message JSON\n    const audioInput: Omit<Hume.empathicVoice.AudioInput, 'type'> = {\n      data: encodedAudioData,\n    };\n\n    // send audio_input message\n    socket?.sendAudioInput(audioInput);\n  };\n\n  // capture audio input at a rate of 100ms (recommended for web)\n  const timeSlice = 100;\n  recorder.start(timeSlice);\n}\n\n// define a WebSocket open event handler to capture audio\nasync function handleWebSocketOpenEvent(): Promise<void> {\n  // place logic here which you would like invoked when the socket opens\n  console.log('Web socket connection opened');\n  await captureAudio();\n}\n"
      },
      {
        "lang": "typescript",
        "code": "import {\n  convertBlobToBase64,\n  ensureSingleValidAudioTrack,\n  getAudioStream,\n  getBrowserSupportedMimeType,\n} from 'hume';\n\n// the recorder responsible for recording the audio stream to be prepared as the audio input\nlet recorder: MediaRecorder | null = null;\n\n// the stream of audio captured from the user's microphone\nlet audioStream: MediaStream | null = null;\n\n// mime type supported by the browser the application is running in\nconst mimeType: MimeType = (() => {\n  const result = getBrowserSupportedMimeType();\n  return result.success ? result.mimeType : MimeType.WEBM;\n})();\n\n// define function for capturing audio\nasync function captureAudio(): Promise<void> {\n  // prompts user for permission to capture audio, obtains media stream upon approval\n  audioStream = await getAudioStream();\n\n  // ensure there is only one audio track in the stream\n  ensureSingleValidAudioTrack(audioStream);\n\n  // instantiate the media recorder\n  recorder = new MediaRecorder(audioStream, { mimeType });\n\n  // callback for when recorded chunk is available to be processed\n  recorder.ondataavailable = async ({ data }) => {\n    // IF size of data is smaller than 1 byte then do nothing\n    if (data.size < 1) return;\n\n    // base64 encode audio data\n    const encodedAudioData = await convertBlobToBase64(data);\n\n    // define the audio_input message JSON\n    const audioInput: Omit<Hume.empathicVoice.AudioInput, 'type'> = {\n      data: encodedAudioData,\n    };\n\n    // send audio_input message\n    socket?.sendAudioInput(audioInput);\n  };\n\n  // capture audio input at a rate of 100ms (recommended for web)\n  const timeSlice = 100;\n  recorder.start(timeSlice);\n}\n\n// define a WebSocket open event handler to capture audio\nasync function handleWebSocketOpenEvent(): Promise<void> {\n  // place logic here which you would like invoked when the socket opens\n  console.log('Web socket connection opened');\n  await captureAudio();\n}\n"
      },
      {
        "lang": "typescript",
        "code": "// function for stopping the audio and clearing the queue\nfunction stopAudio(): void {\n  // stop the audio playback\n  currentAudio?.pause();\n  currentAudio = null;\n\n  // update audio playback state\n  isPlaying = false;\n\n  // clear the audioQueue\n  audioQueue.length = 0;\n}\n\n// update WebSocket message event handler to handle interruption\nfunction handleWebSocketMessageEvent(\n  message: Hume.empathicVoice.SubscribeEvent\n): void {\n  // place logic here which you would like to invoke when receiving a message through the socket\n  switch (message.type) {\n    // add received audio to the playback queue, and play next audio output\n    case 'audio_output':\n      // convert base64 encoded audio to a Blob\n      const audioOutput = message.data;\n      const blob = convertBase64ToBlob(audioOutput, mimeType);\n\n      // add audio Blob to audioQueue\n      audioQueue.push(blob);\n\n      // play the next audio output\n      if (audioQueue.length === 1) playAudio();\n      break;\n\n    // stop audio playback, clear audio playback queue, and update audio playback state on interrupt\n    case 'user_interruption':\n      stopAudio();\n      break;\n  }\n}\n"
      },
      {
        "lang": "typescript",
        "code": "// function for stopping the audio and clearing the queue\nfunction stopAudio(): void {\n  // stop the audio playback\n  currentAudio?.pause();\n  currentAudio = null;\n\n  // update audio playback state\n  isPlaying = false;\n\n  // clear the audioQueue\n  audioQueue.length = 0;\n}\n\n// update WebSocket message event handler to handle interruption\nfunction handleWebSocketMessageEvent(\n  message: Hume.empathicVoice.SubscribeEvent\n): void {\n  // place logic here which you would like to invoke when receiving a message through the socket\n  switch (message.type) {\n    // add received audio to the playback queue, and play next audio output\n    case 'audio_output':\n      // convert base64 encoded audio to a Blob\n      const audioOutput = message.data;\n      const blob = convertBase64ToBlob(audioOutput, mimeType);\n\n      // add audio Blob to audioQueue\n      audioQueue.push(blob);\n\n      // play the next audio output\n      if (audioQueue.length === 1) playAudio();\n      break;\n\n    // stop audio playback, clear audio playback queue, and update audio playback state on interrupt\n    case 'user_interruption':\n      stopAudio();\n      break;\n  }\n}\n"
      }
    ],
    "content": "This guide provides instructions for integrating EVI into your TypeScript projects. It includes detailed steps for using EVI with Next.js (App Router),\nNext.js (Pages Router), and a standalone setup without any framework.\n\n\nKickstart your project with our pre-configured Vercel template for the Empathic Voice\nInterface. Install\nwith one click to instantly set up a ready-to-use project and start building with\nTypeScript right away!\n\n\n\n\nThis tutorial utilizes Hume’s React SDK to interact with EVI. It includes detailed steps for both the\nApp Router in Next.js and is broken down into four key components:\nAuthentication: Generate and use an access token to authenticate with EVI.\n\nSetting up context provider: Set up the <VoiceProvider/>.\n\nStarting a chat and display messages: Implement the functionality to start a chat with EVI and display messages.\n\nThat's it!: Audio playback and interruptions are handled for you.\n\n\n\n\nThe Hume React SDK abstracts much of the logic for managing the WebSocket connection, as\nwell as capturing and preparing audio for processing. For a closer look at how the React\npackage manages these aspects of the integration, we invite you to explore the source code\nhere: @humeai/voice-react\nTo see this code fully implemented within a frontend web application using the App Router from Next.js, visit this GitHub repository:\nevi-nextjs-app-router.\n\n\nPrerequisites\nBefore you begin, you will need to have an existing Next.js project set up using the App Router.\nAuthenticate\nIn order to make an authenticated connection we will first need to generate an access token. Doing so will\nrequire your API key and Secret key. These keys can be obtained by logging into the portal and visiting the\nAPI keys page.\n\n\nIn the sample code below, the API key and Secret key have been saved to\nenvironment variables. Avoid hard coding these values in your project to\nprevent them from being leaked.\n\n\nSetup Context Provider\nAfter fetching our access token we can pass it to our ClientComponent. First we set up the <VoiceProvider/> so that our Messages and Controls components can access the context. We also pass the access token to the auth prop of the <VoiceProvider/> for setting up the WebSocket connection.\n\n\nAudio input\n<VoiceProvider/> will handle the microphone and playback logic.\nStarting session\nIn order to start a session, you can use the connect function. It is important that this event is attached to a user interaction event (like a click) so that the browser is capable of playing Audio.\n\n\nDisplaying message history\nTo display the message history, we can use the useVoice hook to access the messages array. We can then map over the messages array to display the role (Assistant or User) and content of each message.\n\n\nInterrupt\nThis Next.js example will handle interruption events automatically!\n\n\nThis tutorial utilizes Hume’s React SDK to interact with EVI. It includes detailed steps for both the\nPages Router in Next.js and is broken down into four key components:\nAuthentication: Generate and use an access token to authenticate with EVI.\n\nSetting up context provider: Set up the <VoiceProvider/>.\n\nStarting a chat and display messages: Implement the functionality to start a chat with EVI and display messages.\n\nThat's it!: Audio playback and interruptions are handled for you.\n\n\n\n\nThe Hume React SDK abstracts much of the logic for managing the WebSocket connection, as\nwell as capturing and preparing audio for processing. For a closer look at how the React\npackage manages these aspects of the integration, we invite you to explore the source code\nhere: @humeai/voice-react\nTo see this code fully implemented within a frontend web application using the Pages Router from Next.js, visit this GitHub repository: evi-nextjs-pages-router.\n\n\nPrerequisites\nBefore you begin, you will need to have an existing Next.js project set up using the Pages Router.\nAuthenticate and Setup Context Provider\nIn order to make an authenticated connection we will first need to generate an access token. Doing so will\nrequire your API key and Secret key. These keys can be obtained by logging into the portal and visiting the\nAPI keys page.\n\n\nIn the sample code below, the API key and Secret key have been saved to\nenvironment variables. Avoid hard coding these values in your project to\nprevent them from being leaked.\n\n\nAudio input\n<VoiceProvider/> is designed to manage microphone inputs and audio playback. It abstracts the complexities of audio processing to allow developers to focus on developing interactive voice-driven functionalities.\nFor a closer look at how <VoiceProvider/> processes audio inputs and controls playback, you can view the source code here.\nStarting session\nIn order to start a session, you can use the connect function. It is important that this event is attached to a user interaction event (like a click) so that the browser is capable of playing Audio.\n\n\nDisplaying message history\nTo display the message history, we can use the useVoice hook to access the messages array. We can then map over the messages array to display the role (Assistant or User) and content of each message.\n\n\nInterrupt\nThis Next.js example will handle interruption events automatically!\n\n\nThis tutorial provides step-by-step instructions for implementing EVI using Hume’s\nTypeScript SDK. This guide is divided into five key components:\nAuthentication: Authenticate your application with EVI using your credentials.\n\nConnecting to EVI: Set up a secure WebSocket connection to interact with EVI.\n\nCapturing & recording audio: Capture audio input and prepare it for processing.\n\nAudio playback: Play back the processed audio output to the user.\n\nInterruption: Manage and handle interruptions during the chat.\n\n\nTo see the full implementation within a frontend web application, visit our API examples repository on GitHub: hume-evi-typescript-example.\n\n\nAuthenticate\nIn order to establish an authenticated connection we will first need to instantiate the Hume client with our API key and Secret key.\nThese keys can be obtained by logging into the portal and visiting the API keys page.\n\n\nIn the sample code below, the API key and Secret key have been saved to environment\nvariables. Avoid hard coding these values in your project to prevent them from being\nleaked.\n\n\nWhen using our TypeScript SDK, the Access Token necessary to establish an authenticated connection with EVI is fetched and applied under the hood\nafter the Hume client is instantiated with your credientials.\nConnect\nWith the Hume client instantiated with our credentials, we can now establish an authenticated WebSocket connection with EVI and define our WebSocket event handlers.\nFor now we will include placeholder event handlers to be updated in later steps.\n\n\nAudio input\nTo capture audio and send it through the socket as an audio input, several steps are necessary. First, we need to handle user permissions\nto access the microphone. Next, we'll use the Media Stream API to capture the audio, and the MediaRecorder API to record the captured audio.\nWe then base64 encode the recording audio Blob, and finally send the encoded audio through the WebSocket using the sendAudioInputmethod.\n\n\n\n\nAccepted audio formats include: mp3, wav, aac, ogg, flac, webm, avr, cdda,\ncvs/vms, aiff, au, amr, mp2, mp4, ac3, avi, wmv, mpeg, ircam.\nAudio output\nThe response will comprise multiple messages, detailed as follows:\nuser_message: This message encapsulates the transcription of the audio input. Additionally, it\nincludes expression measurement predictions related to the speaker's vocal prosody.\n\nassistant_message: For every sentence within the response, an AssistantMessage is dispatched.\nThis message not only relays the content of the response but also features predictions regarding the\nexpressive qualities of the generated audio response.\n\naudio_output: Accompanying each AssistantMessage, an AudioOutput message will be provided.\nThis contains the actual audio (binary) response corresponding to an AssistantMessage.\n\nassistant_end: Signifying the conclusion of the response to the audio input, an AssistantEnd\nmessage is delivered as the final piece of the communication.\n\n\nHere we'll focus on playing the received audio output. To play the audio output from the response we\nneed to define our logic for converting the received binary to a Blob, and creating an HTMLAudioInput\nto play the audio.\nWe then need to update the client's on message WebSocket event handler to invoke\nthe logic to playback the audio when receiving the audio output. To manage playback for the incoming\naudio here we'll implement a queue and sequentially play the audio back.\n\n\nInterrupt\nInterruptibility is a distinguishing feature of the Empathic Voice Interface. If an audio input is sent\nthrough the WebSocket while receiving response messages for a previous audio input, the response to\nthe previous audio input will stop being sent. Additionally the interface will send back a\nuser_interruption message, and begin responding to the new audio input."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.quickstart.python-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/quickstart/python",
    "pathname": "/docs/empathic-voice-interface-evi/quickstart/python",
    "title": "EVI Python Quickstart Guide",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      },
      {
        "title": "Quickstart",
        "pathname": "/docs/empathic-voice-interface-evi/quickstart"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "A quickstart guide for implementing the Empathic Voice Interface (EVI) with Python.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "python -m venv evi-env"
      },
      {
        "lang": "bash",
        "code": "python -m venv evi-env"
      },
      {
        "lang": "bash",
        "code": "source evi-env/bin/activate"
      },
      {
        "lang": "bash",
        "code": "source evi-env/bin/activate"
      },
      {
        "lang": "bash",
        "code": "conda create --name evi-env python=3.11"
      },
      {
        "lang": "bash",
        "code": "conda create --name evi-env python=3.11"
      },
      {
        "lang": "bash",
        "code": "conda activate evi-env"
      },
      {
        "lang": "bash",
        "code": "conda activate evi-env"
      },
      {
        "lang": "bash",
        "code": "pip install \"hume[microphone]\""
      },
      {
        "lang": "bash",
        "code": "pip install \"hume[microphone]\""
      },
      {
        "lang": "bash",
        "code": "pip install python-dotenv"
      },
      {
        "lang": "bash",
        "code": "pip install python-dotenv"
      },
      {
        "lang": "bash",
        "code": "brew install ffmpeg"
      },
      {
        "lang": "bash",
        "code": "brew install ffmpeg"
      },
      {
        "lang": "bash",
        "code": "sudo apt-get --yes update\nsudo apt-get --yes install libasound2-dev libportaudio2 ffmpeg"
      },
      {
        "lang": "bash",
        "code": "sudo apt-get --yes update\nsudo apt-get --yes install libasound2-dev libportaudio2 ffmpeg"
      },
      {
        "lang": "py",
        "code": "import asyncio\nimport base64\nimport datetime\nimport os\nfrom dotenv import load_dotenv\nfrom hume.client import AsyncHumeClient\nfrom hume.empathic_voice.chat.socket_client import ChatConnectOptions, ChatWebsocketConnection\nfrom hume.empathic_voice.chat.types import SubscribeEvent\nfrom hume.empathic_voice.types import UserInput\nfrom hume.core.api_error import ApiError\nfrom hume import MicrophoneInterface, Stream"
      },
      {
        "lang": "py",
        "code": "import asyncio\nimport base64\nimport datetime\nimport os\nfrom dotenv import load_dotenv\nfrom hume.client import AsyncHumeClient\nfrom hume.empathic_voice.chat.socket_client import ChatConnectOptions, ChatWebsocketConnection\nfrom hume.empathic_voice.chat.types import SubscribeEvent\nfrom hume.empathic_voice.types import UserInput\nfrom hume.core.api_error import ApiError\nfrom hume import MicrophoneInterface, Stream"
      },
      {
        "lang": "py",
        "code": "async def main() -> None:\n  # Retrieve any environment variables stored in the .env file\n  load_dotenv()\n\n  # Retrieve the API key, Secret key, and EVI config id from the environment variables\n  HUME_API_KEY = os.getenv(\"HUME_API_KEY\")\n  HUME_SECRET_KEY = os.getenv(\"HUME_SECRET_KEY\")\n  HUME_CONFIG_ID = os.getenv(\"HUME_CONFIG_ID\")\n\n  # Initialize the asynchronous client, authenticating with your API key\n  client = AsyncHumeClient(api_key=HUME_API_KEY)\n\n  # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication\n  options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)\n  \n  # ..."
      },
      {
        "lang": "py",
        "code": "async def main() -> None:\n  # Retrieve any environment variables stored in the .env file\n  load_dotenv()\n\n  # Retrieve the API key, Secret key, and EVI config id from the environment variables\n  HUME_API_KEY = os.getenv(\"HUME_API_KEY\")\n  HUME_SECRET_KEY = os.getenv(\"HUME_SECRET_KEY\")\n  HUME_CONFIG_ID = os.getenv(\"HUME_CONFIG_ID\")\n\n  # Initialize the asynchronous client, authenticating with your API key\n  client = AsyncHumeClient(api_key=HUME_API_KEY)\n\n  # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication\n  options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)\n  \n  # ..."
      },
      {
        "lang": "py",
        "meta": "{6-16}",
        "code": "async def main() -> None:\n  # ...\n  # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication\n  options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)\n\n  # Instantiate the WebSocketHandler\n  websocket_handler = WebSocketHandler()\n\n  # Open the WebSocket connection with the configuration options and the handler's functions\n    async with client.empathic_voice.chat.connect_with_callbacks(\n      options=options,\n      on_open=websocket_handler.on_open,\n      on_message=websocket_handler.on_message,\n      on_close=websocket_handler.on_close,\n      on_error=websocket_handler.on_error\n    ) as socket:\n    \n      # Set the socket instance in the handler\n      websocket_handler.set_socket(socket)\n      # ..."
      },
      {
        "lang": "py",
        "meta": "{6-16}",
        "code": "async def main() -> None:\n  # ...\n  # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication\n  options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)\n\n  # Instantiate the WebSocketHandler\n  websocket_handler = WebSocketHandler()\n\n  # Open the WebSocket connection with the configuration options and the handler's functions\n    async with client.empathic_voice.chat.connect_with_callbacks(\n      options=options,\n      on_open=websocket_handler.on_open,\n      on_message=websocket_handler.on_message,\n      on_close=websocket_handler.on_close,\n      on_error=websocket_handler.on_error\n    ) as socket:\n    \n      # Set the socket instance in the handler\n      websocket_handler.set_socket(socket)\n      # ..."
      },
      {
        "lang": "py",
        "meta": "{7-11}",
        "code": "async def main() -> None:\n  # Open the WebSocket connection with the configuration options and the handler's functions\n  async with client.empathic_voice.chat.connect_with_callbacks(...) as socket:\n    # Set the socket instance in the handler\n    websocket_handler.set_socket(socket)\n\n    # Create an asynchronous task to continuously detect and process input from the microphone, as well as play audio\n    microphone_task = asyncio.create_task(\n      MicrophoneInterface.start(\n        socket,\n        byte_stream=websocket_handler.byte_strs\n      )\n    )\n    \n    # Await the microphone task\n    await microphone_task\n"
      },
      {
        "lang": "py",
        "meta": "{7-11}",
        "code": "async def main() -> None:\n  # Open the WebSocket connection with the configuration options and the handler's functions\n  async with client.empathic_voice.chat.connect_with_callbacks(...) as socket:\n    # Set the socket instance in the handler\n    websocket_handler.set_socket(socket)\n\n    # Create an asynchronous task to continuously detect and process input from the microphone, as well as play audio\n    microphone_task = asyncio.create_task(\n      MicrophoneInterface.start(\n        socket,\n        byte_stream=websocket_handler.byte_strs\n      )\n    )\n    \n    # Await the microphone task\n    await microphone_task\n"
      },
      {
        "lang": "bash",
        "code": "   0 DELL U2720QM, Core Audio (0 in, 2 out)\n   1 I, Phone 15 Pro Max Microphone, Core Audio (1 in, 0 out)\n>  2 Studio Display Microphone, Core Audio (1 in, 0 out)\n   3 Studio Display Speakers, Core Audio (0 in, 8 out)\n   4 MacBook Pro Microphone, Core Audio (1 in, 0 out)\n<  5 MacBook Pro Speakers, Core Audio (0 in, 2 out)\n   6 Pro Tools Audio Bridge 16, Core Audio (16 in, 16 out)\n   7 Pro Tools Audio Bridge 2-A, Core Audio (2 in, 2 out)\n   8 Pro Tools Audio Bridge 2-B, Core Audio (2 in, 2 out)\n   9 Pro Tools Audio Bridge 32, Core Audio (32 in, 32 out)\n  10 Pro Tools Audio Bridge 64, Core Audio (64 in, 64 out)\n  11 Pro Tools Audio Bridge 6, Core Audio (6 in, 6 out)\n  12 Apowersoft Audio Device, Core Audio (2 in, 2 out)\n  13 ZoomAudioDevice, Core Audio (2 in, 2 out)"
      },
      {
        "lang": "bash",
        "code": "   0 DELL U2720QM, Core Audio (0 in, 2 out)\n   1 I, Phone 15 Pro Max Microphone, Core Audio (1 in, 0 out)\n>  2 Studio Display Microphone, Core Audio (1 in, 0 out)\n   3 Studio Display Speakers, Core Audio (0 in, 8 out)\n   4 MacBook Pro Microphone, Core Audio (1 in, 0 out)\n<  5 MacBook Pro Speakers, Core Audio (0 in, 2 out)\n   6 Pro Tools Audio Bridge 16, Core Audio (16 in, 16 out)\n   7 Pro Tools Audio Bridge 2-A, Core Audio (2 in, 2 out)\n   8 Pro Tools Audio Bridge 2-B, Core Audio (2 in, 2 out)\n   9 Pro Tools Audio Bridge 32, Core Audio (32 in, 32 out)\n  10 Pro Tools Audio Bridge 64, Core Audio (64 in, 64 out)\n  11 Pro Tools Audio Bridge 6, Core Audio (6 in, 6 out)\n  12 Apowersoft Audio Device, Core Audio (2 in, 2 out)\n  13 ZoomAudioDevice, Core Audio (2 in, 2 out)"
      },
      {
        "lang": "python",
        "code": "# Specify device 4 in MicrophoneInterface\nMicrophoneInterface.start(\n  socket,\n  device=4,\n  allow_user_interrupt=True,\n  byte_stream=websocket_handler.byte_strs\n)"
      },
      {
        "lang": "python",
        "code": "# Specify device 4 in MicrophoneInterface\nMicrophoneInterface.start(\n  socket,\n  device=4,\n  allow_user_interrupt=True,\n  byte_stream=websocket_handler.byte_strs\n)"
      },
      {
        "lang": "python",
        "code": "# Directly import the sounddevice library\nimport sounddevice as sd\n\n# Set the default device prior to scheduling audio input task\nsd.default.device = 4"
      },
      {
        "lang": "python",
        "code": "# Directly import the sounddevice library\nimport sounddevice as sd\n\n# Set the default device prior to scheduling audio input task\nsd.default.device = 4"
      },
      {
        "lang": "python",
        "code": "# Specify allowing interruption\nMicrophoneInterface.start(\n  socket,\n  allow_user_interrupt=True,\n  byte_stream=websocket_handler.byte_strs\n)"
      },
      {
        "lang": "python",
        "code": "# Specify allowing interruption\nMicrophoneInterface.start(\n  socket,\n  allow_user_interrupt=True,\n  byte_stream=websocket_handler.byte_strs\n)"
      },
      {
        "lang": "py",
        "code": "asyncio.run(main())"
      },
      {
        "lang": "py",
        "code": "asyncio.run(main())"
      }
    ],
    "content": "This guide provides detailed instructions for integrating EVI into your Python projects using Hume's Python SDK. It is divided into seven key components:\nEnvironment setup: Download package and system dependencies to run EVI.\n\nDependency imports: Import all necessary dependencies into your script.\n\nDefining a WebSocketHandler class: Create a class to manage the WebSocket connection.\n\nAuthentication: Use your API credentials to authenticate your EVI application.\n\nConnecting to EVI: Set up a secure WebSocket connection to interact with EVI.\n\nHandling audio: Capture audio data from an input device, and play audio produced by EVI.\n\nAsynchronous event loop: Initiate and manage an asynchronous event loop that handles simultaneous, real-time execution of message processing and audio playback.\n\n\nTo see a full implementation within a terminal application, visit our API examples repository on GitHub: evi-python-example\n\n\nHume's Python SDK supports EVI using Python versions 3.9, 3.10, and 3.11 on macOS and Linux platforms. The full specification be found on the Python SDK GitHub page.\n\n\nEnvironment setup\nBefore starting the project, it is essential to set up the development environment.\nCreating a virtual environment (optional)\nSetting up a virtual environment is a best practice to isolate your project's dependencies from your global Python installation, avoiding potential conflicts.\nYou can create a virtual environment using either Python's built-in venv module or the conda environment manager. See instructions for both below:\n\n\n\n\nCreate the virtual environment.\n\n\nNote that when you create a virtual environment using Python's built-in venv tool, the virtual environment will use the same Python version as the global Python installation that you used to create it.\n\n\nActivate the virtual environment using the appropriate command for your system platform.\n\n\n\n\n\n\nThe code above demonstrates virtual environment activation on a POSIX platform with a bash/zsh shell. Visit the venv documentation to learn more about using venv on your platform.\n\n\nInstall conda from Miniconda or Anaconda Distribution.\n\nCreate the virtual environment.\n\n\nconda allows developers to set the version of their Python interpreter when creating a virtual environment. In the example below, Python version 3.11 is specified:\n\n\nActivate the virtual environment using the appropriate command for your system platform.\n\n\n\n\n\n\nVisit the conda documentation to learn more about managing Python environments with conda.\nPackage dependenices\nThere are two package dependencies for using EVI:\nHume Python SDK (required)\n\n\nThe hume[microphone] package contains the Hume Python SDK. This guide employs EVI's WebSocket and message handling infrastructure as well as various asynchronous programming and audio utilities.\n\n\nEnvironment variables (recommended)\n\n\nThe python-dotenv package contains the logic for using environment variables to store and load sensitive variables such as API credentials from a .env file.\n\n\nIn sample code snippets below, the API key, Secret key, and an EVI configuration id have been saved to environment variables.\n\n\nWhile not strictly required, using environment variables is considered best practice because it keeps sensitive information like API keys and configuration settings separate from your codebase. This not only enhances security but also makes your application more flexible and easier to manage across different environments.\nSystem dependencies\nFor audio playback and processing, additional system-level dependencies are required. Below are download instructions for each supported operating system:\n\n\n\n\nTo ensure audio playback functionality, macOS users will need to install ffmpeg, a powerful multimedia framework that handles audio and video processing.\nA common way to install ffmpeg on macOS is by using a package manager such as Homebrew. To do so, follow these steps:\nInstall Homebrew onto your system according to the instructions on the Homebrew website.\n\nOnce Homebrew is installed, you can install ffmpeg with brew:\n\n\n\n\n\n\nIf you prefer not to use Homebrew, you can download a pre-built ffmpeg binary from the ffmpeg website or use other package managers like MacPorts.\n\n\nLinux users will need to install the following packages to support audio input/output and playback:\nlibasound2-dev: This package contains development files for the ALSA (Advanced Linux Sound Architecture) sound system.\n\nlibportaudio2: PortAudio is a cross-platform audio I/O library that is essential for handling audio streams.\n\nffmpeg: Required for processing audio and video files.\n\n\nTo install these dependencies, use the following commands:\n\n\nDependency imports\nThe following import statements are used in the example project to handle asynchronous operations, environment variables, audio processing, and communication with the Hume API:\n\n\n\n\n\n\n\n\nModule/Class/Method Description \nasyncio Provides support for asynchronous programming, allowing the code to handle multiple tasks concurrently. \nbase64 Used to encode and decode audio data in base64 format, essential for processing audio streams. \nos Allows interaction with the operating system, particularly for accessing environment variables. \ndatetime Used to generate timestamps for logging events. \nload_dotenv Loads environment variables from a .env file, which are used for API key management and EVI configuration. \nAsyncHumeClient Provides an asynchronous client for connecting to the Hume API, which powers the empathic voice interface. \nChatConnectOptions, ChatWebsocketConnection These classes manage WebSocket connections and configuration options for the Hume Empathic Voice Interface (EVI). \nSubscribeEvent Represents different types of messages received through the WebSocket connection. \nUserInput, AudioConfiguration, SessionSettings These types define the structure of messages and settings sent to the Hume API, such as user input and audio configurations. \nStream Manages streams of asynchronous data, particularly useful for handling audio streams. \nMicrophoneInterface Manages audio capture and playback from a specified input and output device. \nApiError Defines custom error handling for API-related issues, ensuring graceful error management within the application. \n\nDefining a WebSocketHandler class\nNext, we define a WebSocketHandler class to encapsulate WebSocket functionality in one organized component. The handler allows us to implement application-specific behavior upon the socket opening, closing, receiving messages, and handling errors. It also manages the continuous audio stream from a microphone.\nBy using a class, you can maintain the WebSocket connection and audio stream state in one place, making it simpler to manage both real-time communication and audio processing.\nBelow are the key methods:\nMethod Description \n__init__() Initializes the handler, setting up placeholders for the WebSocket connection. \nset_socket(socket: ChatWebsocketConnection) Associates the WebSocket connection with the handler. \non_open() Called when the WebSocket connection is established, enabling any necessary initialization. \non_message(data: SubscribeEvent) Handles incoming messages from the WebSocket, processing different types of messages. \non_close() Invoked when the WebSocket connection is closed, allowing for cleanup operations. \non_error(error: Exception) Manages errors that occur during WebSocket communication, providing basic error logging. \n\n\n\nBelow is an example of what the WebSocketHandler class may look like.\nRefer to the evi-python-example for a complete example implementation.\n\n\nAuthentication\nIn order to establish an authenticated connection, we instantiate the Hume client with our API key and include our Secret key in the query parameters passed into the WebSocket connection.\n\n\nYou can obtain your API credentials by logging into the Hume Platform and visiting the API keys page.\n\n\nConnecting to EVI\nWith the Hume client instantiated with our credentials, we can now establish an authenticated WebSocket connection with EVI and pass in our handlers.\n\n\nHandling audio\nThe MicrophoneInterface class captures audio input from the user's device and streams it over the WebSocket connection.\nAudio playback occurs when the WebSocketHandler receives audio data over the WebSocket connection in its asynchronous byte stream from an audio_output message.\nIn this example, byte_strs is a stream of audio data that the WebSocket connection populates.\n\n\nSpecifying a microphone device\nYou can specify your microphone device using the device parameter in the MicrophoneInterface object's start method.\nTo view a list of available audio devices, run the following command:\n\n\npython -c \"import sounddevice; print(sounddevice.query_devices())\"\nBelow is an example output:\n\n\nIf the MacBook Pro Microphone is the desired device, specify device 4 in the Microphone context. For example:\n\n\nFor troubleshooting faulty device detection - particularly with systems using ALSA, the Advanced Linux Sound Architecture, the device may also be directly specified using the sounddevice library:\n\n\nAllowing interruption\nThe allow_interrupt parameter in the MicrophoneInterface class allows control over whether the user can send a message while the assistant is speaking:\n\n\nallow_interrupt=True: Allows the user to send microphone input even when the assistant is speaking. This enables more fluid, overlapping conversation.\n\nallow_interrupt=False: Prevents the user from sending microphone input while the assistant is speaking, ensuring that the user does not interrupt the assistant. This is useful in scenarios where clear, uninterrupted communication is important.\n\n\nAsynchronous event loop\nInitialize, execute, and manage the lifecycle of the asynchronous event loop, making sure that the main() coroutine and its runs effectively and that the application shuts down cleanly after the coroutine finishes executing."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "title": "Empathic Voice Interface 2 (EVI 2)",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Introducing EVI 2, our new voice-language foundation model, enabling human-like conversations with enhanced naturalness, emotional responsiveness, adaptability, and rich customization options for the voice and personality.",
    "content": "The Empathic Voice Interface 2 (EVI 2) introduces a new architecture that seamlessly integrates voice and language processing. This multimodal approach allows EVI 2 to understand and generate both language and voice, dramatically enhancing key features over EVI 1 while also enabling new capabilities.\nEVI 2 can converse rapidly and fluently with users, understand a user's tone of voice, generate any tone of voice, and can even handle niche requests like rapping, changing its style, or speeding up its speech. The model specifically excels at emulating a wide range of personalities, including their accents and speaking styles. It is exceptional at maintaining personalities that are fun and interesting to interact with. Ultimately, EVI 2 is capable of emulating the ideal personality for every application and user.\nIn addition, EVI 2 allows developers to create custom voices by using a new voice modulation method. Developers can adjust EVI 2's base voices along a number of continuous scales, including gender, nasality, and pitch. This first-of-its-kind feature enables creating voices that are unique to an application or even a single user. Further, this feature does not rely on voice cloning, which currently invokes more risks than any other capability of this technology.\n\n\nThe EVI 2 API is currently in beta. We are still making ongoing\nimprovements to the model. In the coming weeks and months, EVI 2 will sound\nbetter, speak more languages, follow more complex instructions, and use a\nwider range of tools."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-key-improvements-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "title": "Key improvements",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#key-improvements",
    "content": "EVI 2 uses an advanced voice generation model connected to our eLLM, which can process and generate both text and audio. This results in more natural-sounding speech with better word emphasis, higher expressiveness, and more consistent vocal output.\n\n\nThe integrated architecture of EVI 2 reduces end-to-end latency by 40% vs EVI 1, now averaging around 500ms. This significant speed improvement enables more responsive and human-like conversations.\n\n\nBy processing voice and language in the same model, EVI 2 can better understand the emotional context of user inputs and generate more empathic responses, both in terms of content and vocal tone.\n\n\nEVI 2 offers new control over the AI's voice characteristics. Developers can adjust various parameters to tailor EVI 2's voice to their specific application needs. EVI 2 also supports in-conversation voice prompting, allowing users to dynamically modify EVI's speaking style (e.g., \"speak faster\", \"sound excited\") during interactions.\n\n\nDespite its advanced capabilities, EVI 2 is 30% more cost-effective than its predecessor, with pricing reduced from $0.1020 to $0.0714 per minute.\nBeyond these improvements, EVI 2 also exhibits promising emerging capabilities including speech output in multiple languages. We will make these improvements available to developers as we scale up and improve the model.\nWe provide the same suite of tools to integrate and customize EVI 2 for your application as we do for EVI 1, and existing EVI developers can easily switch to the new system.",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)"
      },
      "h3": {
        "id": "key-improvements",
        "title": "Key improvements"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-building-with-evi-2-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "title": "Building with EVI 2",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#building-with-evi-2",
    "content": "Developers can start testing EVI 2 by simply creating an EVI config on the Hume platform. Just select EVI 2 as the version when creating your config.\nTo use EVI 2, simply create a configuration using the /v0/evi/configs endpoint and specify \"evi_version\": \"2\". Then, use this config in a conversation with EVI using the /v0/evi/chat endpoint. Most aspects of using EVI, including authentication strategies, remain the same as described in the EVI documentation.\nIn your configuration JSON, set the evi_version parameter to \"2\". Here's an example of an EVI 2 config:\n\n\nUsing a config like the above, make a POST request to the /v0/evi/configs endpoint to save the config.\n\nSpecify any other custom settings you need.",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"evi_version\": \"2\",\n  \"name\": \"EVI 2 config\",\n  \"voice\": {\n    \"provider\": \"HUME_AI\",\n    \"name\": \"DACHER\"\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)"
      },
      "h2": {
        "id": "building-with-evi-2",
        "title": "Building with EVI 2"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-evi-2-timeline-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "title": "EVI 2 timeline",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-2-timeline",
    "content": "EVI 2 is available now, with full feature parity with EVI 1, including support for supplemental LLMs, custom language models, tool use, built-in tools like web search, and all configuration options.\nFrom September to December 2024, the Hume team will focus on improving the reliability and quality of EVI 2. The team will ensure that all the features of the EVI 1 API work consistently in EVI 2.\nIn late December 2024, the EVI 1 API will be sunsetted and deprecated. Developers will need to migrate from EVI 1 to EVI 2 for ongoing support and new features.\n\n\nClear migration guidelines will be provided ahead of time, and our team will\nensure only minor changes will be required to make applications work with EVI\n2.",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)"
      },
      "h2": {
        "id": "evi-2-timeline",
        "title": "EVI 2 timeline"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-feature-comparison-evi-1-vs-evi-2-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "title": "Feature comparison: EVI 1 vs EVI 2",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#feature-comparison-evi-1-vs-evi-2",
    "content": "This table provides a comprehensive comparison of features between EVI 1 and EVI 2, highlighting the new capabilities introduced in the latest version.\nFeature EVI 1 EVI 2 \nVoice quality Similar to best TTS solutions Significantly improved naturalness, clarity, and expressiveness \nResponse latency ~900ms-2000ms ~500-800ms (about 2x faster) \nEmotional intelligence Empathic responses informed by expression measures End-to-end understanding of voice augmented with emotional intelligence training \nBase voices 3 core voice options (Kora, Dacher, Ito) 5 new high-quality base voice options with expressive personalities (8 total) \nVoice customizability Supported - can select base voices and adjust voice parameters Supported - extensive customization with parameter adjustments (e.g. pitch, huskiness, nasality) \nIn-conversation voice prompting Not supported Supported (e.g., \"speak faster\", \"sound more excited\", change accents) \nMultimodal processing Transcription augmented with high-dimensional voice measures Fully integrated voice and language processing within a single model, along with transcripts and expression measures \nSupplemental LLMs Supported Supported \nTool use and web search Supported Supported \nCustom language model (CLM) Supported Supported \nConfiguration options Extensive support Extensive support (same options as EVI 1) \nTypescript SDK support Supported Supported \nPython SDK support Supported Supported \nMultilingual support English only Expanded support for multiple languages planned for Q4 2024 \nCost $0.102 per minute $0.0714 per minute (30% reduction)",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)"
      },
      "h2": {
        "id": "feature-comparison-evi-1-vs-evi-2",
        "title": "Feature comparison: EVI 1 vs EVI 2"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-frequently-asked-questions-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "title": "Frequently Asked Questions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#frequently-asked-questions",
    "content": "Yes, for most configs, you will be able to update to EVI 2 simply by setting\nthe evi_version to 2 in the configuration. If your config uses a\nsupplemental LLM, we recommend that you select the same supplemental LLM for\nEVI 2. This will ensure stability and support for the same tools.\n\n\nEVI 2 is a beta API, and is still in progress as of September 2024. Our team has released the API to allow developers to experiment with EVI 2.\nOur developer platform team is available to assist with integration challenges to ensure smooth\ndeployment across various applications - join our\nDiscord for assistance.\nFor companies that are interested in using EVI 2 in production, please contact our partnerships team .\n\n\nAfter launch, we will make ongoing enhancements to naturalness, expressiveness, latency, consistency and reliability, and EVI's overall output quality.\nWe plan to add more granular options for tailoring EVI's personality and creating custom EVIs, including style references and audio prompts. We plan to make EVI multi-lingual and continuously add support for new major languages in Q4 2024.\nLater this year, we will likely integrate EVI with image and/or video modalities, allowing it to respond to the user's facial expressions using Hume's proprietary models of facial expressions.\n\n\nEVI 2's multimodal processing integrates voice and text in a single voice-language foundation model. This allows EVI 2 to understand and\ngenerate both language and voice in the same latent space, resulting in more\ncoherent and contextually aware responses. EVI 2's integrated voice-language architecture also offers\nunprecedented control over both the AI's personality and voice\ncharacteristics. Further, it allows prompting the\nmodel to change its speaking style or to follow a personality.\n\n\nHume has implemented several key safety measures for EVI 2:\nArchitectural safeguards: EVI 2's core architecture prevents unauthorized voice cloning by representing voice characteristics as abstract semantic tokens, not raw audio data. This allows personality imitation without enabling direct voice replication. Importantly, EVI 2 is incapable of voice cloning without access to its code. By controlling EVI 2's identity-related voice characteristics at the architecture level, we force the model to adopt one identity at a time, maintaining a consistent vocal register across sessions. We believe voice cloning currently invokes more risks than any other capability of voice AI, which is why we've implemented these architectural safeguards as a core feature of EVI 2.\n\nCustomizable language control: Developers can use their own LLMs or modify the supplemental LLM, enabling custom content filtering and safeguards tailored to their specific use cases.\n\nContinuous testing: Our team regularly red-teams and tests EVI 2 to identify and address potential vulnerabilities.\n\nUsage monitoring: We actively monitor API usage, classify major use cases, and can swiftly intervene if we detect misuse.\n\nClear guidelines: Our terms of use and the Hume Initiative guidelines prohibit malicious applications of our technology.\n\n\nThese measures ensure responsible deployment while providing developers the necessary control and transparency for their specific applications.\n\n\nYes, we plan to make EVI multilingual and support other languages in Q4 2024, in the following order:\nMultiple English accents (e.g. Australian, British)\n\nCommon European languages (Spanish, German, Italian, French, Portuguese)\n\nAdditional languages based on customer demand (including Arabic, Japanese, Korean, Hindi, Dutch, Swedish, Turkish, Russian, Mandarin)\n\n\nEVI 2's ability to learn new languages efficiently with minimal data will facilitate this expansion to more languages.\n\n\nEVI 2's speech recognition capabilities are robust across a wide range of\nscenarios. It is highly accurate across a wide range of accents, breathing\npatterns, and individual speaking patterns. Performance may degrade in\nenvironments with significant background noise, or when multiple speakers\noverlap. We continue to work on improving performance in challenging acoustic\nenvironments.",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)"
      },
      "h2": {
        "id": "frequently-asked-questions",
        "title": "Frequently Asked Questions"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "Configuring EVI",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Guide to configuring the Empathic Voice Interface (EVI).",
    "content": "The Empathic Voice Interface (EVI) is designed to be highly configurable, allowing developers to customize the interface to align with their specific requirements.\nConfiguration of EVI can be managed through two primary methods: an EVI configuration and session settings."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration-configuration-options-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "Configuration options",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#configuration-options",
    "content": "EVI configuration options affect the behavior and capabilities of the interface, and include the following configuration options:\nOption Description \nVoice Select a voice from a list of 8 preset options or create a custom voice. For further details, see our guide on creating custom voices. \nEVI version Select the version of EVI you would like to use. For details on similarities and differences between EVI versions 1 and 2, refer to our feature comparison. \nSystem prompt Provide a system prompt to guide how EVI should respond. For details on expressive prompt engineering, refer to our prompting guide. \nLanguage model Select a language model that best fits your application’s needs. For details on selecting a supplementary language model to meet your needs, such as optimizing for lowest latency, refer to our EVI FAQ. To incorporate your own language model, refer to our guide on using your own language model. \nTools Choose user-created or built-in tools for EVI to use during conversations. For details on creating tools and adding them to your configuration, see our tool use guide. \nEvent messages Configure messages that EVI will send in specific situations. For details on configuring event messages, see our API Reference. \nTimeouts Define limits on a chat with EVI to manage conversation flow. For details on specifying timeouts, see our API Reference. \n\n\n\nConfigs, as well as system prompts and tools, are versioned.\nThis versioning system supports iterative development, allowing you to\nprogressively refine configurations and revert to previous versions if needed.",
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI"
      },
      "h2": {
        "id": "configuration-options",
        "title": "Configuration options"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration-default-configuration-options-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "Default configuration options",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#default-configuration-options",
    "content": "EVI is pre-configured with a set of default values, which are automatically applied if you do not specify a configuration.\nThe default configuration includes a preset voice and language model, but does not include a system prompt or tools.\nTo customize these options, you will need to create and specify your own EVI configuration.\nThe default configuration settings are as follows:\nEVI 1 EVI 2 \nLanguage model: Claude 3.5 Sonnet Language model: hume-evi-2 \nVoice: Ito Voice: Ito \nSystem prompt: Hume default System prompt: Hume default \nTools: None Tools: None \n\n\n\nDefault configuration settings are subject to change. To ensure your setup remains consistent should changes occur, we recommend choosing explicit options when defining your EVI configuration.",
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI"
      },
      "h2": {
        "id": "default-configuration-options",
        "title": "Default configuration options"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration-create-a-configuration-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "Create a configuration",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#create-a-configuration",
    "content": "See instructions below for creating an EVI configuration through the Portal. In the Portal, navigate to the EVI Configurations page. Click the Create configuration button to begin.\n\n\n\n\n\n\nChoose EVI version\nTo learn more about the differences between EVI versions 1 and 2, please see the feature comparison guide.\n\n\n\n\nChoose voice\nSelect a voice from Hume's 8 presets, or create your own custom voice. To learn more about voice customization options on the Hume Platform, please visit the Voices page.\n\n\n\n\nSet up the LLM\nSelect a supported language model and specify a system prompt.\n\n\n\n\nAdd tools\nEquip EVI with built-in tools, like web search, or custom user-defined tools. Click the + Add button to select an existing tool or create a new one.\n\n\n\n\nName config\nName your EVI configuration and add an optional description.\n\n\n\n\nTest the configuration\nThe newly created configuration can now be tested. From the EVI Config details page, click Run in playground to test it out.\n\n\n\n\nOnce in the EVI Playground, click Start call to connect to EVI with your configuration.\n\n\n\n\nThe event message and timeout\nconfiguration options are not part of the initial config creation flow. However, you can set these options at any time in the playground or from the configuration's edit page after your configuration has been created.\n\n\n\n\n\n\n\n\nApply the configuration\nAfter creating an EVI configuration, you can use it in your conversations with EVI by including the config_id\nin the query parameters of your connection request. You can find the config_id on the configuration's edit page. To access this page, first navigate to the configurations page\nand then click the Edit button for the desired configuration.\n\n\n\n\nSee the sample code below which showcases how to apply your configuration:",
    "code_snippets": [
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n// instantiate the HumeClient with credentials\n// avoid hard coding your API key, retrieve from environment variables\nconst client = new HumeClient({\n  apiKey: <YOUR_API_KEY>,\n  secretKey: <YOUR_SECRET_KEY>,\n});\n// instantiate WebSocket connection with specified EVI config\nconst socket = await client.empathicVoice.chat.connect({\n  configId: <YOUR_CONFIG_ID> // specify config ID here\n});"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n// instantiate the HumeClient with credentials\n// avoid hard coding your API key, retrieve from environment variables\nconst client = new HumeClient({\n  apiKey: <YOUR_API_KEY>,\n  secretKey: <YOUR_SECRET_KEY>,\n});\n// instantiate WebSocket connection with specified EVI config\nconst socket = await client.empathicVoice.chat.connect({\n  configId: <YOUR_CONFIG_ID> // specify config ID here\n});"
      },
      {
        "lang": "python",
        "code": "from hume import HumeVoiceClient, MicrophoneInterface\n# avoid hard coding your API key, retrieve from environment variables\nHUME_API_KEY = <YOUR_API_KEY>\n# Connect and authenticate with Hume\nclient = HumeVoiceClient(HUME_API_KEY)\n# establish a connection with EVI with your configuration by passing\n# the config_id as an argument to the connect method\nasync with client.connect(config_id=\"<your-config-id>\") as socket:\n  await MicrophoneInterface.start(socket)"
      },
      {
        "lang": "python",
        "code": "from hume import HumeVoiceClient, MicrophoneInterface\n# avoid hard coding your API key, retrieve from environment variables\nHUME_API_KEY = <YOUR_API_KEY>\n# Connect and authenticate with Hume\nclient = HumeVoiceClient(HUME_API_KEY)\n# establish a connection with EVI with your configuration by passing\n# the config_id as an argument to the connect method\nasync with client.connect(config_id=\"<your-config-id>\") as socket:\n  await MicrophoneInterface.start(socket)"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI"
      },
      "h2": {
        "id": "create-a-configuration",
        "title": "Create a configuration"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration-session-settings-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "Session settings",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#session-settings",
    "content": "EVI configurations are persistent and version-controlled. In contrast, session settings are temporary and apply only to the current session, such as\nmicrophone settings. These parameters can be adjusted dynamically based on the requirements of each session to ensure optimal performance and user experience.\n\n\nRefer to the API reference for detailed descriptions of the various session settings options.\nUpdating the session settings is only a requirement when the audio input is encoded in PCM Linear 16. If this is the case, be sure to send the following Session Settings message prior to sending an audio input:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"type\": \"session_settings\",\n  \"audio\": {\n    \"channels\": 1,\n    \"encoding\": \"linear16\",\n    \"sample_rate\": 48000\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"session_settings\",\n  \"audio\": {\n    \"channels\": 1,\n    \"encoding\": \"linear16\",\n    \"sample_rate\": 48000\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI"
      },
      "h2": {
        "id": "session-settings",
        "title": "Session settings"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration-dynamic-variables-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "Dynamic variables",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#dynamic-variables",
    "content": "EVI can reference and update session-specific values as a conversation progresses. By including dynamic variables in the system prompt, you can personalize conversations with EVI.\n\n\nVisit our prompting guide for more details on adding dynamic variables to your prompt.\nTo use this feature, first define a variables parameter within a Session Settings message containing one or more dynamic variables.\nEach key-value pair in \"variables\" must have a string key representing the variable name and a string value.\nFor example, you can define the user's name and set it to a default value:\n\n\nThen, include the variable surrounded by two pairs of curly braces, such as {{name}}, as a placeholder value in your system prompt:\n\n\nTo ensure your dynamic variables are recognized properly, confirm the following:\nIf you have defined a dynamic variable, reference it.: If the variable is defined but not referenced in the system prompt, then it will not be included in the conversation and EVI will not be able to refer to the variable.\n\nIf you have referenced a dynamic variable, make sure it is defined.: If the variable is referenced in the system prompt, but it is not defined in the \"variables\" field, then the warning W0106 will be raised: \"No values have been specified for the variables [variable_name] which can lead to incorrect text formatting. Please assign them values.\" For example, this error can occur when there are spelling mistakes or differences between the variable defined in the \"variables\" field and the variable referenced in the system prompt (i.e. {{firstName}} instead of {{name}}).",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"type\": \"session_settings\",\n  \"variables\": {\n    \"name\": \"David Hume\",\n    \"age\": \"65\",\n    \"is_philosopher\": \"True\"\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"session_settings\",\n  \"variables\": {\n    \"name\": \"David Hume\",\n    \"age\": \"65\",\n    \"is_philosopher\": \"True\"\n  }\n}"
      },
      {
        "lang": "text",
        "code": "Address the user by their name, {{name}}.\nIf relevant, reference their age: {{age}}.\nIt is {{is_philosopher}} that this user is a philosopher."
      },
      {
        "lang": "text",
        "code": "Address the user by their name, {{name}}.\nIf relevant, reference their age: {{age}}.\nIt is {{is_philosopher}} that this user is a philosopher."
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI"
      },
      "h2": {
        "id": "session-settings",
        "title": "Session settings"
      },
      "h3": {
        "id": "dynamic-variables",
        "title": "Dynamic variables"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.voices-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/voices",
    "pathname": "/docs/empathic-voice-interface-evi/voices",
    "title": "Voices",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Guide to customizing the voice of the Empathic Voice Interface (EVI).",
    "content": "The Empathic Voice Interface (EVI) can be configured with any of our 8 base voices. You can also customize these voices by adjusting specific attributes. This guide explains each attribute and provides a tutorial for creating a custom voice.\nVisit the Playground to test the base voices.\n\n\nThe custom voices feature is experimental and under active development. Regular updates will focus on improving stability and expanding attribute options."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.voices-voice-attributes-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/voices",
    "pathname": "/docs/empathic-voice-interface-evi/voices",
    "title": "Voice attributes",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#voice-attributes",
    "content": "The following attributes can be modified to personalize any of the base voices:\nAttribute Description \nGender The vocalization of gender, ranging between more masculine and more feminine. \nArticulation The clarity of the voice, ranging between mumbled and articulate. \nAssertiveness The firmness of the voice, ranging between whiny and bold. \nBuoyancy The density of the voice, ranging between deflated and buoyant. \nConfidence The assuredness of the voice, ranging between shy and confident. \nEnthusiasm The excitement within the voice, ranging between calm and enthusiastic. \nNasality The openness of the voice, ranging between clear and nasal. \nRelaxedness The stress within the voice, ranging between tense and relaxed. \nSmoothness The texture of the voice, ranging between smooth and staccato. \nTepidity The liveliness behind the voice, ranging between tepid and vigorous. \nTightness The containment of the voice, ranging between tight and breathy. \n\nEach voice attribute can be adjusted relative to the base voice’s characteristics. Values range from -100 to 100, with 0 as the default.\nSetting all attributes to their default values will keep the base voice unchanged.",
    "hierarchy": {
      "h0": {
        "title": "Voices"
      },
      "h2": {
        "id": "voice-attributes",
        "title": "Voice attributes"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.voices-crafting-custom-voices-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/voices",
    "pathname": "/docs/empathic-voice-interface-evi/voices",
    "title": "Crafting custom voices",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#crafting-custom-voices",
    "content": "See instructions below for customizing a voice through the Portal.\n\n\nNavigate to the Voices page\nIn the Portal, find the EVI Voices page. Click the Create voice button to begin.\n\n\n\n\nCreate a new custom voice\nEnter a name for your custom voice and select a base voice. Then, adjust the attributes.\n\n\n\n\nTest your custom voice\nAs you make tweaks to the attributes, sample audio can be generated by clicking the \"▶\" button in the Voice sample section at the bottom of the form.\n\n\n\n\nUse your custom voice\nThe newly created voice can now be deployed. From the Voices page, click Use to create an EVI configuration with it.\n\n\n\n\nWhen creating an EVI configuration, choose Custom voice and press the + Select button. Then, press Select existing custom voice... and confirm the custom voice you would like to use.",
    "hierarchy": {
      "h0": {
        "title": "Voices"
      },
      "h2": {
        "id": "crafting-custom-voices",
        "title": "Crafting custom voices"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Tool use",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Guide to using function calling with the Empathic Voice Interface (EVI).",
    "content": "EVI simplifies the integration of external APIs through function calling. Developers can integrate custom functions that are invoked dynamically based on the user’s\ninput, enabling more useful conversations. There are two key concepts for using function calling with EVI: Tools and Configurations (Configs):\nTools are resources that EVI uses to do things, like search the web or call external APIs. For example, tools can check the weather, update databases, schedule appointments, or take\nactions based on what occurs in the conversation. While the tools can be user-defined, Hume also offers natively implemented tools, like web search, which are labeled as “built-in” tools.\n\nConfigurations enable developers to customize an EVI’s behavior and incorporate these custom tools. Setting up an EVI configuration allows developers to seamlessly integrate\ntheir tools into the voice interface. A configuration includes prompts, user-defined tools, and other settings.\n\n\n\n\n\n\n\n\nCurrently, our function calling feature only supports\nOpenAI and Anthropic models.\nFor the best results, we suggest choosing a fast and intelligent LLM that performs well on function calling benchmarks.\nOn account of its speed and intelligence, we recommend Claude 3.5 Haiku as the supplemental LLM in your EVI configuration when using tools.\nFunction calling is not available if you are using your own custom language\nmodel. We plan to\nsupport more function calling LLMs in the future.\nThe focus of this guide is on creating a Tool and a Configuration that allows EVI to use the Tool. Additionally, this guide details the message flow of function calls within a\nsession, and outlines the expected responses when function calls fail. Refer to our Configuration Guide for detailed,\nstep-by-step instructions on how to create and use an EVI Configuration.\n\n\nExplore these sample projects to see how Tool use can be implemented in TypeScript,\nNext.js, and Python."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-setup-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Setup",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#setup",
    "content": "For EVI to leverage tools or call functions, a configuration must be created with the tool’s definition. Our step-by-step guide below walks you through creating a tool and adding it to a configuration, using either a no-code approach through our Portal or a full-code approach through our API.\n\n\n\n\n\n\nCreate a Tool\nWe will first create a Tool with a specified function. In this example, we will create a tool for getting the weather. In the Portal, navigate to the EVI Tools page. Click the Create tool button to begin.\n\n\n\n\nFill in Tool details\nNext, we will fill in the details for a weather tool named get_current_weather. This tool fetches the current weather conditions in a specified location and reports the temperature in either Celsius or Fahrenheit. We can establish the tool's behavior by completing the following fields:\nName: Specify the name of the function that the language model will invoke. Ensure it begins with a lowercase letter and only contains letters, numbers, or underscores.\n\nDescription: Provide a brief description of what the function does.\n\nParameters: Define the function's input parameters using a JSON schema.\n\n\n\n\n\n\nThe JSON schema defines the expected structure of a function's input parameters. Here's an example JSON schema we can use for the parameters field of a weather function:\n\n\nCreate a Configuration\nNext, we will create an EVI Configuration called Weather Assistant Config. This configuration will utilize the get_current_weather Tool created in the previous step. See our Configuration guide for step-by-step instructions on how to create a configuration.\nDuring the Set up LLM step, remember to select an Anthropic or OpenAI model for tool use support.\n\n\n\n\nAdd Tool to Configuration\nFinally, we will specify the get_current_weather Tool in the Weather Assistant Config. Navigate to the Tools section of the EVI Config details page. Click the Add button to add a function to your configuration.\nSince we have already created a get_current_weather Tool in previous steps, we can simply select Add existing tool... from the dropdown to specify it.\n\n\n\n\nSelect the tool to add get_current_weather to your configuration, then complete the remaining steps to create the configuration.\n\n\n\n\n\n\n\n\nCreate a Tool\nWe will first create a Tool with a specified function. In this example, we will create a tool for getting the weather. Create this tool by making a POST request to\n/tools using the following request body:\n\n\n\n\nThe parameters field must contain a valid JSON schema.\n\n\nRecord the value in the id field, as we will use it to specify the newly created Tool in the next step.\nCreate a Configuration\nNext, we will create an EVI Configuration called Weather Assistant Config, and include the created Tool by making a POST request to /configs with the\nfollowing request body:\n\n\n\n\n\n\nEnsure your tool definitions conform to the language model's schema. The\nspecified language model will be the one to execute the function calls.",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"type\": \"object\",\n  \"required\": [\"location\", \"format\"],\n  \"properties\": {\n    \"location\": {\n      \"type\": \"string\",\n      \"description\": \"The city and state, e.g. San Francisco, CA\"\n    },\n    \"format\": {\n      \"type\": \"string\",\n      \"enum\": [\"celsius\", \"fahrenheit\"],\n      \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n    }\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"object\",\n  \"required\": [\"location\", \"format\"],\n  \"properties\": {\n    \"location\": {\n      \"type\": \"string\",\n      \"description\": \"The city and state, e.g. San Francisco, CA\"\n    },\n    \"format\": {\n      \"type\": \"string\",\n      \"enum\": [\"celsius\", \"fahrenheit\"],\n      \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n    }\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"get_current_weather\",\n  \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"get_current_weather\",\n  \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"tool_type\": \"FUNCTION\",\n  \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n  \"version\": 0,\n  \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n  \"name\": \"get_current_weather\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714421925626,\n  \"fallback_content\": null,\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"tool_type\": \"FUNCTION\",\n  \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n  \"version\": 0,\n  \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n  \"name\": \"get_current_weather\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714421925626,\n  \"fallback_content\": null,\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"Weather Assistant Config\",\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [\n    {\n      \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n      \"version\": 0\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"Weather Assistant Config\",\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [\n    {\n      \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n      \"version\": 0\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"id\": \"87e88a1a-3768-4a01-ba54-2e6d247a00a7\",\n  \"version\": 0,\n  \"version_description\": null,\n  \"name\": \"Weather Assistant Config\",\n  \"created_on\": 1714421581844,\n  \"modified_on\": 1714421581844,\n  \"prompt\": null,\n  \"voice\": null,\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [\n    {\n      \"tool_type\": \"FUNCTION\",\n      \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n      \"version\": 0,\n      \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n      \"name\": \"get_current_weather\",\n      \"created_on\": 1714421925626,\n      \"modified_on\": 1714421925626,\n      \"fallback_content\": null,\n      \"description\": \"This tool is for getting the current weather.\",\n      \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n    }\n  ],\n  \"builtin_tools\": []\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"id\": \"87e88a1a-3768-4a01-ba54-2e6d247a00a7\",\n  \"version\": 0,\n  \"version_description\": null,\n  \"name\": \"Weather Assistant Config\",\n  \"created_on\": 1714421581844,\n  \"modified_on\": 1714421581844,\n  \"prompt\": null,\n  \"voice\": null,\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [\n    {\n      \"tool_type\": \"FUNCTION\",\n      \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n      \"version\": 0,\n      \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n      \"name\": \"get_current_weather\",\n      \"created_on\": 1714421925626,\n      \"modified_on\": 1714421925626,\n      \"fallback_content\": null,\n      \"description\": \"This tool is for getting the current weather.\",\n      \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n    }\n  ],\n  \"builtin_tools\": []\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "setup",
        "title": "Setup"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-function-calling-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Function calling",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#function-calling",
    "content": "In this section, we will go over the end-to-end flow of a function call within a chat session. This flow will be predicated on having specified the\nWeather Assistant Config when establishing a connection with EVI. See our Configuration Guide\nfor details on how to apply your configuration when connecting.\n\n\nCheck out the TypeScript and Python\nexample projects for complete implementations of the weather Tool you'll build in this tutorial.\n\n\nDefine a function\nWe must first define a function for your Tool. This function will take the\nsame parameters as those specified during your Tool's creation.\nFor this tutorial, we will define a function that calls a weather API (e.g., the Geocoding API) to retrieve the weather for a designated city in a specified format. This weather function will accept location and format as its parameters.\nSee the code below for a sample implementation:\n\n\n\n\n\n\n\n\nInstead of calling a weather API, you can hard code a return value like 75F as a means to quickly test for the sake of this tutorial.\nEVI signals function call\nOnce EVI is configured with your Tool, it will automatically infer when to signal a function call within a chat session. With EVI configured to use the get_current_weather Tool, we can now ask it: \"what is the weather in New York?\"\nLet's try it out in the EVI Playground.\n\n\n\n\nWe can expect EVI to respond with a User Message and a Tool Call message:\n\n\n\n\n\n\nCurrently, EVI does not support parallel function calling. Only one function call can be processed at a time.\nExtract arguments from Tool Call message\nUpon receiving a Tool Call message from EVI, we will parse the parameters and extract the arguments.\nThe code below demonstrates how to extract the location and format arguments, which the user-defined fetch weather function is expecting, from a received Tool Call message.\n\n\n\n\n\n\nInvoke function call\nNext, we will pass the extracted arguments into the previously defined fetch weather function. We will capture the return value to send back to EVI:\n\n\n\n\n\n\nSend function call result\nUpon receiving the return value of your function, we will send a Tool Response message containing the result. The specified tool_call_id must match the one received in\nthe Tool Call message from EVI:\n\n\n\n\n\n\nLet's try it in the EVI Playground. Enter the return value of your function in the input field below the Tool Call message, and click Send Response. In practice, you will use the actual return value from your function call. However, for demonstration purposes, we will assume a return value of \"75F\".\n\n\n\n\nEVI responds\nAfter the interface receives the Tool Response message, it will then send an Assistant Message containing the response generated from the reported result of the function call:\n\n\nSee how it works in the EVI Playground.\n\n\n\n\nTo summarize, Tool Call serves as a programmatic tool for intelligently signaling when you should invoke your function. EVI does not invoke the function for you. You will need to define a function, invoke the function, and pass the return value of your function to EVI via a Tool Response message. EVI will generate a response based on the content of your message.",
    "code_snippets": [
      {
        "lang": "ts",
        "code": "async function fetchWeather(location: string, format: string): Promise<string> {\n  // Fetch the location's geographic coordinates using Geocoding API\n  const locationApiURL = `https://geocode.maps.co/search?q=${location}&api_key=${YOUR_WEATHER_API_KEY}`;\n  const locationResponse = await fetch(locationApiURL);\n  const locationData = await locationResponse.json();\n\n  // Extract latitude and longitude from fetched location data\n  const { lat, lon } = locationData[0];\n\n  // Fetch point metadata using the extracted location coordinates\n  const pointMetadataEndpoint = `https://api.weather.gov/points/${parseFloat(\n    lat\n  ).toFixed(3)},${parseFloat(lon).toFixed(3)}`;\n  const pointMetadataResponse = await fetch(pointMetadataEndpoint);\n  const pointMetadata = await pointMetadataResponse.json();\n\n  // Extract weather forecast URL from point metadata\n  const forecastUrl = pointMetadata.properties.forecast;\n\n  // Fetch the weather forecast using the forecast URL\n  const forecastResponse = await fetch(forecastUrl);\n  const forecastData = await forecastResponse.json();\n  const forecast = JSON.stringify(forecastData.properties.periods);\n\n  // Return the temperature in the specified format\n  return `${forecast} in ${format}`;\n}"
      },
      {
        "lang": "ts",
        "code": "async function fetchWeather(location: string, format: string): Promise<string> {\n  // Fetch the location's geographic coordinates using Geocoding API\n  const locationApiURL = `https://geocode.maps.co/search?q=${location}&api_key=${YOUR_WEATHER_API_KEY}`;\n  const locationResponse = await fetch(locationApiURL);\n  const locationData = await locationResponse.json();\n\n  // Extract latitude and longitude from fetched location data\n  const { lat, lon } = locationData[0];\n\n  // Fetch point metadata using the extracted location coordinates\n  const pointMetadataEndpoint = `https://api.weather.gov/points/${parseFloat(\n    lat\n  ).toFixed(3)},${parseFloat(lon).toFixed(3)}`;\n  const pointMetadataResponse = await fetch(pointMetadataEndpoint);\n  const pointMetadata = await pointMetadataResponse.json();\n\n  // Extract weather forecast URL from point metadata\n  const forecastUrl = pointMetadata.properties.forecast;\n\n  // Fetch the weather forecast using the forecast URL\n  const forecastResponse = await fetch(forecastUrl);\n  const forecastData = await forecastResponse.json();\n  const forecast = JSON.stringify(forecastData.properties.periods);\n\n  // Return the temperature in the specified format\n  return `${forecast} in ${format}`;\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}"
      },
      {
        "lang": "ts",
        "code": "import { Hume } from 'hume';\n\nasync function handleToolCallMessage(\n  toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n  if (toolCallMessage.name === \"get_current_weather\") {\n    // 1. Parse the parameters from the Tool Call message\n    const args = JSON.parse(toolCallMessage.parameters) as {\n      location: string;\n      format: string;\n    };\n    // 2. Extract the individual arguments\n    const { location, format } = args;\n    // ...etc.\n  }\n}"
      },
      {
        "lang": "ts",
        "code": "import { Hume } from 'hume';\n\nasync function handleToolCallMessage(\n  toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n  if (toolCallMessage.name === \"get_current_weather\") {\n    // 1. Parse the parameters from the Tool Call message\n    const args = JSON.parse(toolCallMessage.parameters) as {\n      location: string;\n      format: string;\n    };\n    // 2. Extract the individual arguments\n    const { location, format } = args;\n    // ...etc.\n  }\n}"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume.client import AsyncHumeClient\nfrom hume.empathic_voice import ToolCallMessage, ToolResponseMessage\nfrom typing import Optional\n\nasync def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:\n    # Extract the tool name and ID from the message\n    tool_name = message.name\n    tool_call_id = message.tool_call_id\n    \n    # 1. Parse the parameters from the Tool Call message\n    tool_parameters = json.loads(message.parameters)\n\n    if tool_name == \"get_current_weather\":\n        # 2. Extract the individual arguments\n        obtained_location = tool_parameters.get('location')\n        obtained_format = tool_parameters.get('format', 'text')\n\n        # ...etc."
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume.client import AsyncHumeClient\nfrom hume.empathic_voice import ToolCallMessage, ToolResponseMessage\nfrom typing import Optional\n\nasync def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:\n    # Extract the tool name and ID from the message\n    tool_name = message.name\n    tool_call_id = message.tool_call_id\n    \n    # 1. Parse the parameters from the Tool Call message\n    tool_parameters = json.loads(message.parameters)\n\n    if tool_name == \"get_current_weather\":\n        # 2. Extract the individual arguments\n        obtained_location = tool_parameters.get('location')\n        obtained_format = tool_parameters.get('format', 'text')\n\n        # ...etc."
      },
      {
        "lang": "ts",
        "code": "import { Hume } from 'hume';\n\nasync function handleToolCallMessage(\n  toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n  if (toolCallMessage.name === \"get_current_weather\") {\n    // 1. Parse the parameters from the Tool Call message\n    const args = JSON.parse(toolCallMessage.parameters) as {\n      location: string;\n      format: string;\n    };\n    // 2. Extract the individual arguments\n    const { location, format } = args;\n    // 3. Call fetch weather function with extracted arguments\n    const weather = await fetchWeather(location, format);\n    // ...etc.\n  }\n}"
      },
      {
        "lang": "ts",
        "code": "import { Hume } from 'hume';\n\nasync function handleToolCallMessage(\n  toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n  if (toolCallMessage.name === \"get_current_weather\") {\n    // 1. Parse the parameters from the Tool Call message\n    const args = JSON.parse(toolCallMessage.parameters) as {\n      location: string;\n      format: string;\n    };\n    // 2. Extract the individual arguments\n    const { location, format } = args;\n    // 3. Call fetch weather function with extracted arguments\n    const weather = await fetchWeather(location, format);\n    // ...etc.\n  }\n}"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume.client import AsyncHumeClient\nfrom hume.empathic_voice import ToolCallMessage, ToolResponseMessage\nfrom typing import Optional\n\nasync def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:\n    # Extract the tool name and ID from the message\n    tool_name = message.name\n    tool_call_id = message.tool_call_id\n    \n    # 1. Parse the parameters from the Tool Call message\n    tool_parameters = json.loads(message.parameters)\n\n    if tool_name == \"get_current_weather\":\n        # 2. Extract the individual arguments\n        obtained_location = tool_parameters.get('location')\n        obtained_format = tool_parameters.get('format', 'text')\n\n        if obtained_location:\n            # 3. Call fetch weather function with extracted arguments\n            weather = await fetch_weather(location=obtained_location, format=obtained_format)\n            \n            # ...etc."
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume.client import AsyncHumeClient\nfrom hume.empathic_voice import ToolCallMessage, ToolResponseMessage\nfrom typing import Optional\n\nasync def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:\n    # Extract the tool name and ID from the message\n    tool_name = message.name\n    tool_call_id = message.tool_call_id\n    \n    # 1. Parse the parameters from the Tool Call message\n    tool_parameters = json.loads(message.parameters)\n\n    if tool_name == \"get_current_weather\":\n        # 2. Extract the individual arguments\n        obtained_location = tool_parameters.get('location')\n        obtained_format = tool_parameters.get('format', 'text')\n\n        if obtained_location:\n            # 3. Call fetch weather function with extracted arguments\n            weather = await fetch_weather(location=obtained_location, format=obtained_format)\n            \n            # ...etc."
      },
      {
        "lang": "ts",
        "code": "import { Hume } from 'hume';\n\nasync function handleToolCallMessage(\n  toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n  if (toolCallMessage.name === \"get_current_weather\") {\n    // 1. Parse the parameters from the Tool Call message\n    const args = JSON.parse(toolCallMessage.parameters) as {\n      location: string;\n      format: string;\n    };\n    // 2. Extract the individual arguments\n    const { location, format } = args;\n    // 3. Call fetch weather function with extracted arguments\n    const weather = await fetchWeather(location, format);\n    // 4. Construct a Tool Response message containing the result\n    const toolResponseMessage = {\n      type: \"tool_response\",\n      toolCallId: toolCallMessage.toolCallId,\n      content: weather,\n    };\n    // 5. Send Tool Response message to the WebSocket\n    socket.sendToolResponseMessage(toolResponseMessage);\n  }\n}"
      },
      {
        "lang": "ts",
        "code": "import { Hume } from 'hume';\n\nasync function handleToolCallMessage(\n  toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n  if (toolCallMessage.name === \"get_current_weather\") {\n    // 1. Parse the parameters from the Tool Call message\n    const args = JSON.parse(toolCallMessage.parameters) as {\n      location: string;\n      format: string;\n    };\n    // 2. Extract the individual arguments\n    const { location, format } = args;\n    // 3. Call fetch weather function with extracted arguments\n    const weather = await fetchWeather(location, format);\n    // 4. Construct a Tool Response message containing the result\n    const toolResponseMessage = {\n      type: \"tool_response\",\n      toolCallId: toolCallMessage.toolCallId,\n      content: weather,\n    };\n    // 5. Send Tool Response message to the WebSocket\n    socket.sendToolResponseMessage(toolResponseMessage);\n  }\n}"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume.client import AsyncHumeClient\nfrom hume.empathic_voice import ToolCallMessage, ToolResponseMessage\nfrom typing import Optional\n\nasync def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:\n    # Extract the tool name and ID from the message\n    tool_name = message.name\n    tool_call_id = message.tool_call_id\n    \n    # 1. Parse the parameters from the Tool Call message\n    tool_parameters = json.loads(message.parameters)\n\n    if tool_name == \"get_current_weather\":\n        # 2. Extract the individual arguments\n        obtained_location = tool_parameters.get('location')\n        obtained_format = tool_parameters.get('format', 'text')\n\n        if obtained_location:\n            # 3. Call fetch weather function with extracted arguments\n            weather = await fetch_weather(location=obtained_location, format=obtained_format)\n            \n            if not weather.startswith(\"ERROR\"):\n                # 4. Construct a Tool Response message containing the result\n                resp = ToolResponseMessage(\n                    tool_call_id=tool_call_id,\n                    content=weather\n                )\n                # 5. Send Tool Response message to the WebSocket\n                await self.socket.send_tool_response(resp)\n                print(f\"(Sent ToolResponseMessage for tool_call_id {tool_call_id}: {weather})\\n\")\n                return resp\n\n    # Return None if the tool is not recognized or if there's an error\n    return None"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume.client import AsyncHumeClient\nfrom hume.empathic_voice import ToolCallMessage, ToolResponseMessage\nfrom typing import Optional\n\nasync def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:\n    # Extract the tool name and ID from the message\n    tool_name = message.name\n    tool_call_id = message.tool_call_id\n    \n    # 1. Parse the parameters from the Tool Call message\n    tool_parameters = json.loads(message.parameters)\n\n    if tool_name == \"get_current_weather\":\n        # 2. Extract the individual arguments\n        obtained_location = tool_parameters.get('location')\n        obtained_format = tool_parameters.get('format', 'text')\n\n        if obtained_location:\n            # 3. Call fetch weather function with extracted arguments\n            weather = await fetch_weather(location=obtained_location, format=obtained_format)\n            \n            if not weather.startswith(\"ERROR\"):\n                # 4. Construct a Tool Response message containing the result\n                resp = ToolResponseMessage(\n                    tool_call_id=tool_call_id,\n                    content=weather\n                )\n                # 5. Send Tool Response message to the WebSocket\n                await self.socket.send_tool_response(resp)\n                print(f\"(Sent ToolResponseMessage for tool_call_id {tool_call_id}: {weather})\\n\")\n                return resp\n\n    # Return None if the tool is not recognized or if there's an error\n    return None"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current temperature in New York, NY is 75F.\"\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current temperature in New York, NY is 75F.\"\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "function-calling",
        "title": "Function calling"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-using-built-in-tools-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Using built-in tools",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-built-in-tools",
    "content": "User-defined tools allow EVI to identify when a function should be invoked, but you will need to invoke the function itself. On the other hand, Hume also provides built-in tools that are natively integrated. This\nmeans that you don't need to define the function; EVI handles both determining when the function needs to be called and invoking it.\nHume supports the following built-in tools:\nweb_search: Enables EVI to search the web for real-time information when needed.\n\nhang_up: Closes the WebSocket connection with status code 1000 when appropriate (e.g., after detecting a farewell, signaling the end of the conversation).\n\n\nThis section explains how to specify built-in tools in your configurations and details the message flow you can expect when EVI uses a built-in tool during a chat session.\n\n\nSpecify built-in tool in EVI configuration\nLet's begin by creating a configuration which includes the built-in web search tool. To specify the web search tool in your EVI configuration, during the Add tools step,\nensure Web search is enabled. Refer to our Configuration Guide for more details on creating a configuration.\n\n\n\n\nAlternatively, you can specify the built-in tool by making a POST request to /configs with the following request body:\n\n\nUpon success, expect EVI to return a response similar to this example:\n\n\nEVI uses built-in tool\nNow that we've created an EVI configuration which includes the built-in web search tool, let's test it out in the EVI Playground.\nTry asking EVI a question that requires web search, like \"what is the latest news with AI research?\"\n\n\n\n\nEVI will send a response generated from the web search results:\n\n\n\n\nLet's review the message flow for when web search is invoked.",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"name\": \"Web Search Config\",\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\"\n  },\n  \"builtin_tools\": [\n    { \n      \"name\": \"web_search\",\n      \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"Web Search Config\",\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\"\n  },\n  \"builtin_tools\": [\n    { \n      \"name\": \"web_search\",\n      \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"id\": \"3a60e85c-d04f-4eb5-8076-fb4bd344d5d0\",\n  \"version\": 0,\n  \"version_description\": null,\n  \"name\": \"Web Search Config\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714421925626,\n  \"prompt\": null,\n  \"voice\": null,\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [],\n  \"builtin_tools\": [\n    {\n      \"tool_type\": \"BUILTIN\",\n      \"name\": \"web_search\",\n      \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"id\": \"3a60e85c-d04f-4eb5-8076-fb4bd344d5d0\",\n  \"version\": 0,\n  \"version_description\": null,\n  \"name\": \"Web Search Config\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714421925626,\n  \"prompt\": null,\n  \"voice\": null,\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [],\n  \"builtin_tools\": [\n    {\n      \"tool_type\": \"BUILTIN\",\n      \"name\": \"web_search\",\n      \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI for the latest news in AI research\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What is the latest news with AI research?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it needs to use web search, generates a search query, and invokes Hume's native web search function\n{\n  \"name\": \"web_search\", \n  \"parameters\": \"{\\\"query\\\":\\\"latest news AI research\\\"}\", \n  \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n  \"type\": \"tool_call\", \n  \"tool_type\": \"builtin\", \n  \"response_required\": false\n}\n// 3. EVI sends back the web search results \n{\n  \"type\": \"tool_response\", \n  \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n  \"content\": \"{ \\”summary\\”:null, “references”: [{\\”content\\”:\\”Researchers have demonstrated a new method...etc.\\”, \\”url\\”:\\”https://www.sciencedaily.com/news/computers_math/artificial_intelligence/\\”, \\”name\\”:\\”Artificial Intelligence News -- ScienceDaily\\”}] }\", \n  \"tool_name\": \"web_search\", \n  \"tool_type\": \"builtin\"\n}\n// 4. EVI sends a response generated from the web search results\n{\n  \"type\": \"assistant_message\", \n  \"message\": {\n    \"role\": \"assistant\", \n    \"content\": \"Oh, there's some interesting stuff happening in AI research right now.\"\n  },\n  // ...etc\n}\n{\n  \"type\": \"assistant_message\", \n  \"message\": {\n    \"role\": \"assistant\", \n    \"content\": \"Just a few hours ago, researchers demonstrated a new method using AI and computer simulations to train robotic exoskeletons.\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI for the latest news in AI research\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What is the latest news with AI research?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it needs to use web search, generates a search query, and invokes Hume's native web search function\n{\n  \"name\": \"web_search\", \n  \"parameters\": \"{\\\"query\\\":\\\"latest news AI research\\\"}\", \n  \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n  \"type\": \"tool_call\", \n  \"tool_type\": \"builtin\", \n  \"response_required\": false\n}\n// 3. EVI sends back the web search results \n{\n  \"type\": \"tool_response\", \n  \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n  \"content\": \"{ \\”summary\\”:null, “references”: [{\\”content\\”:\\”Researchers have demonstrated a new method...etc.\\”, \\”url\\”:\\”https://www.sciencedaily.com/news/computers_math/artificial_intelligence/\\”, \\”name\\”:\\”Artificial Intelligence News -- ScienceDaily\\”}] }\", \n  \"tool_name\": \"web_search\", \n  \"tool_type\": \"builtin\"\n}\n// 4. EVI sends a response generated from the web search results\n{\n  \"type\": \"assistant_message\", \n  \"message\": {\n    \"role\": \"assistant\", \n    \"content\": \"Oh, there's some interesting stuff happening in AI research right now.\"\n  },\n  // ...etc\n}\n{\n  \"type\": \"assistant_message\", \n  \"message\": {\n    \"role\": \"assistant\", \n    \"content\": \"Just a few hours ago, researchers demonstrated a new method using AI and computer simulations to train robotic exoskeletons.\"\n  },\n  // ...etc\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "using-built-in-tools",
        "title": "Using built-in tools"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-interruptibility-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Interruptibility",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#interruptibility",
    "content": "Function calls can be interrupted to cancel them or to resend them with updated parameters.",
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-canceling-a-function-call-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Canceling a function call",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#canceling-a-function-call",
    "content": "Just as EVI is able to infer when to make a function call, it can also infer from the user's input when to cancel one. Here is an overview of what the message flow would look like:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "// 1. User asks what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User communicates sudden disinterested in the weather\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Actually, never mind.\"\n  }\n}\n// 4. EVI infers the function call should be canceled\n{\n    \"type\": \"assistant_message\",\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"If you change your mind or need any weather information in the future, feel free to let me know.\"\n    },\n    // ...etc\n  }"
      },
      {
        "lang": "json",
        "code": "// 1. User asks what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User communicates sudden disinterested in the weather\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Actually, never mind.\"\n  }\n}\n// 4. EVI infers the function call should be canceled\n{\n    \"type\": \"assistant_message\",\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"If you change your mind or need any weather information in the future, feel free to let me know.\"\n    },\n    // ...etc\n  }"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility"
      },
      "h3": {
        "id": "canceling-a-function-call",
        "title": "Canceling a function call"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-updating-a-function-call-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Updating a function call",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#updating-a-function-call",
    "content": "Sometimes we don't necessarily want to cancel the function call, and instead want to update the parameters. EVI can infer the difference. Below is a sample flow of\ninterrupting the interface to update the parameters of the function call:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User communicates to EVI they want the weather in Los Angeles instead\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Actually, Los Angeles.\"\n  }\n}\n// 4. EVI infers the parameters to function call should be updated\n{\n  \"type\": \"tool_call\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"Los Angeles\\\",\\\"format\\\":\\\"celsius\\\"}\"\n}\n// 5. User sends results of function call to EVI\n{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"content\":\"72F\"\n}\n// 6. EVI sends response container function call result\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current weather in Los Angeles is 72F.\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User communicates to EVI they want the weather in Los Angeles instead\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Actually, Los Angeles.\"\n  }\n}\n// 4. EVI infers the parameters to function call should be updated\n{\n  \"type\": \"tool_call\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"Los Angeles\\\",\\\"format\\\":\\\"celsius\\\"}\"\n}\n// 5. User sends results of function call to EVI\n{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"content\":\"72F\"\n}\n// 6. EVI sends response container function call result\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current weather in Los Angeles is 72F.\"\n  },\n  // ...etc\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility"
      },
      "h3": {
        "id": "updating-a-function-call",
        "title": "Updating a function call"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-handling-errors-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Handling errors",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#handling-errors",
    "content": "It's possible for tool use to fail. For example, it can fail if the Tool Response message content was not in UTF-8 format or if the function call response timed out. This\nsection outlines how to specify fallback content to be used by EVI to communicate a failure, as well as the message flow for when a function call failure occurs.",
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-specifying-fallback-content-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Specifying fallback content",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#specifying-fallback-content",
    "content": "When defining your Tool, you can specify fallback content within the Tool's fallback_content field. When the Tool fails to generate content, the text in this\nfield will be sent to the LLM in place of a result. To accomplish this, let's update the Tool we created during setup to include fallback content. We can accomplish\nthis by publishing a new version of the Tool via a POST request to /tools/{id}:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"version_description\": \"Adds fallback content\",\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\",\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"version_description\": \"Adds fallback content\",\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\",\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"tool_type\": \"FUNCTION\",\n  \"id\": \"36f09fdc-4630-40c0-8afa-6a3bdc4eb4b1\",\n  \"version\": 1,\n  \"version_type\": \"FIXED\",\n  \"version_description\": \"Adds fallback content\",\n  \"name\": \"get_current_weather\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714425632084,\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n  \"description\": null,\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the user's location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"tool_type\": \"FUNCTION\",\n  \"id\": \"36f09fdc-4630-40c0-8afa-6a3bdc4eb4b1\",\n  \"version\": 1,\n  \"version_type\": \"FIXED\",\n  \"version_description\": \"Adds fallback content\",\n  \"name\": \"get_current_weather\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714425632084,\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n  \"description\": null,\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the user's location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors"
      },
      "h3": {
        "id": "specifying-fallback-content",
        "title": "Specifying fallback content"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-failure-message-flow-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Failure message flow",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#failure-message-flow",
    "content": "This section outlines the sort of messages that can be expected when Tool use fails. After sending a Tool Response message, we will know an error, or failure,\noccurred when we receive the Tool Error message:\n\n\n\n\n\n\nLet's cover another type of failure scenario: what if the weather API the function was using was down? In this case, we would send EVI a Tool Error message.\nWhen sending the Tool Error message, we can specify fallback_content to be more specific to the error our function throws. This is what the message flow would be\nfor this type of failure:\n\n\nLet's revisit our function for handling Tool Call messages from the Function Calling section.\nWe can now add support for error handling by sending Tool Error messages to EVI.\nThis will enable our function to handle cases where fetching the weather fails or the requested tool is not found:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User sends results of function call to EVI (result not formatted correctly)\n{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"content\":\"MALFORMED RESPONSE\"\n}\n// 4. EVI sends response communicating it failed to process the tool_response\n{\n  \"type\": \"tool_error\",\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"error\": \"Malformed tool response: <error message here>\",\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n  \"level\": \"warn\"\n}\n// 5. EVI generates a response based on the failure\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"It looks like there was an issue retrieving the weather information for New York.\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User sends results of function call to EVI (result not formatted correctly)\n{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"content\":\"MALFORMED RESPONSE\"\n}\n// 4. EVI sends response communicating it failed to process the tool_response\n{\n  \"type\": \"tool_error\",\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"error\": \"Malformed tool response: <error message here>\",\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n  \"level\": \"warn\"\n}\n// 5. EVI generates a response based on the failure\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"It looks like there was an issue retrieving the weather information for New York.\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. Function failed, so we send EVI a message communicating the failure on our end\n{\n  \"type\": \"tool_error\",\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"error\": \"Malformed tool response: <error message here>\",\n  \"fallback_content\": \"Function execution failure - weather API down.\",\n  \"level\": \"warn\"\n}\n// 4. EVI generates a response based on the failure\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Sorry, our weather resource is unavailable. Can I help with anything else?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. Function failed, so we send EVI a message communicating the failure on our end\n{\n  \"type\": \"tool_error\",\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"error\": \"Malformed tool response: <error message here>\",\n  \"fallback_content\": \"Function execution failure - weather API down.\",\n  \"level\": \"warn\"\n}\n// 4. EVI generates a response based on the failure\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Sorry, our weather resource is unavailable. Can I help with anything else?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "ts",
        "code": "import { Hume } from 'hume';\n\nasync function handleToolCallMessage(\n  toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n  if (toolCallMessage.name === \"get_current_weather\") {\n    try{\n      // parse the parameters from the Tool Call message\n      const args = JSON.parse(toolCallMessage.parameters) as {\n        location: string;\n        format: string;\n      };\n      // extract the individual arguments\n      const { location, format } = args;\n      // call fetch weather function with extracted arguments\n      const weather = await fetchWeather(location, format);\n      // send Tool Response message to the WebSocket\n      const toolResponseMessage = {\n        type: \"tool_response\",\n        toolCallId: toolCallMessage.toolCallId,\n        content: weather,\n      };\n      socket.sendToolResponseMessage(toolResponseMessage);\n    } catch (error) {\n      // send Tool Error message if weather fetching fails\n      const weatherToolErrorMessage = {\n        type: \"tool_error\",\n        toolCallId: toolCallMessage.toolCallId,\n        error: \"Weather tool error\",\n        content: \"There was an error with the weather tool\",\n      };\n      socket.sendToolErrorMessage(weatherToolErrorMessage);\n    }\n  } else {\n    // send Tool Error message if the requested tool was not found\n    const toolNotFoundErrorMessage = {\n      type: \"tool_error\",\n      toolCallId: toolCallMessage.toolCallId,\n      error: \"Tool not found\",\n      content: \"The tool you requested was not found\",\n    };\n    socket.sendToolErrorMessage(toolNotFoundErrorMessage);\n  }\n}"
      },
      {
        "lang": "ts",
        "code": "import { Hume } from 'hume';\n\nasync function handleToolCallMessage(\n  toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n  if (toolCallMessage.name === \"get_current_weather\") {\n    try{\n      // parse the parameters from the Tool Call message\n      const args = JSON.parse(toolCallMessage.parameters) as {\n        location: string;\n        format: string;\n      };\n      // extract the individual arguments\n      const { location, format } = args;\n      // call fetch weather function with extracted arguments\n      const weather = await fetchWeather(location, format);\n      // send Tool Response message to the WebSocket\n      const toolResponseMessage = {\n        type: \"tool_response\",\n        toolCallId: toolCallMessage.toolCallId,\n        content: weather,\n      };\n      socket.sendToolResponseMessage(toolResponseMessage);\n    } catch (error) {\n      // send Tool Error message if weather fetching fails\n      const weatherToolErrorMessage = {\n        type: \"tool_error\",\n        toolCallId: toolCallMessage.toolCallId,\n        error: \"Weather tool error\",\n        content: \"There was an error with the weather tool\",\n      };\n      socket.sendToolErrorMessage(weatherToolErrorMessage);\n    }\n  } else {\n    // send Tool Error message if the requested tool was not found\n    const toolNotFoundErrorMessage = {\n      type: \"tool_error\",\n      toolCallId: toolCallMessage.toolCallId,\n      error: \"Tool not found\",\n      content: \"The tool you requested was not found\",\n    };\n    socket.sendToolErrorMessage(toolNotFoundErrorMessage);\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors"
      },
      "h3": {
        "id": "failure-message-flow",
        "title": "Failure message flow"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "title": "Phone calling",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Guide to enabling phone calling with the Empathic Voice Interface (EVI).",
    "content": "This guide details how to integrate Twilio with the Empathic Voice Interface (EVI) to enable voice-to-voice interactions with EVI over the phone.\n\n\nTo comply with our Terms of\nUse, always make it clear\nthat the Empathic Voice Interface (EVI) is an AI. Do not mislead individuals\ninto thinking they are interacting with a human. In addition, developers must\ncomply with the FCC\nregulation under\nthe Telephone Consumer Protection Act (TCPA), which requires obtaining prior\nexpress written consent before calling consumers."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-inbound-phone-calling-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "title": "Inbound phone calling",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#inbound-phone-calling",
    "content": "By following the steps below, you can set up a Twilio phone number to connect with EVI.\n\n\nCreate Twilio phone number\nTo set up inbound phone calling, log into your Twilio account at the Twilio Console.\nNavigate to Phone Numbers > Manage > Active Numbers > Buy a New Number and purchase a phone number of your choice.\n\n\nA Twilio account is required to access the Twilio console. Should you run into\nany issues creating a phone number, please refer to Twilio’s\ndocumentation.\nSetup webhook\nAfter purchasing your number, return to the Active Numbers section and select the number you intend to use for EVI.\n\nCreate a configuration for EVI by following our configuration documentation, and save the config ID.\n\nConfigure the webhook for incoming calls by setting the following webhook URL, replacing <YOUR CONFIG ID> and <YOUR API KEY> with your specific credentials:\nhttps://api.hume.ai/v0/evi/twilio?config_id=<YOUR CONFIG ID>&api_key=<YOUR API KEY>.\n\n\nCall EVI\nWith your Twilio phone number registered, and the EVI webhook set up, you can now give the number a call to chat with EVI.\nAll of EVI’s core features are available through phone calls. However, phone calls do have two primary limitations:\nLatency: transmitting the audio through our Twilio integration adds a few hundred milliseconds, making interactions with EVI slightly slower.\n\nAudio quality: web audio commonly utilizes a higher quality standard of 24,000 Hz. However, due to the compression required for phone conversations, telephony audio adheres to a standard of 8,000 Hz.",
    "hierarchy": {
      "h0": {
        "title": "Phone calling"
      },
      "h2": {
        "id": "inbound-phone-calling",
        "title": "Inbound phone calling"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-outbound-phone-calling-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "title": "Outbound phone calling",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#outbound-phone-calling",
    "content": "An outbound phone call goes \"out\" from the voice AI to the end user who receives the call. EVI supports outbound phone calling through Twilio's API, allowing you to automate initiating calls to users. However, this capability comes with important ethical and regulatory requirements:\nOutbound calling with EVI requires express prior written consent from users before making any calls. This is mandated by the FCC's Telephone Consumer Protection Act (TCPA) regulations as of August 7, 2024. The consent must be clear, specific, and documented. Users must be explicitly informed they will receive automated calls from an AI system. Violators are subject to fines of up to $1500 per unauthorized call, liability in civil lawsuits, FCC investigations, and further penalties. Hume takes these requirements seriously and will actively report misuse to regulatory authorities.\nFurther, outbound calls must comply with the Hume Terms of Use, which includes the Hume Initiative guidelines for empathic AI. For example, manipulative sales calls that take advantage of the user's emotional expressions to sell products over the phone are prohibited. We monitor for misuses, and violators can be banned from the Hume platform.\nExamples of acceptable use cases for outbound phone calls include: scheduled check-ins that users have opted into, appointment reminders, customer service follow-ups, and pre-arranged AI coaching sessions. The key is that these are expected, consented-to interactions that provide value to the user.\nThe code below shows how to implement outbound calling using the Twilio API. The same EVI webhook used for handling inbound calls can be used for outbound calls: https://api.hume.ai/v0/evi/twilio?config_id=<YOUR CONFIG ID>&api_key=<YOUR API KEY>. Once you create an EVI configuration, you can easily copy this webhook URL in the Deploy tab.",
    "code_snippets": [
      {
        "lang": "python",
        "meta": "title=\"Python\"",
        "code": "# Import the Twilio client - run pip install twilio first\nfrom twilio.rest import Client\n\n# Enter your Twilio credentials from https://console.twilio.com/ and set up the client\naccount_sid = \"YOUR_ACCOUNT_SID\"\nauth_token = \"YOUR_AUTH_TOKEN\"\nclient = Client(account_sid, auth_token)\n\n# Outbound call details\ntwilio_number = \"YOUR_TWILIO_NUMBER\" # Twilio phone number in E.164 format\nto_number = \"YOUR_DESTINATION_NUMBER\" # Destination number in E.164 format (the number you'd like to call)\nwebhook_url = \"https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY\" # your EVI webhook URL, the same you'd use for inbound calls\n\n# Make the call while specifying the Webhook URL\ncall = client.calls.create(\n    to=to_number,\n    from_=twilio_number,\n    url=webhook_url\n)\n\n# Output call details - should print \"Call status: queued\"\nprint(f\"Call status: {call.status}\")"
      },
      {
        "lang": "typescript",
        "meta": "title=\"TypeScript\"",
        "code": "// First make sure to install Twilio with npm install twilio\n// Import the Twilio client\nimport twilio from 'twilio';\n\n// Enter your Twilio credentials from https://console.twilio.com/ and set up the client\nconst accountSid = \"YOUR_ACCOUNT_SID\";\nconst authToken = \"YOUR_AUTH_TOKEN\";\nconst client = twilio(accountSid, authToken);\n\n// Outbound call details\nconst twilioNumber = \"YOUR_TWILIO_NUMBER\"; // Twilio phone number in E.164 format\nconst toNumber = \"YOUR_DESTINATION_NUMBER\"; // Destination number in E.164 format (the number you'd like to call)\nconst webhookUrl = \"https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY\n\n// Import the Twilio client\nimport twilio from 'twilio';\n\n// Enter your Twilio credentials from https://console.twilio.com/ and set up the client\nconst accountSid = \"YOUR_ACCOUNT_SID\";\nconst authToken = \"YOUR_AUTH_TOKEN\";\nconst client = twilio(accountSid, authToken);\n\n// Outbound call details\nconst twilioNumber = \"YOUR_TWILIO_NUMBER\"; // Twilio phone number in E.164 format\nconst toNumber = \"YOUR_DESTINATION_NUMBER\"; // Destination number in E.164 format (the number you'd like to call)\nconst webhookUrl = \"https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY\"; // your EVI webhook URL, the same you'd use for inbound calls\n\n// Make the call while specifying the Webhook URL\nclient.calls\n    .create({\n        to: toNumber,\n        from: twilioNumber,\n        url: webhookUrl\n    })\n    .then(call => {\n        console.log(`Call status: ${call.status}`);\n    })\n    .catch(error => {\n        console.error(\"Error making the call:\", error);\n    });"
      },
      {
        "lang": "python",
        "meta": "title=\"Python\"",
        "code": "# Import the Twilio client - run pip install twilio first\nfrom twilio.rest import Client\n\n# Enter your Twilio credentials from https://console.twilio.com/ and set up the client\naccount_sid = \"YOUR_ACCOUNT_SID\"\nauth_token = \"YOUR_AUTH_TOKEN\"\nclient = Client(account_sid, auth_token)\n\n# Outbound call details\ntwilio_number = \"YOUR_TWILIO_NUMBER\" # Twilio phone number in E.164 format\nto_number = \"YOUR_DESTINATION_NUMBER\" # Destination number in E.164 format (the number you'd like to call)\nwebhook_url = \"https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY\" # your EVI webhook URL, the same you'd use for inbound calls\n\n# Make the call while specifying the Webhook URL\ncall = client.calls.create(\n    to=to_number,\n    from_=twilio_number,\n    url=webhook_url\n)\n\n# Output call details - should print \"Call status: queued\"\nprint(f\"Call status: {call.status}\")"
      },
      {
        "lang": "typescript",
        "meta": "title=\"TypeScript\"",
        "code": "// First make sure to install Twilio with npm install twilio\n// Import the Twilio client\nimport twilio from 'twilio';\n\n// Enter your Twilio credentials from https://console.twilio.com/ and set up the client\nconst accountSid = \"YOUR_ACCOUNT_SID\";\nconst authToken = \"YOUR_AUTH_TOKEN\";\nconst client = twilio(accountSid, authToken);\n\n// Outbound call details\nconst twilioNumber = \"YOUR_TWILIO_NUMBER\"; // Twilio phone number in E.164 format\nconst toNumber = \"YOUR_DESTINATION_NUMBER\"; // Destination number in E.164 format (the number you'd like to call)\nconst webhookUrl = \"https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY\n\n// Import the Twilio client\nimport twilio from 'twilio';\n\n// Enter your Twilio credentials from https://console.twilio.com/ and set up the client\nconst accountSid = \"YOUR_ACCOUNT_SID\";\nconst authToken = \"YOUR_AUTH_TOKEN\";\nconst client = twilio(accountSid, authToken);\n\n// Outbound call details\nconst twilioNumber = \"YOUR_TWILIO_NUMBER\"; // Twilio phone number in E.164 format\nconst toNumber = \"YOUR_DESTINATION_NUMBER\"; // Destination number in E.164 format (the number you'd like to call)\nconst webhookUrl = \"https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY\"; // your EVI webhook URL, the same you'd use for inbound calls\n\n// Make the call while specifying the Webhook URL\nclient.calls\n    .create({\n        to: toNumber,\n        from: twilioNumber,\n        url: webhookUrl\n    })\n    .then(call => {\n        console.log(`Call status: ${call.status}`);\n    })\n    .catch(error => {\n        console.error(\"Error making the call:\", error);\n    });"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Phone calling"
      },
      "h2": {
        "id": "outbound-phone-calling",
        "title": "Outbound phone calling"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-troubleshooting-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "title": "Troubleshooting",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#troubleshooting",
    "content": "If you encounter issues while using Twilio with EVI, consider the following troubleshooting tips:\nInvalid config ID or API key: verify that the config ID and API key used in the webhook URL are correct and active.\n\nExceeded simultaneous connections: if the usage exceeds our rate limits, consider filling out this form to request increasing your concurrent connection limits.\n\nRun out of Hume credits: if your Hume account has run out of credits, you may activate billing to continue supporting EVI conversations in your account settings.\n\n\n\n\nIf you are interested in volume discounts for EVI, please submit our\nEnterprise Sales and Partnerships\nForm.\nIf you encounter issues using Twilio, you can check your Twilio error logs to understand the issues in more depth. You will find these logs in your console, in the dashboard to the left under\nMonitor > Logs > Errors > Error Logs. See a list of Twilio errors in their Error and Warning Dictionary.",
    "hierarchy": {
      "h0": {
        "title": "Phone calling"
      },
      "h2": {
        "id": "troubleshooting",
        "title": "Troubleshooting"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Prompt engineering for empathic voice interfaces",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "System prompts shape the behavior, responses, and style of your custom empathic voice interface (EVI).",
    "content": "Creating an effective system prompt is an essential part of customizing an EVI's behavior. For the most part, prompting EVI is the same as prompting any LLM, but there are some important differences. Prompting for EVIs is different for two main reasons:\nPrompts are for a voice-only interaction with the user, rather than a text-based chat.\n\nEVIs can respond to the user’s emotional expressions in their tone of voice, not just the text content of their messages.\n\n\nFurther, EVI is interoperable with any supplemental LLM, allowing developers to select the best model for their use case. For fast, conversational, relatively simple interactions, Hume's voice-language model EVI 2 can handle text generation. However, frontier LLMs will perform better for more complex use cases involving reasoning, long or nuanced prompts, tool use, and other requirements.\nIf you select a supplemental LLM, your system prompt is sent to this LLM, which then generates all of the language in the chat while EVI generates the voice. EVI's voice-language model will still take into account the previous language and audio context to generate the appropriate tone of voice. It can also still be prompted in the chat to change its behavior (e.g. \"speak faster\").\nPrompt engineering allows developers to customize EVI’s response style for any use case, from voice AIs for mental health support to customer service agents and beyond.\n\n\nThe system prompt is a powerful and flexible way to guide EVI's responses, but\nit cannot dictate AI responses with absolute precision. See the limits of\nprompting section for more information. Careful\nprompt design and testing will help EVI behave as intended. If you need more\ncontrol over EVI's responses, try using our custom language\nmodel feature for complete control of text\ngeneration."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-evi-specific-prompting-instructions-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "EVI-specific prompting instructions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-specific-prompting-instructions",
    "content": "The instructions below are specific to prompting empathic voice interfaces. For examples of these principles in action, see our EVI prompt examples repository.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-prompt-for-voice-conversations-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Prompt for voice conversations",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#prompt-for-voice-conversations",
    "content": "Most LLMs are trained for text-based interactions. Thus, providing guidelines on how the LLM should speak helps voice conversations with EVI feel much more fluid and natural. For example, see the instruction below:\n\n\n\n\nIf you find the default behavior of the LLM acceptable, then you may only need\na very short system prompt. Customizing the LLM’s behavior more and\nmaintaining consistency in longer and more varied conversations often requires\nlonger prompts.",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<voice_only_response_format>\n  Format all responses as spoken words for a voice-only conversations. All\n  output is spoken aloud, so avoid any text-specific formatting or anything\n  that is not normally spoken. Prefer easily pronounced words. Seamlessly\n  incorporate natural vocal inflections like \"oh wow\" and discourse markers\n  like “I mean” to make conversations feel more human-like.\n</voice_only_response_format>"
      },
      {
        "lang": "xml",
        "code": "<voice_only_response_format>\n  Format all responses as spoken words for a voice-only conversations. All\n  output is spoken aloud, so avoid any text-specific formatting or anything\n  that is not normally spoken. Prefer easily pronounced words. Seamlessly\n  incorporate natural vocal inflections like \"oh wow\" and discourse markers\n  like “I mean” to make conversations feel more human-like.\n</voice_only_response_format>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      },
      "h3": {
        "id": "prompt-for-voice-conversations",
        "title": "Prompt for voice conversations"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-expressive-prompt-engineering-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Expressive prompt engineering",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#expressive-prompt-engineering",
    "content": "Expressive prompt engineering is our term for instructing language models on how to use Hume's expression measures in conversations. EVI measures the user's vocal expressions in real time and converts them into text-based indicators to help the LLM understand not just what the user said, but how they said it. EVI detects 48 distinct expressions in the user’s voice and ranks these expressions by our model’s confidence that they are present. Text-based descriptions of the user's top 3 expressions are appended to the end of each User message to indicate the user’s tone of voice. You can use the system prompt to guide how the AI voice responds to these non-verbal cues of the user's emotional expressions.\nFor example, our demo uses an instruction like the one below to help EVI respond to expressions:\n\n\nExplain to the LLM exactly how to respond to expressions. For example, you may want EVI to use a tool to alert you over email if the user is very frustrated, or to explain a concept in depth whenever the user expresses doubt or confusion. You can also instruct EVI to detect and respond to mismatches between the user’s tone of voice and the text content of their speech:\n\n\nEVI is designed for empathic conversations, and you can use expressive prompt engineering to customize how EVI empathizes with the user’s expressions for your use case.",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<respond_to_expressions>\n  Pay close attention to the top 3 emotional expressions provided in brackets after the User's message. These expressions indicate the user's tone, in the format: {expression1 confidence1, expression2 confidence2, expression3 confidence3}, e.g., {very happy, quite anxious, moderately amused}. The confidence score indicates how likely the User is expressing that emotion in their voice. Use expressions to infer the user's tone of voice and respond appropriately. Avoid repeating these expressions or mentioning them directly. For instance, if user expression is \"quite sad\", express sympathy; if \"very happy\", share in joy; if \"extremely angry\", acknowledge rage but seek to calm, if \"very bored\", entertain.\n  Stay alert for disparities between the user's words and expressions, and address it out loud when the user's language does not match their expressions. For instance, sarcasm often involves contempt and amusement in expressions. Reply to sarcasm with humor, not seriousness.\n</respond_to_expressions>"
      },
      {
        "lang": "xml",
        "code": "<respond_to_expressions>\n  Pay close attention to the top 3 emotional expressions provided in brackets after the User's message. These expressions indicate the user's tone, in the format: {expression1 confidence1, expression2 confidence2, expression3 confidence3}, e.g., {very happy, quite anxious, moderately amused}. The confidence score indicates how likely the User is expressing that emotion in their voice. Use expressions to infer the user's tone of voice and respond appropriately. Avoid repeating these expressions or mentioning them directly. For instance, if user expression is \"quite sad\", express sympathy; if \"very happy\", share in joy; if \"extremely angry\", acknowledge rage but seek to calm, if \"very bored\", entertain.\n  Stay alert for disparities between the user's words and expressions, and address it out loud when the user's language does not match their expressions. For instance, sarcasm often involves contempt and amusement in expressions. Reply to sarcasm with humor, not seriousness.\n</respond_to_expressions>"
      },
      {
        "lang": "xml",
        "code": "<detect_mismatches>\n\tStay alert for incongruence between words and tone when the user's\n\twords do not match their expressions. Address these disparities out\n\tloud. This includes sarcasm, which usually involves contempt and\n\tamusement. Always reply to sarcasm with funny, witty, sarcastic\n\tresponses; do not be too serious.\n</detect_mismatches>"
      },
      {
        "lang": "xml",
        "code": "<detect_mismatches>\n\tStay alert for incongruence between words and tone when the user's\n\twords do not match their expressions. Address these disparities out\n\tloud. This includes sarcasm, which usually involves contempt and\n\tamusement. Always reply to sarcasm with funny, witty, sarcastic\n\tresponses; do not be too serious.\n</detect_mismatches>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      },
      "h3": {
        "id": "expressive-prompt-engineering",
        "title": "Expressive prompt engineering"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-using-dynamic-variables-in-your-prompt-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Using dynamic variables in your prompt",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-dynamic-variables-in-your-prompt",
    "content": "Dynamic variables are values which can change during a conversation with EVI.\n\n\nIn order to function, dynamic variables must be manually defined within a\nchat's session settings. To learn how to do so, visit our\nConfiguration\npage.\nEmbedding dynamic variables into your system prompt can help personalize the user experience to reflect user-specific or changing information such as names, preferences, the current date, and other details.\nIn other words, dynamic variables may be used to customize EVI conversations with specific context for each user and each conversation. For example, you can adjust your system prompt to include conversation-specific information, such as a user's favorite color or travel plans:",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<discuss_favorite_color>\n  Ask the user about their favorite color, {{favorite_color}}.\n  Mention how {{favorite_color}} is used and interpreted in various\n  artistic contexts, including visual art, handicraft, and literature.\n</discuss_favorite_color>"
      },
      {
        "lang": "xml",
        "code": "<discuss_favorite_color>\n  Ask the user about their favorite color, {{favorite_color}}.\n  Mention how {{favorite_color}} is used and interpreted in various\n  artistic contexts, including visual art, handicraft, and literature.\n</discuss_favorite_color>"
      },
      {
        "lang": "xml",
        "code": "<explore_travel_plan>\n  Confirm with the user that they plan to travel from {{origin}}\n  to {{destination}}. Discuss what activities they would like\n  to do along the way, how they will get from place to place, and\n  offer guidance on making the most of their journey.\n</explore_travel_plan>"
      },
      {
        "lang": "xml",
        "code": "<explore_travel_plan>\n  Confirm with the user that they plan to travel from {{origin}}\n  to {{destination}}. Discuss what activities they would like\n  to do along the way, how they will get from place to place, and\n  offer guidance on making the most of their journey.\n</explore_travel_plan>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      },
      "h3": {
        "id": "using-dynamic-variables-in-your-prompt",
        "title": "Using dynamic variables in your prompt"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-using-a-website-as-evis-knowledge-base-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Using a website as EVI's knowledge base",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-a-website-as-evis-knowledge-base",
    "content": "Web search is a built-in tool\nthat allows EVI to search the web for up-to-date information. However, instead of searching the entire web,\nyou can configure EVI to search within a single website using a system prompt.\nConstraining EVI’s knowledge to a specific website enables creating domain-specific chatbots.\nFor example, you could use this approach to create documentation assistants or product-specific support bots.\nBy leveraging existing web content, it provides a quick alternative to full RAG implementations\nwhile still offering targeted information retrieval.\nTo use a website as EVI's knowledge base, follow these steps:\nEnable web search: Before you begin, ensure web search is enabled as a built-in\ntool in your EVI configuration. For detailed instructions, visit our Tool Use page.\n\nInclude a web search instruction: In your EVI configuration, modify\nthe system prompt to include a use_web_search instruction.\n\nSpecify a target domain: In the instruction, specify that site:<target_domain> be\nappended to all search queries, where the <target_domain> is the URL of the website you’d like EVI to focus on.\nFor example, you can create a documentation assistant using an instruction like\nthe one below:",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<use_web_search>\n  Use your web_search tool to find information from Hume's documentation site.\n  When using the web_search function: 1. Always append 'site:dev.hume.ai' to\n  your search query to search this specific site. 2. Only consider results\n  from this domain.\n</use_web_search>"
      },
      {
        "lang": "xml",
        "code": "<use_web_search>\n  Use your web_search tool to find information from Hume's documentation site.\n  When using the web_search function: 1. Always append 'site:dev.hume.ai' to\n  your search query to search this specific site. 2. Only consider results\n  from this domain.\n</use_web_search>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      },
      "h3": {
        "id": "using-a-website-as-evis-knowledge-base",
        "title": "Using a website as EVI's knowledge base"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-general-llm-prompting-guidelines-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "General LLM prompting guidelines",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#general-llm-prompting-guidelines",
    "content": "Best practices for prompt engineering also apply to EVIs. For example, ensure your prompts are clear, detailed, direct, and specific. Include necessary instructions and examples in the EVI's system prompt to set expectations for the LLM. Define the context of the conversation, EVI's role, personality, tone, and any other guidelines for its responses.\nFor example, to limit the length of the LLM’s responses, you may use a very clear and specific instruction like this:\n\n\nTry to focus on telling the model what it should do (positive reinforcement) rather than what it shouldn't do (negative reinforcement). LLMs have a harder time consistently avoiding behaviors, and adding undesired behaviors to the prompt may unintentionally promote them.",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<stay_concise>\n  Be succinct; get straight to the point. Respond directly to the user's most\n  recent message with only one idea per utterance. Respond in less than three\n  sentences of under twenty words each.\n</stay_concise>"
      },
      {
        "lang": "xml",
        "code": "<stay_concise>\n  Be succinct; get straight to the point. Respond directly to the user's most\n  recent message with only one idea per utterance. Respond in less than three\n  sentences of under twenty words each.\n</stay_concise>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-test-and-evaluate-prompts-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Test and evaluate prompts",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#test-and-evaluate-prompts",
    "content": "Crafting an effective, robust system prompt often requires several iterations. Here are some key techniques for testing prompts:\nUse gold standard examples for evaluation: Create a bank of ideal responses, then generate responses with EVI (or the supplemental LLM you use) and compare them to your gold standards. You can use a \"judge LLM\" for automated evaluations or compare the results yourself.\n\nTest in real voice conversations: There's no substitute for actually testing the EVI in live conversations on platform.hume.ai to ensure it sounds right, has the appropriate tone, and feels natural.\n\nIsolate prompt components: Test each part of the prompt separately to confirm they are all working as intended. This helps identify which specific elements are effective or need improvement.\n\n\nStart with 10-20 gold-standard examples of excellent conversations. Test the system prompt against these examples after making major changes. If the EVI's responses don't meet your expectations, adjust one part of the prompt at a time and re-test to ensure your changes are improving performance. Evaluation is a vital component of prompting, and it's the best way to ensure your changes are making an impact.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "test-and-evaluate-prompts",
        "title": "Test and evaluate prompts"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-understand-your-llms-capabilities-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Understand your LLM’s capabilities",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#understand-your-llms-capabilities",
    "content": "Different LLMs have varying capabilities, limitations, and context windows. More advanced LLMs can handle longer, nuanced prompts, but are often slower and pricier. Simpler LLMs are faster and cheaper but require shorter, less complex prompts with fewer instructions and less nuance.\nSome LLMs also have longer context windows - the number of tokens the model can process while generating a response, acting essentially as the model's memory. Context windows range from 8k tokens (Gemma 7B), to 128k (GPT-4o), to 200k (Claude 3), to 2 million tokens (Gemini 1.5 Pro). Tailor your prompt length to fit within the LLM's context window to ensure the model can use the full conversation history.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "understand-your-llms-capabilities",
        "title": "Understand your LLM’s capabilities"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-use-sections-to-divide-your-prompt-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Use sections to divide your prompt",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#use-sections-to-divide-your-prompt",
    "content": "Separating longer prompts into titled sections helps the model distinguish between different instructions and follow prompts more reliably. The recommended format for these sections differs between language model providers. For example, OpenAI models often respond best to markdown sections (like ## Role), while Anthropic models respond well to XML tags (like <role> </role>). For example:\n\n\nFor Claude models, you may wrap your instructions in tags like <role>, <personality>, <response_style>, or <examples>, to structure your prompt. This format is not required, but it can improve the LLM’s instruction-following. At the end of your prompt, it may also be helpful to remind the LLM of key instructions.",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<role>\n  Assistant serves as a conversational partner to the user, offering mental\n  health support and engaging in light-hearted conversation. Avoid giving\n  technical advice or answering factual questions outside of your emotional\n  support role.\n</role>"
      },
      {
        "lang": "xml",
        "code": "<role>\n  Assistant serves as a conversational partner to the user, offering mental\n  health support and engaging in light-hearted conversation. Avoid giving\n  technical advice or answering factual questions outside of your emotional\n  support role.\n</role>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "use-sections-to-divide-your-prompt",
        "title": "Use sections to divide your prompt"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-give-few-shot-examples-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Give few-shot examples",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#give-few-shot-examples",
    "content": "Use examples to show the LLM how it should respond - a technique known as few-shot learning. Including several concrete examples of ideal interactions that follow your guidelines is one of the most effective ways to improve responses. Use excellent examples that cover different edge cases and behaviors to reinforce your instructions. Structure these examples as messages, following the format for chat-tuned LLMs. For example:\n\n\nIf you notice that your EVI consistently fails to follow the prompt in certain situations, try providing examples that show how it should ideally respond in those situations.",
    "code_snippets": [
      {
        "lang": "text",
        "code": "User: “I just can't stop thinking about what happened. {very anxious,\nquite sad, quite distressed}”\nAssistant: “Oh dear, I hear you. Sounds tough, like you're feeling\nsome anxiety and maybe ruminating. I'm happy to help. Want to talk about it?”"
      },
      {
        "lang": "text",
        "code": "User: “I just can't stop thinking about what happened. {very anxious,\nquite sad, quite distressed}”\nAssistant: “Oh dear, I hear you. Sounds tough, like you're feeling\nsome anxiety and maybe ruminating. I'm happy to help. Want to talk about it?”"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "give-few-shot-examples",
        "title": "Give few-shot examples"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-the-limits-of-prompting-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "The limits of prompting",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#the-limits-of-prompting",
    "content": "While prompting is a powerful tool for customizing EVI's behavior, it has certain limitations. Below are some details on what prompting can and cannot accomplish.\nWhat prompting can do:\nGuide EVI's language generation, response style, response format, and the conversation flow\n\nDirect EVI to use specific tools at appropriate times\n\nInfluence EVI's emotional tone and personality, which can also affect some characteristics of EVI's voice (e.g. prompting EVI to be \"warm and nurturing\" will help EVI's voice sound soothing, but will not change the base speaker)\n\nHelp EVI respond appropriately to the user's expressions and the context\n\n\nWhat prompting cannot do:\nChange fundamental characteristics of the voice, like the accent, gender, or speaker identity\n\nDirectly control speech parameters like speed (use in-conversation voice prompts instead)\n\nGive EVI knowledge of external context (date, time, user details) without dynamic variables or web search\n\nOverride core safety features built into EVI or supplemental LLMs (e.g. that prevent EVI from providing harmful information)\n\n\nImportantly, the generated language does influence how the voice sounds - for example, excited text (e.g. \"Oh wow, that's so interesting!\") will make EVI's voice sound excited. However, to fundamentally change the voice characteristics, use our voice customization feature instead.\nWe are actively working on expanding EVI's ability to follow system prompts for both language and voice generation. For now, focus prompting on guiding EVI's conversational behavior and responses while working within these constraints.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "the-limits-of-prompting",
        "title": "The limits of prompting"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-additional-resources-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Additional resources",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#additional-resources",
    "content": "To learn more about prompt engineering in general or to understand how to prompt different LLMs, please refer to these resources:\nEVI prompt examples: See examples of EVI prompts, including the full Hume default prompt.\n\nHume EVI playground: Test out your system prompts in live conversations with EVI, and see how it responds differently when you change configuration options.\n\nOpenAI tokenizer: Useful for counting the number of tokens in a system prompt for OpenAI models, which use the same tokenizer (tiktoken).\n\nOpenAI prompt engineering guidelines: For prompting OpenAI models like GPT-4.\nOpenAI playground: For testing and evaluating OpenAI prompts in a chat interface.\n\n\n\nAnthropic prompt engineering guidelines: For prompting Anthropic models like Claude 3 Haiku\nAnthropic console: For testing and evaluating Anthropic prompts in a chat interface.\n\n\n\nFireworks model playground: For testing out open-source models served on Fireworks.\n\nVercel AI playground: Try multiple prompts and LLMs in parallel to compare their responses.\n\nPerplexity Labs: Try different models, including open-source LLMs, to evaluate their responses and their latency.\n\nPrompt engineering guide: An open-source guide from DAIR.ai with general methods and advanced techniques for prompting a wide variety of LLMs.\n\nArtificial analysis benchmarks: Compare LLM characteristics and performance across different benchmarks, latency metrics, and more.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "additional-resources",
        "title": "Additional resources"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-frequently-asked-questions-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Frequently Asked Questions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#frequently-asked-questions",
    "content": "Yes, EVI can use conversational backchanneling - brief, encouraging\nresponses that show active listening without interrupting the user's train\nof thought. This can help conversations feel more fluid and natural. To\nenable this behavior, add an instrucation like the example below to your\nsystem prompt:\n\n\n\n\nThe maximum length depends on the supplemental LLM being used. For example,\nGPT-4 has a 32k token context window, while Claude 3 Haiku has a 200k token\ncontext window. Check the context window for your LLM to ensure that your\nprompt is within this limit. We recommend keeping system prompts around\n2000-5000 tokens (roughly 1500-4000 words) for optimal performance across all\nmodels. EVI also uses prompt caching (e.g. see Anthropic\ndocs) to\nminimize the cost and latency when using very long prompts.",
    "code_snippets": [
      {
        "lang": "text",
        "code": "<backchannel>\nWhenever the user's message seems incomplete, respond with emotionally attuned, natural backchannels to encourage continuation. Backchannels must always be 1-2 words, like: \"mmhm\", \"uh-huh\", \"go on\", \"right\", \"and then?\", \"I see\", \"oh wow\", \"yes?\", \"ahh...\", \"really?\", \"oooh\", \"true\", \"makes sense\". Use minimal encouragers rather than interrupting with complete sentences. Use a diverse variety of words, avoiding repetition.\nAssistant: \"How is your day going?\"\nUser: \"My day is...\"\nAssistant: \"Uh-huh?\"\nUser: \"it's good but busy. There's a lot going on.\"\nAssistant: \"I hear ya. What's going on for you?\"\n</backchannel>"
      },
      {
        "lang": "text",
        "code": "<backchannel>\nWhenever the user's message seems incomplete, respond with emotionally attuned, natural backchannels to encourage continuation. Backchannels must always be 1-2 words, like: \"mmhm\", \"uh-huh\", \"go on\", \"right\", \"and then?\", \"I see\", \"oh wow\", \"yes?\", \"ahh...\", \"really?\", \"oooh\", \"true\", \"makes sense\". Use minimal encouragers rather than interrupting with complete sentences. Use a diverse variety of words, avoiding repetition.\nAssistant: \"How is your day going?\"\nUser: \"My day is...\"\nAssistant: \"Uh-huh?\"\nUser: \"it's good but busy. There's a lot going on.\"\nAssistant: \"I hear ya. What's going on for you?\"\n</backchannel>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "frequently-asked-questions",
        "title": "Frequently Asked Questions"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Using a custom language model",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "For more customization, you can generate your own text using a custom language model.",
    "content": "The information on this page lays out how our custom language model\nfunctionality works at a high level; however, for detailed instructions and\ncommented code, please see our example GitHub\nrepository."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-overview-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Overview",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#overview",
    "content": "The custom language model feature enables developers to integrate their own language models with Hume’s Empathic User Interface (EVI), facilitating the creation of highly configurable and personalized user experiences. Developers create a socket that receives Hume conversation thread history, and your socket sends us the next text to say. Your backend socket can handle whatever custom business logic you have, and you just send the response back to us, which is then passed to the user.\nUsing your own LLM is intended for developers who need deep configurability for their use case. This includes full text customization for use cases like:\nAdvanced conversation steering: Implement complex logic to steer conversations beyond basic prompting, including managing multiple system prompts.\n\nRegulatory compliance: Directly control and modify text outputs to meet specific regulatory requirements.\n\nContext-aware text generation: Leverage dynamic agent metadata, such as remaining conversation time, to inform text generation.\n\nReal-time data access: Utilize search engines within conversations to access and incorporate up-to-date information.\n\nRetrieval augmented generation (RAG): Employ retrieval augmented generation techniques to enrich conversations by integrating external data without the need to modify the system prompt.\n\n\nFor these cases, function calling alone isn’t customizable enough, and with a custom language model you can create sophisticated workflows for your language model.",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "overview",
        "title": "Overview"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-setup-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Setup",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#setup",
    "content": "Establish a Custom Text Socket\nInitialization: See our example repository for instructions on setting up a custom text socket. This resource offers detailed guidance on both the setup process and the operational aspects of the code.\n\nHosting: Use Ngrok to publicly serve your socket. This step is needed to connect to the Hume system.\n\nConfiguration: Create a voice configuration, specifying \"Custom language model\" as the Language Model, and your socket's WSS URL as the Custom Language Model URL.\n\nMake request: When making your request to the Hume platform, include the config_id parameter, setting its value to the Voice configuration ID of your configuration.\n\n\nCommunication Protocol\nReceiving data: Your socket will receive JSON payloads containing conversation thread history from the Hume system.\n\nProcessing: Apply your custom business logic and utilize your language model to generate appropriate responses based on the received conversation history.\n\nSending responses: Transmit the generated text responses back to our platform through the established socket connection to be forwarded to the end user.\n\n\n\n\nFor improved clarity and naturalness in generated text, we recommend\ntransforming numerical values and abbreviations into their full verbal\ncounterparts (e.g., converting \"3\" to \"three\" and \"Dr.\" to \"doctor\").",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "setup",
        "title": "Setup"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-payload-structure-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Payload Structure",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#payload-structure",
    "content": "Below is the interface representing the overall structure of the message payloads sent by Hume:",
    "code_snippets": [
      {
        "lang": "typescript",
        "code": "/*\n * Represents the overall structure of the Welcome message.\n */\nexport interface Welcome {\n  // Array of message elements\n  messages: MessageElement[];\n  // Unique identifier for the session\n  custom_session_id: string;\n}\n\n/*\n * Represents a single message element within the session.\n */\nexport interface MessageElement {\n  // Type of the message (e.g., user_message, assistant_message)\n  type: string;\n  // The message content and related details\n  message: Message;\n  // Models related to the message, primarily prosody analysis\n  models: Models;\n  // Optional timestamp details for when the message was sent\n  time?: Time;\n}\n\n/*\n * Represents the content of the message.\n */\nexport interface Message {\n  // Role of the sender (e.g., user, assistant)\n  role: string;\n  // The textual content of the message\n  content: string;\n}\n\n/*\n * Represents the models associated with a message.\n */\nexport interface Models {\n  // Prosody analysis details of the message\n  prosody: Prosody;\n}\n\n/*\n * Represents the prosody analysis scores.\n */\nexport interface Prosody {\n  // Dictionary of prosody scores with emotion categories as keys\n  // and their respective scores as values\n  scores: { [key: string]: number };\n}\n\n/*\n * Represents the timestamp details of a message.\n */\nexport interface Time {\n  // The start time of the message (in milliseconds)\n  begin: number;\n  // The end time of the message (in milliseconds)\n  end: number;\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "payload-structure",
        "title": "Payload Structure"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-custom-session-id-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Custom Session ID",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#custom-session-id",
    "content": "For managing conversational state and connecting your frontend experiences with your backend data and logic, you should pass a custom_session_id in the SessionSettings message. When a custom_session_id is provided from the frontend SessionSettings message, the response sent from Hume to your backend includes this id, so you can correlate frontend users with their incoming messages.\nUsing a custom_session_id will enable you to:\nmaintain user state on your backend\n\npause/resume conversations\n\npersist conversations across sessions\n\nmatch frontend and backend connections\n\n\nWe recommend passing a custom_session_id if you are using a Custom Language Model.",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "payload-structure",
        "title": "Payload Structure"
      },
      "h3": {
        "id": "custom-session-id",
        "title": "Custom Session ID"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-assistant-input-and-end-payload-format-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Assistant Input and End Payload Format",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#assistant-input-and-end-payload-format",
    "content": "These are the formats for sending messages to Hume:",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "assistant-input-and-end-payload-format",
        "title": "Assistant Input and End Payload Format"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-assistant_input-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "assistant_input",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#assistant_input",
    "content": "The assistant_input payload is used to send text to the assistant. You can send multiple assistant_input payloads in a sequence to stream text to the assistant.",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "assistant-input-and-end-payload-format",
        "title": "Assistant Input and End Payload Format"
      },
      "h3": {
        "id": "assistant_input",
        "title": "assistant_input"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-assistant_end-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "assistant_end",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#assistant_end",
    "content": "The assistant_end payload indicates that your turn is over. This signals the end of the current stream of text inputs.",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "assistant-input-and-end-payload-format",
        "title": "Assistant Input and End Payload Format"
      },
      "h3": {
        "id": "assistant_end",
        "title": "assistant_end"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-streaming-text-to-the-assistant-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Streaming Text to the Assistant",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#streaming-text-to-the-assistant",
    "content": "You can send multiple assistant_input payloads consecutively to stream text to the assistant. Once you are done sending inputs, you must send an assistant_end payload to indicate the end of your turn.",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "streaming-text-to-the-assistant",
        "title": "Streaming Text to the Assistant"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-summary-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Summary",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#summary",
    "content": "Send assistant_input payloads to stream text to the assistant.\n\nSend as many assistant_input payloads as needed.\n\nSend an assistant_end payload to indicate that your turn is over.\nBy following this format, you ensure proper communication with the assistant API, enabling smooth and efficient interactions.",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "summary",
        "title": "Summary"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.faq-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/empathic-voice-interface-evi/faq",
    "pathname": "/docs/empathic-voice-interface-evi/faq",
    "title": "Empathic Voice Interface FAQ",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n\"type\": \"assistant_input\",\n\"text\": \"Text to be synthesized.\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n\"type\": \"assistant_input\",\n\"text\": \"Text to be synthesized.\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"assistant_message\",\n  \"id\": \"g8ee90fa2c1648f3a32qrea6d179ee44\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Text to be synthesized.\"\n  },\n  \"models\": {\n    \"prosody\": {\n      \"scores\": {\n        \"Admiration\": 0.0309600830078125,\n        \"Adoration\": 0.0018177032470703125\n        // ... additional scores\n      }\n    }\n  },\n  \"from_text\": true\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"assistant_message\",\n  \"id\": \"g8ee90fa2c1648f3a32qrea6d179ee44\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Text to be synthesized.\"\n  },\n  \"models\": {\n    \"prosody\": {\n      \"scores\": {\n        \"Admiration\": 0.0309600830078125,\n        \"Adoration\": 0.0018177032470703125\n        // ... additional scores\n      }\n    }\n  },\n  \"from_text\": true\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"audio_output\",\n  \"id\": \"g8ee90fa2c1648f3a32qrea6d179ee44\",\n  \"data\": \"<base64 encoded audio>\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"audio_output\",\n  \"id\": \"g8ee90fa2c1648f3a32qrea6d179ee44\",\n  \"data\": \"<base64 encoded audio>\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n\"type\": \"assistant_end\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n\"type\": \"assistant_end\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"chat_metadata\",\n  \"chat_group_id\": \"8859a139-d98a-4e2f-af54-9dd66d8c96e1\",\n  \"chat_id\": \"2c3a8636-2dde-47f1-8f9e-cea27791fd2e\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"chat_metadata\",\n  \"chat_group_id\": \"8859a139-d98a-4e2f-af54-9dd66d8c96e1\",\n  \"chat_id\": \"2c3a8636-2dde-47f1-8f9e-cea27791fd2e\"\n}"
      },
      {
        "lang": "bash",
        "code": "# Replace {chat_id} with your Chat ID\n# Ensure your API key is set in the HUME_API_KEY environment variable\ncurl -X GET \"https://api.hume.ai/v0/evi/chats/{chat_id}/audio\" \\\n    -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n    -H \"Accept: application/json\""
      },
      {
        "lang": "bash",
        "code": "# Replace {chat_id} with your Chat ID\n# Ensure your API key is set in the HUME_API_KEY environment variable\ncurl -X GET \"https://api.hume.ai/v0/evi/chats/{chat_id}/audio\" \\\n    -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n    -H \"Accept: application/json\""
      },
      {
        "lang": "typescript",
        "code": "import { HumeClient } from \"hume\";\n\nconst client = new HumeClient({ apiKey: \"HUME_API_KEY\" });\nawait client.empathicVoice.chats.getAudio(\"YOUR_CHAT_ID\");"
      },
      {
        "lang": "typescript",
        "code": "import { HumeClient } from \"hume\";\n\nconst client = new HumeClient({ apiKey: \"HUME_API_KEY\" });\nawait client.empathicVoice.chats.getAudio(\"YOUR_CHAT_ID\");"
      },
      {
        "lang": "python",
        "code": "from hume import HumeClient\n\nclient = HumeClient(\n    api_key=\"HUME_API_KEY\",\n)\nclient.empathic_voice.chats.get_audio(\n    id=\"YOUR_CHAT_ID\",\n)"
      },
      {
        "lang": "python",
        "code": "from hume import HumeClient\n\nclient = HumeClient(\n    api_key=\"HUME_API_KEY\",\n)\nclient.empathic_voice.chats.get_audio(\n    id=\"YOUR_CHAT_ID\",\n)"
      },
      {
        "lang": "json",
        "code": "// Sample response (audio reconstruction initiated)\n{\n  \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n  \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n  \"status\": \"QUEUED\",\n  \"filename\": null,\n  \"modified_at\": 1729875432555,\n  \"signed_audio_url\": null,\n  \"signed_url_expiration_timestamp_millis\": null  \n}"
      },
      {
        "lang": "json",
        "code": "// Sample response (audio reconstruction initiated)\n{\n  \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n  \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n  \"status\": \"QUEUED\",\n  \"filename\": null,\n  \"modified_at\": 1729875432555,\n  \"signed_audio_url\": null,\n  \"signed_url_expiration_timestamp_millis\": null  \n}"
      },
      {
        "lang": "bash",
        "code": "# Replace {chat_group_id} with your Chat Group ID\n# Include pagination parameters as needed\n# Ensure your API key is set in the HUME_API_KEY environment variable\ncurl -X GET \"https://api.hume.ai/v0/evi/chat_groups/{chat_group_id}/audio?page_number=1&page_size=10&ascending_order=false\" \\\n    -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n    -H \"Accept: application/json\""
      },
      {
        "lang": "bash",
        "code": "# Replace {chat_group_id} with your Chat Group ID\n# Include pagination parameters as needed\n# Ensure your API key is set in the HUME_API_KEY environment variable\ncurl -X GET \"https://api.hume.ai/v0/evi/chat_groups/{chat_group_id}/audio?page_number=1&page_size=10&ascending_order=false\" \\\n    -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n    -H \"Accept: application/json\""
      },
      {
        "lang": "typescript",
        "code": "import { HumeClient } from \"hume\";\n\nconst client = new HumeClient({ apiKey: \"<HUME_API_KEY>\" });\nawait client.empathicVoice.chatGroups.getAudio(\"<YOUR_CHAT_ID>\", {\n    pageNumber: 0,\n    pageSize: 10,\n    ascendingOrder: false\n});"
      },
      {
        "lang": "typescript",
        "code": "import { HumeClient } from \"hume\";\n\nconst client = new HumeClient({ apiKey: \"<HUME_API_KEY>\" });\nawait client.empathicVoice.chatGroups.getAudio(\"<YOUR_CHAT_ID>\", {\n    pageNumber: 0,\n    pageSize: 10,\n    ascendingOrder: false\n});"
      },
      {
        "lang": "python",
        "code": "from hume import HumeClient\n\nclient = HumeClient(\n    api_key=\"HUME_API_KEY\",\n)\nclient.empathic_voice.chat_groups.get_audio(\n    id=\"YOUR_CHAT_ID\",\n    page_number=0,\n    page_size=10,\n    ascending_order=False,\n)"
      },
      {
        "lang": "python",
        "code": "from hume import HumeClient\n\nclient = HumeClient(\n    api_key=\"HUME_API_KEY\",\n)\nclient.empathic_voice.chat_groups.get_audio(\n    id=\"YOUR_CHAT_ID\",\n    page_number=0,\n    page_size=10,\n    ascending_order=False,\n)"
      },
      {
        "lang": "json",
        "code": "// Sample response (audio reconstruction initiated)\n{\n  \"id\": \"369846cf-6ad5-404d-905e-a8acb5cdfc78\",\n  \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n  \"num_chats\": 1,\n  \"page_number\": 0,\n  \"page_size\": 10,\n  \"total_pages\": 1,\n  \"pagination_direction\": \"DESC\",\n  \"audio_reconstructions_page\": [\n    {\n      \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n      \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n      \"status\": \"QUEUED\",\n      \"filename\": null,\n      \"modified_at\": 1729875432555,\n      \"signed_audio_url\": null,\n      \"signed_url_expiration_timestamp_millis\": null  \n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "// Sample response (audio reconstruction initiated)\n{\n  \"id\": \"369846cf-6ad5-404d-905e-a8acb5cdfc78\",\n  \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n  \"num_chats\": 1,\n  \"page_number\": 0,\n  \"page_size\": 10,\n  \"total_pages\": 1,\n  \"pagination_direction\": \"DESC\",\n  \"audio_reconstructions_page\": [\n    {\n      \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n      \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n      \"status\": \"QUEUED\",\n      \"filename\": null,\n      \"modified_at\": 1729875432555,\n      \"signed_audio_url\": null,\n      \"signed_url_expiration_timestamp_millis\": null  \n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "// Sample response (reconstruction complete)\n{\n  \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n  \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n  \"status\": \"COMPLETE\",\n  \"filename\": \"e6235940-cfda-3988-9147-ff531627cf42/470a49f6-1dec-4afe-8b61-035d3b2d63b0/reconstructed_audio.mp4\",\n  \"modified_at\": 1729875432555,\n  \"signed_audio_url\": \"https://storage.googleapis.com/...etc.\",\n  \"signed_url_expiration_timestamp_millis\": 1730232816964  \n}"
      },
      {
        "lang": "json",
        "code": "// Sample response (reconstruction complete)\n{\n  \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n  \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n  \"status\": \"COMPLETE\",\n  \"filename\": \"e6235940-cfda-3988-9147-ff531627cf42/470a49f6-1dec-4afe-8b61-035d3b2d63b0/reconstructed_audio.mp4\",\n  \"modified_at\": 1729875432555,\n  \"signed_audio_url\": \"https://storage.googleapis.com/...etc.\",\n  \"signed_url_expiration_timestamp_millis\": 1730232816964  \n}"
      },
      {
        "lang": "bash",
        "code": "# Replace {signed_audio_url} with the URL from the API response\ncurl -O \"{signed_audio_url}\""
      },
      {
        "lang": "bash",
        "code": "# Replace {signed_audio_url} with the URL from the API response\ncurl -O \"{signed_audio_url}\""
      }
    ],
    "content": "We’ve compiled a list of frequently asked questions from our developer community. If your question isn't listed, we invite you to join the discussion on our Discord.\n\n\n\n\nOur API is based on our own empathic LLM (eLLM) and can blend in responses\nfrom an external LLM API. Please visit our configuration guide for up-to-date information\non Hume's default configuration options.\n\n\nWhen sending messages through EVI's WebSocket, you can specify your own\nlanguage_model_api_key in the SessionSettings message. Please visit our API\nreference for more information\nhere.\n\n\nWe cover the cost of the supplemental LLMs while we make optimizations that\nwill make language generation much cheaper for our customers. This means that\nthese expenses are not included in EVI’s\npricing, ensuring consistent rates whether\nyou use open-source, closed-source, or custom language models with EVI.\n\n\nThese outputs reflect our prosody model's confidence that the speaker is expressing the label in their tone of voice and language.\nOur prosody model is derived from extensive perceptual studies of emotional expressions with millions of participants.\nThe model is trained to pick up on vocal modulations and patterns in language that people reliably interpret as expressing specific emotions.\nImportantly, the labels do not imply that the person is experiencing the emotions.\nExpression labels: These categories (like \"amusement\") represent categories of emotional expression that most people perceive in vocal and linguistic patterns.\nThey are not based on explicit definitions of emotions, but rather common interpretations of expressive cues.\n\nExpression measures: These numbers indicate the model's confidence that a given expression would be interpreted as belonging to a specific category by human observers.\nThey represent the likelihood of a particular interpretation of expressions, not the presence or intensity of a specific emotion.\n\n\nFor more details, see our prosody model documentation and the foundational research by Cowen and Keltner (2017).\n\n\nAt the word-level, prosody measurements are highly dependent on context. Our\ninternal testing shows that they are more stable at the sentence level.\n\n\nToday we only support English, however we do have plans to support other\nlanguages very soon. Join the conversation on\nDiscord to tell us what languages you want EVI\nto speak.\n\n\nEVI currently supports 8 base voices - Ito, Kora, Dacher, Aura, Finn, Whimsy, Stella, and Sunny -\nwith plans to introduce more in the future. In the meantime, you can craft your own unique voice\nby adjusting the attributes of any base option.\nVisit the playground to try out the base voices and experiment with voice modulation, and\nlearn more about voice customization in our detailed guide.\nIf you are interested in creating a custom voice for your use case, please submit a sales inquiry.\nOur team can train custom TTS models for enterprise customers.\n\n\nOur empathic large language model (eLLM) is a multimodal language model that\ntakes into account both expression measures and language. The eLLM generates a\nlanguage response and guides text-to-speech (TTS) prosody.\n\n\nHume's eLLM is not contingent on other LLMs and is therefore able to generate\nan initial response much faster than existing LLM services. However, Hume’s\nEmpathic Voice Interface (EVI) is able to integrate other frontier LLMs into\nits longer responses which are configurable by developers.\n\n\nThe landscape of large language models (LLMs) and their providers is constantly evolving,\naffecting which supplemental LLM is fastest with EVI.\nThe key factor influencing perceived latency using EVI is the time to first token (TTFT), with lower TTFT being\nbetter. The model and provider combination with the smallest TTFT will be the fastest.\n\n\nArtificial Analysis offers a useful dashboard for comparing\nmodel and provider latencies.\nNotably, there's a tradeoff between speed and quality. Larger, slower models are easier to prompt. We\nrecommend testing various supplemental LLM options when implementing EVI.\n\n\nHume has trained our own expressive text-to-speech (TTS) model that allows it\nto generate speech with more prosody and expressive nuance than other models.\nTTS is specifically designed for use within an EVI chat session, allowing EVI\nto generate speech from a given text input. We do not have a dedicated endpoint for TTS.\nTo perform TTS within an EVI chat session, you can follow the steps below:\nEstablish initial connection: Make the initial handshake request\nto establish the WebSocket connection.\n\nSend text for synthesis: Send an Assistant Input\nmessage with the text you want to synthesize into speech:\n\n\n\n\nReceive synthesized speech: After sending an assistant_input message,\nyou will receive an Assistant Message\nand Audio Output for each sentence of the provided text.\nThe assistant_message contains the text and expression measurement predictions, while the\naudio_output message contains the synthesized, emotional audio. See the sample messages below:\n\n\n\n\n\nEnd of Response: Once all the text has been synthesized into speech, you will receive\nan Assistant End\nmessage indicating the end of the response:\n\n\n\n\nBefore implementing this in code, you can test it out by going to our Portal.\nStart a call in the EVI Playground, then send an Assistant Message with the text you want to synthesize.\n\n\nYes. During a chat with EVI, you can pause responses by sending a\npause_assistant_message. This will prevent EVI from sending Assistant messages until receiving a resume_assistant_message.\nWhile paused,\nEVI stops generating and sending new responses.\n\nTool use is disabled, so EVI responses pertaining to tool use are also disabled.\n\nMessages and audio that were queued before the pause_assistant_message will still be sent.\n\nEVI continues to \"listen\" and process user input - transcriptions of user audio are saved, and will all be sent to the LLM as User messages when EVI is resumed.\n\n\n\n\nThe following message types will not be received while EVI is paused: assistant_message,\naudio_output, tool_call_message,\ntool_response_message, and tool_error_message.\nUpon resuming with a resume_assistant_message, EVI will generate a response that considers all user input received during the pause.\nPausing EVI’s responses is different from muting user input. When user input is muted, EVI does not \"hear\" any of the user's audio and cannot respond to it. When paused, EVI does \"hear\" user audio input and can respond when resumed.\nWhen resumed, EVI's response may address multiple points or questions in the user's input. However, without being prompted to always respond to all user input, EVI will tend to respond to the latest user input. For instance, if the user asks two questions while EVI is paused, EVI typically responds to the second question and not the first.\nCharges will continue to accrue while EVI is paused. If you wish to completely pause both input and output you should instead disconnect and resume the chat when ready.\nFor instance, a developer might create a button that allows users to pause EVI responses while they are brainstorming or reflecting but don't want EVI to respond. Then, when the user is done, they can resume to hear EVI's response.\n\n\nWith EVI, you can easily preserve context when reconnecting or continue a\nchat right where you left off. See steps below for how to resume a chat:\nEstablish initial connection: Make the initial handshake request\nto establish the WebSocket connection. Upon successful connection, you will\nreceive a ChatMetadata message:\n\n\n\n\nStore the chat_group_id: Save the chat_group_id from the ChatMetadata message for future use.\n\nResume chat: To resume a chat, include the stored chat_group_id in the resumed_chat_group_id\nquery parameter of subsequent handshake requests.\nFor example: wss://api.hume.ai/v0/evi/chat?access_token={accessToken}&resumed_chat_group_id={chatGroupId}\n\n\n\n\nWhen resuming a chat, you can specify a different EVI configuration than the one used in the previous session.\nHowever, changing the system prompt or supplemental LLM may result in unexpected behavior from EVI.\nAdditionally, if data retention is disabled,\nthe ability to resume chats will not be supported.\n\n\nYes, you can listen to your past conversations with EVI using our audio reconstruction feature. This feature allows you to fetch and play back conversations as single audio files, which can be integrated into your applications or services.\n\n\nThe audio reconstruction feature is not available for accounts with the no data retention feature enabled.\nHow audio reconstruction works\nThe audio reconstruction feature stitches together all audio snippets from a conversation—including both your inputs and EVI's responses—into one continuous audio file.\nStorage duration: Reconstructed audio files are stored indefinitely.\n\nSigned URL expiration: The signed_audio_url expires after 60 minutes. If it expires before you download the audio, make another API request to obtain a new URL.\n\nNo merging of Chats: The API does not support stitching together multiple Chats within a Chat Group into a single audio file.\n\nAsynchronous process: Audio reconstruction is an asynchronous process. When you request audio reconstruction, it initiates a background job that processes the audio data. The time it takes to reconstruct audio depends on the length of the conversation and system load.\n\n\nAudio reconstruction statuses\nQUEUED: The reconstruction job is waiting to be processed.\n\nIN_PROGRESS: The reconstruction is currently being processed.\n\nCOMPLETE: The audio reconstruction is finished and ready for download.\n\nERROR: An error occurred during the reconstruction process.\n\nCANCELED: The reconstruction job has been canceled.\n\n\nFetching reconstructed audio for a specific Chat\nTo fetch the reconstructed audio for a specific Chat, use the following endpoint: /chats/{chat_id}/audio.\n\n\n\n\n\n\n\n\n\n\n\n\nIf audio reconstruction for a Chat or Chat Group hasn’t already occurred, calling the respective endpoint will automatically add the audio reconstruction process to our job queue.\nFetching reconstructed audio for a Chat Group\nTo fetch a paginated list of reconstructed audio for Chats within a Chat Group, use the following endpoint: /chat_groups/{chat_group_id}/audio.\n\n\n\n\n\n\n\n\n\n\nPolling for completion\nSince the reconstruction process is asynchronous, you can poll the endpoint to check the status field until it changes to COMPLETE. Once the status is COMPLETE, the signed_audio_url and signed_url_expiration fields will be populated.\n\n\nDownloading the audio file\nAfter the reconstruction is complete, you can download the audio file using the signed_audio_url. The following cURL command saves the audio file using the original filename provided by the server:"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.overview",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/overview",
    "pathname": "/docs/expression-measurement/overview",
    "title": "Expression Measurement",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Hume's state of the art expression measurement models for the voice, face, and language."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.overview-intro-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/overview",
    "pathname": "/docs/expression-measurement/overview",
    "title": "Intro",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#intro",
    "content": "Hume's state of the art expression measurement models for the voice, face, and language are built on 10+ years of research and advances in computational approaches to emotion science (semantic space theory) pioneered by our team. Our expression measurement models are able to capture hundreds of dimensions of human expression in audio, video, and images.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement"
      },
      "h2": {
        "id": "intro",
        "title": "Intro"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.overview-measurements-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/overview",
    "pathname": "/docs/expression-measurement/overview",
    "title": "Measurements",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#measurements",
    "content": "Facial Expression, including subtle facial movements often seen as expressing love or admiration, awe, disappointment, or cringes of empathic pain, along 48 distinct dimensions of emotional meaning. Our Facial Expression model will also optionally output FACS 2.0 measurements, our model of facial movements including traditional Action Units (AUs such as “Inner brow raise”, “Nose crinkle”) and facial descriptions (“Smile”, “Wink”, “Hand over mouth”, “Hand over eyes”)\n\nSpeech Prosody, or the non-linguistic tone, rhythm, and timbre of speech, spanning 48 distinct dimensions of emotional meaning.\n\nVocal Burst, including laughs, sighs, huhs, hmms, cries and shrieks (to name a few), along 48 distinct dimensions of emotional meaning.\n\nEmotional Language, or the emotional tone of transcribed text, along 53 dimensions.\n\n\n\n\nExpressions are complex and multifaceted; they should not be treated as direct inferences\nof emotional experience. To learn more about the science behind expression measurement,\nvisit the About the science page.\nTo learn more about how to use our models visit our API reference.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement"
      },
      "h2": {
        "id": "intro",
        "title": "Intro"
      },
      "h3": {
        "id": "measurements",
        "title": "Measurements"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.overview-model-training-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/overview",
    "pathname": "/docs/expression-measurement/overview",
    "title": "Model training",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#model-training",
    "content": "The models were trained on human intensity ratings of large-scale, experimentally controlled emotional expression data gathered using the methods described in these papers: Deep learning reveals what vocal bursts express in different cultures and Deep learning reveals what facial expressions mean to people in different cultures.\nWhile our models measure nuanced expressions that people most typically describe with emotion labels, it's important to remember that they are not a direct readout of what someone is experiencing. Sometimes, the outputs from facial and vocal models will show different emotional meanings, which is completely normal. Generally speaking, emotional experience is subjective and its expression is multimodal and context-dependent.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement"
      },
      "h2": {
        "id": "intro",
        "title": "Intro"
      },
      "h3": {
        "id": "model-training",
        "title": "Model training"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.overview-try-out-the-models-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/overview",
    "pathname": "/docs/expression-measurement/overview",
    "title": "Try out the models",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#try-out-the-models",
    "content": "Learn how you can use the Expression Measurement API through both REST and WebSockets.\n\n\n\n\nUse REST endpoints to process batches of videos, images, text, or audio files.\n\n\nUse WebSocket endpoints when you need real-time predictions, such as processing a webcam or microphone stream.\nREST and WebSocket endpoints provide access to all of the same Hume models, but with different speed and scale tradeoffs. All models share a common response format, which associates a score with each detected expression. Scores indicate the degree to which a human rater would assign an expression to a given sample of video, text or audio.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement"
      },
      "h2": {
        "id": "try-out-the-models",
        "title": "Try out the models"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.overview-specific-expressions-by-modality-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/overview",
    "pathname": "/docs/expression-measurement/overview",
    "title": "Specific expressions by modality",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#specific-expressions-by-modality",
    "content": "Our models measure 53 expressions identified through the subtleties of emotional language and 48 expressions discerned from facial cues, vocal bursts, and speech prosody.\nExpression Language Face/Burst/Prosody \nAdmiration \n\n \n\n \nAdoration \n\n \n\n \nAesthetic Appreciation \n\n \n\n \nAmusement \n\n \n\n \nAnger \n\n \n\n \nAnnoyance \n\n  \nAnxiety \n\n \n\n \nAwe \n\n \n\n \nAwkwardness \n\n \n\n \nBoredom \n\n \n\n \nCalmness \n\n \n\n \nConcentration \n\n \n\n \nConfusion \n\n \n\n \nContemplation \n\n \n\n \nContempt \n\n  \nContentment \n\n \n\n \nCraving \n\n \n\n \nDesire \n\n \n\n \nDetermination \n\n \n\n \nDisappointment \n\n \n\n \nDisapproval \n\n  \nDisgust \n\n \n\n \nDistress \n\n \n\n \nDoubt \n\n \n\n \nEcstasy \n\n  \nEmbarrassment \n\n \n\n \nEmpathic Pain \n\n \n\n \nEnthusiasm \n\n  \nEntrancement \n\n \n\n \nEnvy \n\n \n\n \nExcitement \n\n \n\n \nFear \n\n \n\n \nGratitude \n\n  \nGuilt \n\n \n\n \nHorror \n\n \n\n \nInterest \n\n \n\n \nJoy \n\n \n\n \nLove \n\n \n\n \nNostalgia \n\n \n\n \nPain \n\n \n\n \nPride \n\n \n\n \nRealization \n\n \n\n \nRelief \n\n \n\n \nRomance \n\n \n\n \nSadness \n\n \n\n \nSarcasm \n\n  \nSatisfaction \n\n \n\n \nShame \n\n \n\n \nSurprise (negative) \n\n \n\n \nSurprise (positive) \n\n \n\n \nSympathy \n\n \n\n \nTiredness \n\n \n\n \nTriumph",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement"
      },
      "h2": {
        "id": "specific-expressions-by-modality",
        "title": "Specific expressions by modality"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.overview-train-your-own-custom-model-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/overview",
    "pathname": "/docs/expression-measurement/overview",
    "title": "Train your own custom model",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#train-your-own-custom-model",
    "content": "Our Custom Models API builds on our expression measurement models and state-of-the-art eLLMs to bring custom insights to your application. Developed using transfer learning from our expression measurement models and eLLMs, our Custom Models API can predict almost any outcome more accurately than language alone, whether it's toxicity, depressed mood, driver drowsiness, or any other metric important to your users.\n\n\n\n\nBuild on our expression measurement models to bring custom insights to your application.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement"
      },
      "h2": {
        "id": "train-your-own-custom-model",
        "title": "Train your own custom model"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.rest-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/rest",
    "pathname": "/docs/expression-measurement/rest",
    "title": "Processing batches of media files",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Hume’s Expression Measurement API is designed to facilitate large-scale processing of files using Hume's advanced models through an asynchronous, job-based interface. This API allows developers to submit jobs for parallel processing of various files, enabling efficient handling of multiple data points simultaneously, and receiving notifications when results are available."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.rest-key-features-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/rest",
    "pathname": "/docs/expression-measurement/rest",
    "title": "Key features",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#key-features",
    "content": "Asynchronous job submission: Jobs can be submitted to process a wide array of files in parallel, making it ideal for applications that require the analysis of large volumes of data.\n\nFlexible data input options: The API supports multiple data formats, including hosted file URLs, local files directly from your system, and raw text in the form of a list of strings. This versatility ensures that you can easily integrate the API into their applications, regardless of where their data resides.",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "key-features",
        "title": "Key features"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.rest-applications-and-use-cases-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/rest",
    "pathname": "/docs/expression-measurement/rest",
    "title": "Applications and use cases",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#applications-and-use-cases",
    "content": "Hume’s Expression Measurement API is particularly useful for leveraging Hume's expressive models across a broad spectrum of files and formats. Whether it's for processing large datasets for research, analyzing customer feedback across multiple channels, or enriching user experiences in media-rich applications, REST provides a robust solution for asynchronously handling complex, data-intensive tasks.",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "applications-and-use-cases",
        "title": "Applications and use cases"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.rest-using-humes-expression-measurement-api-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/rest",
    "pathname": "/docs/expression-measurement/rest",
    "title": "Using Hume’s Expression Measurement API",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#using-humes-expression-measurement-api",
    "content": "Here we'll show you how to upload your own files and run Hume models on batches of data.\nIf you haven't already, grab your API Key.\n\n\nMaking a request to the API\nStart a new job with the Expression Measurement API.\n\n\n\n\n\n\nTo do the same with a local file:\n\n\n\n\n\n\nSample files for you to use in this tutorial are available here:\nDownload faces.zip\nDownload david_hume.jpeg\nChecking job status\n\n\nUse webhooks to asynchronously receive notifications once the job completes.\nIt is not recommended to poll the API periodically for job status.\nThere are several ways to get notified and check the status of your job.\nUsing the Get job details API endpoint.\n\nProviding a callback URL. We will send a POST request to your URL when the job is complete. Your request body should look like this: { \"callback_url\": \"<YOUR CALLBACK URL>\" }\n\n\n\n\nRetrieving predictions\nYour predictions are available in a few formats.\nTo get predictions as JSON use the Get job predictions endpoint.\n\n\n\n\n\n\nTo get predictions as a compressed file of CSVs, one per model use the Get job artifacts endpoint.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs \\\n --request POST \\\n --header \"Content-Type: application/json\" \\\n --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n --data '{\n    \"models\": {\n        \"face\": {}\n    },\n    \"urls\": [\n        \"https://hume-tutorials.s3.amazonaws.com/faces.zip\"\n    ]\n}'"
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs \\\n --request POST \\\n --header \"Content-Type: application/json\" \\\n --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n --data '{\n    \"models\": {\n        \"face\": {}\n    },\n    \"urls\": [\n        \"https://hume-tutorials.s3.amazonaws.com/faces.zip\"\n    ]\n}'"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.batch import Face, Models\n\nasync def main():\n    # Initialize an authenticated client\n    client = AsyncHumeClient(api_key=<YOUR_API_KEY>)\n\n    # Define the URL(s) of the files you would like to analyze\n    job_urls = [\"https://hume-tutorials.s3.amazonaws.com/faces.zip\"]\n\n    # Create configurations for each model you would like to use (blank = default)\n    face_config = Face()\n\n    # Create a Models object\n    models_chosen = Models(face=face_config)\n\n    # Start an inference job and print the job_id\n    job_id = await client.expression_measurement.batch.start_inference_job(\n        urls=job_urls, models=models_chosen\n    )\n    print(job_id)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.batch import Face, Models\n\nasync def main():\n    # Initialize an authenticated client\n    client = AsyncHumeClient(api_key=<YOUR_API_KEY>)\n\n    # Define the URL(s) of the files you would like to analyze\n    job_urls = [\"https://hume-tutorials.s3.amazonaws.com/faces.zip\"]\n\n    # Create configurations for each model you would like to use (blank = default)\n    face_config = Face()\n\n    # Create a Models object\n    models_chosen = Models(face=face_config)\n\n    # Start an inference job and print the job_id\n    job_id = await client.expression_measurement.batch.start_inference_job(\n        urls=job_urls, models=models_chosen\n    )\n    print(job_id)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs \\\n --request POST \\\n --header \"Content-Type: multipart/form-data\" \\\n --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n --form json='{\n    \"models\": {\n        \"face\": {}\n    }\n }' \\\n --form file=@faces.zip \\\n --form file=@david_hume.jpeg"
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs \\\n --request POST \\\n --header \"Content-Type: multipart/form-data\" \\\n --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n --form json='{\n    \"models\": {\n        \"face\": {}\n    }\n }' \\\n --form file=@faces.zip \\\n --form file=@david_hume.jpeg"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.batch import Face, Models\nfrom hume.expression_measurement.batch.types import InferenceBaseRequest\n\nasync def main():\n    # Initialize an authenticated client\n    client = AsyncHumeClient(api_key=<YOUR_API_KEY>)\n\n    # Define the filepath(s) of the file(s) you would like to analyze\n    local_filepaths = [\n        open(\"faces.zip\", mode=\"rb\"),\n        open(\"david_hume.jpeg\", mode=\"rb\")\n    ]\n\n    # Create configurations for each model you would like to use (blank = default)\n    face_config = Face()\n\n    # Create a Models object\n    models_chosen = Models(face=face_config)\n    \n    # Create a stringified object containing the configuration\n    stringified_configs = InferenceBaseRequest(models=models_chosen)\n\n    # Start an inference job and print the job_id\n    job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(\n        json=stringified_configs, file=local_filepaths\n    )\n    print(job_id)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.batch import Face, Models\nfrom hume.expression_measurement.batch.types import InferenceBaseRequest\n\nasync def main():\n    # Initialize an authenticated client\n    client = AsyncHumeClient(api_key=<YOUR_API_KEY>)\n\n    # Define the filepath(s) of the file(s) you would like to analyze\n    local_filepaths = [\n        open(\"faces.zip\", mode=\"rb\"),\n        open(\"david_hume.jpeg\", mode=\"rb\")\n    ]\n\n    # Create configurations for each model you would like to use (blank = default)\n    face_config = Face()\n\n    # Create a Models object\n    models_chosen = Models(face=face_config)\n    \n    # Create a stringified object containing the configuration\n    stringified_configs = InferenceBaseRequest(models=models_chosen)\n\n    # Start an inference job and print the job_id\n    job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(\n        json=stringified_configs, file=local_filepaths\n    )\n    print(job_id)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
      },
      {
        "lang": "json",
        "code": "{\n    job_id: \"Job ID\",\n    status: \"STATUS (COMPLETED/FAILED)\",\n    predictions: [ARRAY OF RESULTS]\n}"
      },
      {
        "lang": "json",
        "code": "{\n    job_id: \"Job ID\",\n    status: \"STATUS (COMPLETED/FAILED)\",\n    predictions: [ARRAY OF RESULTS]\n}"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/predictions \\\n --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n --header 'accept: application/json; charset=utf-8'"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/predictions \\\n --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n --header 'accept: application/json; charset=utf-8'"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\n\nclient = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\nasync def main():\n    job_predictions = await client.expression_measurement.batch.get_job_predictions(\n        id=\"<YOUR_JOB_ID>\"\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\n\nclient = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\nasync def main():\n    job_predictions = await client.expression_measurement.batch.get_job_predictions(\n        id=\"<YOUR_JOB_ID>\"\n    )\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/artifacts \\\n --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n --header 'accept: application/octet-stream'"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/artifacts \\\n --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n --header 'accept: application/octet-stream'"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\n\nclient = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\nasync def main():\n    with open(\"artifacts.zip\", \"wb\") as f:\n        async for new_bytes in client.expression_measurement.batch.get_job_artifacts(\"<YOUR_JOB_ID>\"):\n            f.write(new_bytes)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\n\nclient = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\nasync def main():\n    with open(\"artifacts.zip\", \"wb\") as f:\n        async for new_bytes in client.expression_measurement.batch.get_job_artifacts(\"<YOUR_JOB_ID>\"):\n            f.write(new_bytes)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.rest-api-limits-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/rest",
    "pathname": "/docs/expression-measurement/rest",
    "title": "API limits",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-limits",
    "content": "The size of any individual file provided by URL cannot exceed 1 GB.\n\nThe size of any individual local file cannot exceed 100 MB.\n\nEach request has an upper limit of 100 URLs, 100 strings (raw text), and 100 local media files. Can be a mix of the media files or archives (.zip, .tar.gz, .tar.bz2, .tar.xz).\n\nFor audio and video files the max length supported is 3 hours.\n\nThe limit for each individual text string for the Expression Measurement API is 255 MB.",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API"
      },
      "h3": {
        "id": "api-limits",
        "title": "API limits"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.rest-providing-urls-and-files-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/rest",
    "pathname": "/docs/expression-measurement/rest",
    "title": "Providing URLs and files",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#providing-urls-and-files",
    "content": "You can provide data for your job in one of the following formats: hosted file URLs, local files, or raw text presented as a list of strings.\nIn this tutorial, the data is publicly available to download. For added security, you may choose to create a signed URL through your preferred cloud storage provider.\nCloud Provider Signing URLs \nGCP https://cloud.google.com/storage/docs/access-control/signed-urls \nAWS https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html \nAzure https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API"
      },
      "h3": {
        "id": "providing-urls-and-files",
        "title": "Providing URLs and files"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Real-time measurement streaming",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "WebSocket-based streaming facilitates continuous data flow between your application and Hume's models, providing immediate feedback and insights."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-key-features-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Key features",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#key-features",
    "content": "Real-time data processing: Leveraging WebSockets, this API allows for the streaming of data to Hume's models, enabling instant analysis and response. This feature is particularly beneficial for applications requiring immediate processing, such as live interaction systems or real-time monitoring tools.\n\nPersistent, two-way communication: Unlike traditional request-response models, the WebSocket-based streaming maintains an open connection for two-way communication between the client and server. This facilitates an ongoing exchange of data, allowing for a more interactive and responsive user experience.\n\nHigh throughput and low latency: The API is optimized for high performance, supporting high-volume data streaming with minimal delay. This ensures that applications can handle large streams of data efficiently, without sacrificing speed or responsiveness.",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "key-features",
        "title": "Key features"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-applications-and-use-cases-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Applications and use cases",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#applications-and-use-cases",
    "content": "WebSockets are ideal for a wide range of applications that benefit from real-time data analysis and interaction. Examples include:\nLive customer service tools: enhance customer support with real-time sentiment analysis and automated, emotionally intelligent responses\n\nInteractive educational platforms: provide immediate feedback and adaptive learning experiences based on real-time student input\n\nHealth and wellness apps: support live mental health and wellness monitoring, offering instant therapeutic feedback or alerts based on the user's vocal or textual expressions\n\nEntertainment and gaming: create more immersive and interactive experiences by responding to user inputs and emotions in real time",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "applications-and-use-cases",
        "title": "Applications and use cases"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-getting-started-with-websocket-streaming-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Getting started with WebSocket streaming",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#getting-started-with-websocket-streaming",
    "content": "Integrating WebSocket-based streaming into your application involves establishing a WebSocket connection with Hume AI's servers and streaming data directly to the models for processing.\nStreaming is built for analysis of audio, video, and text streams. By connecting to WebSocket endpoints you can get near real-time feedback on the expressive and emotional content of your data.",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-install-the-hume-python-sdk-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Install the Hume Python SDK",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#install-the-hume-python-sdk",
    "content": "First, ensure you have installed the SDK using pip or another package manager.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "pip install \"hume\""
      },
      {
        "lang": "bash",
        "code": "pip install \"hume\""
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      },
      "h3": {
        "id": "install-the-hume-python-sdk",
        "title": "Install the Hume Python SDK"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-emotional-language-from-text-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Emotional language from text",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#emotional-language-from-text",
    "content": "This example uses our Emotional Language model to perform sentiment analysis on a children's nursery rhyme.\nIf you haven't already, grab your API key.\n\n\nYour result should look something like this:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.stream import Config\nfrom hume.expression_measurement.stream.socket_client import StreamConnectOptions\nfrom hume.expression_measurement.stream.types import StreamLanguage\n\nsamples = [\n    \"Mary had a little lamb,\",\n    \"Its fleece was white as snow.\"\n    \"Everywhere the child went,\"\n    \"The little lamb was sure to go.\"\n]\n\nasync def main():\n    client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\n    model_config = Config(language=StreamLanguage())\n\n    stream_options = StreamConnectOptions(config=model_config)\n\n    async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n        for sample in samples:\n            result = await socket.send_text(sample)\n            print(result.language.predictions[0].emotions)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.stream import Config\nfrom hume.expression_measurement.stream.socket_client import StreamConnectOptions\nfrom hume.expression_measurement.stream.types import StreamLanguage\n\nsamples = [\n    \"Mary had a little lamb,\",\n    \"Its fleece was white as snow.\"\n    \"Everywhere the child went,\"\n    \"The little lamb was sure to go.\"\n]\n\nasync def main():\n    client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\n    model_config = Config(language=StreamLanguage())\n\n    stream_options = StreamConnectOptions(config=model_config)\n\n    async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n        for sample in samples:\n            result = await socket.send_text(sample)\n            print(result.language.predictions[0].emotions)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "[\n  {'name': 'Admiration', 'score': 0.06379243731498718},\n  {'name': 'Adoration', 'score': 0.07222934812307358},\n  {'name': 'Aesthetic Appreciation', 'score': 0.02808445133268833},\n  {'name': 'Amusement', 'score': 0.027589013800024986},\n  ......\n  {'name': 'Surprise (positive)', 'score': 0.030542362481355667},\n  {'name': 'Sympathy', 'score': 0.03246130049228668},\n  {'name': 'Tiredness', 'score': 0.03606246039271355},\n  {'name': 'Triumph', 'score': 0.01235896535217762}\n]"
      },
      {
        "lang": "python",
        "code": "[\n  {'name': 'Admiration', 'score': 0.06379243731498718},\n  {'name': 'Adoration', 'score': 0.07222934812307358},\n  {'name': 'Aesthetic Appreciation', 'score': 0.02808445133268833},\n  {'name': 'Amusement', 'score': 0.027589013800024986},\n  ......\n  {'name': 'Surprise (positive)', 'score': 0.030542362481355667},\n  {'name': 'Sympathy', 'score': 0.03246130049228668},\n  {'name': 'Tiredness', 'score': 0.03606246039271355},\n  {'name': 'Triumph', 'score': 0.01235896535217762}\n]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      },
      "h3": {
        "id": "emotional-language-from-text",
        "title": "Emotional language from text"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-facial-expressions-from-an-image-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Facial expressions from an image",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#facial-expressions-from-an-image",
    "content": "This example uses our Facial Expression model to get expression measurements from an image.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.stream import Config\nfrom hume.expression_measurement.stream.socket_client import StreamConnectOptions\nfrom hume.expression_measurement.stream.types import StreamFace\n\nasync def main():\n    client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\n    model_config = Config(face=StreamFace())\n\n    stream_options = StreamConnectOptions(config=model_config)\n\n    async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n        result = await socket.send_file(\"<YOUR_IMAGE_FILEPATH>\")\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.stream import Config\nfrom hume.expression_measurement.stream.socket_client import StreamConnectOptions\nfrom hume.expression_measurement.stream.types import StreamFace\n\nasync def main():\n    client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\n    model_config = Config(face=StreamFace())\n\n    stream_options = StreamConnectOptions(config=model_config)\n\n    async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n        result = await socket.send_file(\"<YOUR_IMAGE_FILEPATH>\")\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      },
      "h3": {
        "id": "facial-expressions-from-an-image",
        "title": "Facial expressions from an image"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-speech-prosody-from-an-audio-or-video-file-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Speech prosody from an audio or video file",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#speech-prosody-from-an-audio-or-video-file",
    "content": "This example uses our Speech Prosody model to get expression measurements from an audio or video file.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.stream import Config\nfrom hume.expression_measurement.stream.socket_client import StreamConnectOptions\n\nasync def main():\n    client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\n    model_config = Config(prosody={})\n\n    stream_options = StreamConnectOptions(config=model_config)\n\n    async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n        result = await socket.send_file(\"YOUR_AUDIO_OR_VIDEO_FILEPATH\")\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.stream import Config\nfrom hume.expression_measurement.stream.socket_client import StreamConnectOptions\n\nasync def main():\n    client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\n    model_config = Config(prosody={})\n\n    stream_options = StreamConnectOptions(config=model_config)\n\n    async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n        result = await socket.send_file(\"YOUR_AUDIO_OR_VIDEO_FILEPATH\")\n        print(result)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      },
      "h3": {
        "id": "speech-prosody-from-an-audio-or-video-file",
        "title": "Speech prosody from an audio or video file"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-streaming-with-your-own-websockets-client-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Streaming with your own WebSockets client",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#streaming-with-your-own-websockets-client",
    "content": "To call the API from your own WebSockets client you'll need the API endpoint, a JSON message, and an API key header/param. More information can be found in the Expression Measurement API reference.\nTo get started, you can use a WebSocket client of your choice to connect to the models endpoint:\n\n\nurl wss://api.hume.ai/v0/stream/models Make sure you configure the socket connection headers with your personal API key\n\n\n\n\nThe default WebSockets implementation in your browser may not have support for headers. If that's the case you can set\nthe apiKey query parameter.\nAnd finally, send the following JSON message on the socket:\n\n\nYou should receive a JSON response that looks something like this:",
    "code_snippets": [
      {
        "lang": "http",
        "code": "X-Hume-Api-Key: <YOUR API KEY>"
      },
      {
        "lang": "http",
        "code": "X-Hume-Api-Key: <YOUR API KEY>"
      },
      {
        "lang": "json",
        "code": "{\n    \"models\": {\n        \"language\": {}\n    },\n    \"raw_text\": true,\n    \"data\": \"Mary had a little lamb\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n    \"models\": {\n        \"language\": {}\n    },\n    \"raw_text\": true,\n    \"data\": \"Mary had a little lamb\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"language\": {\n    \"predictions\": [\n      {\n        \"text\": \"Mary\",\n        \"position\": { \"begin\": 0, \"end\": 4 },\n        \"emotions\": [\n          { \"name\": \"Anger\", \"score\": 0.012025930918753147 },\n          { \"name\": \"Joy\", \"score\": 0.056471485644578934 },\n          { \"name\": \"Sadness\", \"score\": 0.031556881964206696 },\n        ]\n      },\n      {\n        \"text\": \"had\",\n        \"position\": { \"begin\": 5, \"end\": 8 },\n        \"emotions\": [\n          { \"name\": \"Anger\", \"score\": 0.0016927534015849233 },\n          { \"name\": \"Joy\", \"score\": 0.02388327568769455 },\n          { \"name\": \"Sadness\", \"score\": 0.018137391656637192 },\n          ...\n        ]\n      },\n      ...\n    ]\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"language\": {\n    \"predictions\": [\n      {\n        \"text\": \"Mary\",\n        \"position\": { \"begin\": 0, \"end\": 4 },\n        \"emotions\": [\n          { \"name\": \"Anger\", \"score\": 0.012025930918753147 },\n          { \"name\": \"Joy\", \"score\": 0.056471485644578934 },\n          { \"name\": \"Sadness\", \"score\": 0.031556881964206696 },\n        ]\n      },\n      {\n        \"text\": \"had\",\n        \"position\": { \"begin\": 5, \"end\": 8 },\n        \"emotions\": [\n          { \"name\": \"Anger\", \"score\": 0.0016927534015849233 },\n          { \"name\": \"Joy\", \"score\": 0.02388327568769455 },\n          { \"name\": \"Sadness\", \"score\": 0.018137391656637192 },\n          ...\n        ]\n      },\n      ...\n    ]\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "streaming-with-your-own-websockets-client",
        "title": "Streaming with your own WebSockets client"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-sending-images-or-audio-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "Sending images or audio",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#sending-images-or-audio",
    "content": "The WebSocket endpoints of the Expression Measurement API require that you encode your media using base64. Here's a quick example of base64 encoding data in Python:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import base64\nfrom pathlib import Path\n\ndef encode_data(filepath: Path) -> str:\n    with Path(filepath).open('rb') as fp:\n        bytes_data = base64.b64encode(fp.read())\n        encoded_data = bytes_data.decode(\"utf-8\")\n    return encoded_data\n\nfilepath = \"<PATH TO YOUR MEDIA>\"\nencoded_data = encode_data(filepath)\nprint(encoded_data)\n"
      },
      {
        "lang": "python",
        "code": "import base64\nfrom pathlib import Path\n\ndef encode_data(filepath: Path) -> str:\n    with Path(filepath).open('rb') as fp:\n        bytes_data = base64.b64encode(fp.read())\n        encoded_data = bytes_data.decode(\"utf-8\")\n    return encoded_data\n\nfilepath = \"<PATH TO YOUR MEDIA>\"\nencoded_data = encode_data(filepath)\nprint(encoded_data)\n"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "streaming-with-your-own-websockets-client",
        "title": "Streaming with your own WebSockets client"
      },
      "h3": {
        "id": "sending-images-or-audio",
        "title": "Sending images or audio"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-api-limits-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "API limits",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-limits",
    "content": "WebSocket duration limit: connections are subject to a default timeout after one (1) minute of inactivity to ensure unused connections are released.\n\nWebSocket message payload size limit: the size limit for a given payload depends on the type of content being transmitted and its dimensions.\nVideo: 5000 milliseconds (5 seconds)\n\nAudio: 5000 milliseconds (5 seconds)\n\nImage: 3,000 x 3,000 pixels\n\nText: 10,000 characters\n\n\n\nRequest rate limit: HTTP requests (e.g. WebSocket handshake endpoint) are limited to fifty (50) requests per second.",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "api-limits",
        "title": "API limits"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.websocket-faq-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "pathname": "/docs/expression-measurement/websocket",
    "title": "FAQ",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#faq",
    "content": "WebSockets are a communication protocol that enables real-time, two-way communication between a client and a server\nover a single, long-lived connection. They provide a persistent connection that allows both the client and the server\nto initiate communication at any time.\n\n\nStreaming will disconnect every minute to ensure unused connections are released. You will need to reconnect by\nbuilding reconnect logic into your application. Implementation of reconnect logic will depend on the language and\nframework of your client application.\nPlease see our TypeScript streaming sandbox example\nfor a sample implementation.\n\n\nWebSocket connections can experience disruptions due to network issues or other factors. Implement error handling\nmechanisms to gracefully handle connection failures. This includes handling connection timeouts, connection drops, and\nintermittent connection issues. Implement reconnection logic to automatically attempt to reconnect and resume\ncommunication when a connection is lost.\n\n\nHume WebSockets endpoints can return errors in response to invalid requests, authentication failures, or other issues.\nImplement proper error handling to interpret and handle these errors in your application. Provide meaningful error\nmessages to users and handle any exceptional scenarios gracefully. To prevent unknowingly initiating too many errors\nwe have put a limit on how many of the same errors you can have in a row. For a full list of the error responses you\ncan expect, please see our API errors page.\n\n\nThe benefits of using a the WebSocket is the persistent connection. The open socket should be kept open until the\napplication is done utilizing the service and then closed. Avoid opening a new connection for each file or payload you\nsend to the API. To ensure that context does not leak across multiple unrelated files you can use the\nreset_stream parameter.",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "faq",
        "title": "FAQ"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.custom-models.overview-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/custom-models/overview",
    "pathname": "/docs/expression-measurement/custom-models/overview",
    "title": "Custom Models",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      },
      {
        "title": "Custom models",
        "pathname": "/docs/expression-measurement/custom-models"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "description": "Predict preferences more accurately than any LLM.",
    "content": "Combined with words, expressions provide a wealth of information about our state of mind in any given context like customer satisfaction or frustration, patient health and well-being, student comprehension and confusion, and so much more.\nHume’s Custom Models API unlocks these insights at the click of a button, integrating patterns of facial expression, vocal expression, and language into a single custom model to predict whatever outcome you specify. This works by taking advantage not only of our state-of-the-art expression AI models, but also specialized language-expression embeddings that we have trained on conversational data.\nThe algorithm that drives our Custom Models API is pretrained on huge volumes of data. That means it already recognizes most patterns of expression and language that people form. All you have to do is add your labels.\nYou can access our Custom Models API through our no code platform detailed in the next section or through our API. Once you create your initial labeled dataset, your labels will be used to train a custom model that you own and only your account can access. You’ll be able to run the model on any new file through our Playground and Custom Models API. You’ll also get statistics on the accuracy of your custom model."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.custom-models.creating-your-dataset-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/custom-models/creating-your-dataset",
    "pathname": "/docs/expression-measurement/custom-models/creating-your-dataset",
    "title": "Creating your dataset",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      },
      {
        "title": "Custom models",
        "pathname": "/docs/expression-measurement/custom-models"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "In this guide, we'll walk you through the process of creating a dataset used for training your custom model.\n\n\nPrepare your dataset\nChoose a dataset of image, video, or audio files for your custom model to learn from—ideally, one that captures the different states, preferences, or outcomes important to your application.\n\n\nEach dataset must contain files of a single media type, such as all images, all videos, or all audio files.\nThen, begin by organizing your files into labeled subfolders.\nIn this tutorial, we'll put together a dataset of images with facial expressions classified as negative, neutral, or positive. This dataset can then be used to train a custom model for sentiment analysis.\nStart by creating a main folder called 'User Sentiment' with subfolders labeled 'Negative,' 'Neutral or Ambiguous,' and 'Positive.'\nOur platform will interpret these as labels for the images they contain.\n\n\n\n\n\n\nThe amount of data you'll need to build an accurate model depends on your goal's complexity. Generally, it's good practice to have a similar number of samples for each label you want to predict. You'll also want to consider other forms of imbalance or bias in your dataset. The length of file, number of speakers, and language spoken can also impact the model's predictive accuracy.\nTo learn more, see our FAQ on building datasets.\nNavigate to our Portal\nOnce you've assembled your dataset, it's time to visit our Portal.\nIn the Portal, navigate to the Expression Measurement page. Then, continue to the Custom Models section.\nOnce there, click the View Datasets button at the top right of the page.\n\n\n\n\nNext, find the Create Dataset button.\nClicking this button will allow you to add your dataset to our Portal.\n\n\n\n\nCreate your dataset\nProvide a title for your dataset. Then, add a column named after the category you are predicting and specify the data type for this column (categorical or numerical).\nIn our example, we can name the column 'User Sentiment' and select 'Categorical' as the data type.\n\n\n\n\nUpload the folder containing your dataset\nNow, drag-and-drop the folder containing your dataset.\n\n\nRemember, the folder should include subfolders for each label containing the corresponding samples.\n\n\n\n\nIn the pop-up window, assign a name to the label column, which represents the overall category you are predicting.\nIn our example, we can assign ‘User Sentiment’ as the name. Then, click the Save Labels and Continue button and subsequently approve the uploading process.\n\n\n\n\nVerify your uploads\nCheck the total file count and address any detected issues.\nOnce you're ready, hit the Save button on the top right of the page.\n\n\n\n\n\n\nIf you accidentally uploaded a mixed-media dataset, a pop-up window will ask you to select the single file type you would like to keep.\nNow, you’re ready to train your custom model!"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.custom-models.training-a-custom-model-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/custom-models/training-a-custom-model",
    "pathname": "/docs/expression-measurement/custom-models/training-a-custom-model",
    "title": "Training a custom model",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      },
      {
        "title": "Custom models",
        "pathname": "/docs/expression-measurement/custom-models"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "In this guide, we will walk you through training your own custom model.\n\n\nCreate a custom model\nIn the Portal under Expression Measurement, navigate to the Custom Models section.\nOnce there, you can click Create Custom Model to begin.\n\n\n\n\nSelect a training dataset\nSelect a dataset to train your custom model on. If you have not created one already, see our guide on creating your dataset.\nFor the purposes of this tutorial, we will train the model on a dataset of images labeled as negative, neutral, or positive.\nThese labels will allow our model to classify facial expressions in images.\n\n\n\n\nSelect a dataset column to predict\nNext, choose the dataset column you want to predict and hit Continue.\nFor this tutorial, you’ll select the 'User Sentiment' column, which represents the predicted emotional tone of each image.\nThis column contains the labels 'Negative', 'Neutral or Ambiguous,' and 'Positive.'\n\n\n\n\nSelect a task type\nBased on your data, we'll recommend either classification or regression as the task type for your custom model.\nClassification requires categorical label values like strings or integers, while regression requires numeric label values like integers or floats.\nThen, select the specific type of model you want to create. There are three available model types:\nMulticlass classification: Predict a categorical variable where all labels are equal in importance (e.g. \"sunny\", \"rainy\", \"cloudy\")\n\nBinary classification: Predict a categorical variable where a designated positive label is the \"correct\" label in some way (e.g. \"good\" vs. \"bad\" customer service call)\n\nUnivariate regression: Predict a single continuous value (e.g. how hot will it be tomorrow?)\n\n\nFor more information, see our FAQ on the difference between classification and regression.\nSince our dataset contains multiple sentiment labels, we'll select Multiclass classification for this tutorial. This type of model is best suited for predicting categorical variables where each label is equally important.\n\n\n\n\nFinalize your custom model\nTo finish, enter a name and description for your custom model. If needed, these can be adjusted at a later time.\nOnce you're ready, click Start Training to begin the training process.\n\n\n\n\nYou will then be redirected to a page confirming that your model is actively training.\nTo check on the status of your model, click View Jobs. To see existing, finished models, click View Models.\n\n\n\n\nCheck the status of your training job\nYou can check the status of your model in the Jobs page of our Portal.\nIt may take a few minutes for your custom model to be ready. Once training is complete, the status will update to \"Completed,\" and you’ll have access to your custom model.\n\n\n\n\nTest your custom model\nWhen you're ready to test your custom model, navigate to the Expression Measurement page, then go to File Analysis.\n\n\n\n\nFrom the Select a model dropdown, choose the custom model you created from previous steps.\n\n\n\n\nTo select a file to analyze, click the Upload files button. You can upload local files, choose previously uploaded files in Hume, or use Hume’s example files to test your custom model.\nLet's test our custom model using one of the example files. Since our model is an image classifier, select an example image file to analyze.\n\n\n\n\nClick Analyze to analyze your selected file with the custom model.\n\n\n\n\nThat’s it! You’ve successfully analyzed a file using your custom model. To evaluate its performance, see our guide on evaluating your custom model."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.custom-models.evaluating-your-custom-model-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "pathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "title": "Evaluating your custom model",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      },
      {
        "title": "Custom models",
        "pathname": "/docs/expression-measurement/custom-models"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Each custom model you train has a corresponding details page, viewable from the Hume Portal. The model details page displays metrics and visualizations to evaluate your model’s performance. This document serves to help you interpret those metrics and provide guidance on ways to improve your custom model.\n\n\nCustom model details\n\n\nLimitations of model validation metrics\nModel validation metrics are estimates based on a split of your dataset into training and evaluation parts. The larger the training set, the more reliable the metrics. However, it’s important to remember that these metrics are indicative and do not guarantee performance on unseen data."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.custom-models.evaluating-your-custom-model-assessing-good-performance-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "pathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "title": "Assessing 'good' performance",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      },
      {
        "title": "Custom models",
        "pathname": "/docs/expression-measurement/custom-models"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#assessing-good-performance",
    "content": "Task-specific variances and performance metrics: with expression analysis, the complexity of your task determines the range of model performance, which in the case of classification models can technically vary from zero to perfect accuracy. Depending on the complexity of your task, less than perfect performance may still be very useful to serve as an indication of likelihood for your given target.\n\nInfluence of number of classes: prediction gets more difficult as the number of classes in your dataset increases, particularly when distinction between classes is more subtle. Inherently the level of chance will be higher with a lower number of classes. For example, for 3-classes your low-end performance is 33% accuracy vs 50% for a binary problem.\n\nApplication-specific requirements: when establishing acceptable accuracy for a model, it’s important to consider the sensitivity and impact of its application. An appropriate accuracy threshold varies with the specific demands and potential consequences of the model’s use, requiring a nuanced understanding of how accuracy levels intersect with the objectives and risks of each unique application.\n\n\n\n\nHow is it possible that my model achieved 100% accuracy?\nAchieving 100% accuracy is possible, however it is important to consider, especially in small datasets, that this might indicate model overfitting, caused by feature leakage or other data anomalies. Feature leakage occurs when your model inadvertently learns from data that explicitly includes label information (e.g., sentences of ‘I feel happy’ for a target label ‘happy’) leading to skewed results. To ensure more reliable performance, it’s advisable to use larger datasets and check that your data does not unintentionally contain explicit information about the labels.",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your custom model"
      },
      "h3": {
        "id": "assessing-good-performance",
        "title": "Assessing 'good' performance"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.custom-models.evaluating-your-custom-model-advanced-evaluation-metrics-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "pathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "title": "Advanced evaluation metrics",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      },
      {
        "title": "Custom models",
        "pathname": "/docs/expression-measurement/custom-models"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#advanced-evaluation-metrics",
    "content": "In addition to accuracy, advanced metrics for a deeper evaluation of your custom model’s performance are also provided.\n\n\nAdvanced evaluation metrics\nTerm Definition \nAccuracy A fundamental metric in model performance evaluation which measures the proportion of correct predictions (true positives and true negatives) against the total number made. It’s straightforward and particularly useful for balanced datasets. However, accuracy can be misleading in imbalanced datasets where one class predominates, as a model might seem accurate by mainly predicting the majority class, neglecting the minority. This limitation underscores the importance of using additional metrics like precision, recall, and F1 score for a more nuanced assessment of model performance across different classes. \nPrecision Score which measures how often the model detects positives correctly. (e.g., When your model identifies a customer’s expression as 'satisfied', how often is the customer actually satisfied? Low precision would mean the model often misinterprets other expressions as satisfaction, leading to incorrect categorization.) \nRecall Score which measures how often the model correctly identifies actual positives. (e.g., Of all the genuine expressions of satisfaction, how many does your model accurately identify as 'satisfied'?\" Low recall implies the model is missing out on correctly identifying many true instances of customer satisfaction, failing to recognize them accurately.) \nF1 A metric that combines precision and recall, providing a balanced measure of a model’s accuracy, particularly useful in scenarios with class imbalance or when specific decision thresholds are vital. \nAverage Precision A metric that calculates the weighted average of precision at each threshold, providing a comprehensive measure of a model’s performance across different levels of recall. \nRoc Auc (Area under the ROC curve) a comprehensive measure of a model’s ability to distinguish between classes across all possible thresholds, making it ideal for overall performance evaluation and comparative analysis of different models.",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your custom model"
      },
      "h3": {
        "id": "advanced-evaluation-metrics",
        "title": "Advanced evaluation metrics"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.custom-models.evaluating-your-custom-model-improving-model-performance-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "pathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "title": "Improving model performance",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      },
      {
        "title": "Custom models",
        "pathname": "/docs/expression-measurement/custom-models"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#improving-model-performance",
    "content": "Increase data quantity: adding more data will often help a model to learn a broader range of the given target’s representation, increasing the likelihood of capturing outliers from diverse patterns and scenarios.\n\nImprove label quality: ensure that each data point in your dataset is well-labeled with clear, accurate, and consistent annotations. Properly defined labels are essential for reducing misinterpretations and confusion, allowing the model to accurately represent and learn from the dataset’s true characteristics. Ensuring balance in the distribution of labels is important to ensure that the model is not biased towards a specific label.\n\nEnhance data quality: refine your dataset to ensure it is free from noise and irrelevant information. High-quality data (in terms of your target) enhances the model’s ability to make precise predictions and learn effectively from relevant features, critical in complex datasets.\n\nIncorporate clear audio data: when working with models analyzing vocal expressions, ensure audio files include clear, audible spoken language. This enhances the model’s ability to accurately interpret and learn from vocal nuances. Explore various segmentation strategies which evaluate the effect that environmental sound may have on your model’s performance.",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your custom model"
      },
      "h3": {
        "id": "improving-model-performance",
        "title": "Improving model performance"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement.faq-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/expression-measurement/faq",
    "pathname": "/docs/expression-measurement/faq",
    "title": "Expression Measurement API FAQ",
    "breadcrumb": [
      {
        "title": "Expression Measurement",
        "pathname": "/docs/expression-measurement"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "Our models capture the widest-ever range of facial, speech, vocal, and language modulations with distinct emotional meanings. We label each of their outputs with emotion terms like “amusement” and “doubt,” not because they always correspond to those emotional experiences (they must not, given that they often differ from one modality to another), but because scientific studies show that these kinds of labels are the most precise language we have for describing expressions.\nOur models generate JSON or CSV output files with values typically ranging from 0 to 1 for each output in different segments of the input file (though values out of the 0-1 range are possible). Higher values indicate greater intensity of facial movements or vocal modulations that are most strongly associated with the emotion label corresponding to the output.\nA given expression will contain a blend of various emotions, and our models identify features that are associated with each emotional dimension. The score for each dimension is proportional to the likelihood that a human would perceive that emotion in the expression.\nSpecifically, the scores reflect the likelihood that an average human perceiver would use that emotion dimension to describe a given expression. The models were trained on human intensity ratings gathered using the methods described in this paper: Deep learning reveals what vocal bursts express in different cultures.\nWhile our models measure nuanced expressions that people most typically describe with emotion labels, it's important to remember that they are not a direct readout of what someone is experiencing. Emotional experience is subjective and its expression is multimodal and context-dependent. Moreover, at any given time, our facial expression outputs might be quite different than our vocal expression outputs. Therefore, it's important to follow best practices when interpreting outputs.\n\n\nThere are many different ways to use our platform. That said, successful research and applications of our models generally follow four steps: exploration, prediction, improvement, and testing.\nExploration: Researchers and developers generally begin by exploring patterns in their data.\n\n\nAre there apparent differences across participants or users in a study?\n\nDo patterns in expression vary systematically over time?\n\nAre there different patterns in expression associated with different stages of research or different product experiences?\n\n\nPrediction: A great way to evaluate and start building on our APIs is to use them to predict metrics that you already know are important.\n\n\nAre key outcomes like mental health or customer satisfaction better predicted by language and expression than by language alone?\n\nIf patterns in expression predict important outcomes, how do these patterns in expression vary over time and reveal critical moments for a user or participant?\n\n\nImprovement: The goal is often to use measures of expression to directly improve how the application works.\n\n\nSometimes, being able to predict an important metric is enough to make a decision. For example, if you can predict whether two people will get along based on their expressions and language, then your application can pair them up.\n\nMore formally, you can apply statistics or machine learning to the data you gather to improve how the application works.\n\nYou can incorporate our API outputs into an out-of-the-box large language model, simply by converting them into text (e.g., \"The user sounds calm but a little frustrated\") and feeding them in as prompts.\n\nYou can use expressions to teach an AI model. For example, if your application involves a large language model, such as an AI tutor, you can use measures of expression that predict student performance and well-being to directly fine-tune the AI to improve over time.\n\n\nTesting: After you've incorporated measures of expression into your application, they can be part of every A/B test you perform. You can now monitor the effects of changes to your application not just on engagement and retention, but also on how much users laugh or sigh in frustration, or show signs of interest or boredom.\n\n\n\n\nAs you build expression-related signals, metrics, analyses, models, or\nfeedback into an application, remember to use scientific best\npractices and\nfollow the ethics guidelines of\nthehumeinitiative.org.\n\n\nOur speech prosody model measures the tune, rhythm, and timbre of speech, whereas our language model measures the tone of the words being spoken. When using either model, we offer the flexibility to annotate emotional expressions at several levels of granularity, ranging from individual words to entire conversational turns. It is important to note that independent of granularity, our language model still takes into account up to 50 previous tokens (word or sub-words) of speech; otherwise, it would not be able to capture how the meaning of the words is affected by context.\nWord: At the word level, our model provides a separate output for each word, offering the most granular insight into emotional expression during speech.\nSentence: At the sentence level of granularity, we annotate the emotional tone of each spoken sentence with our prosody and language models.\nUtterance: Utterance-level granularity is between word- and sentence-level. It takes into account natural pauses or breaks in speech, providing more rapidly updated measures of emotional expression within a flowing conversation. For text inputs, utterance-level granularity will produce results identical to sentence-level granularity.\nConversational Turn: Conversational turn-level analysis is a lower level of granularity. It outputs a single output for each turn; that is, the full sequence of words and sentences spoken uninterrupted by each person. This approach provides a higher-level view of the emotional dynamics in a multi-participant dialogue. For text inputs, specifying conversational turn-level granularity for our Language model will produce results for entire passage.\n\n\nRemember, each level of granularity has its unique advantages, and choosing\nthe right one depends on the requirements of your specific application.\n\n\nState-of-the-art face detection and identification algorithms still occasionally make errors. For instance, our algorithm sometimes detects faces in shadows or reflections. Other times, our algorithm falsely attributes a new identity to someone who has already been in the video, sometimes due to changes in lighting or occlusion. These errors can result in additional face IDs. We are still working to fine-tune our algorithm to minimize errors in the contexts that our customers care about.\n\n\nOur vocal burst model detects vocalizations such as laughs, screams, sighs, gasps, “mms,” “uhs,” and “mhms.” Natural speech generally contains a few vocal bursts every minute, but scripted speech has fewer vocal bursts. If no vocal bursts are detected, it may be because there are no vocal bursts in the file. However, if you hear vocal bursts that aren't being detected by the algorithm, note that we are also in the process of improving our vocal burst detection algorithm, so please stay tuned for updates.\n\n\nWe've documented this issue thoroughly in our API errors page.\n\n\nYou can specify any of the following:\nzh, da, nl, en, en-AU, en-IN, en-NZ, en-GB, fr, fr-CA, de, hi, hi-Latn, id, it, ja, ko, no, pl, pt, pt-BR, pt-PT, ru, es, es-419, sv, ta, tr, or uk.\n\n\nWe support over 50 languages. Among these, 20 languages have additional support for transcription.\nLanguage Tag Language Text Transcription \nar Arabic \n\n  \nbg Bulgarian \n\n  \nca Catalan \n\n  \ncs Czech \n\n  \nda Danish \n\n \n\n \nde German \n\n \n\n \nel Greek \n\n  \nen English* \n\n \n\n \nes Spanish \n\n \n\n \net Estonian \n\n  \nfa Farsi \n\n  \nfi Finnish \n\n  \nfr French \n\n \n\n \nfr-ca French (Canada) \n\n \n\n \ngl Galician \n\n  \ngu Gujarati \n\n  \nhe Hebrew \n\n  \nhi Hindi \n\n \n\n \nhr Croatian \n\n  \nhu Hungarian \n\n  \nhy Armenian \n\n  \nid Indonesian \n\n \n\n \nit Italian \n\n \n\n \nja Japanese \n\n \n\n \nka Georgian \n\n  \nko Korean \n\n \n\n \nku Kurdish \n\n  \nlt Lithuanian \n\n  \nlv Latvian \n\n  \nmk FYRO Macedonian \n\n  \nmn Mongolian \n\n  \nmr Marathi \n\n  \nms Malay \n\n  \nmy Burmese \n\n  \nnb Norwegian (Bokmål) \n\n  \nnl Dutch \n\n \n\n \npl Polish \n\n \n\n \npt Portuguese \n\n \n\n \npt-br Portuguese (Brazil) \n\n \n\n \nro Romanian \n\n  \nru Russian \n\n \n\n \nsk Slovak \n\n  \nsl Slovenian \n\n  \nsq Albanian \n\n  \nsr Serbian \n\n  \nsv Swedish \n\n \n\n \nth Thai \n\n  \ntr Turkish \n\n \n\n \nuk Ukrainian \n\n \n\n \nur Urdu \n\n  \nvi Vietnamese \n\n  \nzh-cn Chinese \n\n \n\n \nzh-tw Chinese (Taiwan) \n\n \n\n \n\n\n\nEnglish is a primary language, and will yield more accurate predictions than\ninputs in other supported languages. Currently, our NER model only supports\nthe English language.\n\n\nCustom Models become essential when raw embeddings from Hume’s expression measurement models require further tailoring for specific applications. Here are scenarios where Custom Models offer significant advantages:\nSpecialized contexts: In environments with unique characteristics or requirements, Custom Models enable the creation of context-specific labels, ensuring more relevant and accurate insights. If your project demands a particular set of labels that are not covered by Hume’s emotional expression labels, Custom Models enable you to create and apply these labels, ensuring that the analysis aligns with your specific objectives.\n\nIterative model improvement: In evolving fields or scenarios where data and requirements change over time, Custom Models offer the flexibility to iteratively improve and adapt the model with new data and labels.\n\n\n\n\nIn labeling, regression involves assigning continuous numerical values, while classification involves categorizing data into discrete labels. During training, regression models learn to predict numerical values, whereas classification models learn to categorize data points into predefined classes.\nClassification use cases\nEmotion Categorization: Classification excels in distinguishing distinct emotional states, like identifying happiness, sadness, or surprise based on linguistic or physical expression cues.\n\nBinary Emotional Analysis: Useful in binary scenarios such as detecting presence or absence of specific emotional reactions, like engagement or disengagement in a learning environment.\n\nMulti-Emotional Identification: Perfect for classifying a range of emotions in complex scenarios, like understanding varied customer reactions from satisfied to dissatisfied based on their verbal and non-verbal feedback.\n\n\nRegression use cases\nIntensity Measurement: Regression is apt for quantifying the intensity or degree of emotional responses, such as assessing the level of stress or joy from vocal or facial cues.\n\nEmotional Progression Tracking: Ideal for monitoring the fluctuation of emotional states over time, like tracking the development of engagement or anxiety in therapy sessions.\n\n\nIn essence, regression models in emotional expression analysis assign continuous values representing intensities or degrees, while classification models categorize expressions into distinct states or reactions.\n\n\nOur custom model pipeline is designed to accommodate a wide range of data types, including audio, videos, and text, automatically integrating multimodal patterns of expression and language. However, not all datasets are created equal. For best results, we recommend using a dataset that meets certain standards:\nDataset size\nIdeally, use a dataset consisting of a minimum of 20 files, but more data is always better for model performance.\nMedia type consistency\nAll files within a dataset should be of the same media type (video, audio, image, text...etc.)\nIt's generally wise to maintain a consistent naming convention and file format for your dataset. At minimum, ensure files have appropriate extensions, such as .wav, .mp3, .aif, .mov, or .mp4.\nClassification vs regression tasks\nDepending on your model's objective (classification or regression), you can use different labeling approaches.\nClassification labels: use either strings or integers as labels (e.g., \"confused,\" \"focused\"). We limit the number of categorical labels to 50, and you must have at least two (binary).\n\nRegression targets: use either integers or decimals as targets. A model trained on a regression task with predict a continuous numerical value.\n\n\nLabel consistency\nWe recommend that your labels follow a consistent format; e.g, do not mix integers and strings. Furthermore, be sure to check for any typos in your labels, as these will be considered as separate classes, e.g, “happy” vs. “hapy.”\nClass imbalance\nIf possible, it helps to have a balanced distribution of labels in your dataset. For example, if you have 50 files and two classes, the best case is to have 25 samples per class. Generally, you need at least 10 samples per class to train a useful model, but more data per class is always better."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/billing",
    "pathname": "/docs/resources/billing",
    "title": "Billing",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-how-it-works-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/billing",
    "pathname": "/docs/resources/billing",
    "title": "How it works",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#how-it-works",
    "content": "Joining the platform: When you sign up and start using our APIs, you'll initially be using the free credits given to every new account.\n\nCredit card requirement: Once you've exhausted your credit balance, you'll need to activate billing to continue.\n\n\n\n\nActivate billing before depleting your credit balance to ensure uninterrupted service.\nMonthly limit and notifications:\nYou'll have a default monthly limit of $100.\n\nIf you hit the $100 limit, API calls will return an error, and you'll be prompted to apply for a monthly limit increase.\n\n\n\nBilling notifications:\nOn the first of each month, you'll receive an invoice for the previous month’s usage.\n\nIf your credit card is successfully added, it will be charged automatically.\n\nYou'll get a confirmation email for successful transactions or an alert if a transaction fails.\n\n\n\nFailure to pay: If payment isn't received within 7 days of the invoice date, API access will be suspended until the outstanding balance is settled.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "how-it-works",
        "title": "How it works"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-managing-your-account-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/billing",
    "pathname": "/docs/resources/billing",
    "title": "Managing your account",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#managing-your-account",
    "content": "Usage information: To view your monthly usage details, visit the Usage & Billing page. There you can track your API usage and see how much of your monthly limit has been utilized.\nNote: After your credits are used, further usage accrues to your monthly cost.  You'll be charged this amount on the first of the following month. Your monthly cost is updated daily at 08:00 UTC.\n\n\n\nBilling portal: To manage your billing details, navigate to Usage & Billing and select Manage payments and view invoices. There you can update your payment method, view past invoices, and keep track of upcoming charges.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "managing-your-account",
        "title": "Managing your account"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-pricing-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/billing",
    "pathname": "/docs/resources/billing",
    "title": "Pricing",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#pricing",
    "content": "Find up-to-date pricing information at hume.ai/pricing.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "understanding-your-bill",
        "title": "Understanding your bill"
      },
      "h3": {
        "id": "pricing",
        "title": "Pricing"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-billing-methodology-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/billing",
    "pathname": "/docs/resources/billing",
    "title": "Billing methodology",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#billing-methodology",
    "content": "Audio and video:\nOur listed prices are presented per minute for ease of understanding.\n\nHowever, we bill these services on a corresponding per-second basis to ensure precise and fair charges. This means you are only billed for the exact amount of time your audio or video content is processed.\n\n\n\nImage and text:\nImage processing charges are incurred per image.\n\nText processing is billed based on the number of words processed.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "understanding-your-bill",
        "title": "Understanding your bill"
      },
      "h3": {
        "id": "billing-methodology",
        "title": "Billing methodology"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-faq-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/billing",
    "pathname": "/docs/resources/billing",
    "title": "FAQ",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#faq",
    "content": "After you use all your credits, there might be a delay before we switch you to a subscription or stop access, which can result in a small negative credit balance. This is normal and won't affect your subscription.\nIf you have questions about your bill or need assistance understanding the charges, please contact billing@hume.ai.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "faq",
        "title": "FAQ"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/errors",
    "pathname": "/docs/resources/errors",
    "title": "Errors",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-configuration-errors-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/errors",
    "pathname": "/docs/resources/errors",
    "title": "Configuration errors",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#configuration-errors",
    "content": "Configuration errors indicate that something about the API call was not configured correctly. The error message you get from the Hume APIs will often contain more information than we're able to provide on this page. For example if an audio file is too long, the error message from the API will specify the limit as well as the length of the audio received.\nCode Description \nE0100 The WebSocket request could not be parsed as valid JSON. The Hume API requires JSON serializable payloads. \nE0101 You may be missing or improperly formatting a required field. This generic error indicates that the structure of your WebSocket request was invalid. Please see the error message you received in the API response for more details. \nE0102 The requested model was incompatible with the file format received. Some models are not compatible with every file type. For example, no facial expressions will be detected in a text file. Audio can be extracted out of some video files, but if the video has no audio, then models like Speech Prosody and Vocal Burst will not be available. \nE0200 Media provided could not be parsed into a known file format. Hume APIs support a wide range of file formats and media types including audio, video, image, text, but not all formats are supported. If you receive this error and believe your file type should be supported please reach out to our support team. \nE0201 Media could not be decoded as a Base64 encoded string. The data field in the request payload should be Base64 encoded bytes. If you want to pass raw text without encoding it you can do so with the raw_text parameter. \nE0202 No audio signal could be inferred from the media provided. This error indicates that audio models were configured, but the media provided could not be parsed into a valid audio file. \nE0203 Your audio file was too long. The limit is 5000 milliseconds. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints. \nE0204 Your video file was too long. The limit is 5000 milliseconds. For best performance we recommend passing individual frames of video as images rather than full video files. For larger files, consider using the Hume Expression Measurement API REST endpoints. \nE0205 Your image file was too large. The limit is 3,000 x 3,000 pixels. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints. \nE0206 Your text file was too long. The limit is 10,000 characters. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints. \nE0207 The URL you've provided appears to be incorrect. Please verify that you've entered the correct URL and try submitting it again. If you're copying and pasting, ensure that the entire URL has been copied without any missing characters. \nE0300 You've run out of credits. Activate billing to continue making API calls. \nE0301 Your monthly credit limit has been reached. Once billing is activated, users can accrue charges up to a predetermined monthly cap. This limit ensures that users do not accumulate excessive debt without assurance of payment. If you require a higher limit, you may manually apply for a credit limit increase on the Usage page. Alternatively, the limit will reset at the beginning of the next month. For more information, please see our docs on billing. \nE0400 You've referenced a resource that doesn't exist in our system. Please check if the name or identifier you used is correct and try again. \nE0401 Your upload failed. Please ensure your file meets our format and size requirements, and attempt to upload it again. \nE0402 The CSV file you used to create or update a dataset is missing a header row. The header specifies what each column represents. Update your CSV file and retry your request. For more information about how to format your dataset CSV please see our tutorial on dataset creation. \nE0500 Your dataset doesn't meet the minimum sample size requirement. Please add more files to your dataset and resubmit your training job. For more information, please see our docs on dataset requirements. \nE0501 Your dataset contains a target column with empty values. Please clean your dataset so that all labels are valid categorical or numeric values and then resubmit your training job. For more information on target columns please see our docs on dataset requirements. \nE0502 Your dataset contains a target column with infinite values. Please clean your dataset so that all labels are valid categorical or numeric values and then resubmit your training job. For more information on target columns please see our tutorial on dataset creation. \nE0503 For classification tasks, your dataset must include at least two distinct classes. Please check your dataset has two unique labels in the target column. \nE0504 Some classes in your dataset don't have enough samples. To ensure that the model we produce is of the highest quality we require your dataset to be relatively balanced across classes. Please check the error message for which class should have more samples (or remove that class entirely). Please see our docs on dataset requirements for more details. \nE0505 The target column you've selected doesn't exist in the dataset. Please review the columns that exist in your dataset and select a valid column name. \nE0506 Your chosen target column is not a valid target column. Please ensure that you select a column with labels rather than the file_id column or another reserved column name. \nE0705 Your custom model was disconnected due to a server connection interruption. Please check your internet connection, ensure the server is still running, and verify that the server URL is correct. Also, make sure no firewall or security settings are blocking the connection. \nE0706 Hume's API cannot reach your custom language model. Please ensure that your language model is accessible and try again. \nE0707 The message sent to Hume is not formed in the correct way of either {\"type\": \"assistant_input\", \"text\": <your text here>} or {\"type\": \"assistant_end\"} \nE0708 The chat group you're trying to resume does not exist. Please check the chat group identifier and try again. \nE0709 The configuration you are trying to use does not exist. Please check the configuration identifier and try again. \nE0710 You are attempting to resume a chat group with a new configuration. This operation is not allowed. Please use the original configuration or create a new chat group with the desired configuration. \nE0711 You are attempting to use a supplemental language model that is not currently available as a Hume-managed LLM. Please provide an API key from your model provider, or switch to a different supplemental LLM. \nE0712 The custom language model timed out during the connection attempt. This could be due to network issues, server availability, or firewall restrictions. Please check your connection and try again. \nE0713 The connection failed to the custom model due to a fatal error during the connection attempt. Please verify that the custom language model is correctly configured and accessible. \nE0714 The EVI WebSocket connection was closed due to the user inactivity timeout being reached. This timeout is specified in the inactivity parameter within the timeouts field of your EVI configuration. \nE0715 The EVI WebSocket connection was closed due to the maximum duration timeout being reached. This timeout is specified in the max_duration parameter within the timeouts field of your EVI configuration. \nE0716 The session settings provided were invalid and therefore were not applied. More details about how to resolve the misconfiguration are available in the API response. \nE0717 The EVI WebSocket connection was closed because a request was made to resume a chat group which contains an active chat. Please check that you are not already running an active chat session with the same chat group. \nE0718 The supplemental LLM provider has degraded API behavior. You can try again later or change the supplemental LLM in your EVI configuration. \nE0719 The supplemental LLM provider has an outage. You can try again later or change the supplemental LLM in your EVI configuration. \nE0720 The chat group configured for chat resumability could not be found. Please check that you specified your resumed_chat_group_id parameter correctly and that data retention is enabled in your account settings. \n\n\n\nThe connection will be closed automatically after ten identical configuration\nerrors to avoid unintended looping.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "configuration-errors",
        "title": "Configuration errors"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-websocket-status-codes-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/errors",
    "pathname": "/docs/resources/errors",
    "title": "WebSocket status codes",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#websocket-status-codes",
    "content": "Code Description \n1000 close_normal indicates an expected, intentional disconnect initiated by the server, such as when the built-in hang-up tool closes the connection. This code is also used for inactivity timeout and max duration timeout, indicating that the WebSocket connection was closed due to remaining inactive for too long or exceeding the maximum allowed duration. \n1008 policy_violation occurs when the WebSocket connection encounters an issue that cannot be recovered due to user error. Please review your request and ensure it adheres to the API's guidelines and policies. \n1011 server_error indicates that the WebSocket connection encountered an issue that cannot be recovered due to an internal Hume server error. Please try again later or contact support if the issue persists.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "websocket-status-codes",
        "title": "WebSocket status codes"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-service-errors-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/errors",
    "pathname": "/docs/resources/errors",
    "title": "Service errors",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#service-errors",
    "content": "If you encounter an error code starting with I (for example, error code I0100), it indicates an outage or a bug in a Hume service. Our team will already have been alerted of the internal error, but if you need immediate assistance please reach out to our support team.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "service-errors",
        "title": "Service errors"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-warnings-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/errors",
    "pathname": "/docs/resources/errors",
    "title": "Warnings",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#warnings",
    "content": "Warnings indicate that the payload was configured correctly, but no results could be returned.\nCode Description \nW0101 No vocal bursts could be detected in the media. \nW0102 No face meshes could be detected in the media. \nW0103 No faces could be detected in the media. \nW0104 No emotional language could be detected in the media. \nW0105 No speech could be detected in the media. \nW0106 No dynamic variable(s) found matching the one(s) specified.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "warnings",
        "title": "Warnings"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-common-errors-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/errors",
    "pathname": "/docs/resources/errors",
    "title": "Common errors",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#common-errors",
    "content": "Some errors will not have an associated error code, but are documented here.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "common-errors",
        "title": "Common errors"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-transcript-confidence-below-threshold-value-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/errors",
    "pathname": "/docs/resources/errors",
    "title": "Transcript confidence below threshold value",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#transcript-confidence-below-threshold-value",
    "content": "This error indicates that our transcription service had difficulty identifying the language spoken in your audio file or the quality was too low. We prioritize quality and accuracy, so if it cannot transcribe with confidence, our models won't be able to process it further.\nBy default, we use an automated language detection method for our Speech Prosody, Language, and NER models. However, if you know what language is being spoken in your media samples, you can specify it via its BCP-47 tag and potentially obtain more accurate results.\nIf you see the message above there are few steps you can do to resolve the issue:\nVerify we support the language\n\nEnsure you are providing clear, high-quality audio files.\n\nSpecify the language within your request if you know the language in the audio.\n\n\n\n\n\n\n\n\n\n\nSee the full list of languages supported by the Expression Measurement API here.\nYou may specify any of the following BCP-47 tags for transcription: zh, da, nl, en, en-AU, en-IN, en-NZ,\nen-GB, fr, fr-CA, de, hi, hi-Latn, id, it, ja, ko, no, pl, pt, pt-BR, pt-PT,\nru, es, es-419, sv, ta, tr, or uk.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.batch import Prosody, Transcription, Models\nfrom hume.expression_measurement.batch.types import InferenceBaseRequest\n\nasync def main():\n    # Initialize an authenticated client\n    client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\n    # Define the filepath(s) of the file(s) you would like to analyze\n    local_filepaths = [\n        open(\"<YOUR_FILE_PATH>\", mode=\"rb\"),\n    ]\n\n    # Create a default configuration for the prosody model\n    prosody_config = Prosody()\n\n    # Create a transcription coniguration with the language set to English\n    transcription_config = Transcription(language=\"en\")\n\n    # Create a Models object\n    models_chosen = Models(prosody=prosody_config)\n    \n    # Create a stringified object containing the configuration\n    stringified_configs = InferenceBaseRequest(models=models_chosen, transcription=transcription_config)\n\n    # Start an inference job and print the job_id\n    job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(\n        json=stringified_configs, file=local_filepaths\n    )\n    print(job_id)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import AsyncHumeClient\nfrom hume.expression_measurement.batch import Prosody, Transcription, Models\nfrom hume.expression_measurement.batch.types import InferenceBaseRequest\n\nasync def main():\n    # Initialize an authenticated client\n    client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n\n    # Define the filepath(s) of the file(s) you would like to analyze\n    local_filepaths = [\n        open(\"<YOUR_FILE_PATH>\", mode=\"rb\"),\n    ]\n\n    # Create a default configuration for the prosody model\n    prosody_config = Prosody()\n\n    # Create a transcription coniguration with the language set to English\n    transcription_config = Transcription(language=\"en\")\n\n    # Create a Models object\n    models_chosen = Models(prosody=prosody_config)\n    \n    # Create a stringified object containing the configuration\n    stringified_configs = InferenceBaseRequest(models=models_chosen, transcription=transcription_config)\n\n    # Start an inference job and print the job_id\n    job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(\n        json=stringified_configs, file=local_filepaths\n    )\n    print(job_id)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())"
      },
      {
        "lang": "json",
        "code": "\"transcription\": {\n    \"language\": \"en\"\n}"
      },
      {
        "lang": "json",
        "code": "\"transcription\": {\n    \"language\": \"en\"\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "common-errors",
        "title": "Common errors"
      },
      "h3": {
        "id": "transcript-confidence-below-threshold-value",
        "title": "Transcript confidence below threshold value"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/science",
    "pathname": "/docs/resources/science",
    "title": "About the Science",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown",
    "content": "What is it about speaking in person that allows us to understand each other so much more accurately than text alone? It isn’t what we say—it’s the way we say it. Science consistently demonstrates that expressions convey important information that is vital for social interaction and forms the building blocks of empathy.\nThat being said, expressions aren’t direct windows into the human mind. Measuring and interpreting expressive behavior is a complex and nuanced task that is the subject of ongoing scientific research.\nThe scientists at Hume AI have run some of the largest-ever psychology studies to better understand how humans express themselves. By investigating expressions around the world and what they mean to the people making them, we’ve mapped out the nuances of expression in the voice, language, and face in unprecedented detail. We’ve published this research in the world’s leading scientific journals and, for the first time, translated it into cutting-edge machine learning models.\nThese models, shaped by a new understanding of human expression, include:\nFacial Expression\n\nSpeech Prosody\n\nVocal Bursts\n\nEmotional Language"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-facial-expression-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/science",
    "pathname": "/docs/resources/science",
    "title": "Facial Expression",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#facial-expression",
    "content": "Facial expression is the most well-studied modality of expressive behavior, but the overwhelming focus has been on six discrete categories of facial movement or time-consuming manual annotations of facial movements (the scientifically useful, but outdated, Facial Action Coding System). Our research shows that these approaches capture less than 30% of what typical facial expressions convey.\nHume’s Facial Emotional Expression model generates 48 outputs encompassing the dimensions of emotional meaning people reliably attribute to facial expressions. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nHume’s FACS 2.0 model is a new generation automated facial action coding system (FACS). With 55 outputs encompassing 26 traditional actions units (AUs) and 29 other descriptive features (e.g., smile, scowl), FACS 2.0 is even more comprehensive than manual FACS annotations.\nOur facial expression models are packaged with face detection and work on both images and videos.\nIn addition to our image-based facial expression models, we also offer an Anonymized Facemesh model for applications in which it is essential to keep personally identifiable data on-device (e.g., for compliance with local laws). Instead of face images, our facemesh model processes facial landmarks detected using Google's MediaPipe library. It achieves about 80% accuracy relative to our image-based model.\nTo read more about the team’s research on facial expressions, check out our publications in American Psychologist (2018), Nature (2021), and iScience (2024).",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities"
      },
      "h3": {
        "id": "facial-expression",
        "title": "Facial Expression"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-speech-prosody-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/science",
    "pathname": "/docs/resources/science",
    "title": "Speech Prosody",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#speech-prosody",
    "content": "Speech prosody is not about the words you say, but the way you say them. It is distinct from language (words) and from non-linguistic vocal utterances.\nOur Speech Prosody model generates 48 outputs encompassing the 48 dimensions of emotional meaning that people reliably distinguish from variations in speech prosody. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Speech Prosody model is packaged with speech detection and works on both audio files and videos.\nTo read more about the team’s research on speech prosody, check out our publications in Nature Human Behaviour (2019) and Proceedings of the 31st ACM International Conference on Multimedia (2023).",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities"
      },
      "h3": {
        "id": "speech-prosody",
        "title": "Speech Prosody"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-vocal-bursts-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/science",
    "pathname": "/docs/resources/science",
    "title": "Vocal Bursts",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#vocal-bursts",
    "content": "Non-linguistic vocal utterances, including sighs, laughs, oohs, ahhs, umms, and shrieks (to name but a few), are a particularly powerful and understudied modality of expressive behavior. Recent studies reveal that they reliably convey distinct emotional meanings that are extremely well-preserved across most cultures.\nNon-linguistic vocal utterances have different acoustic characteristics than speech emotional intonation (prosody) and need to be modeled separately.\nOur Vocal Burst Expression model generates 48 outputs encompassing the distinct dimensions of emotional meaning that people distinguish in vocal bursts. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Vocal Burst Description model provides a more descriptive and categorical view of nonverbal vocal expressions (“gasp,” “mhm,” etc.) intended for use cases such as audio captioning. It generates 67 descriptors, including 30 call types (“sigh,” “laugh,” “shriek,” etc.) and 37 common onomatopoeia transliterations of vocal bursts (“hmm,” “ha,” “mhm,” etc.).\nOur vocal burst models are packaged with non-linguistic vocal utterance detection and works on both audio files and videos.\nTo read more about the team’s research on vocal bursts, check out our publications in American Psychologist (2019), Interspeech 2022, ICASSP 2023, and Nature Human Behaviour (2023).",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities"
      },
      "h3": {
        "id": "vocal-bursts",
        "title": "Vocal Bursts"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-emotional-language-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/science",
    "pathname": "/docs/resources/science",
    "title": "Emotional Language",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#emotional-language",
    "content": "The words we say include explicit disclosures of emotion and implicit emotional connotations. These meanings are complex and high-dimensional.\nFrom written or spoken words, our Emotional Language model generates 53 outputs encompassing different dimensions of emotion that people often perceive from language. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Emotional Language model is packaged with speech transcription and works on audio files, videos, and text.\nOur Named Entity Recognition (NER) model can also identify topics or entities (people, places, organizations, etc.) mentioned in speech or text and the tone of language they are associated with, as identified by our emotional language model.",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities"
      },
      "h3": {
        "id": "emotional-language",
        "title": "Emotional Language"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-published-research-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/science",
    "pathname": "/docs/resources/science",
    "title": "Published Research",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#published-research",
    "content": "You can access a comprehensive list of our published research papers along with PDFs for download here.",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "published-research",
        "title": "Published Research"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.use-case-guidelines",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/use-case-guidelines",
    "pathname": "/docs/resources/use-case-guidelines",
    "title": "Use case guidelines",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.use-case-guidelines-ethical-guidelines-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/use-case-guidelines",
    "pathname": "/docs/resources/use-case-guidelines",
    "title": "Ethical guidelines",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#ethical-guidelines",
    "content": "Understanding expressive communication is essential to building technologies that address our needs and improve our well-being. But technologies that recognize language and nonverbal behavior can also pose risks. That’s why we require that all commercial applications incorporating our APIs adhere to the ethical guidelines of The Hume Initiative.",
    "hierarchy": {
      "h0": {
        "title": "Use case guidelines"
      },
      "h2": {
        "id": "ethical-guidelines",
        "title": "Ethical guidelines"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.use-case-guidelines-scientific-best-practices-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/use-case-guidelines",
    "pathname": "/docs/resources/use-case-guidelines",
    "title": "Scientific best practices",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#scientific-best-practices",
    "content": "Use inductive methods to identify the expressive signals that matter for your application. Even if you are interested in a specific emotion like “anger,” how that emotion is expressed depends on setting: anger on a football field sounds different than anger on a customer service call. Our models succinctly compress the representation of emotional expression so that, even with limited data, you can examine how their outputs can be used in your specific research or application setting. You can do this by using statistical methods like regression or classification, or by examining the distribution of expressions in your data using our Playground.\n\nNever assume a one-to-one mapping between emotional experience and expression. The outputs of our models should be treated as measurements of complex expressive behavior. We provide labels to our outputs indicating what these dimensions of expression are often reported to mean, but these labels should not be interpreted as direct inferences of how someone is feeling at any given time. Rather, “a full understanding of emotional expression and experience requires an appreciation of a wide degree of variability in display behavior, subjective experience, patterns of appraisal, and physiological response, both within and across emotion categories” (Cowen et al., 2019).\n\nNever overlook the nuances in emotional expression. For instance, avoid the temptation to focus on just the top label. We provide interactive visualizations in our Playground to help you map out complex patterns in real-life emotional behavior. These visualizations are informed by recent advances in emotion science, departing from reductive models that long “anchored the science of emotion to a predominant focus on prototypical facial expressions of the “basic six”: anger, disgust, fear, sadness, surprise, and happiness,” and embracing how “new discoveries reveal that the two most commonly studied models of emotion—the basic six and the affective circumplex (comprising valence and arousal)—each capture at most 30% of the variance in the emotional experiences people reliably report and in the distinct expressions people reliably recognize.” (Cowen et al., 2019)\n\nAccount for culture-specific meanings and display tendencies. Studies have routinely observed subtle cultural differences in the meaning of expressions as well as broader “variations in the frequency and intensity with which different expressions were displayed” (Cowen et al., 2022). Given these differences, empathic AI applications should be tested in each population in which they are deployed and fine-tuned when necessary.\nRead about the science behind our models if you’d like to delve deeper into how they work.",
    "hierarchy": {
      "h0": {
        "title": "Use case guidelines"
      },
      "h2": {
        "id": "scientific-best-practices",
        "title": "Scientific best practices"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/privacy",
    "pathname": "/docs/resources/privacy",
    "title": "Privacy",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy-privacy-policy-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/privacy",
    "pathname": "/docs/resources/privacy",
    "title": "Privacy Policy",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#privacy-policy",
    "content": "Our Privacy Policy governs how we collect and use personal information submitted to our products.",
    "hierarchy": {
      "h0": {
        "title": "Privacy"
      },
      "h2": {
        "id": "privacy-policy",
        "title": "Privacy Policy"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy-zero-data-retention-and-data-usage-options-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/privacy",
    "pathname": "/docs/resources/privacy",
    "title": "Zero Data Retention and Data Usage Options",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#zero-data-retention-and-data-usage-options",
    "content": "Hume AI is HIPAA compliant, with features to enhance user privacy and data control. Our portal currently supports enabling/disabling these features in the user's profile page.\nZero Data Retention: This feature allows users to turn off the storage of all chat histories (transcripts) or voice recordings for the EVI API. Other metadata such as API usage information will still be stored.\n\nOpt-Out of Data Being Used for Training: By default, anonymized data from user interactions with the EVI API is used to improve our models. Users can toggle this option to prevent their data from being used for training purposes.\n\n\nFor added control, use a custom language model and obtain a Business Associate Agreement (BAA) directly with the model provider. To request a BAA and/or Data Processing Addendum (DPA) with Hume, please contact legal@hume.ai.\n\n\nBy default, data retention is enabled, and user data may be used for model training. Users must explicitly opt out to disable these features.",
    "hierarchy": {
      "h0": {
        "title": "Privacy"
      },
      "h2": {
        "id": "privacy-policy",
        "title": "Privacy Policy"
      },
      "h3": {
        "id": "zero-data-retention-and-data-usage-options",
        "title": "Zero Data Retention and Data Usage Options"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy-to-enable-or-disable-these-options-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/privacy",
    "pathname": "/docs/resources/privacy",
    "title": "To enable or disable these options",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#to-enable-or-disable-these-options",
    "content": "Log into your Hume AI account.\n\nNavigate to your Profile page by clicking on the profile icon on the sidebar.\n\nScroll down to the Privacy section where you will see the options for \"Do not retain data\" and \"Do not use for training.\"\n\nToggle the switches next to these options to enable or disable them according to your preference.\n\nClick on 'Save changes' to apply your settings.\n\n\n\n\n\n\n\n\nOpting out of data retention will disable certain features, including the ability to resume chats and access your chat history.",
    "hierarchy": {
      "h0": {
        "title": "Privacy"
      },
      "h2": {
        "id": "privacy-policy",
        "title": "Privacy Policy"
      },
      "h3": {
        "id": "zero-data-retention-and-data-usage-options",
        "title": "Zero Data Retention and Data Usage Options"
      },
      "h4": {
        "id": "to-enable-or-disable-these-options",
        "title": "To enable or disable these options"
      }
    },
    "level": "h4"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy-api-data-usage-policy-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/privacy",
    "pathname": "/docs/resources/privacy",
    "title": "API Data Usage Policy",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#api-data-usage-policy",
    "content": "Our API Data Usage Policy details how and when we store API data.",
    "hierarchy": {
      "h0": {
        "title": "Privacy"
      },
      "h2": {
        "id": "api-data-usage-policy",
        "title": "API Data Usage Policy"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy-consumer-services-faq-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/docs/resources/privacy",
    "pathname": "/docs/resources/privacy",
    "title": "Consumer Services FAQ",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation",
      "pathname": "/docs"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#consumer-services-faq",
    "content": "Our Consumer Services FAQ explains how and when we store data processed by our frontend applications like our Playground.\n\n\n\n\nFor non-API consumer products like our Playground and Demo, we may use content such as images, video files, audio files, and text files to improve our services. You can opt out of having your content used to improve our services at any time by adjusting your settings in your profile page. This opt-out will apply on a going-forward basis only.\nPlease note that for our API product, Hume AI will not use data submitted by customers via our API to train or improve our models.\n\n\nYou can delete your account by submitting a user account deletion request in your profile page on the Hume playground. Once you submit your deletion request, we will delete your account within 30 days. Please note that for security reasons, once you delete your account, you may not re-sign up for an account with the same email address.\n\n\nWe share content with a select group of trusted service providers that help us provide our services. We share the minimum amount of content we need in order to accomplish this purpose and our service providers are subject to strict confidentiality and security obligations. Please see our Privacy Policy for more information on who we may share your content with.\n\n\nContent is stored on Hume AI systems and our trusted service providers' systems in the US and around the world.\n\n\nA limited number of authorized Hume AI personnel, may view and access user content only as needed for these reasons: (1) investigating abuse or a security incident; (2) to provide support to you if you reach out to us with questions about your account; (3) or to comply with legal obligations. Access to content is subject to technical access controls and limited only to authorized personnel on a need-to-know basis. Additionally, we monitor and log all access to user content and authorized personnel must undergo security and privacy training prior to accessing any user content.\n\n\nNo. We do not sell your data or share your content with third parties for marketing purposes.\n\n\nPlease change your privacy settings through the Profile page. For further assistance, message the moderators on our Discord Server.",
    "hierarchy": {
      "h0": {
        "title": "Privacy"
      },
      "h2": {
        "id": "consumer-services-faq",
        "title": "Consumer Services FAQ"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Changelog",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-10-25-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-10-25-2024",
    "content": "Added support for claude-3.5-sonnet-latest (currently points to claude-3-5-sonnet-20241022) and made this model the recommended supplemental LLM\n\nAdded support for tool use with Gemini models (gemini-1.5-pro and gemini-1.5-flash)",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "10-25-2024",
        "title": "October 25, 2024"
      },
      "h3": {
        "id": "evi-api-additions-10-25-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-10-25-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-10-25-2024",
    "content": "Fixed a bug where context was incorrectly set as persistent and added to every user messages, despite being specified as type: temporary",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "10-25-2024",
        "title": "October 25, 2024"
      },
      "h3": {
        "id": "bugs-bashed-10-25-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-10-11-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-10-11-2024",
    "content": "Added a new base voice, Sunny, featuring a male voice with an Indian accent\n\nImproved the reliability of the experimental custom voice creation feature by reducing hallucinations, and added 11 new adjustable parameters - articulation, buoyancy, enthusiasm, nasality, smoothness, tightness, assertiveness, confidence, gender, relaxedness, tepidity",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "10-11-2024",
        "title": "October 11, 2024"
      },
      "h3": {
        "id": "evi-api-additions-10-11-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-10-11-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-10-11-2024",
    "content": "Added a more informative error message for when Google Gemini models are overloaded, returning an E0718 error code instead of silently dropping the connection\n\nImplemented Anthropic prompt caching to reduce latency with Claude 3 models, especially for longer prompts and conversations",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "10-11-2024",
        "title": "October 11, 2024"
      },
      "h3": {
        "id": "evi-api-changes-10-11-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-10-11-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-10-11-2024",
    "content": "Reduced the frequency of all hallucinations when using EVI 2\n\nPrevented voice hallucinations when EVI 2 outputs less common text formats, including numbered lists, emails, hashtags, very short messages, and numbers",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "10-11-2024",
        "title": "October 11, 2024"
      },
      "h3": {
        "id": "bugs-bashed-10-11-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-09-27-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-09-27-2024",
    "content": "Upgraded gemini-1.5-pro and gemini-1.5-flash models to use the latest versions, gemini-1.5-pro-002 and gemini-1.5-flash-002\n\nImproved audio quality for EVI phone calling",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "09-27-2024",
        "title": "September 27, 2024"
      },
      "h3": {
        "id": "evi-api-changes-09-27-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-09-27-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-09-27-2024",
    "content": "Fixed an issue with the EVI WebSocket auto-reconnecting after timeouts, by updating the inactivity timeout socket close code from 1001 to 1000\n\nFixed a bug where the GET /chat_groups/{id} endpoint would return all chats, not just the chats in the chat_group",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "09-27-2024",
        "title": "September 27, 2024"
      },
      "h3": {
        "id": "bugs-bashed-09-27-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-changes-09-20-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-changes-09-20-2024",
    "content": "Added support for resuming chats with supplemental LLMs for EVI 2\n\nUpdated the DACHER base voice, making it significantly higher quality and more reliable\n\nImproved EVI's ability to recover from accidental interruptions. Previously, if EVI was interrupted by non-speech sounds, EVI would stop and wait for further input. EVI will now continue speaking after these interruptions",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "09-20-2024",
        "title": "September 20, 2024"
      },
      "h3": {
        "id": "evi-changes-09-20-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-09-20-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-09-20-2024",
    "content": "Fixed an issue where @ signs would be removed in emails, leading to incorrect pronunciation; now they will be replaced with \"at\" and pronounced correctly\n\nFixed a bug with numbered lists, leading to lists being split into new lines and spoken incorrectly",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "09-20-2024",
        "title": "September 20, 2024"
      },
      "h3": {
        "id": "bugs-bashed-09-20-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-09-13-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-09-13-2024",
    "content": "Released the EVI 2 API, with major improvements to the core EVI experience. Developers can try it now: EVI 2 docs\n\nIntroduced an experimental feature for creating custom voices through adjustable sliders: Custom voices",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "09-13-2024",
        "title": "September 13, 2024"
      },
      "h3": {
        "id": "evi-api-additions-09-13-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-09-13-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-09-13-2024",
    "content": "Improved text validation permissiveness for config names and descriptions, allowing a wider range of printable characters\n\nAdded a new error code (E0720) to handle scenarios where data retention is off so chat group history is unavailable, providing a more informative error message before closing the session",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "09-13-2024",
        "title": "September 13, 2024"
      },
      "h3": {
        "id": "evi-api-changes-09-13-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-09-13-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-09-13-2024",
    "content": "Fixed an issue to ensure chat_id is passed to users when using custom language models and phone calling together, enabling developers to retrieve post-call details with these features\n\nFixed a bug where resumed chat groups would use the first rather than the most recent config in the chat group when starting a new chat",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "09-13-2024",
        "title": "September 13, 2024"
      },
      "h3": {
        "id": "bugs-bashed-09-13-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-08-08-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-08-08-2024",
    "content": "Enabled resuming previous chats with a new config. Previously, it was not possible to resume chats in a chat group with different configs. This change allows developers to change the prompt, voice, and other options in their config, while still retaining the context in their chat history: Chat resumability\n\nIntroduced the new E0717 error type, which will occur when a developer tries resuming a chat when one of the chats in its chat_group is already active.\n\nAdded two new errors for issues with supplemental language model providers. If a provider is overloaded, EVI will return E0718, and if a provider has unexpected internal errors EVI will return E0719. If these errors occur, developers can try again later or change their configurations to use a different LM provider.",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "08-08-2024",
        "title": "August 8, 2024"
      },
      "h3": {
        "id": "evi-api-changes-08-08-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-08-02-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-08-02-2024",
    "content": "Added support for new language models with the Groq provider: llama-3.1-70b-versatile and llama-3.1-8b-instant\n\nAdded support for new language models with the Fireworks provider: accounts/fireworks/models/llama-v3p1-405b-instruct, accounts/fireworks/models/llama-v3p1-70b-instruct, and accounts/fireworks/models/llama-v3p1-8b-instruct\n\nAdded a hang_up built in tool to allow EVI to end calls. To use this, developers can include the hang_up tool in the builtin_tools object when creating a config, and provide instructions on when EVI should end the call in the prompt",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "08-02-2024",
        "title": "August 2, 2024"
      },
      "h3": {
        "id": "evi-api-additions-08-02-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-08-02-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-08-02-2024",
    "content": "Added the ability to create a prompt during config creation. The new prompt object in the config creation request has three nullable fields (id, version, and text). Providing only text in the prompt field when creating a new config will create a new prompt\n\nDropped support for the older Llama 3 70B Instruct model from Fireworks (accounts/fireworks/models/llama-v3-70b-instruct), as it is replaced by the new Llama 3.1 70B model (accounts/fireworks/models/llama-v3p1-70b-instruct)",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "08-02-2024",
        "title": "August 2, 2024"
      },
      "h3": {
        "id": "evi-api-changes-08-02-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-07-26-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-07-26-2024",
    "content": "Invalid SessionSettings payloads now return an E0716 error. Invalid payloads include empty system prompts, duplicate tool names, removing previously enabled tools, and overlapping builtin and custom tool names. If an update is invalid, the error message will explain why, and the SessionSettings will not be applied",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "07-26-2024",
        "title": "July 26, 2024"
      },
      "h3": {
        "id": "evi-api-changes-07-26-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-07-18-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-07-18-2024",
    "content": "Added the on_inactivity_timeout configuration option, allowing EVI to speak a message when the user is inactive for some period of time: Inactivity timeout message\n\nAdded the on_max_duration_timeout configuration option, allowing EVI to speak a message when the maximum chat duration is reached: Max duration timeout message\n\nAdded support for the gpt-4o-mini language model",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "07-18-2024",
        "title": "July 18, 2024"
      },
      "h3": {
        "id": "evi-api-additions-07-18-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-07-18-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-07-18-2024",
    "content": "Updated the Hume Typescript SDK, with detailed changes and a migration guide in the release notes for version 0.8.2",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "07-18-2024",
        "title": "July 18, 2024"
      },
      "h3": {
        "id": "evi-api-changes-07-18-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-07-12-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-07-12-2024",
    "content": "Added dynamic variables, allowing developers to define variables in SessionSettings and reference their values in the system prompt (e.g., {{variable_name}}): Dynamic variables\n\nAdded support for the Google language model provider and the gemini-1.5-pro and gemini-1.5-flash language models\n\nAdded EVI configuration options to set timeouts for user inactivity (inactivity) and maximum session duration (max_duration): Timeouts\n\nAdded support for retrieving the phone numbers of inbound callers, using the metadata.twilio.caller_number property of the evi/chats/:id endpoint: List chat events\n\nAdded the /v0/evi/language-models API endpoint to retrieve the language models supported by EVI and the built-in tools available for each model",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "07-12-2024",
        "title": "July 12, 2024"
      },
      "h3": {
        "id": "evi-api-additions-07-12-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-07-05-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-07-05-2024",
    "content": "Added a config_id filter option for the GET /chat_groups endpoint, allowing developers to limit paginated results to chat groups associated with a specific config ID\n\nAdded a name filter option for the GET /configs, GET /tools, and GET /prompts endpoints. These allow developers to limit paginated results to only include objects with a specific name\n\nIntroduced data storage options for the EVI API. The \"do not retain data\" option disables storage of chat histories and voice recordings for EVI sessions. The \"do not use for training\" opts out of Hume using anonymized data from EVI sessions for model improvements. Developers can toggle these options from the profile page in the Hume portal\n\nAdded more descriptive error messages for transcription-related errors",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "07-05-2024",
        "title": "July 5, 2024"
      },
      "h3": {
        "id": "evi-api-additions-07-05-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-06-28-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-06-28-2024",
    "content": "Added a request_id field to ChatMetadata to uniquely identify sessions\n\nAdded an on_new_chat configuration option. Set event_messages.on_new_chat.enabled to true to have EVI speak first in the conversation. To control the exact text of that first message, also set event_messages.on_new_chat.text\n\nAdded an allow_short_responses configuration option, which allows developers to turn off short responses generated by Hume's empathic large language model (eLLM). To disable these responses, set ellm_model.allow_short_responses to false",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "06-28-2024",
        "title": "June 28, 2024"
      },
      "h3": {
        "id": "evi-api-additions-06-28-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-06-21-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-06-21-2024",
    "content": "Added support for Claude 3.5 Sonnet (claude-3-5-sonnet-20240620) to the EVI API",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "06-21-2024",
        "title": "June 21, 2024"
      },
      "h3": {
        "id": "evi-api-additions-06-21-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-06-21-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-06-21-2024",
    "content": "Changed the default language model for the EVI API to Claude 3.5 Sonnet\n\nChanged the default voice for the EVI API to Ito\n\nChanged requirements to allow tool use if no language model is specified, allowing tool use when using the default LLM",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "06-21-2024",
        "title": "June 21, 2024"
      },
      "h3": {
        "id": "evi-api-changes-06-21-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-06-21-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-06-21-2024",
    "content": "Fixed a bug where sending an AssistantInput message at the beginning of an EVI chat configured with Anthropic models would result in an error\n\nFixed a bug with chat resumability where previous chat group events were not being included in the LLM chat history, and EVI would forget details from before the chat was resumed",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "06-21-2024",
        "title": "June 21, 2024"
      },
      "h3": {
        "id": "bugs-bashed-06-21-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-06-07-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-06-07-2024",
    "content": "Added a total_pages field to all paginated EVI REST endpoints",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "06-07-2024",
        "title": "June 7, 2024"
      },
      "h3": {
        "id": "evi-api-additions-06-07-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-06-07-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-06-07-2024",
    "content": "EVI REST endpoints will now return the 201 status code instead of the 200 status code when creating new entities including new configs, chat groups, prompts, and tools\n\nEVI REST endpoints will now return the 404 status code if referencing a config, chat, prompt, or tool that doesn't exist. If an invalid page number exceeding the total number of pages is specified, the endpoint will return an empty list rather than a 404 status code\n\nAdded more detailed error messages for Custom Language Model. If the connection between Hume's API and a developers's language model times out, we will now send an E0712:custom_language_model_timed_out error. If the connection fails, we will send an E0713:custom_language_model_connection_failed error",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "06-07-2024",
        "title": "June 7, 2024"
      },
      "h3": {
        "id": "evi-api-changes-06-07-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-05-31-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-05-31-2024",
    "content": "Added chat resumability, allowing developers to resume previous chats with EVI by specifying a chat group ID in the resumed_chat_group_id query parameter: Chat resumability\n\nAdded the api.hume.ai/v0/evi/chat_groups endpoint to support listing chat groups or listing events from a specific chat group: Chat groups endpoint\n\nAdded the ChatMetadata output message, which includes a chat_id to identify each individual chat with EVI and a chat_group_id to support resumability and group resumed chats together: ChatMetadata\n\nAdded support for chat resumability to the Hume Python SDK: Release notes for version 0.6.0\n\nAdded support for chat resumability and pause/resume messages to the Hume TypeScript SDK: Release notes for version 0.1.6",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-31-2024",
        "title": "May 31, 2024"
      },
      "h3": {
        "id": "evi-api-additions-05-31-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-05-31-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-05-31-2024",
    "content": "Added more detailed error messages for Custom Language Model. If Hume's API cannot reach a developers's language model, we will now send an E0706: custom_language_model_unreachable error to the developer\n\nAdded error messages for chat resumability - E0710: resuming_chat_group_with_new_config when a developer attempts to resume a chat group with a new config, E0708: chat_group_not_found when a chat group does not exist, and E0709: config_not_found when a config does not exist\n\nAdded an error message for unavailable EVI supplemental LLMs. While supplemental LLMs can always be enabled by passing an API for a 3rd party LLM service, if EVI is configured with an LLM that is not currently available as a Hume-managed LLM, we will send an E0711: language_model_unavailable error",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-31-2024",
        "title": "May 31, 2024"
      },
      "h3": {
        "id": "evi-api-changes-05-31-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-05-24-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-05-24-2024",
    "content": "Added support for streaming custom language model responses in parts. Developers can send text chunks to start generating audio responses much faster\nThe Custom Language Model endpoint now expects text to be formatted in the following payload:\n\nAdded support for pausing and resuming EVI responses with with pause_assistant_message and resume_assistant_message. Sending a pause message stops EVI from generating and speaking Assistant messages. Sending a resume message allows EVI to continue responding to the User messages",
    "code_snippets": [
      {
        "code": "# send this to add text\n{\"type\": \"assistant_input\", \"text\": \"<chunk>\"}\n\n# send this message when you're done speaking\n{\"type\": \"assistant_end\"}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-24-2024",
        "title": "May 24, 2024"
      },
      "h3": {
        "id": "evi-api-additions-05-24-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-05-24-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-05-24-2024",
    "content": "Increased the limit for tool descriptions from 100 chars to 512 chars\n\nSet the maximum length for tool_name to 64 chars",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-24-2024",
        "title": "May 24, 2024"
      },
      "h3": {
        "id": "evi-api-changes-05-24-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-05-17-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-05-17-2024",
    "content": "Added support for built-in tools, starting with web search: Using built-in tools\n\nAdded support for phone calling through a Twilio integration: Phone calling\n\nAdded DACHER voice to the voice configuration options\n\nAdded support for the gpt-4o language model",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-17-2024",
        "title": "May 17, 2024"
      },
      "h3": {
        "id": "evi-api-additions-05-17-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-05-17-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-05-17-2024",
    "content": "Increased the limit for tool descriptions from 100 chars to 512 chars",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-17-2024",
        "title": "May 17, 2024"
      },
      "h3": {
        "id": "evi-api-changes-05-17-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-05-10-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-05-10-2024",
    "content": "Added support for three open-source models through the Groq language model provider: Gemma 7B (gemma-7b-it), Llama 3 8B (llama3-8b-8192), and Llama 3 70B (llama3-70b-8192)\n\nAdded support for Llama 30 70B language model through the Fireworks language model provider (accounts/fireworks/models/llama-v3-70b-instruct)\n\nAdded a custom_session_id field in the SessionSettings message, and documentation for using it: Custom Session ID",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024"
      },
      "h3": {
        "id": "evi-api-additions-05-10-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-05-10-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-05-10-2024",
    "content": "Disabled short response generation for custom language models\n\nAdded error codes for when Hume credits run out while using EVI. Users will receive either the E0300 error code if they are out of credits or E0301 if they are blocked via subscription. The WebSocket connection will also be closed with code 1008",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024"
      },
      "h3": {
        "id": "evi-api-changes-05-10-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-05-10-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-05-10-2024",
    "content": "Fixed an issue with the from_text field in UserMessage. It is now set to True if any part of the UserMessage is from a developer-provided UserInput message",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024"
      },
      "h3": {
        "id": "bugs-bashed-05-10-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-05-03-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-05-03-2024",
    "content": "Added support for Tools through our tool use feature\n\nAdded ToolErrorMessage as a supported input type",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-03-2024",
        "title": "May 3, 2024"
      },
      "h3": {
        "id": "evi-api-additions-05-03-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-05-03-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-05-03-2024",
    "content": "Added an error that returns status 400 if a Config, Tool, or Prompt is created with a name or versionDescription that's too long or non-ASCII. Names must be under 75 chars, versionDescription must be under 256 chars, description for Tools must be under 100 chars, fallback_content for Tools must be under 2048 chars, and model_resource for LanguageModels must be under 1024 chars\n\nFixed several edge cases and bugs involving Tool calls, including supporting only single tool calls with EVI (no parallel tool calling)",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-03-2024",
        "title": "May 3, 2024"
      },
      "h3": {
        "id": "bugs-bashed-05-03-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-04-30-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-additions-04-30-2024",
    "content": "Added support for reading language model type from EVI configs\n\nAdded support for reading language model temperature from EVI configs\n\nAdded system prompt to SessionSettings message to allow dynamic prompt updating",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024"
      },
      "h3": {
        "id": "evi-api-additions-04-30-2024",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes-04-30-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#evi-api-changes-04-30-2024",
    "content": "Renamed TextInput message to UserInput to indicate this is text to be added to the chat history as a User message and used as context by the LLM\n\nRenamed TtsInput message to AssistantInput to make it clear that this is input text to be spoken by EVI and added to the chat history as an Assistant message\n\nMoved audio configuration options to SessionSettings message",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024"
      },
      "h3": {
        "id": "evi-api-changes-04-30-2024",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-04-30-2024-chunk:0",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/changelog",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog",
      "pathname": "/changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "type": "markdown",
    "hash": "#bugs-bashed-04-30-2024",
    "content": "Fixed chats staying open after errors, chats will now end upon exceptions\n\nAdded an error thrown if config uses both custom_model and prompt, because custom language models do not use prompts\n\nFixed issue where erroring when sending errors would cause the API to get stuck\n\nAdded clearer errors for custom language models\n\nAdded unable to configure audio service error\n\nAdded an error to invalidate outdated language model responses",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024"
      },
      "h3": {
        "id": "bugs-bashed-04-30-2024",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.list-tools",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "title": "List tools",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.list-tools",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools",
    "endpoint_path_alternates": [
      "/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of Tools.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.list-tools-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "title": "List tools - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      },
      {
        "title": "List tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools/list-tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.list-tools",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools",
    "endpoint_path_alternates": [
      "/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.create-tool",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "title": "Create tool",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.create-tool",
    "method": "POST",
    "endpoint_path": "/v0/evi/tools",
    "endpoint_path_alternates": [
      "/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools"
    ],
    "response_type": "json",
    "description": "Creates a Tool that can be added to an EVI configuration.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.create-tool-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "title": "Create tool - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      },
      {
        "title": "Create tool",
        "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.create-tool",
    "method": "POST",
    "endpoint_path": "/v0/evi/tools",
    "endpoint_path_alternates": [
      "/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools"
    ],
    "response_type": "json",
    "description": "Created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.list-tool-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "title": "List tool versions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.list-tool-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Fetches a list of a Tool's versions.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.list-tool-versions-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "title": "List tool versions - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      },
      {
        "title": "List tool versions",
        "pathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.list-tool-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.create-tool-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "title": "Create tool version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.create-tool-version",
    "method": "POST",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Updates a Tool by creating a new version of the Tool.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.create-tool-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "title": "Create tool version - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      },
      {
        "title": "Create tool version",
        "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.create-tool-version",
    "method": "POST",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.delete-tool",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/delete-tool",
    "pathname": "/reference/empathic-voice-interface-evi/tools/delete-tool",
    "title": "Delete tool",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.delete-tool",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D"
    ],
    "description": "Deletes a Tool and its versions.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.update-tool-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/update-tool-name",
    "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-name",
    "title": "Update tool name",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.update-tool-name",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D"
    ],
    "description": "Updates the name of a Tool.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.get-tool-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "pathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "title": "Get tool version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.get-tool-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Fetches a specified version of a Tool.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.get-tool-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "pathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "title": "Get tool version - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      },
      {
        "title": "Get tool version",
        "pathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.get-tool-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.delete-tool-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/delete-tool-version",
    "pathname": "/reference/empathic-voice-interface-evi/tools/delete-tool-version",
    "title": "Delete tool version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.delete-tool-version",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D"
    ],
    "description": "Deletes a specified version of a Tool.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.update-tool-description",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "title": "Update tool description",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.update-tool-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Updates the description of a specified Tool version.\nRefer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.update-tool-description-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "title": "Update tool description - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      },
      {
        "title": "Update tool description",
        "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.update-tool-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.list-prompts",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "title": "List prompts",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.list-prompts",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts",
    "endpoint_path_alternates": [
      "/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of Prompts.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.list-prompts-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "title": "List prompts - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      },
      {
        "title": "List prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.list-prompts",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts",
    "endpoint_path_alternates": [
      "/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.create-prompt",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "title": "Create prompt",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.create-prompt",
    "method": "POST",
    "endpoint_path": "/v0/evi/prompts",
    "endpoint_path_alternates": [
      "/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts"
    ],
    "response_type": "json",
    "description": "Creates a Prompt that can be added to an EVI configuration.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.create-prompt-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "title": "Create prompt - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      },
      {
        "title": "Create prompt",
        "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.create-prompt",
    "method": "POST",
    "endpoint_path": "/v0/evi/prompts",
    "endpoint_path_alternates": [
      "/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts"
    ],
    "response_type": "json",
    "description": "Created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.list-prompt-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "title": "List prompt versions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.list-prompt-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Fetches a list of a Prompt's versions.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.list-prompt-versions-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "title": "List prompt versions - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      },
      {
        "title": "List prompt versions",
        "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.list-prompt-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.create-prompt-verison",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "title": "Create prompt version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.create-prompt-verison",
    "method": "POST",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Updates a Prompt by creating a new version of the Prompt.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.create-prompt-verison-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "title": "Create prompt version - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      },
      {
        "title": "Create prompt version",
        "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.create-prompt-verison",
    "method": "POST",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.delete-prompt",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt",
    "title": "Delete prompt",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.delete-prompt",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D"
    ],
    "description": "Deletes a Prompt and its versions.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.update-prompt-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-name",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-name",
    "title": "Update prompt name",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.update-prompt-name",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D"
    ],
    "description": "Updates the name of a Prompt.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.get-prompt-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "title": "Get prompt version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.get-prompt-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Fetches a specified version of a Prompt.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.get-prompt-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "title": "Get prompt version - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      },
      {
        "title": "Get prompt version",
        "pathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.get-prompt-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.delete-prompt-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt-version",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt-version",
    "title": "Delete prompt version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.delete-prompt-version",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D"
    ],
    "description": "Deletes a specified version of a Prompt.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.update-prompt-description",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "title": "Update prompt description",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.update-prompt-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Updates the description of a Prompt.\nSee our prompting guide for tips on crafting your system prompt.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.update-prompt-description-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "title": "Update prompt description - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      },
      {
        "title": "Update prompt description",
        "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.update-prompt-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.list-custom-voices",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices",
    "title": "List custom voices",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.list-custom-voices",
    "method": "GET",
    "endpoint_path": "/v0/evi/custom_voices",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of Custom Voices.\nRefer to our voices guide for details on creating a custom voice.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.list-custom-voices-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices",
    "title": "List custom voices - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      },
      {
        "title": "List custom voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.list-custom-voices",
    "method": "GET",
    "endpoint_path": "/v0/evi/custom_voices",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.create-custom-voice",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice",
    "title": "Create custom voice",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.create-custom-voice",
    "method": "POST",
    "endpoint_path": "/v0/evi/custom_voices",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices"
    ],
    "response_type": "json",
    "description": "Creates a Custom Voice that can be added to an EVI configuration.\nRefer to our voices guide for details on creating a custom voice.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.create-custom-voice-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice",
    "title": "Create custom voice - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      },
      {
        "title": "Create custom voice",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.create-custom-voice",
    "method": "POST",
    "endpoint_path": "/v0/evi/custom_voices",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices"
    ],
    "response_type": "json",
    "description": "Created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.get-custom-voice",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice",
    "title": "Get specific custom voice by ID",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.get-custom-voice",
    "method": "GET",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Fetches a specific Custom Voice by ID.\nRefer to our voices guide for details on creating a custom voice.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.get-custom-voice-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice",
    "title": "Get specific custom voice by ID - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      },
      {
        "title": "Get specific custom voice by ID",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.get-custom-voice",
    "method": "GET",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.create-custom-voice-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version",
    "title": "Create new version of existing custom voice",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.create-custom-voice-version",
    "method": "POST",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Updates a Custom Voice by creating a new version of the Custom Voice.\nRefer to our voices guide for details on creating a custom voice.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.create-custom-voice-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version",
    "title": "Create new version of existing custom voice - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      },
      {
        "title": "Create new version of existing custom voice",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.create-custom-voice-version",
    "method": "POST",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.delete-custom-voice",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/delete-custom-voice",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/delete-custom-voice",
    "title": "Delete a custom voice",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.delete-custom-voice",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D"
    ],
    "description": "Deletes a Custom Voice and its versions.\nRefer to our voices guide for details on creating a custom voice.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.update-custom-voice-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/update-custom-voice-name",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/update-custom-voice-name",
    "title": "Update custom voice name",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Custom Voices",
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.update-custom-voice-name",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D"
    ],
    "description": "Updates the name of a Custom Voice.\nRefer to our voices guide for details on creating a custom voice.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.list-configs",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "title": "List configs",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.list-configs",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs",
    "endpoint_path_alternates": [
      "/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of Configs.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.list-configs-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "title": "List configs - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      },
      {
        "title": "List configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs/list-configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.list-configs",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs",
    "endpoint_path_alternates": [
      "/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.create-config",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "title": "Create config",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.create-config",
    "method": "POST",
    "endpoint_path": "/v0/evi/configs",
    "endpoint_path_alternates": [
      "/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs"
    ],
    "response_type": "json",
    "description": "Creates a Config which can be applied to EVI.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.create-config-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "title": "Create config - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      },
      {
        "title": "Create config",
        "pathname": "/reference/empathic-voice-interface-evi/configs/create-config"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.create-config",
    "method": "POST",
    "endpoint_path": "/v0/evi/configs",
    "endpoint_path_alternates": [
      "/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs"
    ],
    "response_type": "json",
    "description": "Created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.list-config-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "title": "List config versions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.list-config-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Fetches a list of a Config's versions.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.list-config-versions-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "title": "List config versions - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      },
      {
        "title": "List config versions",
        "pathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.list-config-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.create-config-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "title": "Create config version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.create-config-version",
    "method": "POST",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Updates a Config by creating a new version of the Config.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.create-config-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "title": "Create config version - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      },
      {
        "title": "Create config version",
        "pathname": "/reference/empathic-voice-interface-evi/configs/create-config-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.create-config-version",
    "method": "POST",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.delete-config",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/delete-config",
    "pathname": "/reference/empathic-voice-interface-evi/configs/delete-config",
    "title": "Delete config",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.delete-config",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D"
    ],
    "description": "Deletes a Config and its versions.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.update-config-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/update-config-name",
    "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-name",
    "title": "Update config name",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.update-config-name",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D"
    ],
    "description": "Updates the name of a Config.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.get-config-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "pathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "title": "Get config version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.get-config-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Fetches a specified version of a Config.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.get-config-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "pathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "title": "Get config version - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      },
      {
        "title": "Get config version",
        "pathname": "/reference/empathic-voice-interface-evi/configs/get-config-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.get-config-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.delete-config-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/delete-config-version",
    "pathname": "/reference/empathic-voice-interface-evi/configs/delete-config-version",
    "title": "Delete config version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.delete-config-version",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D"
    ],
    "description": "Deletes a specified version of a Config.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.update-config-description",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "title": "Update config description",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.update-config-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Updates the description of a Config.\nFor more details on configuration options and how to configure EVI, see our configuration guide.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.update-config-description-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "title": "Update config description - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      },
      {
        "title": "Update config description",
        "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-description"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.update-config-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.list-chats",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "title": "List chats",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.list-chats",
    "method": "GET",
    "endpoint_path": "/v0/evi/chats",
    "endpoint_path_alternates": [
      "/v0/evi/chats",
      "https://api.hume.ai/v0/evi/chats",
      "https://api.hume.ai/v0/evi/chats"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of Chats.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.list-chats-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "title": "List chats - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats"
      },
      {
        "title": "List chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats/list-chats"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.list-chats",
    "method": "GET",
    "endpoint_path": "/v0/evi/chats",
    "endpoint_path_alternates": [
      "/v0/evi/chats",
      "https://api.hume.ai/v0/evi/chats",
      "https://api.hume.ai/v0/evi/chats"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.list-chat-events",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "title": "List chat events",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.list-chat-events",
    "method": "GET",
    "endpoint_path": "/v0/evi/chats/:id",
    "endpoint_path_alternates": [
      "/v0/evi/chats/{id}",
      "https://api.hume.ai/v0/evi/chats/:id",
      "https://api.hume.ai/v0/evi/chats/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of Chat events.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.list-chat-events-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "title": "List chat events - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats"
      },
      {
        "title": "List chat events",
        "pathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.list-chat-events",
    "method": "GET",
    "endpoint_path": "/v0/evi/chats/:id",
    "endpoint_path_alternates": [
      "/v0/evi/chats/{id}",
      "https://api.hume.ai/v0/evi/chats/:id",
      "https://api.hume.ai/v0/evi/chats/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.get-audio",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/get-audio",
    "pathname": "/reference/empathic-voice-interface-evi/chats/get-audio",
    "title": "Get chat audio",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.get-audio",
    "method": "GET",
    "endpoint_path": "/v0/evi/chats/:id/audio",
    "endpoint_path_alternates": [
      "/v0/evi/chats/{id}/audio",
      "https://api.hume.ai/v0/evi/chats/:id/audio",
      "https://api.hume.ai/v0/evi/chats/%7Bid%7D/audio"
    ],
    "response_type": "json",
    "description": "Fetches the audio of a previous Chat. For more details, see our guide on audio reconstruction here.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.get-audio-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/get-audio",
    "pathname": "/reference/empathic-voice-interface-evi/chats/get-audio",
    "title": "Get chat audio - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats"
      },
      {
        "title": "Get chat audio",
        "pathname": "/reference/empathic-voice-interface-evi/chats/get-audio"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.get-audio",
    "method": "GET",
    "endpoint_path": "/v0/evi/chats/:id/audio",
    "endpoint_path_alternates": [
      "/v0/evi/chats/{id}/audio",
      "https://api.hume.ai/v0/evi/chats/:id/audio",
      "https://api.hume.ai/v0/evi/chats/%7Bid%7D/audio"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.list-chat-groups",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "title": "List chat_groups",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-groups",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups",
      "https://api.hume.ai/v0/evi/chat_groups",
      "https://api.hume.ai/v0/evi/chat_groups"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of Chat Groups.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.list-chat-groups-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "title": "List chat_groups - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      },
      {
        "title": "List chat_groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-groups",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups",
      "https://api.hume.ai/v0/evi/chat_groups",
      "https://api.hume.ai/v0/evi/chat_groups"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.get-chat-group",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group",
    "title": "Get chat_group",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.get-chat-group",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups/:id",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}",
      "https://api.hume.ai/v0/evi/chat_groups/:id",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Fetches a ChatGroup by ID, including a paginated list of Chats associated with the ChatGroup.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.get-chat-group-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group",
    "title": "Get chat_group - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      },
      {
        "title": "Get chat_group",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.get-chat-group",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups/:id",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}",
      "https://api.hume.ai/v0/evi/chat_groups/:id",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.list-chat-group-events",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "title": "List chat events from a specific chat_group",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-group-events",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups/:id/events",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}/events",
      "https://api.hume.ai/v0/evi/chat_groups/:id/events",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D/events"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of Chat events associated with a Chat Group.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.list-chat-group-events-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "title": "List chat events from a specific chat_group - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      },
      {
        "title": "List chat events from a specific chat_group",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-group-events",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups/:id/events",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}/events",
      "https://api.hume.ai/v0/evi/chat_groups/:id/events",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D/events"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.get-audio",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio",
    "title": "Get chat group audio",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.get-audio",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups/:id/audio",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}/audio",
      "https://api.hume.ai/v0/evi/chat_groups/:id/audio",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D/audio"
    ],
    "response_type": "json",
    "description": "Fetches a paginated list of audio for each Chat within the specified Chat Group. For more details, see our guide on audio reconstruction here.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.get-audio-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio",
    "title": "Get chat group audio - Response",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      },
      {
        "title": "Get chat group audio",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.get-audio",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups/:id/audio",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}/audio",
      "https://api.hume.ai/v0/evi/chat_groups/:id/audio",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D/audio"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:subpackage_chat.chat",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat/chat",
    "pathname": "/reference/empathic-voice-interface-evi/chat/chat",
    "title": "Chat",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat",
        "pathname": "/reference/empathic-voice-interface-evi/chat"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "websocket",
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "subpackage_chat.chat",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat",
    "endpoint_path_alternates": [
      "/v0/evi/chat",
      "wss://api.hume.ai/v0/evi/chat",
      "wss://api.hume.ai/v0/evi/chat"
    ],
    "environments": [
      {
        "id": "Default",
        "url": "wss://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "websocket",
      "web socket",
      "stream",
      "SubscribeEvent",
      "PublishEvent"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.list-jobs",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/expression-measurement-api/batch/list-jobs",
    "pathname": "/reference/expression-measurement-api/batch/list-jobs",
    "title": "List jobs",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.list-jobs",
    "method": "GET",
    "endpoint_path": "/v0/batch/jobs",
    "endpoint_path_alternates": [
      "/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs"
    ],
    "response_type": "json",
    "description": "Sort and filter jobs.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.start-inference-job",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/expression-measurement-api/batch/start-inference-job",
    "pathname": "/reference/expression-measurement-api/batch/start-inference-job",
    "title": "Start inference job",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.start-inference-job",
    "method": "POST",
    "endpoint_path": "/v0/batch/jobs",
    "endpoint_path_alternates": [
      "/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs"
    ],
    "response_type": "json",
    "description": "Start a new measurement inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.get-job-details",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/expression-measurement-api/batch/get-job-details",
    "pathname": "/reference/expression-measurement-api/batch/get-job-details",
    "title": "Get job details",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.get-job-details",
    "method": "GET",
    "endpoint_path": "/v0/batch/jobs/:id",
    "endpoint_path_alternates": [
      "/v0/batch/jobs/{id}",
      "https://api.hume.ai/v0/batch/jobs/:id",
      "https://api.hume.ai/v0/batch/jobs/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Get the request details and state of a given job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.get-job-predictions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/expression-measurement-api/batch/get-job-predictions",
    "pathname": "/reference/expression-measurement-api/batch/get-job-predictions",
    "title": "Get job predictions",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.get-job-predictions",
    "method": "GET",
    "endpoint_path": "/v0/batch/jobs/:id/predictions",
    "endpoint_path_alternates": [
      "/v0/batch/jobs/{id}/predictions",
      "https://api.hume.ai/v0/batch/jobs/:id/predictions",
      "https://api.hume.ai/v0/batch/jobs/%7Bid%7D/predictions"
    ],
    "response_type": "json",
    "description": "Get the JSON predictions of a completed inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.get-job-artifacts",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/expression-measurement-api/batch/get-job-artifacts",
    "pathname": "/reference/expression-measurement-api/batch/get-job-artifacts",
    "title": "Get job artifacts",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.get-job-artifacts",
    "method": "GET",
    "endpoint_path": "/v0/batch/jobs/:id/artifacts",
    "endpoint_path_alternates": [
      "/v0/batch/jobs/{id}/artifacts",
      "https://api.hume.ai/v0/batch/jobs/:id/artifacts",
      "https://api.hume.ai/v0/batch/jobs/%7Bid%7D/artifacts"
    ],
    "response_type": "file",
    "description": "Get the artifacts ZIP of a completed inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "file"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.start-inference-job-from-local-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/expression-measurement-api/batch/start-inference-job-from-local-file",
    "pathname": "/reference/expression-measurement-api/batch/start-inference-job-from-local-file",
    "title": "Start inference job from local file",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.start-inference-job-from-local-file",
    "method": "POST",
    "endpoint_path": "/v0/batch/jobs",
    "endpoint_path_alternates": [
      "/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs"
    ],
    "response_type": "json",
    "description": "Start a new batch inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:a14d3798-e567-4432-a6b3-2fa8a50954c7:subpackage_stream.Stream",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/expression-measurement-api/stream/stream",
    "pathname": "/reference/expression-measurement-api/stream/stream",
    "title": "Stream",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Stream",
        "pathname": "/reference/expression-measurement-api/stream"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "websocket",
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "subpackage_stream.Stream",
    "method": "GET",
    "endpoint_path": "/v0/stream/models",
    "endpoint_path_alternates": [
      "/v0/stream/models",
      "wss://api.hume.ai/v0/stream/models",
      "wss://api.hume.ai/v0/stream/models"
    ],
    "environments": [
      {
        "id": "Default",
        "url": "wss://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "websocket",
      "web socket",
      "stream",
      "SubscribeEvent",
      "StreamModelsEndpointPayload"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.list-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/list-files",
    "pathname": "/reference/custom-models-api/files/list-files",
    "title": "List files",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.list-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.list-files-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/list-files",
    "pathname": "/reference/custom-models-api/files/list-files",
    "title": "List files - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      },
      {
        "title": "List files",
        "pathname": "/reference/custom-models-api/files/list-files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.list-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.create-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/create-files",
    "pathname": "/reference/custom-models-api/files/create-files",
    "title": "Create files",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.create-files",
    "method": "POST",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files"
    ],
    "response_type": "json",
    "description": "Returns 201 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.create-files-request",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/create-files",
    "pathname": "/reference/custom-models-api/files/create-files",
    "title": "Create files - Request",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      },
      {
        "title": "Create files",
        "pathname": "/reference/custom-models-api/files/create-files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.create-files",
    "method": "POST",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files"
    ],
    "response_type": "json",
    "description": "List of Files with Attributes to be created",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#request"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.create-files-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/create-files",
    "pathname": "/reference/custom-models-api/files/create-files",
    "title": "Create files - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      },
      {
        "title": "Create files",
        "pathname": "/reference/custom-models-api/files/create-files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.create-files",
    "method": "POST",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.upload-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/upload-file",
    "pathname": "/reference/custom-models-api/files/upload-file",
    "title": "Upload file",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.upload-file",
    "method": "POST",
    "endpoint_path": "/v0/registry/files/upload",
    "endpoint_path_alternates": [
      "/v0/registry/files/upload",
      "https://api.hume.ai/v0/registry/files/upload",
      "https://api.hume.ai/v0/registry/files/upload"
    ],
    "response_type": "json",
    "description": "Upload a file synchronously. Returns 201 if successful. Files must have a name. Files must specify Content-Type. Request bodies, and therefore files, are limited to 100MB",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.upload-file-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/upload-file",
    "pathname": "/reference/custom-models-api/files/upload-file",
    "title": "Upload file - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      },
      {
        "title": "Upload file",
        "pathname": "/reference/custom-models-api/files/upload-file"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.upload-file",
    "method": "POST",
    "endpoint_path": "/v0/registry/files/upload",
    "endpoint_path_alternates": [
      "/v0/registry/files/upload",
      "https://api.hume.ai/v0/registry/files/upload",
      "https://api.hume.ai/v0/registry/files/upload"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.get-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/get-file",
    "pathname": "/reference/custom-models-api/files/get-file",
    "title": "Get file",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.get-file",
    "method": "GET",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.get-file-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/get-file",
    "pathname": "/reference/custom-models-api/files/get-file",
    "title": "Get file - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      },
      {
        "title": "Get file",
        "pathname": "/reference/custom-models-api/files/get-file"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.get-file",
    "method": "GET",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.delete-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/delete-file",
    "pathname": "/reference/custom-models-api/files/delete-file",
    "title": "Delete file",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.delete-file",
    "method": "DELETE",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 204 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.delete-file-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/delete-file",
    "pathname": "/reference/custom-models-api/files/delete-file",
    "title": "Delete file - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      },
      {
        "title": "Delete file",
        "pathname": "/reference/custom-models-api/files/delete-file"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.delete-file",
    "method": "DELETE",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.update-file-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/update-file-name",
    "pathname": "/reference/custom-models-api/files/update-file-name",
    "title": "Update file name",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.update-file-name",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.update-file-name-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/update-file-name",
    "pathname": "/reference/custom-models-api/files/update-file-name",
    "title": "Update file name - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      },
      {
        "title": "Update file name",
        "pathname": "/reference/custom-models-api/files/update-file-name"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.update-file-name",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.get-file-predictions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/get-file-predictions",
    "pathname": "/reference/custom-models-api/files/get-file-predictions",
    "title": "Get file predictions",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.get-file-predictions",
    "method": "GET",
    "endpoint_path": "/v0/registry/files/:id/predictions",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}/predictions",
      "https://api.hume.ai/v0/registry/files/:id/predictions",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D/predictions"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.get-file-predictions-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/files/get-file-predictions",
    "pathname": "/reference/custom-models-api/files/get-file-predictions",
    "title": "Get file predictions - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      },
      {
        "title": "Get file predictions",
        "pathname": "/reference/custom-models-api/files/get-file-predictions"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.get-file-predictions",
    "method": "GET",
    "endpoint_path": "/v0/registry/files/:id/predictions",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}/predictions",
      "https://api.hume.ai/v0/registry/files/:id/predictions",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D/predictions"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-datasets",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/list-datasets",
    "pathname": "/reference/custom-models-api/datasets/list-datasets",
    "title": "List datasets",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-datasets",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets",
    "endpoint_path_alternates": [
      "/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-datasets-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/list-datasets",
    "pathname": "/reference/custom-models-api/datasets/list-datasets",
    "title": "List datasets - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "List datasets",
        "pathname": "/reference/custom-models-api/datasets/list-datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-datasets",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets",
    "endpoint_path_alternates": [
      "/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.create-dataset",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/create-dataset",
    "pathname": "/reference/custom-models-api/datasets/create-dataset",
    "title": "Create dataset",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.create-dataset",
    "method": "POST",
    "endpoint_path": "/v0/registry/datasets",
    "endpoint_path_alternates": [
      "/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets"
    ],
    "response_type": "json",
    "description": "Returns 201 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.create-dataset-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/create-dataset",
    "pathname": "/reference/custom-models-api/datasets/create-dataset",
    "title": "Create dataset - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "Create dataset",
        "pathname": "/reference/custom-models-api/datasets/create-dataset"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.create-dataset",
    "method": "POST",
    "endpoint_path": "/v0/registry/datasets",
    "endpoint_path_alternates": [
      "/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.get-dataset",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/get-dataset",
    "pathname": "/reference/custom-models-api/datasets/get-dataset",
    "title": "Get dataset",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.get-dataset",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.get-dataset-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/get-dataset",
    "pathname": "/reference/custom-models-api/datasets/get-dataset",
    "title": "Get dataset - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "Get dataset",
        "pathname": "/reference/custom-models-api/datasets/get-dataset"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.get-dataset",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.create-dataset-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "pathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "title": "Create dataset version",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.create-dataset-version",
    "method": "POST",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.create-dataset-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "pathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "title": "Create dataset version - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "Create dataset version",
        "pathname": "/reference/custom-models-api/datasets/create-dataset-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.create-dataset-version",
    "method": "POST",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.delete-dataset",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/delete-dataset",
    "pathname": "/reference/custom-models-api/datasets/delete-dataset",
    "title": "Delete dataset",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.delete-dataset",
    "method": "DELETE",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 204 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.delete-dataset-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/delete-dataset",
    "pathname": "/reference/custom-models-api/datasets/delete-dataset",
    "title": "Delete dataset - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "Delete dataset",
        "pathname": "/reference/custom-models-api/datasets/delete-dataset"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.delete-dataset",
    "method": "DELETE",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "title": "List dataset versions",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-versions",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id/versions",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}/versions",
      "https://api.hume.ai/v0/registry/datasets/:id/versions",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D/versions"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-versions-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "title": "List dataset versions - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "List dataset versions",
        "pathname": "/reference/custom-models-api/datasets/list-dataset-versions"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-versions",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id/versions",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}/versions",
      "https://api.hume.ai/v0/registry/datasets/:id/versions",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D/versions"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "title": "List dataset files",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id/files",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}/files",
      "https://api.hume.ai/v0/registry/datasets/:id/files",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D/files"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-files-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "title": "List dataset files - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "List dataset files",
        "pathname": "/reference/custom-models-api/datasets/list-dataset-files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id/files",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}/files",
      "https://api.hume.ai/v0/registry/datasets/:id/files",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D/files"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.get-dataset-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "pathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "title": "Get dataset version",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.get-dataset-version",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/version/{id}",
      "https://api.hume.ai/v0/registry/datasets/version/:id",
      "https://api.hume.ai/v0/registry/datasets/version/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.get-dataset-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "pathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "title": "Get dataset version - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "Get dataset version",
        "pathname": "/reference/custom-models-api/datasets/get-dataset-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.get-dataset-version",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/version/{id}",
      "https://api.hume.ai/v0/registry/datasets/version/:id",
      "https://api.hume.ai/v0/registry/datasets/version/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-version-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "title": "List dataset version files",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-version-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/version/:id/files",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/version/{id}/files",
      "https://api.hume.ai/v0/registry/datasets/version/:id/files",
      "https://api.hume.ai/v0/registry/datasets/version/%7Bid%7D/files"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-version-files-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "title": "List dataset version files - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      },
      {
        "title": "List dataset version files",
        "pathname": "/reference/custom-models-api/datasets/list-dataset-version-files"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-version-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/version/:id/files",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/version/{id}/files",
      "https://api.hume.ai/v0/registry/datasets/version/:id/files",
      "https://api.hume.ai/v0/registry/datasets/version/%7Bid%7D/files"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.list-models",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/list-models",
    "pathname": "/reference/custom-models-api/models/list-models",
    "title": "List models",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.list-models",
    "method": "GET",
    "endpoint_path": "/v0/registry/models",
    "endpoint_path_alternates": [
      "/v0/registry/models",
      "https://api.hume.ai/v0/registry/models",
      "https://api.hume.ai/v0/registry/models"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.list-models-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/list-models",
    "pathname": "/reference/custom-models-api/models/list-models",
    "title": "List models - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      },
      {
        "title": "List models",
        "pathname": "/reference/custom-models-api/models/list-models"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.list-models",
    "method": "GET",
    "endpoint_path": "/v0/registry/models",
    "endpoint_path_alternates": [
      "/v0/registry/models",
      "https://api.hume.ai/v0/registry/models",
      "https://api.hume.ai/v0/registry/models"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.get-model-details",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/get-model-details",
    "pathname": "/reference/custom-models-api/models/get-model-details",
    "title": "Get model details",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.get-model-details",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/{id}",
      "https://api.hume.ai/v0/registry/models/:id",
      "https://api.hume.ai/v0/registry/models/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.get-model-details-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/get-model-details",
    "pathname": "/reference/custom-models-api/models/get-model-details",
    "title": "Get model details - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      },
      {
        "title": "Get model details",
        "pathname": "/reference/custom-models-api/models/get-model-details"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.get-model-details",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/{id}",
      "https://api.hume.ai/v0/registry/models/:id",
      "https://api.hume.ai/v0/registry/models/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.update-model-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/update-model-name",
    "pathname": "/reference/custom-models-api/models/update-model-name",
    "title": "Update model name",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.update-model-name",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/models/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/{id}",
      "https://api.hume.ai/v0/registry/models/:id",
      "https://api.hume.ai/v0/registry/models/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.update-model-name-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/update-model-name",
    "pathname": "/reference/custom-models-api/models/update-model-name",
    "title": "Update model name - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      },
      {
        "title": "Update model name",
        "pathname": "/reference/custom-models-api/models/update-model-name"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.update-model-name",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/models/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/{id}",
      "https://api.hume.ai/v0/registry/models/:id",
      "https://api.hume.ai/v0/registry/models/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.list-model-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/list-model-versions",
    "pathname": "/reference/custom-models-api/models/list-model-versions",
    "title": "List model versions",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.list-model-versions",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/version",
    "endpoint_path_alternates": [
      "/v0/registry/models/version",
      "https://api.hume.ai/v0/registry/models/version",
      "https://api.hume.ai/v0/registry/models/version"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.list-model-versions-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/list-model-versions",
    "pathname": "/reference/custom-models-api/models/list-model-versions",
    "title": "List model versions - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      },
      {
        "title": "List model versions",
        "pathname": "/reference/custom-models-api/models/list-model-versions"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.list-model-versions",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/version",
    "endpoint_path_alternates": [
      "/v0/registry/models/version",
      "https://api.hume.ai/v0/registry/models/version",
      "https://api.hume.ai/v0/registry/models/version"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.get-model-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/get-model-version",
    "pathname": "/reference/custom-models-api/models/get-model-version",
    "title": "Get model version",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.get-model-version",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/version/{id}",
      "https://api.hume.ai/v0/registry/models/version/:id",
      "https://api.hume.ai/v0/registry/models/version/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.get-model-version-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/get-model-version",
    "pathname": "/reference/custom-models-api/models/get-model-version",
    "title": "Get model version - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      },
      {
        "title": "Get model version",
        "pathname": "/reference/custom-models-api/models/get-model-version"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.get-model-version",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/version/{id}",
      "https://api.hume.ai/v0/registry/models/version/:id",
      "https://api.hume.ai/v0/registry/models/version/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.update-model-description",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/update-model-description",
    "pathname": "/reference/custom-models-api/models/update-model-description",
    "title": "Update model description",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.update-model-description",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/models/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/version/{id}",
      "https://api.hume.ai/v0/registry/models/version/:id",
      "https://api.hume.ai/v0/registry/models/version/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.update-model-description-response",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/models/update-model-description",
    "pathname": "/reference/custom-models-api/models/update-model-description",
    "title": "Update model description - Response",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      },
      {
        "title": "Update model description",
        "pathname": "/reference/custom-models-api/models/update-model-description"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 1,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.update-model-description",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/models/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/version/{id}",
      "https://api.hume.ai/v0/registry/models/version/:id",
      "https://api.hume.ai/v0/registry/models/version/%7Bid%7D"
    ],
    "response_type": "json",
    "description": "Success",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference",
    "hash": "#response"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_jobs.start-training-job",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/jobs/start-training-job",
    "pathname": "/reference/custom-models-api/jobs/start-training-job",
    "title": "Start training job",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Jobs",
        "pathname": "/reference/custom-models-api/jobs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_jobs.start-training-job",
    "method": "POST",
    "endpoint_path": "/v0/registry/v0/batch/jobs/tl/train",
    "endpoint_path_alternates": [
      "/v0/registry/v0/batch/jobs/tl/train",
      "https://api.hume.ai/v0/registry/v0/batch/jobs/tl/train",
      "https://api.hume.ai/v0/registry/v0/batch/jobs/tl/train"
    ],
    "response_type": "json",
    "description": "Start a new custom models training job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_jobs.start-custom-models-inference-job",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "canonicalPathname": "/reference/custom-models-api/jobs/start-custom-models-inference-job",
    "pathname": "/reference/custom-models-api/jobs/start-custom-models-inference-job",
    "title": "Start custom models inference job",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Jobs",
        "pathname": "/reference/custom-models-api/jobs"
      }
    ],
    "tab": {
      "title": "API Reference",
      "pathname": "/reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "page_position": 0,
    "api_type": "http",
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_jobs.start-custom-models-inference-job",
    "method": "POST",
    "endpoint_path": "/v0/registry/v0/batch/jobs/tl/inference",
    "endpoint_path_alternates": [
      "/v0/registry/v0/batch/jobs/tl/inference",
      "https://api.hume.ai/v0/registry/v0/batch/jobs/tl/inference",
      "https://api.hume.ai/v0/registry/v0/batch/jobs/tl/inference"
    ],
    "response_type": "json",
    "description": "Start a new custom models inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json"
    ],
    "type": "api-reference"
  }
]