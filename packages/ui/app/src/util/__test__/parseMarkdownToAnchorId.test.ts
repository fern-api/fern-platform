import { parseMarkdownPageToAnchorTag } from "../resolver/resolveDocsContent";

const humanloopMarkdown = [
    "## Update Logs API\n\nWe've introduced the ability to patch Logs for Prompts and Tools. This can come in useful in scenarios where certain characteristics of your Log are delayed that you may want to add later, such as the output, or if you have a process of redacting inputs that takes time.\n\nNote that not all fields support being patched, so start by referring to our [V5 API References](api-reference/prompts). From there, you can submit updates to your previously created logs.\n",
    "## Search files by path\n\nWe've extended our search interface to include file paths, allowing you to more easily find and navigate to related files that you've grouped under a directory.\n\n![Search dialog showing file paths](file:b3484531-3d5b-437f-b6cf-da9ba1d103b8)\n\nBring up this search dialog by clicking \"Search\" near the top of the left-hand sidebar, or by pressing `Cmd+K`.\n",
    "## Updated Gemini 1.5 models\n\nHumanloop supports the three newly released Gemini 1.5 models.\n\nStart using these improved models by specifying one of the following model names in your Prompts:\n\n- `gemini-1.5-pro-exp-0827` The improved Gemini 1.5 Pro model\n- `gemini-1.5-flash-exp-0827` The improved Gemini 1.5 Flash model\n- `gemini-1.5-flash-8b-exp-0827` The smaller Gemini 1.5 Flash variant\n\nMore details on these models can be viewed [here](https://ai.google.dev/gemini-api/docs/models/experimental-models#available-models).\n",
    "## Custom attributes for Files\n\nYou can now include custom attributes to determine the unique version of your file definitions on Humanloop. \n\nThis allows you to make the version depend on data custom to your application that Humanloop may not be aware of. \n\nFor example, if there are feature flags or identifiers that indicate a different configuration of your system that may impact the behaviour of your Prompt or Tool.\n\n`attributes` can be submitted via the v5 API endpoints. When added, the attributes are visible on the Version Drawer and in the Editor.\n\n![Metadata on versions](file:6d8cb8d6-aa2a-4ce5-a639-3ad9049632b5)\n",
    "## Improved popover UI\n\nWe've expanded the information shown in the version popover so that it is easier to identify which version you are working with.\n\nThis is particularly useful in places like the Logs table and within Evaluation reports, where you may be working with multiple versions of a Prompt, Tool, or Evaluator and need to preview the contents.\n\n![Improved version popover](file:25f0da7f-6531-4d12-861c-be670a51542b)\n",
    '## Evaluate uncommitted versions\n\nYou can now evaluate versions without committing them first. This means you can draft a version of a Prompt in the editor and simultaneously evaluate it in the evaluations tab, speeding up your iteration cycle.\n\nThis is a global change that allows you to load and use uncommitted versions. Uncommitted versions are created automatically when a new version of a Prompt, Tool, or Evaluator is run in their respective editors or called via the API. These versions will now appear in the version pickers underneath all your committed versions.\n\nTo evaluate an uncommitted version, simply select it by using the hash (known as the "version id") when setting up your evaluation.\n\n![Uncommitted versions in the version picker](file:adf7e8e9-dfb9-4534-96b8-51652ff7312b)\n',
    "## Human Evaluator upgrades\n\nWe've made significant upgrades to Human Evaluators and related workflows to improve your ability to gather Human judgments (sometimes referred to as \"feedback\") in assessing the quality of your AI applications.\n\nHere are some of the key improvements:\n\n- Instead of having to define a limited feedback schema tied to the settings of a specific Prompt, you can now **define your schema with a Human Evaluator file and reuse it across multiple Prompts and Tools** for both monitoring and offline evaluation purposes.\n- You are no longer restricted to the default types of `Rating`, `Actions` and `Issues` when defining your feedback schemas from the UI. We've introduced a **more flexible Editor interface supporting different return types** and valence controls.\n- We've extended the scope of Human Evaluators so that they can now **also be used with Tools and other Evaluators** (useful for validating AI judgments) in the same way as with Prompts.\n- We've **improved the Logs drawer UI for applying feedback** to Logs. In particular, we've made the buttons more responsive.\n\nTo set up a Human Evaluator, create a new file. Within the file creation dialog, click on **Evaluator**, then click on **Human**.\nThis will create a new Human Evaluator file and bring you to its Editor. Here, you can choose a `Return type` for the Evaluator and configure the feedback schema.\n\n![Tone evaluator set up with options and instructions](file:1c0e1aa3-86a5-4b37-87a6-167e420c71be)\n\nYou can then reference this Human Evaluator within the `Monitoring` dropdown of Prompts, Tools, and other Evaluators, as well as when configuring reports in their `Evaluations` tab.\n\nWe've set up default `Rating` and `Correction` Evaluators that will be automatically attached to all Prompts new and existing. We've migrated all your existing Prompt specific feedback schemas to Human Evaluator files and these will continue to work as before with no disruption.\n\nCheck out our updated document for further details on how to use Human Evaluators:\n\n- [Create a Human Evaluator](/docs/v5/evaluation/guides/human-evaluator)\n- [Capture End User Feedback](/docs/v5/observability/guides/capture-user-feedback)\n- [Run a Human Evaluation](/docs/v5/evaluation/guides/run-human-evaluation)\n",
    "## Evaluations improvements\n\nWe've made improvements to help you evaluate the components of your AI applications, quickly see issues and explore the full context of each evaluation.\n\n### A clearer Evaluation tab in Logs\n\nWe've given the Log drawer's Evaluation tab a facelift. You can now clearly see what the results are for each of the connected Evaluators.\n\nThis means that it's now easier to debug the judgments applied to a Log, and if necessary, re-run code/AI Evaluators in-line.\n\n![Log drawer's Evaluation tab with the \"Run again\" menu open](file:936085e1-6e4c-4566-9e39-7ceed0505e9b)\n\n### Ability to re-run Evaluators\n\nWe have introduced the ability to re-run your Evaluators against a specific Log. This feature allows you to more easily address and fix issues with previous Evaluator judgments for specific Logs.\n\nYou can request a re-run of that Evaluator by opening the menu next to that Evaluator and pressing the \"Run Again\" option.\n\n### Evaluation popover\n\nIf you hover over an evaluation result, you'll now see a popover with more details about the evaluation including any intermediate results or console logs without context switching.\n\n![Evaluation popover](file:46c2d018-0bd4-4d10-958b-6d24383033e7)\n\n### Updated Evaluator Logs table\n\nThe Logs table for Evaluators now supports the functionality as you would expect from our other Logs tables. This will make it easier to filter and sort your Evaluator judgments.\n",
    "## More Code Evaluator packages\n\nWe have expanded the packages available in the Evaluator Python environment. The new packages we've added are: `continuous-eval`, `jellyfish`, `langdetect`, `nltk`, `scikit-learn`, `spacy`, `transformers`. The full list of packages can been seen in our [Python environment reference](/docs/v5/reference/python-environment). \n\nWe are actively improving our execution environment so if you have additional packages you'd like us to support, please do not hesitate to get in touch.\n\n",
    '## OpenAI Structured Outputs\n\nOpenAI have introduced [Structured Outputs](https://openai.com/index/introducing-structured-outputs-in-the-api/) functionality to their API.\n\nThis feature allows the model to more reliably adhere to user defined JSON schemas for use cases like information extraction, data validation, and more.\n\nWe\'ve extended our `/chat` (in v4) and `prompt/call` (in v5) endpoints to support this feature. There are two ways to trigger Structured Outputs in the API:\n\n1. **Tool Calling:** When defining a tool as part of your Prompt definition, you can now include a `strict=true` flag. The model will then output JSON data that adheres to the tool `parameters` schema definition.\n\n```python\n""" Example using our v5 API. """\nfrom humanloop import Humanloop\n\nclient = Humanloop(\n    api_key="YOUR_API_KEY",\n)\n\nclient.prompts.call(\n    path="person-extractor",\n    prompt={\n        "model": "gpt-4o",\n        "template": [\n            {\n                "role": "system",\n                "content": "You are an information extractor.",\n            },\n        ],\n        "tools": [\n            {\n                "name": "extract_person_object",\n                "description": "Extracts a person object from a user message.",\n                # New parameter to enable structured outputs\n                "strict": True,\n                "parameters": {\n                    "type": "object",\n                    "properties": {\n                        "name": {\n                            "type": "string",\n                            "name": "Full name",\n                            "description": "Full name of the person",\n                        },\n                        "address": {\n                            "type": "string",\n                            "name": "Full address",\n                            "description": "Full address of the person",\n                        },\n                        "job": {\n                            "type": "string",\n                            "name": "Job",\n                            "description": "The job of the person",\n                        }\n                    },\n                    # These fields need to be defined in strict mode\n                    "required": ["name", "address", "job"],\n                    "additionalProperties": False,\n                },\n            }\n        ],\n    },\n    messages=[\n        {\n            "role": "user",\n            "content": "Hey! I\'m Jacob Martial, I live on 123c Victoria street, Toronto and I\'m a software engineer at Humanloop.",\n        },\n    ],\n    stream=False,\n)\n\n```\n\n2. **Response Format:** We have expanded the `response_format` with option `json_schema` and a request parameter to also include an optional `json_schema` field where you can pass in the schema you wish the model to adhere to.\n\n```python\n\nclient.prompts.call(\n    path="person-extractor",\n    prompt={\n        "model": "gpt-4o",\n        "template": [\n            {\n                "role": "system",\n                "content": "You are an information extractor.",\n            },\n        ],\n        "response_format":{\n            "type": "json_schema",\n            # New parameter to enable structured outputs\n            "response_format": {\n                "type": "json_schema",\n                "json_schema": {\n                    "name": "person_object",\n                    "strict": True,\n                    "schema": {\n                        "type": "object",\n                        "properties": {\n                            "name": {\n                                "type": "string",\n                                "name": "Full name",\n                                "description": "Full name of the person"\n                            },\n                            "address": {\n                                "type": "string",\n                                "name": "Full address",\n                                "description": "Full address of the person"\n                            },\n                            "job": {\n                                "type": "string",\n                                "name": "Job",\n                                "description": "The job of the person"\n                            }\n                        },\n                        "required": ["name", "address", "job"],\n                        "additionalProperties": False}\n                    }\n                }\n            },\n        },\n    messages=[\n        {\n            "role": "user",\n            "content": "Hey! I\'m Jacob Martial, I live on 123c Victoria street, Toronto and I\'m a software engineer at Humanloop.",\n        },\n    ],\n    stream=False,\n)\n```\nThis new response formant functionality is only supported by the latest OpenAPI model snapshots `gpt-4o-2024-08-06` and `gpt-4o-mini-2024-07-18`.\n\nWe will also be exposing this functionality in our Editor UI soon too!\n',
    "## Improved Code Evaluator Debugging\n\nWe've added the ability to view the Standard Output (Stdout) for your Code Evaluators. \n\nYou can now use `print(...)` statements within your code to output intermediate results to aid with debugging.\n\nThe Stdout is available within the Debug console as you iterate on your Code Evaluator:\n\n![DebugConsole](file:0f95d6fa-2604-4e9c-af06-934ed28f6190)\n\nAdditionally, it is stored against the Evaluator Log for future reference:\n\n![EvaluatorLog](file:dc441cf4-4e22-41bc-94a2-dd35604804db)\n",
    "## Select multiple Versions when creating an Evaluation\n\nOur Evaluations feature allows you to benchmark Versions of a same File. We've made the form for creating new Evaluations simpler by allowing the selection of multiple in the picker dialog. Columns will be filled or inserted as needed.\n\nAs an added bonus, we've made adding and removing columns feel smoother with animations. The form will also scroll to newly-added columns.\n\n![](file:2b93b51a-69db-441f-87e3-651ab69278d3)\n",
    "## Faster log queries\n\nYou should notice that queries against your logs should load faster and the tables should render more quickly.\n\nWe're still making more enhancements so keep an eye for more speed-ups coming soon!\n",
    "## gpt-4o-mini support\n\nLatest model from OpenAI, GPT-4o-mini, has been added. It's a smaller version of the GPT-4o model which shows GPT-4 level performance with a model that is 60% cheaper than gpt-3.5-turbo.\n\n- Cost: 15 cents per million input tokens, 60 cents per million output tokens\n- Performance: MMLU score of 82%\n",
    "## Enhanced code Evaluators\nWe've introduced several enhancements to our code Evaluator runtime environment to support additional packages, environment variables, and improved runtime output.\n\n### Runtime environment\nOur Code Evaluator now logs both `stdout` and `stderr` when executed and environment variables can now be accessed via the `os.environ` dictionary, allowing you to retrieve values such as `os.environ['HUMANLOOP_API_KEY']` or `os.environ['PROVIDER_KEYS']`.\n\n### Python packages\nPreviously, the selection of Python packages we could support was limited. We are now able to accommodate customer-requested packages. If you have specific package requirements for your eval workflows, please let us know!\n",
    "## Gemini 1.5 Flash support\n\nGemini 1.5 Flash is Googles most efficient model to date with a long context window and great latency.\n\nWhile it’s smaller than 1.5 Pro, it’s highly capable of multimodal reasoning with a 1 million token length context window.\n\nFind out more about Flash's [availability and pricing](https://blog.google/technology/developers/gemini-gemma-developer-updates-may-2024/)\n",
    "## Committing and deploying UX improvements\n\nWe've made some improvements to the user experience around committing and deploying changes to your evaluators, tools and datasets.\n\nNow, each editor has a consistent and reliable loading and saving experience. You can choose prior versions in the dropdown, making it easier to toggle between versions.\n\nAnd, as you commit, you'll also get the option to immediately deploy your changes. This reduces the number of steps needed to get your changes live.\n\nAdditional bug fixes:\n\n- Fixed the flickering issue on the datasets editor\n- Fixed the issue where the evaluator editor would lose the state of the debug drawer on commit.\n",
    "## Claude 3.5 Sonnet support\n\nClaude 3.5 Sonnet is now in Humanloop!\n\nSonnet is the latest and most powerful model from Anthropic.\n\n**2x the speed, 1/5th the cost, yet smarter than Claude 3 Opus.**\n\nAnthropic have now enabled streaming of tool calls too, which is supported in Humanloop now too.\n\nAdd your Anthropic key and select Sonnet in the Editor to give it a go.\n\n![Sonnet](file:bd15e98e-9047-4c4e-9930-43d6ea614309)\n",
    "## Prompt and Tool version drawer in Evaluation reports\n\nYou can now click on the Prompt and Tool version tags within your Evaluation report to open a drawer with details. This helps provide the additional context needed when reasoning with the results without having to navigate awa\n\n![Prompt drawer in Evaluation report](file:8a3b49ec-a587-4491-9789-ee5f6fe8f2d4)\n",
    "## Status of Human Evaluators\n\nWith Humanloop Evaluation Reports, you can leverage multiple Evaluators for comparing your Prompt and Tool variations. Evaluators can be of different types: code, AI or Human and the progress of the report is dependent on collecting all the required judgements. Human judgments generally take longer than the rest and are collected async by members of your team.\n\n![Human Evaluators](file:1ac21e49-0632-41e8-bc50-074a864eec35)\n\nTo better support this workflow, we've improved the UX around monitoring the status of judgments, with a new progress bar. Your Human Evaluators can now also update the status of the report when they're done.\n\n![Human Evaluators](file:a029a4a3-fbec-464e-813d-012afe716bed)\n\nWe've also added the ability to cancel ongoing Evaluations that are pending or running. Humanloop will then stop generating Logs and running Evaluators for this Evaluation report.\n",
    "## Faster Evaluations\n\nFollowing the recent upgrades around Evaluation reports, we've improved the batching and concurrency for both calling models and getting the evaluation results. This has increased the speed of Evaluation report generation by 10x and the reports now update as new batches of logs and evaluations are completed to give a sense of intermediary progress.\n",
    "## Evaluation Comparison Reports\n\nWe've released Evaluation reports, which allows you to easily compare the performance of your different Prompts or Tools across multiple different [Evaluator](/docs/evaluators) criteria.\n\nThis generalises our previous concept of Evaluation runs, extending it with multiple complimentary changes with getting more from your evals. All your existing Evaluation runs have been migrated to Evaluation reports with a single evaluated Prompt or Tool. You can easily extend these existing runs to cover additional Evaluators and Prompts/Tools with out having to regenerate existing logs.\n\n<img src=\"file:58431b82-a567-4519-a6ae-dbf482f7e2bc\" />\n\n### Feature breakdown\n\nWe've introduced a new **stats comparison view**, including a radar chart that gives you a quick overview of how your versions compare across all Evaluators. Below it, your evaluated versions are shown in columns, forming a grid with a row per Evaluator you've selected.\n\nThe performance of each version for a given Evaluator is shown in a chart, where bar charts are used for boolean results, while box plots are used for numerical results providing an indication of variance within your Dataset.\n\nEvaluation reports also introduce an **automatic deduplication** feature, which utilizes previous logs to avoid generating new logs for the same inputs. If a log already exists for a given evaluated-version-and-datapoint pair, it will automatically be reused. You can also override this behavior and force the generation of new logs for a report by creating a **New Batch** in the setup panel.\n\n<img src=\"file:bd475dc7-fa92-40f7-9ac3-838f2121e24f\" />\n\n### How to use Evaluation reports\n\nTo get started, head over to the Evaluations tab of the Prompt you'd like to evaluate, and click **Evaluate** in the top right.\n\nThis will bring you to a page where you can set up your Evaluation, choosing a Dataset, some versions to Evaluate and compare, and the Evaluators you'd like to use.\n\n![](file:4e16a5f6-d748-46ef-b191-6a79e973cff2)\n\nWhen you click **Save**, the Evaluation report will be created, and any missing Logs will be generated.\n\n### What's next\n\nWe're planning on improving the functionality of Evaluation reports by adding a more comprehensive detailed view, allowing you to get a more in-depth look at the generations produced by your Prompt versions. Together with this, we'll also be improving Human evaluators so you can better annotate and aggregate feedback on your generations.\n",
    "## Azure Model Updates\n\nYou can now access the latest versions of GPT-4 and GPT-4o hosted on Azure in the Humanloop Editor and via our Chat endpoints.\n\nOnce you've configured your Azure key and endpoint in your organization's provider settings, the model versions will show up in the Editor dropown as follows:\n\nFor more detail, please see the [API documentation](https://docs.humanloop.com/reference/logs_list) on our Logs endpoints.\n\n![](file:f2620b93-d55d-4e02-999b-6f5f6b98eeff)\n",
    "## Improved Logs Filtering\n\nWe've improved the ability to filter logs by time ranges. The API logs filter parameters for `start_date` and `end_date` now supports querying with more granularity. Previously the filters were limited to dates, such as **2024-05-22**, now you can use hourly ranges as well, such as **2024-05-22 13:45**.\n\nFor more detail, please see the [API documentation](https://docs.humanloop.com/reference/logs_list) on our Logs endpoints.\n",
    '## Monitoring with deployed Evaluators\n\nYou can now connect deployed Evaluator versions for online monitoring of your Prompts and Tools.\n\nThis enables you to update Evaluators for multiple Prompt or Tools when you deploy a new Evaluator version.\n\n<img src="file:3c2908ea-1e73-4b6f-8fd4-c0163b329dd1" />\n',
    '## GPT-4o\n\nSame day support for OpenAIs new GPT4-Omni model! You can now use this within the Humanloop Editor and chat APIs.\n\nFind out more from OpenAI [here](https://openai.com/index/hello-gpt-4o/).\n\n<img src="file:a88ca7ab-b189-4aab-b9b6-1d4f76552ebc" />\n',
    '## Logs for Evaluators\n\nFor AI and Code Evaluators, you can now inspect and reference their logs as with Prompts and Tools. This provides greater transparency into how they are being used and improves the ability to debug and improve.\n\nFurther improvements to Human Evaluators are coming very soon...\n\n<img src="file:eb776aca-ac45-4044-8cbc-bd136ad22507"\n\nalt="Creating a new Evaluator file" />\n',
    '## Improved Evaluator management\n\nEvaluators are now first class citizens alongside Prompts, Tools and Datasets. This allows for easier re-use, version control and helps with organising your workspace within directories.\n\nYou can create a new Evaluator by choosing **Evaluator** in the File creation dialog in the sidebar or on your home page.\n\n<img src="file:75b1d931-b397-4cd8-b39f-12b7bfbe6478" alt="Creating a new Evaluator file" />\n\n### Migration and backwards compatibility\n\nWe\'ve migrated all of your Evaluators previously managed within **Prompts > Evaluations > Evaluators** to new Evaluator files. All your existing Evaluation runs will remain unchanged and online Evaluators will continue to work as before. Moving forward you should use the new Evaluator file to make edits and manage versions.\n',
    '## Log drawer in Editor\n\nYou can now open up the Log drawer directly in the Editor.\n\nThis enables you to see exactly what was sent to the provider as well as the tokens used and cost. You can also conveniently add feedback and run evaluators on that specific Log, or add it to a dataset.\n\nTo show the Logs just click the arrow icon beside each generated message or completion.\n\n<img src="file:66a272a7-d845-49db-9418-3c476bc8bdbd" />\n\n\n<img src="file:fd8507e7-ed89-413f-ab9e-bffa15009209" />\n',
    '## Groq support (Beta)\n\nWe have introduced support for models available on Groq to Humanloop. You can now try out the blazingly fast generations made with the open-source models (such as Llama 3 and Mixtral 8x7B) hosted on Groq within our Prompt Editor.\n\n<img src="file:c9e9ce00-485f-4746-9dd3-1e13805e9dc7" />\n\n\nGroq achieves [faster throughput](https://artificialanalysis.ai/models/llama-3-instruct-70b/providers)  using specialized hardware, their LPU Inference Engine. More information is available in their [FAQ](https://wow.groq.com/why-groq/) and on their website.\n\n<br />\n\nNote that their API service, GroqCloud, is still in beta and low rate limits are enforced.\n',
    '## Llama 3\n\n[Llama 3](https://llama.meta.com/llama3/), Meta AI\'s latest openly-accessible model, can now be used in the Humanloop Prompt Editor. \n\nLlama 3 comes in two variants: an 8-billion parameter model that performs similarly to their previous 70-billion parameter Llama 2 model, and a new 70-billion parameter model. Both of these variants have an expanded context window of 8192 tokens. \n\nMore details and benchmarks against other models can be found on their [blog post](https://ai.meta.com/blog/meta-llama-3/) and [model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).\n\nHumanloop supports Llama 3 on the Replicate model provider, and on the newly-introduced Groq model provider.\n\n<img src="file:f3958ca2-1eaf-465b-a7af-38e36ac03d7f" />\n',
    '## Anthropic tool support (Beta)\n\nOur Editor and deployed endpoints now supports tool use with the Anthropic\'s Claude3 models. Tool calling with Anthropic is still in Beta, so streaming is not important.\n\nIn order to user tool calling for Claude in Editor you therefore need to first turn off streaming mode in the menu dropdown to the right of the load button.\n\n<img src="file:cc4c1cf4-6dc2-43f1-bb05-39471ff7abe9" />\n',
    '## Cost, Tokens and Latency\n\nWe now compute Cost, Tokens and Latency for all Prompt logs by default across all model providers.\n\nThese values will now appear automatically as graphs in your Dashboard, as columns in your logs table and will be displayed in our Version and Log drawers.\n\n<img src="file:1e26371c-2c3c-4587-a157-15918e63a4a8" />\n',
    '## Cohere Command-r\n\nWe\'ve expanded the Cohere models with the latest command-r suite. You can now use these models in our Editor and via our APIs once you have set your Cohere API key.\n\nMore details can be found on their [blog post](https://cohere.com/blog/command-r-plus-microsoft-azure).\n\n<img src="file:00cb93d3-7ff2-4261-9868-aac6e420e776" />\n',
    '## Dataset Files & Versions\n\nIn our recent release, we promoted **Datasets** from being attributes managed within the context of a single Prompt, to a **first-class Humanloop file type** alongside Prompts and Tools.\n\n<img src="file:409176c2-3574-4700-8e55-adb1b530640b" />\n\nThis means you can curate Datasets and share them for use across any of the Prompts in your organization. It also means you get the full power of our **file versioning system**, allowing you **track and commit every change** you make Datasets and their Datapoints, with attribution and commit messages inspired by Git.\n\n<img src="file:36fca146-3bfc-4cd6-9a36-2de920b66c0e" />\n\nIt\'s now easy to understand which version of a Dataset was used in a given Evaluation run, and whether the most recent edits to the Dataset were included or not.\n\nRead more on how to get started with datasets [here](/docs/datasets).\n\nThis change lays the foundation for lots more improvements we have coming to Evaluations in the coming weeks. Stay tuned!\n',
    '## Mixtral 8x7B\n\nKeeping you up to date with the latest open models, we\'ve added support for Mixtral 8x7B to our Editor with a [Replicate integration](https://replicate.com/).\n\n<img src="file:03830ee9-4762-4277-bc07-b33b366134d6" />\n\n\nMixtral 8x7B outperforms LLaMA 2 70B (already supported in Editor) with faster inference, with performance comparable to that of GPT-3.5. More details are available in its [release announcement](https://mistral.ai/news/mixtral-of-experts/).\n\n## Additional Replicate models support via API\n\nThrough the Replicate model provider additional open models can be used by specifying a model name via the API. The model name should be of a similar form as the ref used when calling `replicate.run(ref)` using [Replicate\'s Python SDK](https://github.com/replicate/replicate-python).\n\nFor example, Vicuna, an open-source chatbot model based on finetuning LLaMA can be used with the following model name alongside `provider: "replicate"` in your Prompt version.  \n`replicate/vicuna-13b:6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b`\n',
    '## Surfacing uncommitted Versions\n\nWe now provide the ability to access your uncommitted Prompt Versions and associated Logs.\n\nAdding to our recent changes around the [Commit flow for Versions](https://docs.humanloop.com/changelog/prompts-and-committing-prompt-versions), we\'ve added the ability to view any uncommitted versions in your Versions and Logs tables. This can be useful if you need to recover or compare to a previous state during your Prompt engineering and Evaluation workflows.\n\nUncommitted Versions are created when you make generations in our Editor without first committing what you are working on. In future, it will also be possible to create uncommitted versions when logging or generating using the API. \n\nWe\'ve added new filter tabs to the Versions and Logs table to enable this:\n\n<img src="file:e3fc8e10-e238-4489-b79c-a89bb2a325cf" alt="New **All** and From **Committed By Versions** filter tabs on the logs table." />\n\n\n<img src="file:96f19c0d-9c61-49c2-bc42-d056f14f7143" alt="New **Committed** and **Uncommitted** tabs on the Versions table of your Prompt dashboard." />\n',
    "## Improved navigation & sidebar\n\nWe've introduced a sidebar for easier navigation between your Prompts and Tools. \n\nAs new language models unlock more complex use cases, you'll be setting up and connecting Prompts, Tools, and Evaluators. The new layout better reflects these emerging patterns, and switching between your files is now seamless with the directory tree in the sidebar.\n\n![](file:2e5e8b35-ad6a-4025-858e-7f7f328ac90d)\n\nYou can also bring up the search dialog with **Cmd+K** and switch to another file using only your keyboard.\n",
    '## Claude 3\n\nIntroducing same day support for the Claude 3 - Anthropics new industry leading models. Read more about the release [here](https://www.anthropic.com/news/claude-3-family).\n\nThe release contains three models in ascending order of capability: _Haiku_, _Sonnet_, and _Opus_. This suite provides users with the different options to balance intelligence, speed, and cost for their specific use-cases.\n\n<img src="file:54b5cb77-cdb9-4111-bcca-42e22ad39e38" />\n\n\n## **Key take aways**\n\n1. **Performance** - a new leader. The largest of the 3 models, Opus, is claimed to outperform GPT-4 and Gemini Ultra on key benchmarks such as MMLU and Hellaswag. It even reached 84.9% on the Humaneval coding test set (vs GPT-4’s 67%) 🤯\n2. **200k context window** with near-perfect recall on selected benchmarks. Opus reports 99% accuracy on the NIAH test, which measures how accurately a model can recall information given to it in a large corpus of data.\n3. **Opus has vision**. Anthropic claim that performance here is on par with that of other leading models (ie GPT-4 and Gemini). They say it’s most useful for inputting graphs, slides etc. in an enterprise setting.\n4. **Pricing** - as compared to OpenAI:\n\nOpus - $75 (2.5x GPT-4 Turbo)  \nSonnet - $15 (50% of GPT-4 Turbo)  \nHaiku - $1.25 (1.6x GPT-3.5)\n\n5. **How you can use it**: The Claude 3 family is now available on Humanloop. Bring your API key to test, evaluate and deploy the publicly available models - Opus and Sonnet.\n',
    '## New Tool creation flow\n\nYou can now create Tools in the same way as you create Prompts and Directories. This is helpful as it makes it easier to discover Tools and easier to quickly create new ones. \n\n![](file:f7e9da40-0b4d-4a59-a6da-b249b5aae020)\n\nTo create a new Tool simply press the New button from the directory of your choice and select one of our supported Tools, such as JSON Schema tool for function calling or our Pinecone tool to integrate with your RAG pipelines.\n\n## Tool editor and deployments\nYou can now manage and edit your Tools in our new Tool Editor. This is found in each Tool file and lets you create and iterate on your tools. As well, we have introduced deployments to Tools, so you can better control which versions of a tool are used within your Prompts.\n\n![](file:142bac55-4e58-40b1-ab6a-e96f62007c7a)\n\n### Tool Editor\n\nThis replaces the previous Tools section which has been removed. The editor will let you edit  any of the tool types that Humanloop supports (JSON Schema, Google, Pinecone, Snippet, Get API) and commit new Versions. \n\n![](file:e804007d-24d2-4372-a26a-03aed418addc)\n\n### Deployment\n\nTools can now be deployed. You can pick a version of your Tool and deploy it. When deployed it can be used and referenced in a Prompt editor.\n\nAnd example of this, if you have a version of a Snippet tool with the signature `snippet(key)` with a key/value pair of "_helpful_"/"_You are a helpful assistant_". You decide you would rather change the value to say "_You are a funny assistant_", you can commit a version of the Tool with the updated key. This wont affect any of your prompts that reference the Snippet tool until you Deploy the second version, after which each prompt will automatically start using the funny assistant prompt.\n\n## Prompt labels and hover cards\n\nWe\'ve rolled out a unified label for our Prompt Versions to allow you to quickly identify your Prompt Versions throughout our UI. As we\'re rolling out these labels across the app, you\'ll have a consistent way of interacting with and identifying your Prompt Versions.\n\n<img src="file:e9d84285-a257-44cf-adbb-ee1869f1e324" alt="Label and hover card for a deployed Prompt Version" />\n\n\nThe labels show the deployed status and short ID of the Prompt Version. When you hover over these labels, you will see a card that displays the commit message and authorship of the committed version.\n\nYou\'ll be able to find these labels in many places across the app, such as in your Prompt\'s deployment settings, in the Logs drawer, and in the Editor.\n\n<img src="file:c3de0a56-a34d-4f75-ac6c-a6487e7dc8d1" alt="The Prompt Version label and hover card in a Prompt Editor" />\n\n\nAs a quick tip, the color of the checkmark in the label indicates that this is a version that has been deployed. If the Prompt Version has not been deployed, the checkmark will be black. \n\n<img src="file:f1e84d10-b0c2-48fe-bc5e-5a3e33ad5563" alt="A Prompt Version that has not been deployed" />\n\n## Committing Prompt Versions\n\nBuilding on our terminology improvements from Project -> Prompt, we\'ve now updated Model Configs -> Prompt Versions to improve consistency in our UI. \n\nThis is part of a larger suite of changes to improve the workflows around how entities are managed on Humanloop and to make them easier to work with and understand. We will also be following up soon with a new and improved major version of our API that encapsulates all of our terminology improvements.\n\nIn addition to just the terminology update, we\'ve improved our Prompt versioning functionality to now use `commits` that can take `commit messages`, where you can describe how you\'ve been iterating on your Prompts. \n\nWe\'ve removed the need for names (and our auto-generated placeholder names) in favour of using explicit commit messages.  \n\n<img src="file:e36a8906-128b-4b2d-ade7-d7c8cc41598b" />\n\n\nWe\'ll continue to improve the version control and file types support over the coming weeks. \n\nLet us know if you have any questions around these changes!\n',
    '## Online evaluators for monitoring Tools\n\nYou can now use your online evaluators for monitoring the logs sent to your Tools. The results of this can be seen in the graphs on the Tool dashboard as well as on the Logs tab of the Tool.\n\n![](file:89780614-f804-469e-8140-bdebd2faef35)\n\nTo enable Online Evaluations follow the steps seen in our [Evaluate models online](/docs/guides/evaluate-models-online) guide.\n\n## Logging token usage\n\nWe\'re now computing and storing the number of tokens used in both the requests to and responses from the model.\n\nThis information is available in the logs table UI and as part of the [log response](/api-reference/logs/get) in the API. Furthermore you can use the token counts as inputs to your code and LLM based evaluators.\n\nThe number of tokens used in the request is called `prompt_tokens` and the number of tokens used in the response is called `output_tokens`.\n\nThis works consistently across all model providers and whether or not you are you are streaming the responses. OpenAI, for example, do not return token usage stats when in streaming mode.\n\n<img src="file:29df692f-35d5-4982-9f6e-0fa520008ee2" />\n',
    '## Prompt Version authorship\n\nYou can now view who authored a Prompt Version. \n\n<img src="file:d4a90834-cbb2-4dea-b01e-2603b97e1174" alt="Prompt Version authorship in the Prompt Version slideover" />\n\n\nWe\'ve also introduced a popover showing more Prompt Version details that shows when you mouseover a Prompt Version\'s ID.\n\n<img src="file:9779719d-bcb3-482e-99d3-7888f9a4a253" alt="Prompt Version popover in the Logs slideover" />\n\n\nKeep an eye out as we\'ll be introducing this in more places across the app.\n',
    '## Filterable and sortable evaluations overview\n\nWe\'ve made improvements to the evaluations runs overview page to make it easier for your team to find interesting or important runs.\n\n![](file:e53375a6-2e9e-4c41-9d1c-7ca976e91a6a)\n\nThe charts have been updated to show a single datapoint per run. Each chart represents a single evaluator, and shows the performance of the prompt tested in that run, so you can see at a glance how the performance your prompt versions have evolved through time, and visually spot the outliers. Datapoints are color-coded by the dataset used for the run.\n\nThe table is now paginated and does not load your entire project\'s list of evaluation runs in a single page load. The page should therefore load faster for teams with a large number of runs.\n\nThe columns in the table are now filterable and sortable, allowing you to - for example - filter just for the completed runs which test two specific prompt versions on a specific datasets, sorted by their performance under a particular evaluator.\n\n<img src="file:a2631602-d4ae-4664-a8a6-d4aab612a28f" alt="Here, we\'ve filtered the table on completed runs that tested three specific prompt versions of interest, and sorted to show those with the worst performance on the Valid JSON evaluator." />\n',
    "## Projects rename and file creation flow\n\nWe've renamed `Projects` to `Prompts` and `Tools` as part of our move towards managing `Prompts`, `Tools`, `Evaluators` and `Datasets` as special-cased and strictly versioned files in your Humanloop directories. \n\nThis is a purely cosmetic change for now. Your Projects (now Prompts and Tools) will continue to behave exactly the same. This is the first step in a whole host of app layout, navigation and API improvements we have planned in the coming weeks. \n\nIf you are curious, please reach out to learn more.\n\n<img src=\"file:4afbd7d2-0cb3-4a6a-b747-47f85ed8a48a\" />\n\n**New creation flow**\n\nWe've also updated our file creation flow UI. When you go to create projects you'll notice they are called Prompts now.\n\n![](file:e2dd0ea6-ef34-4c0d-9326-cecb4a651546)\n",
    '## Control logging level\n\nWe\'ve added a `save` flag to all of our endpoints that generate logs on Humanloop so that you can control whether the request and response payloads that may contain sensitive information are persisted on our servers or not.\n\nIf `save` is set to `false` then no `inputs`, `messages` our `outputs` of any kind (including the raw provider request and responses) are stored on our servers. This can be helpful for sensitive use cases where you can\'t for example risk PII leaving your system.\n\nDetails of the model configuration and any metadata you send are still stored. Therefore you can still benefit from certain types of evaluators such as human feedback, latency and cost, as well as still track important metadata over time that may not contain sensitive information.\n\nThis includes all our [chat](/api-reference/chats/create) and [completion](/api-reference/completions/create) endpoint variations, as well as our explicit [log](/api-reference/logs/log) endpoint.\n\n```python\nfrom humanloop import Humanloop\n\n# You need to initialize the Humanloop SDK with your API Keys\nhumanloop = Humanloop(api_key="<YOUR Humanloop API KEY>")\n\n# humanloop.complete_deployed(...) will call the active model config on your project.\n# You can optionally set the save flag to False\ncomplete_response = humanloop.complete_deployed(\n  \tsave=False,\n    project="<YOUR UNIQUE PROJECT NAME>",\n    inputs={"question": "I have inquiry about by life insurance policy. Can you help?"},\n)\n\n# You can still retrieve the data_id and output as normal\ndata_id = complete_response.data[0].id\noutput = complete_response.data[0].output\n\n# And log end user feedback that will still be stored\nhumanloop.feedback(data_id=data_id, type="rating", value="good")\n\n\n```\n\n## Logging provider request\n\nWe\'re now capturing the raw provider request body alongside the existing provider response for all logs generated from our [deployed endpoints](/docs/guides/chat-using-the-sdk).\n\nThis provides more transparency into how we map our provider agnostic requests to specific providers. It can also effective for helping to troubleshoot the cases where we return well handled provider errors from our API.\n\n<img src="file:df73e41f-b803-4db8-8632-0684d358cfe1" />\n',
    "## Add Evaluators to existing runs\n\nYou can now add an evaluator to any existing evaluation run. This is helpful in situations where you have no need to regenerate logs across a dataset, but simply want to run new evaluators across the existing run. By doing this instead of launching a fresh run, you can the save significant time & costs associated with unnecessarily regenerating logs, especially when working with large datasets.\n\n<img src=\"file:7305ebd6-06b2-4bb6-88bc-bc1514ab34b7\" alt=\"Use the **Add Evaluator** button to run more evaluators across the logs in an existing evaluation run. This can be done on any runs, including those still running or already completed.\" />\n\n## Improved Evaluation Debug Console\n\nWe've enhanced the usability of the debug console when creating and modifying evaluators. Now you can more easily inspect the data you are working with, and understand the root causes of errors to make debugging quicker and more intuitive.\n\n![](file:bc536dc8-391c-443b-b62e-1829648f8b13)\n\nOn any row in the debug console, click the arrow next to a testcase to inspect the full entity in a slideover panel.\n\nAfter clicking **Run** to generate a log from a testcase, you can inspect the full log right from the debug console, giving you clearer access to error messages or the model-generated content, as in the example below.\n\n![](file:60ec0a1b-d147-47c9-adaa-0fc56fd9f768)\n\n## LLM Evaluators\n\nWe expect this feature to be most useful in the case of creating and debugging LLM evaluators. You can now inspect the log of the LLM evaluation itself right from the debug console, along with the original testcase and model-generated log, as described above.\n\nAfter clicking **Run** on a testcase in the debug console, you'll see the **LLM Evaluation Log** column populated with a button that opens a full drawer.\n\n![](file:1293952e-c745-4430-b4a5-583dfc03f136)\n\nThis is particularly helpful to verify that your evaluation prompt was correctly populated with data from the underlying log and testcase, and to help understand why the LLM's evaluation output may not have been parsed correctly into the output values.\n\n![](file:8820da4f-19cd-435a-ac15-d291476113e3)\n\n## Tool projects\n\nWe have upgraded projects to now also work for tools. Tool projects are automatically created for tools you define as part of your model config [in the Editor](/docs/guides/create-a-tool-in-the-editor) as well as tools [managed at organization level](/docs/guides/link-a-jsonschema-tool).\n\nIt is now easier to access the logs from your tools and manage different versions like you currently do for your prompts.\n\n![](file:86338da3-06f9-4f9e-a6e3-8827986f0dab)\n\n### Tool versioning\n\nIn the dashboard view, you can see the different versions of your tools. This will soon be expanded to link you to the source config and provide a more comprehensive view of your tool's usage.\n\n### Logs\n\nAny logs submitted via the SDK that relate to these tools will now appear in the Logs view of these projects. You can see this by following our [sessions guide](https://dash.readme.com/project/humanloop/v4.0/docs/logging-session-traces) and logging a new tool via the SDK. This also works natively with online Evaluators, so you can start to layer in observability for the individual non-LLM components of your session\n\n### Offline Evaluations via SDK\n\nYou can trigger evaluations on your tools projects similar to how you would for an LLM project with model configs. This can be done by logging to the tool project, creating a dataset, and triggering an evaluation run. A good place to start would be the [Set up evaluations using API](/docs/guides/evaluations-using-api) guide.\n\n## Support for new OpenAI Models\n\nFollowing [OpenAI's latest model releases](https://openai.com/blog/new-embedding-models-and-api-updates), you will find support for all the latest models in our **Playground** and **Editor**.\n\n### GPT-3.5-Turbo and GPT-4-Turbo\n\nIf your API key has access to the models, you'll see the new release `gpt-4-0125-preview` and `gpt-3.5-turbo-0125` available when working in Playground and Editor. These models are more capable and cheaper than their predecessors - see the OpenAI release linked above for full details.\n\n![](file:cb05ed41-c3ec-4980-8199-7d5a977f8b2c)\n\nWe also support the new `gpt-4-turbo-preview` model alias, which points to the latest `gpt-4-turbo` model without specifying a specific version.\n\n### Embedding Models\n\nFinally, the new embedding models - `text-embedding-3-small` and `text-embedding-3-large` are also available for use via Humanloop. The `small` model is 5x cheaper than the previous generation `ada-002` embedding model, while the larger model significantly improves performance and maps to a much larger embedding space.\n",
    "## Improved evaluation run launcher\n\nWe've made some usability enhancements to the launch experience when setting up batch generation & evaluation runs. \n\nIt's now clearer which model configs, datasets and evaluators you've selected. It's also now possible to specify whether you want the logs to be generated in the Humanloop runtime, or if you're going to post the logs from your own infrastructure via the API.\n\n![](file:6d04a0c2-3485-40d9-bbfb-04e9516b942d)\n\n### Cancellable evaluation runs\n\nOccasionally, you may launch an evaluation run and then realise that you didn't configure it quite the way you wanted. Perhaps you want to use a different model config or dataset, or would like to halt its progress for some other reason. \n\nWe've now made evaluation runs cancellable from the UI - see the screenshot below. This is especially helpful if you're running evaluations over large datasets, where you don't want to unnecessarily consume provider credits. \n\n<img src=\"file:cf59f8f6-1b46-4042-aa33-c2a2e179c049\" alt=\"Cancellation button in the evaluation run page.\" />\n",
    "## Faster offline evaluations\n\nWe've introduced batching to our offline Evaluations to significantly speed up runtime performance and also improved the robustness to things going wrong mid-run.\n\nIn addition to our recent [enhancements to the Evaluations API](changelog:evaluation-api-enhancements), we've also made some significant improvements to our underlying orchestration framework which should mean your evaluation runs are now faster and more reliable. In particular, we now **batch generations** across the run - by default in groups of five, being conscious of potential rate limit errors (though this will soon be configurable). \n\nEach batch runs its generations concurrently, so you should see much faster completion times - especially in runs across larger datasets.\n",
    '## Evaluation API enhancements\n\nWe\'ve started the year by enhancing our evaluations API to give you more flexibility for self-hosting whichever aspects of the evaluation workflow you need to run in your own infrastructure - while leaving the rest to us!\n\n### Mixing and matching the Humanloop-runtime with self-hosting\n\nConceptually, evaluation runs have two components:\n\n1. Generation of logs for the datapoints using the version of the model you are evaluating.\n2. Evaluating those logs using Evaluators.\n\nNow, using the Evaluations API, Humanloop offers the ability to generate logs either within the Humanloop runtime, or self-hosted (see our [guide on external generations for evaluations](/docs/guides/evaluating-externally-generated-logs)).\n\nSimilarly, evaluating of the logs can be performed in the Humanloop runtime (using evaluators that you can define in-app), or self-hosted (see our [guide on self-hosted evaluations](/docs/guides/self-hosted-evaluations)).\n\nIt is now possible to mix-and-match self-hosted and Humanloop-runtime logs and evaluations in any combination you wish.\n\nWhen creating an Evaluation (via the improved UI dialogue or via the API), you can set the new `hl_generated` flag to `False` to indicate that you are posting the logs from your own infrastructure. You can then also include an evaluator of type `External` to indicate that you will post evaluation results from your own infrastructure.\n\n<img src="file:d0ed2a50-4e12-46ce-834a-1ca6504ec313" />\n\nYou can now also include multiple evaluators on any run, and these can include a combination of `External` (i.e. self-hosted) and Humanloop-runtime evaluators.\n',
    "## Human Evaluators\n\nWe've introduced a new special type of 'Human' Evaluator to compliment our existing code and AI based Evaluators.\n\nThere are many important evaluation use cases that require input from your internal domain experts, or product teams. Typically this is where you would like a gold standard judgement of how your LLM app is performing.\n\n<img src=\"file:6e61784d-b6d8-44d0-8dd7-3afaae98408f\" />\n\nOur new Human Evaluator allows you to trigger a batch evaluation run as normal (from our UI as part of your prompt engineering process, or using our SDK as part of your CI/CD pipeline) and then queues the results ready for a human to provide feedback.\n\nOnce completed, the feedback is aggregated to give a top-line summary of how the model is performing. It can also be combined with automatic code and AI evaluators in a single run.\n\n<img src=\"file:0b561f49-e625-4a03-a752-29e0fdb355e3\" />\n\nSet up your first Human Evaluator run by following [our guide.](/docs/guides/evaluating-with-human-feedback)\n\n## Return inputs flag\n\nWe've introduced a `return_inputs` flag on our chat and completion endpoints to improve performance for larger payloads.\n\nAs context model windows get increasingly larger, for example [Claude with 200k tokens](https://www.anthropic.com/index/claude-2-1), it's important to make sure our APIs remain performant. A contributor to response times is the size of the response payload being sent over the wire.\n\nWhen you set this new flag to false, our responses will no longer contain the `inputs` that were sent to the model and so can be significantly smaller. This is the first in a sequence of changes to add more control to the caller around API behaviour.\n\nAs always, we welcome suggestions, requests, and feedback should you have any.\n\n## Gemini\n\nYou can now use Google's latest LLMs, [Gemini](https://blog.google/technology/ai/google-gemini-ai/), in Humanloop.\n\n### Setup\n\nTo use Gemini, first go to [https://makersuite.google.com/app/apikey](https://makersuite.google.com/app/apikey) and generate an API key. Then, save this under the \"Google\" provider on [your API keys page](http://app.humanloop.com/account/api-keys).\n\nHead over to the playground, and you should see `gemini-pro` and `gemini-pro-vision` in your list of models.\n\n<img src=\"file:d0026061-38be-48ab-be78-51a287d55303\" />\n\nYou can also now use Gemini through the Humanloop API's `/chat`endpoints.\n\n### Features\n\nGemini offers support for multi-turn chats, tool calling, and multi-modality.\n\nHowever, note that while `gemini-pro` supports multi-turn chats and tool calling, it does not support multi-modality. On the other hand, `gemini-pro-vision` supports multi-modality but not multi-turn chats or tool calling. Refer to [Gemini's docs](https://ai.google.dev/models/gemini) for more details.\n\nWhen providing images to Gemini, we've maintained compatibility with OpenAI's API. This means that when using Humanloop, you can provide images either via a HTTP URL or with a base64-encoded data URL.\n",
    '## Chat sessions in Editor\n\nYour chat messages in Editor are now recorded as part of a session so you can more easily keep track of conversations.\n\n<img src="file:0b4c2865-dabe-4d7c-a88b-dcc7a2fe3ecd" />\n\nAfter chatting with a saved prompt, go to the sessions tab and your messages will be grouped together.\n\nIf you want to do this with the API, it can be as simple as setting the `session_reference_id`– see [docs on sessions](/docs/guides/logging-session-traces).\n',
    '## Environment logs\n\nLogs for your deployed prompts will now be tagged with the corresponding [environment](/docs/guides/deploy-to-an-environment).\n\nIn your logs table, you can now filter your logs based on environment:\n\n<img src="file:60eed402-cf7c-4bb0-bac1-1f85823f4c4e" />\n\nYou can now also pass an `environment` tag when using the explicit [/log ](/api-reference/logs/log) endpoint; helpful for use cases such as [orchestrating your own models](/docs/guides/use-your-own-model-provider).\n',
    "## Improved Evaluator UI\n\nWe've improved the experience of creating and debugging your evaluators.\n\nNow that you can [access any property of the objects you're testing](/api-reference/changelog#llm-evals---improved-data-access) we've cleaned up the debug panel to make easier to view the testcases that you load from a dataset or from your projects.\n\n<img src=\"file:0d6b4f03-f20f-437c-afd3-b1706e854408\" />\n\nWe've also clarified what the return types are expected as you create your evaluators.\n\n## Prompt diffs\n\nFollowing our recent [introduction of our .prompt file](/docs/guides/prompt-file-format), you can now compare your model configs within a project with our new 'diff' view.\n\n![](file:64028d2c-48e6-4df9-a22e-326ca5565a1e)\n\nAs you modify and improve upon your model configs, you might want to remind yourself of the changes that were made between different versions of your model config. To do so, you can now select 2 model configs in your project dashboard and click **Compare** to bring up a side-by-side comparison between them. Alternatively, open the actions menu and click **Compare to deployed**.\n\n<img src=\"file:97da97f9-039e-4c7b-a050-ad10a5fd69be\" />\n\nThis diff compares the .prompt files representing the two model configs, and will highlight any differences such as in the model, hyperparameters, or prompt template.\n\n## LLM evals - improved data access\n\nIn order to help you write better LLM evaluator prompts, you now have finer-grained access to the objects you are evaluating.\n\nIt's now possible to access any part of the `log` and `testcase` objects using familiar syntax like `log.messages[0].content`. Use the debug console to help understand what the objects look like when writing your prompts.\n\n![](file:dfc85d34-3eb9-40dc-a462-446c7a7f5ecc)\n",
    "## Tool linking\n\nIt's now possible to manage tool definitions globally for your organization and re-use them across multiple projects by linking them to your model configs.\n\nPrior to this change, if you wanted to re-use the same tool definition across multiple model configs, you had to copy and paste the JSON schema snippet defining the name, description and parameters into your Editor for each case. And if you wanted to make changes to this tool, you would have to recall which model configs it was saved to prior and update them inline 1 by 1.\n\nYou can achieve this tool re-use by first defining an instance of our new `JsonSchema` tool available as another option in your global `Tools` tab. Here you can define a tool once, such as `get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')`, and then link that to as many model configs as you need within the Editor as shown below.\n\nImportantly, updates to the `get_current_weather` `JsonSchema` tool defined here will then propagate automatically to all the model configs you've linked it to, without having to publish new versions of the prompt.\n\nThe old behaviour of defining the tool inline as part of your model config definition is still available for the cases where you do want changes in the definition of the tool to lead to new versions of the model-config.\n\n## Set up the tool\n\nNavigate to the [tools tab](https://app.humanloop.com/hl-test/tools) in your organisation and select the JsonSchema tool card.\n\n![](file:39713272-68f1-4fa1-8c90-b2dcbec79e78)\n\nWith the dialog open, define your tool with `name`, `description`, and `parameters` values. Our guide for using [OpenAI Function Calling in the playground](/docs/guides/create-a-tool-in-the-editor) can be a useful reference in this case.\n\n## Using the tool\n\nIn the editor of your target project, link the tool by pressing the `Add Tool` button and selecting your `get_current_weather` tool.\n\n![](file:05cc0701-5fb2-4027-af5a-41a2bc153804)\n",
    "## Improved log table UI\n\nWe've updated how we show logs and datapoints in their respective tables. You can now see the stack of inputs and messages in a cleaner interface rather than having them spread into separate columns.\n\n<img src=\"file:f602c1a4-e9cb-4a58-ad32-d1d2ad281a54\" alt=\"Part of the updated Log Table. Inputs are now stacked with a more consistent and less-busy UI.\" />\n\nThere will be more updates soon to improve how logs and prompts are shown in tables and the drawers soon, so if you have ideas for improvements please let us know.\n\n## Introducing .prompt files\n\nWe're introducing a .prompt file format for representing model configs in a format that's both human-readable and easy to work with.\n\nFor certain use cases it can be helpful for engineers to also store their prompts alongside their app's source code in their favourite version control system. The .prompt file is the appropriate artefact for this.\n\nThese .prompt files can be retrieved through both the API and through the Humanloop app.\n\n### Exporting via API\n\nTo fetch a .prompt file via the API, make `POST` request to `https://api.humanloop.com/v4/model-configs/{id}/export`, where `{id}` is the ID of the model config (beginning with `config_`).\n\n### Export from Humanloop\n\nYou can also export an existing model config as a .prompt file from the app. Find the model config within the project's dashboard's table of model configs and open the actions menu by clicking the three dots. Then click **Export .prompt**. (You can also find this button within the drawer that opens after clicking on on a model config's row).\n\n<img src=\"file:1d84ac0c-ddf5-4d62-ad03-56b8a65f1c8c\" />\n\n### Editor\n\nAdditionally, we've added the ability to view and edit your model configs in a .prompt file format when in Editor. Press **Cmd-Shift-E** when in editor to swap over to a view of your .prompt file.\n\n<img src=\"file:267943d7-e82e-4682-a97a-df5a35298c7b\" />\n\nMore details on our .prompt file format are available [here](/docs/guides/prompt-file-format). We'll be building on this and making it more powerful. Stay tuned.\n",
    '## Improved RBACs\n\nWe\'ve introduced more levels to our roles based access controls (RBACs).\n\nWe now distinguish between different roles to help you better manage your organization\'s access levels and permissions on Humanloop.\n\nThis is the first in a sequence of upgrades we are making around RBACs.\n\n## Organization roles\n\nEveryone invited to the organization can access all projects currently (controlling project access coming soon).\n\nA user can be one of the following rolws:\n\n**Admin:**The highest level of control. They can manage, modify, and oversee the organization\'s settings and have full functionality across all projects.\n\n**Developer:**(Enterprise tier only) Can deploy prompts, manage environments, create and add API keys, but lacks the ability to access billing or invite others.\n\n**Member:**(Enterprise tier only) The basic level of access. Can create and save prompts, run evaluations, but not deploy. Can not see any org-wide API keys.\n\n## RBACs summary\n\nHere is the full breakdown of roles and access:\n\n| Action                         | Member | Developer | Admin |\n| :----------------------------- | :----- | :-------- | :---- |\n| Create and manage Prompts      | ✔️     | ✔️        | ✔️    |\n| Inspect logs and feedback      | ✔️     | ✔️        | ✔️    |\n| Create and manage evaluators   | ✔️     | ✔️        | ✔️    |\n| Run evaluations                | ✔️     | ✔️        | ✔️    |\n| Create and manage datasets     | ✔️     | ✔️        | ✔️    |\n| Create and manage API keys     |        | ✔️        | ✔️    |\n| Manage prompt deployments      |        | ✔️        | ✔️    |\n| Create and manage environments |        | ✔️        | ✔️    |\n| Send invites                   |        |           | ✔️    |\n| Set user roles                 |        |           | ✔️    |\n| Manage billing                 |        |           | ✔️    |\n| Change organization settings   |        |           | ✔️    |\n\n## Self hosted evaluations\n\nWe\'ve added support for managing [evaluations](/docs/guides/evaluate-your-model) outside of Humanloop in your own code.\n\nThere are certain use cases where you may wish to run your evaluation process outside of Humanloop, where the evaluator itself is defined in your code as opposed to being defined using our Humanloop runtime.\n\nFor example, you may have implemented an evaluator that uses your own custom model, or has to interact with multiple systems. In which case, it can be difficult to define these as a simple code or [LLM evaluator](/docs/guides/use-llms-to-evaluate-logs) within your Humanloop project.\n\nWith this kind of setup, our users have found it very beneficial to leverage the datasets they have curated on Humanloop, as well as consolidate all of the results alongside the prompts stored on Humanloop.\n\nTo better support this setting, we\'re releasing additional API endpoints and SDK utilities. We\'ve added endpoints that allow you to:\n\n- Retrieve your curated datasets\n- Trigger evaluation runs\n- Send evaluation results for your datasets generated using your custom evaluators\n\nBelow is a code snippet showing how you can use the latest version of the Python SDK to log an evaluation run to a Humanloop project. For a full explanation, see our [guide](/docs/guides/self-hosted-evaluations) on self-hosted evaluations.\n\n```python\nfrom humanloop import Humanloop\n\nAPI_KEY = ...\nhumanloop = Humanloop(api_key=API_KEY)\n\n# 1. Retrieve a dataset\nDATASET_ID = ...\ndatapoints = humanloop.datasets.list_datapoints(DATASET_ID).records\n\n# 2. Create an external evaluator\nevaluator = humanloop.evaluators.create(\n    name="My External Evaluator",\n    description="An evaluator that runs outside of Humanloop runtime.",\n    type="external",\n    arguments_type="target_required",\n    return_type="boolean",\n)\n# Or, retrieve an existing one:\n# evaluator = humanloop.evaluators.get(EVALUATOR_ID)\n\n# 3. Retrieve a model config\nCONFIG_ID = ...\nmodel_config = humanloop.model_configs.get(CONFIG_ID)\n\n# 4. Create the evaluation run\nPROJECT_ID = ...\nevaluation_run = humanloop.evaluations.create(\n    project_id=PROJECT_ID,\n    config_id=CONFIG_ID,\n    evaluator_ids=[EVALUATOR_ID],\n    dataset_id=DATASET_ID,\n)\n\n# 5. Iterate the datapoints and trigger generations\nlogs = []\nfor datapoint in datapoints:\n    log = humanloop.chat_model_config(\n        project_id=PROJECT_ID,\n        model_config_id=model_config.id,\n        inputs=datapoint.inputs,\n        messages=[\n            {key: value for key, value in dict(message).items() if value is not None}\n            for message in datapoint.messages\n        ],\n        source_datapoint_id=datapoint.id,\n    ).data[0]\n    logs.append((log, datapoint))\n\n# 6. Evaluate the results.\n#    In this example, we use an extremely simple evaluation, checking for an exact\n#    match between the target and the model\'s actual output.\nfor (log, datapoint) in logs:\n    # The datapoint target tells us the correct answer.\n    target = str(datapoint.target["answer"])\n\n    # The log output is what the model said.\n    model_output = log.output\n\n    # The evaluation is a boolean, indicating whether the model was correct.\n    result = target == model_output\n\n    # Post the result back to Humanloop.\n    evaluation_result_log = humanloop.evaluations.log_result(\n        log_id=log.id,\n        evaluator_id=evaluator.id,\n        evaluation_run_external_id=evaluation_run.id,\n        result=result,\n    )\n\n# 7. Complete the evaluation run.\nhumanloop.evaluations.update_status(id=evaluation_run.id, status="completed")\n\n```\n\n## Chat response\n\nWe\'ve updated the response models of all of our [/chat](/api-reference/chats/create) API endpoints to include an output message object.\n\nUp to this point, our `chat` and `completion` endpoints had a unified response model, where the `content` of the assistant message returned by OpenAI models was provided in the common `output` field for each returned sample. And any tool calls made were provided in the separate `tool_calls` field.\n\nWhen making subsequent chat calls, the caller of the API had to use these fields to create a message object to append to the history of messages. So to improve this experience we now added an `output_message` field to the chat response. This is additive and does not represent a breaking change.\n\n**Before:**\n\n```json\n{\n    "project_id": "pr_GWx6n0lv6xUu3HNRjY8UA",\n    "data": [\n        {\n            "id": "data_Vdy9ZoiFv2B7iYLIh15Jj",\n            "index": 0,\n            "output": "Well, I gotta say, ...",\n            "raw_output": "Well, I gotta say...",\n            "finish_reason": "length",\n            "model_config_id": "config_VZAPd51sJH7i3ZsjauG2Q",\n            "messages": [\n                {\n                    "content": "what\'s your best guess...",\n                    "role": "user",\n                }\n            ],\n            "tool_calls": null\n        }\n    ],\n...\n...\n...\n}\n```\n\n**After:**\n\n```json\n{\n    "project_id": "pr_GWx6n0lv6xUu3HNRjY8UA",\n    "data": [\n        {\n            "id": "data_Vdy9ZoiFv2B7iYLIh15Jj",\n\t\t\t\t\t\t"output_message": {\n                "content": "Well, I gotta say, ...",\n                "name": null,\n                "role": "assistant",\n                "tool_calls": null\n            },\n            "index": 0,\n            "output": "Well, I gotta say, ...",\n            "raw_output": "Well, I gotta say...",\n            "finish_reason": "length",\n            "model_config_id": "config_VZAPd51sJH7i3ZsjauG2Q",\n            "messages": [\n                {\n                    "content": "what\'s your best guess...",\n                    "role": "user",\n                }\n            ],\n            "tool_calls": null,\n        }\n    ],\n...\n...\n...\n}\n```\n\n## Snippet tool\n\nWe\'ve added support for managing common text \'snippets\' (or \'passages\', or \'chunks\') that you want to reuse across your different prompts.\n\nThis functionality is provided by our new _Snippet tool_. A Snippet tool acts as a simple key/value store, where the key is the name of the common re-usable text snippet and the value is the corresponding text.\n\nFor example, you may have some common persona descriptions that you found to be effective across a range of your LLM features. Or maybe you have some specific formatting instructions that you find yourself re-using again and again in your prompts.\n\nBefore now, you would have to copy and paste between your editor sessions and keep track of which projects you edited. Now you can instead inject the text into your prompt using the Snippet tool.\n\n## Set up the tool\n\nNavigate to the [tools tab](https://app.humanloop.com/hl-test/tools) in your organisation and select the Snippet tool card.\n\n![](file:e719065d-af4c-478a-ae7f-6e4814626047)\n\nWhen the dialog opens, start adding your key/value pairs. In the example below we\'ve defined an Assistants snippet tool that can be used manage some common persona descriptions we feed to the LLM.\n\n<Info> \nYou can have up to 10 key/value snippets in a single snippet tool.\n</Info>\n\nThe **name** field will be how you\'ll access this tool in the editor. By setting the value as _assistant_ below it means in the editor you\'ll be able to access this specific tool by using the syntax `{{ assistant(key) }}`.\n\nThe **key** is how you\'ll access the snippet later, so it\'s recommended to choose something short and memorable.\n\nThe **value** is the passage of text that will be included in your prompt when it is sent to the model.\n\n![](file:cea1b530-2232-4fe6-84e1-aeffd1c4f200)\n\n## Use the tool\n\nNow your Snippets are set up, you can use it to populate strings in your prompt templates across your projects. Double curly bracket syntax is used to call a tool in the template. Inside the curly brackets you call the tool.\n\n![](file:9dde0d2c-9d66-4d8e-8aee-13533e95772a)\n\nThe tool requires an input value to be provided for the key. In our [editor environment](https://app.humanloop.com/playground) the result of the tool will be shown populated top right above the chat.\n\nAbove we created an Assistants tool. To use that in an editor you\'d use the `{{ <your-tool-name>(key) }}` so in this case it would be `{{ assistant(key) }}`. When adding that you get an inputs field appear where you can specify your `key`, in the screenshot above we used the `helpful` key to access the `You are a helpful assistant. You like to tell jokes and if anyone asks your name is Sam.`string. This input field can be used to experiment with different key/value pairs to find the best one to suit your prompt.\n\n<Warning title="The snippet will only render in the preview after running the chat">\nIf you want to see the corresponding snippet to the key you either need to first run the conversation to fetch the string and see it in the preview.\n</Warning>\n\nIf you have a specific key you would like to hardcode in the prompt, you can define it using the literal key value: `{{ <your-tool-name>("key") }}`, so in this case it would be `{{ assistant("helpful") }}`.\n\n![](file:9de89628-14bf-4774-b92f-2e43e36864a8)\n\nThis is particularly useful because you can define passages of text once in a snippet tool and reuse them across multiple prompts, without needing to copy/paste them and manually keep them all in sync.\n\n## What\'s next\n\nExplore our other tools such as the Google or Pinecone Search. If you have other ideas for helpful integrations please reach out and let us know.\n',
    '## Quality-of-life app improvements\n\nWe\'ve been shipping some quality-of-life "little big things" to improve your every day usage of the platform.\n\n### Project switcher throughout the app\n\nWe\'ve added the project switcher throughout the app so its easier to jump between Projects from anywhere\n\n<img src="file:4c9de4bb-4d67-45ec-b5ac-3153ae4fe9ea" alt="The project switcher is now available everywhere." />\n\n\n### We\'ve tidied up the Editor\n\nWith all the new capabilities and changes (tools, images and more) we need to keep a tight ship to stop things from becoming too busy.\n\nWe\'re unifying how we show all your logged generations, in the editor, and in the logs and sessions. We\'ve also changed the font to Inter to be legible at small font sizes. \n\n<img src="file:2719aa72-0ff7-44ec-8cd9-bd74816bc6de" alt="The Editor and other places have had a clean up to aid the new capabilites of tool calling and vision." />\n\n\n### No more accidental blank messages\n\nWe\'ve also fixed issues where empty messages would get appended to the chat.\n\n### We\'ve improved keyboard navigation\n\nThe keyboard shortcuts have been updated so its now easier to navigate in the log tables (up/down keys), and to run generations in Editor (cmd/ctrl + enter). \n\n## Thanks for all your requests and tips. Please keep the feedback coming!\n',
    "## Claude 2.1\n\nToday, Anthropic released its latest model, **Claude 2.1**, and we've added support for it in the Humanloop app.\n\n<img src=\"file:3cfcfe2b-6dce-4f5e-a4c0-92ff18554b1f\" />\n\n\nThe new model boasts a 200K context window and a reported 2x decrease in hallucination rates.\n\nAdditionally, this model introduces tool use to the line-up of Anthropic models. The feature is presently in beta preview, and we'll be adding support for it to Humanloop in the coming days.\n\nRead more about Claude 2.1 in the [official release notes](https://www.anthropic.com/index/claude-2-1).\n",
    '## Parallel tool calling\n\nWe\'ve added support for parallel tool calls in our Editor and API.\n\nWith the release of the latest OpenAI turbo models, the model can choose to respond with more than one tool call for a given query; this is referred to as [parallel tool calling](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling).\n\n### Editor updates\n\nYou can now experiment with this new feature in our Editor:\n\n- Select one of the [new turbo models](/api-reference/changelog#new-openai-turbos) in the model dropdown.\n- Specify a tool in your model config on the left hand side.\n- Make a request that would require multiple calls to answer correctly.\n- As shown here for a weather example, the model will respond with multiple tool calls in the same message\n\n<img src="file:42f9b11d-1acf-4eeb-9006-33d1a1b38bf9" />\n\n### API implications\n\nWe\'ve added an additional field `tool_calls` to our chat endpoints response model that contains the array of tool calls returned by the model. The pre-existing `tool_call` parameter remains but is now marked as deprecated.\n\nEach element in the `tool_calls` array has an id associated to it. When providing the tool response back to the model for one of the tool calls, the `tool_call_id` must be provided, along with `role=tool` and the `content` containing the tool response.\n\n```python\nfrom humanloop import Humanloop\n\n# Initialize the Humanloop SDK with your API Keys\nhumanloop = Humanloop(api_key="<YOUR Humanloop API KEY>")\n\n# form of message when providing the tool response to the model\nchat_response = humanloop.chat_deployed(\n    project_id="<YOUR PROJECT ID>",\n  \tmessages: [\n      {\n        "role": "tool",\n        "content": "Horribly wet"\n        "tool_call_id": "call_dwWd231Dsdw12efoOwdd"\n      }\n   ]\n)\n```\n\n## Python SDK improvements\n\nWe\'ve improved the response models of our [Python SDK](https://github.com/humanloop/humanloop-python#raw-http-response) and now give users better control over HTTPs timeout settings.\n\n### Improved response model types\n\nAs of **versions >= 0.6.0**, our Python SDK methods now return [Pydantic](https://docs.pydantic.dev/latest/) models instead of typed dicts. This improves developer ergonomics around typing and validations.\n\n- Previously, you had to use the [...] syntax to access response values:\n\n```python\nchat_response = humanloop.chat(\n        # parameters\n    )\nprint(chat_response.project_id)\n```\n\n- With Pydantic-based response values, you now can use the . syntax to access response values. To access existing response model from \\< 0.6.0, use can still use the .raw namespace as specified in the [Raw HTTP Response section](https://github.com/humanloop/humanloop-python#raw-http-response).\n\n```python\nchat_response = humanloop.chat(\n        # parameters\n    )\nprint(chat_response.project_id)\n```\n\n> 🚧 Breaking change\n>\n> Moving to >= 0.6.0 does represent a breaking change in the SDK. The underlying API remains unchanged.\n\n### Support for timeout parameter\n\nThe default timeout used by [aiohttp](https://docs.aiohttp.org/en/stable/), which our SDK uses is 300 seconds. For very large prompts and the latest models, this can cause timeout errors to occur.\n\nIn the latest version of Python SDKs, we\'ve increased the default timeout value to 600 seconds and you can update this configuration if you are still experiencing timeout issues by passing the new timeout argument to any of the SDK methods. For example passing`timeout=1000` will override the timeout to 1000 seconds.\n\n## Multi-modal models\n\nWe\'ve introduced support for multi-modal models that can take both text and images as inputs!\n\nWe\'ve laid the foundations for multi-modal model support as part of our Editor and API. The first model we\'ve configured is OpenAI\'s [GPT-4 with Vision (GPT-4V)](https://platform.openai.com/docs/guides/vision/vision). You can now select `gpt-4-vision-preview` in the models dropdown and add images to your chat messages via the API.\n\nLet us know what other multi-modal models you would like to see added next!\n\n### Editor quick start\n\nTo get started with GPT-4V, go to the Playground, or Editor within your project.\n\n- Select `gpt-4-vision-preview` in the models dropdown.\n- Click the **Add images** button within a user\'s chat message.\n- To add an image, either type a URL into the Image URL textbox or select "Upload image" to upload an image from your computer. If you upload an image, it will be converted to a Base64-encoded data URL that represents the image.\n- Note that you can add multiple images\n\n<img src="file:e855e602-bf02-4447-a74d-e40738b65aa8" />\n\nTo view the images within a log, find the log within the logs table and click on it to open it in a drawer. The images in each chat message be viewed within this drawer.\n\n<img src="file:d9a5539b-bfe7-4ae7-9a2e-332b3b4c2871" />\n\n### API quick start\n\nAssuming you have deployed your `gpt-4-vision-preview` based model config, you can now also include images in messages via the API.\n\n```python\nfrom humanloop import Humanloop\n\n# Initialize the Humanloop SDK with your API Keys\nhumanloop = Humanloop(api_key="<YOUR Humanloop API KEY>")\n\n# humanloop.chat_deployed(...) will call the active model config on your project.\nchat_response = humanloop.chat_deployed(\n    project_id="<YOUR PROJECT ID>",\n  \tmessages: [\n      {\n        "role": "user",\n        "content": [\n          {\n            "type": "image_url",\n            "image_url": {\n              "detail": "high",\n              "url": "https://www.acomaanimalclinictucson.com/wp-content/uploads/2020/04/AdobeStock_288690671-scaled.jpeg"\n            }\n          }\n        ]\n)\n```\n\nAny generations made will also be viewable from within your projects logs table.\n\n### Limitations\n\nThere are some know limitations with the current preview iteration of OpenAI\'s GPT-4 model to be aware of:\n\n- Image messages are only supported by the `gpt-4-vision-preview` model in chat mode.\n- GPT-4V model does not support tool calling or JSON mode.\n- You cannot add images to the first `system` message.\n\n## JSON mode and seed parameters\n\nWe\'ve introduced new model config parameters for **JSON mode** and **Seed** in our Editor and API.\n\nWith the introduction of the new [OpenAI turbo models](https://docs.humanloop.com/changelog/gpt4-turbo-preview) you can now set additional properties that impact the behaviour of the model; `response_format` and `seed`.\n\n<Note title="Further details"> \n> \nSee further guidance from OpenAI on the JSON response format [here](https://platform.openai.com/docs/guides/text-generation/json-mode) and reproducing outputs using the seed parameter [here](https://platform.openai.com/docs/guides/text-generation/reproducible-outputs).\n</Note>\n\nThese new parameters can now optionally contribute to your model config in our Editor and API. Updated values for `response_format` or `seed` will constitute new versions of your model on Humanloop.\n\n<img src="file:21bd404d-0e6a-4456-aaed-152f816c7198" />\n\n<Warning title="JSON mode prompts">\nWhen using JSON mode with the new turbo models, you should still include formatting instructions in your prompt.\n\nIn fact, if you do not include the word \'json\' anywhere in your prompt, OpenAI will return a validation error currently.\n</Warning>\n',
    '## LLM Evaluators\n\nUntil now, it\'s been possible to trigger LLM-based evaluations by writing Python code that uses the Humanloop API to trigger the LLM generations.\n\nToday, in order to make this increasingly important workflow simpler and more intuitive, we\'re releasing **LLM Evaluators**, which require no Python configuration.\n\nFrom the Evaluations page, click **New Evaluator** and select LLM Evaluator.\n\n<img src="file:ebaae28d-93de-41b8-b1c0-7e1e16fd2c9d" alt="You can now choose between the existing Python Evaluators and our new LLM Evaluators." />\n\n\nInstead of a code editor, the right hand side of the page is now a prompt editor for defining instructions to the LLM Evaluator. Underneath the prompt, you can configure the parameters of the Evaluator (things like model, temperature etc.) just like any normal model config.\n\n<img src="file:c41eca92-8f48-4f50-a409-e1fa0561eca7" alt="LLM Evaluator Editor." />\n\n\nIn the prompt editor, you have access to a variety of variables that correspond to data from the underlying Log that you are trying to evaluate. These use the usual `{{ variable }}` syntax, and include:\n\n- `log_inputs` - the input variables that were passed in to the prompt template when the Log was generated\n- `log_prompt` - the fully populated prompt (if it was a completion mode generation)\n- `log_messages` - a JSON representation of the messages array (if it was a chat mode generation)\n- `log_output` - the output produced by the model\n- `log_error` - if the underlying Log was an unsuccessful generation, this is the error that was produced\n- `testcase` - when in offline mode, this is the testcase that was used for the evaluation.\n\nTake a look at some of the presets we\'ve provided on the left-hand side of the page for inspiration.\n\n<img src="file:84dfa783-9611-4fd5-804c-725d158e7f3c" alt="LLM Evaluator presets. You\'ll likely need to tweak these to fit your use case." />\n\n\nAt the bottom of the page you can expand the debug console - this can be used verify that your Evaluator is working as intended. We\'ve got further enhancements coming to this part of the Evaluator Editor very soon.\n\nSince an LLM Evaluator is just another model config managed within Humanloop, it gets its own project. When you create an LLM Evaluator, you\'ll see that a new project is created in your organisation with the same name as the Evaluator. Every time the Evaluator produces a Log as part of its evaluation activity, that output will be visible in the Logs tab of that project.\n\n## Improved evaluator editor\n\nGiven our current focus on delivering a best-in-class evaluations experience, we\'ve promoted the Evaluator editor to a full-page screen in the app.\n\n![](file:6877a67b-925f-4911-a7ed-622aa6b0bd26)\n\nIn the left-hand pane, you\'ll find drop-downs to: \n\n- Select the mode of the Evaluator - either Online or Offline, depending on whether the Evaluator is intended to run against pre-defined testcases or against live production Logs\n- Select the return type of the Evaluator - either boolean or number\n\nUnderneath that configuration you\'ll find a collection of presets.\n\n<img src="file:8eb5eae9-e467-4731-8e86-1bde3af4b777" alt="Preset selector." />\n',
    "## Evaluation comparison charts\n\nWe've added comparison charts to the evaluation runs page to help you better compare your evaluation results. These can be found in the evaluations run tab for each of your projects. \n\n![](file:cf4f0380-92c5-4059-93ef-ed062f49c85c)\n\n### Comparing runs\n\nYou can use this to compare specific evaluation runs by selecting those in the runs table. If you don't select any specific rows the charts show an averaged view of all the previous runs for all the evaluators. \n\n![](file:0371e7c6-d071-4c02-bbe3-aede69e1448e)\n\n### Hiding a chart\n\nTo hide a chart for a specific evaluator you can hide the column in the table and it will hide the corresponding chart. \n\n![](file:14d7d707-b8c3-4e29-b4b6-3c5e2a0d75cb)\n",
    '## Comparison mode in Editor\n\nYou can now compare generations across Model Configs and inputs in Editor!\n\n![](file:2f417763-ccf9-42ef-8046-6ec2c3dfe62e)\n\n### Quick start\n\nTo enter comparison mode, click **New panel** in the dropdown menu adds a new blank panel to the right. \n\n**Duplicate panel** adds a new panel containing the same information as your current panel.\n\n[<img src="file:bb4c626b-adcb-46bc-bb1f-8719a2d59407" alt="Clicking **New panel** in the dropdown menu..." />\n\n\n<img src="file:8623574f-434c-47a1-bc36-79243528ab63" alt="... will open a new panel to the right." />\n\n\nEach panel is split into two section: a Model Config section at the top and an Inputs & Chat section at the bottom. These can be collapsed and resized to suit your experimentation.\n\nIf you\'ve made changes in one panel, you can copy the changes you\'ve made using the **Copy** button in the subsection\'s header and paste it in the target panel using its corresponding **Paste** button.\n\n<img src="file:a7a2501d-e968-4d7b-95e6-44f7a15e899e" alt="The **Copy** button on the left panel will copy the new chat template..." />\n\n\n<img src="file:2e870e1f-e2cf-4536-8bfd-5a1b4ff79c34" alt="... and the **Paste** button on the right panel will then update its chat template." />\n\n\n### Other changes\n\nOur recently-introduced local history has also been upgraded to save your full session even when you have multiple panels open.\n\nThe toggle to completion mode and the button to open history have now been moved into the new dropdown menu.\n\n<img src="file:696fa2bb-1f15-4053-9c01-34310f0f5f95" />\n',
    "## Improved evaluation runs\n\nYou can now trigger runs against multiple model configs simultaneously. \n\nThis improves your ability to compare and evaluate changes  across your prompts. We've also removed the summary cards. In their place, we've added a table that supports sorting and rearranging of columns to help you better interrogate results.\n\n### Multiple model configs\n\nTo run evaluations against multiple model configs it's as simple as selecting the targeted model configs in the run dialog, similar to before, but multiple choices are now supported. This will trigger multiple evaluation runs at once, with each model config selected as a target.\n\n![](file:6a3b23ad-17c1-474c-a394-f109a259d9a5)\n\n### Evaluation table\n\nWe've updated our evaluation runs with a table to help view the outcomes of runs in a more condensed form. It also allows you to sort results and trigger re-runs easier. As new evaluators are included, a column will be added automatically to the table. \n\n![](file:6be7325b-6be9-4bc2-a936-fbb97ec5e5f9)\n\n### Re-run previous evaluations\n\nWe've exposed the re-run option in the table to allow you to quickly trigger runs again, or use older runs as a way to preload the dialog and change the parameters such as the target dataset or model config. \n\n![](file:e9f17e97-492f-4ccc-b66c-b93ea3cda3cd)\n\n## New OpenAI turbos\n\nOff the back of OpenAI's [dev day](https://devday.openai.com/) we've added support for the new turbo [models](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) that were announced:\n\n- **gpt-4-1106-preview**\n- **gpt-3.5-turbo-1106**\n\nBoth of these models add a couple of nice capabilities: \n\n- Better instruction following performance\n- JSON mode that forces the model to return valid JSON\n- Can call multiple tools at once\n- Set a seed for reproducible outputs\n\nYou can now access these in your Humanloop Editor and via the API.\n\n<img src=\"file:1d05cae0-527a-41af-a522-957514e63b38\" />\n",
    "## Improved logs drawer\n\nYou can now resize the message section in the Logs and Session drawers, allowing you to review your logs more easily. \n\n![](file:344e1ad5-5081-45f8-a7be-ca15502da63a)\n\nTo resize the message section we've introduced a resize bar that you can drag up or down to give yourself the space needed. To reset the layout back to default just give the bar a double click.\n",
    "## Local editor history\n\nThe Humanloop playground and editor now save history locally as you make edits, giving you complete peace of mind that your precisely-crafted prompts will not be lost due to an accidental page reload or navigating away.\n\n![](file:2cf0cd78-60d9-4fea-82d6-e71829c87112)\n\nLocal history entries will be saved as you use the playground (e.g. as you modify your model config, make generations, or add messages). These will be visible under the **Local** tab within the history side panel. Local history is saved to your browser and is only visible to you.\n\nOur shared history feature, where all playground generations are saved, has now been moved under the **Shared** tab in the history side panel.\n",
    '## Project folders\n\nYou can now organize your projects into folders! \n\nLogging in to Humanloop will bring you to the new page where you can start arranging your projects.\n\n![](file:5d595c49-d1d1-43af-b2e1-a88684b04bec)\n\nNavigate into folders and open projects by clicking on the row. To go back to a parent folder, click on the displayed breadcrumbs (e.g. "Projects" or "Development" in the above screenshot).\n\n***\n\n### Search\n\nSearching will give you a list of directories and projects with a matching name.\n\n![](file:c7e752e8-c8a2-4fb7-9af8-b78929c2b5cb)\n\n### Moving multiple projects\n\nYou can move a group of projects and directories by selecting them and moving them together.\n\n1. Select the projects you want to move.  \n   Tip: Put your cursor on a project row and press [x] to select the row.\n2. To move the selected projects into a folder, drag and drop them into the desired folder.\n\n![](file:39666185-d4b1-4587-967a-08cd4be768ed)\n\nTo move projects out of a folder and into a parent folder, you can drag and drop them onto the parent folder breadcrumbs:\n\n![](file:0cbd0288-5e5c-4f47-a110-c600df3aced1)\n\nTo move projects into deeply nested folders, it might be easier to select your target directory manually. To do so, select the projects you wish to move and then click the blue **Actions** button and then click **Move ...** to bring up a dialog allowing you to move the selected projects.\n\n![](file:e3cad91b-6e37-4f3c-b691-9bcbd3eea9ae)\n\n![](file:7136d451-1ab2-4faf-9c08-de2804366cdc)\n\n***\n\nIf you prefer the old view, we\'ve kept it around for now. Let us know what you\'re missing from the new view so we can improve it.\n\n<img src="file:321b68f1-ca8a-4c2a-95e2-e4ee6723633d" alt="The [Go to old layout] button will take you to the previous view without folders." />\n',
    '## Datasets\n\nWe\'ve introduced **Datasets** to Humanloop. Datasets are collections of **Datapoints**, which represent input-output pairs for an LLM call.\n\nWe recently released **Datasets** in our Evaluations beta, by the name **Evaluation Testsets**. We\'re now promoting the concept to a first-class citizen within your projects. If you\'ve previously been using testsets in the evaluations beta, you\'ll see that your testsets have now automatically migrated to datasets.\n\nDatasets can be created via CSV upload, converting from existing Logs in your project, or by API requests.\n\nSee our [guides on datasets](/docs/guides/datasets), which show how to upload from CSV and perform a batch generation across the whole dataset.\n\n<img src="file:8caa2d11-9ab5-44a4-9fda-13d8e6e743ac" alt="A single dataset that has been added to a project, with 9 datapoints." />\n\nClicking into a dataset, you can explore its datapoints.\n\n<img src="file:dede0b57-05d0-45a4-b4d3-e1ab8202c38c" alt="Datapoints are pre-defined input-output pairs." />\n\nA dataset contains a collection of prompt variable **inputs** (the dynamic values which are interpolated into your model config prompt template at generation-time), as well as a collection of **messages** forming the chat history, and a **target** output with data representing what we expect the model to produce when it runs on those inputs.\n\nDatasets are useful for evaluating the behaviour of you model configs across a well-defined collection of test cases. You can use datasets to check for regressions as you iterate your model configs, knowing that you are checking behaviour against a deterministic collection of known important examples.\n\nDatasets can also be used as collections of input data for **fine-tuning** jobs.\n',
    '## GET API tool\n\nWe\'ve added support for a tool that can make GET calls to an external API.\n\nThis can be used to dynamically retrieve context for your prompts. For example, you may wish to get additional information about a user from your system based on their ID, or look up additional information based on a query from a user.\n\nTo set up the tool you need to provide the following details for your API:\n\n| Tool parameter   | Description                                                                 | Example                            |\n| ---------------- | --------------------------------------------------------------------------- | ---------------------------------- |\n| Name             | A unique tool name to reference as a call signature in your prompts         | `get_api_tool`                     |\n| URL              | The URL for your API endpoint                                               | https://your-api.your-domain.com   |\n| API Key Header   | The authentication header required by your endpoint.                        | `X-API-KEY`                        |\n| API Key          | The API key value to use in the authentication header.                      | `sk_1234567891011121314`           |\n| Query parameters | A comma delimited list of the query parameters to set when making requests. | user_query, client_id              |\n\n### Define your API\n\nFirst you will need to define your API. For demo purposes, we will create a [mock endpoint in postman](https://learning.postman.com/docs/designing-and-developing-your-api/mocking-data/setting-up-mock/). Our [mock endpoint](https://www.postman.com/humanloop/workspace/humanloop/request/12831443-9c48e591-b7b2-4a17-b56a-8050a133e1b5) simply returns details about a mock user given their `user_id`. \n\nA call to our Mock API in Python is as follows; note the query parameter `user_id`\n\n```python\nimport requests\n\nurl = "https://01a02b84-08c5-4e53-b283-a8c2beef331c.mock.pstmn.io/users?user_id=01234567891011"\nheaders = {\n  \'X-API-KEY\': \'<API KEY VALUE>\'\n}\nresponse = requests.request("GET", url, headers=headers)\nprint(response.text)\n\n```\n\nAnd returns the response:\n\n```json\n{\n  "user_id", "012345678910",\n  "name": "Albert",\n  "company": "Humanloop",\n  "role": "Engineer"\n}\n```\n\nWe can now use this tool to inject information for a given user into our prompts.\n\n### Set up the tool\n\nNavigate to the [tools tab](https://app.humanloop.com/hl-test/tools) in your organisation and select the `Get API Call ` tool card:\n\n<img src="file:3eabc0df-d5b7-4c44-894a-d42171a76925" />\n\n\nConfigure the tool with your API details:\n\n<img src="file:f62090e3-c781-4a24-9997-7d3614bd2d5b" />\n\n\n### Use the tool\n\nNow your API tool is set up, you can use it to populate input variables in your prompt templates. Double curly bracket syntax is used to call a tool in the template. The call signature is the unique tool name with arguments for the query parameters defined when the tool was set up. \n\nIn our mock example, the signature will be:  `get_user_api(user_id)`.\n\nAn example prompt template using this tool is: \n\n```shell\nYou are a helpful assistant. Please draft an example job role summary for the following user:\n\nUser details: {{ get_user_api(user_id) }}\nKeep it short and concise.\n```\n\nThe tool requires an input value to be provided for user_id. In our [playground environment](https://app.humanloop.com/playground) the result of the tool will be shown populated top right above the chat:\n\n<img src="file:37af4775-e407-465f-bf03-34b1acd2d739" />\n\n\n### What\'s next\n\nExplore more complex examples of context stuffing such as defining your own custom RAG service.\n',
    '## Evaluations improvements\n\nWe\'ve released a couple of minor useability improvements in the evaluations workflow.\n\n### Summary statistics for evaluation runs\n\nWhen reviewing past runs of evaluations, you can now see summary statistics for each evaluator before clicking into the detail view, allowing for easier comparison between runs.\n\n![](file:4bb11707-7bae-405d-ad4e-cb09136a745b)\n\n### Re-running evaluations\n\nTo enable easier re-running of past evaluations, you can now click the **Re-run** button in the top-right of the evaluation detail view.\n\n![](file:496d2e13-c46b-4c9a-98c4-dba8065d14b7)\n\n## Editor - copy tools\n\nOur Editor environment let\'s users incorporate [OpenAI function calling](https://openai.com/blog/function-calling-and-other-api-updates) into their prompt engineering workflows by defining tools. Tools are made available to the model as functions to call using the same universal JSON schema format. \n\nAs part of this process it can be helpful to copy the full JSON definition of the tool for quickly iterating on new versions, or copy and pasting it into code. You can now do this directly from the tool definition in Editor:\n\n<img src="file:a92bd986-ab61-4799-acd4-5ccfb3dcfeba" />\n\n\nSelecting the Copy button adds the full JSON definition of the tool to your clipboard:\n\n```json\n{\n  "name": "get_current_weather",\n  "description": "Get the current weather in a given location",\n  "parameters": {\n    "type": "object",\n    "properties": {\n      "location": {\n        "type": "string",\n        "name": "Location",\n        "description": "The city and state, e.g. San Francisco, CA"\n      },\n      "unit": {\n        "type": "string",\n        "name": "Unit",\n        "enum": [\n          "celsius",\n          "fahrenheit"\n        ]\n      }\n    },\n    "required": [\n      "location"\n    ]\n  }\n}\n```\n\n## Single sign on (SSO)\n\nWe\'ve added support for SOO to our signup, login and invite flows. By default users can now use their Gmail accounts to access Humanloop. \n\nFor our enterprise customers, this also unlocks the ability for us to more easily support their SAML-based single sign-on (SSO) set ups.  \n\n<img src="file:ed407265-5a2d-4d6d-9d66-ff69ceda699f" />\n',
    '## Organization slug in URLs\n\nWe have altered routes specific to your organization to include the organization slug. The organization slug is a unique value that was derived from your organization name when your organization was created.\n\nFor project paths we\'ve dropped the `projects` label in favour of a more specific `project` label. \n\nAn example of what this looks like can be seen below:\n\n<img src="file:7ca599e6-d3cb-4c6b-a786-2574e153b067" />\n\n<Check title="Existing bookmarks and links will continue to work">\nWhen a request is made to one of the legacy URL paths, we\'ll redirect it to the corresponding new path. Although the legacy routes are still supported, we encourage you to update your links and bookmarks to adopt the new naming scheme.\n</Check>\n\n### Updating your organization slug\n\nThe organization slug can be updated by organization administrators. This can be done by navigating to the [general settings](https://app.humanloop.com/account/organization) page. Please exercise caution when changing this, as it will affect the URLs across the organization. \n\n![](file:d4a0f7f9-8e38-454d-96f0-5904ec0b7a1f)\n',
    '## Allow trusted email domains\n\nYou can now add **trusted email domains** to your organization. Adding trusted email domains allows new users, when creating an account with a matching email, to join your organization without requiring an invite.\n\n### Managing trusted domains\n\nAdding and removing trusted email domains is controlled from your organizations [General settings](https://app.humanloop.com/account/organization) page.\n\n<Info> \nOnly Admins can manage trusted domains for an organization.\n</Info>\n\nTo add a new trusted domain press the **Add domain** button and enter the domains trusted by your organization. The domains added here will check against new users signing up to Humanloop and if there is a match those users will be given the option to join your organization. \n\n<img src="file:0e781f8f-374c-4cf3-90ee-61cac91e1a35" />\n\n\n### Signup for new users\n\nNew users signing up to Humanloop will see the following screen when they signup with an email that matches and organizations trusted email domain. By pressing Join they will be added to the matching organization. \n\n<img src="file:bb069f9a-3de0-43bc-82a2-a0a78ac5484b" />\n',
    '## Editor - insert new message within existing chat\n\nYou can now insert a new message within an existing chat in our Editor.  Click the plus button that appears between the rows.\n\n<img src="file:65a7dbaf-91ab-4867-b03a-49e1a274d66f" />\n',
    "## Claude instant 1.2\n\nWe've added support for Anthropic's latest model Claude instant 1.2! Claude Instant is the faster and lower-priced yet still very capable model from Anthropic, great for use cases where low latency and high throughput are required. \n\nYou can use Claude instant 1.2 directly within the Humanloop playground and deployment workflows.\n\nRead more about the latest Claude instant model [here](https://www.anthropic.com/index/releasing-claude-instant-1-2).\n",
    "## Offline evaluations with testsets\n\nWe're continuing to build and release more functionality to Humanloop's evaluations framework!\n\nOur first release provided the ability to run **online evaluators** in your projects. Online evaluators allow you to monitor the performance of your live deployments by defining functions which evaluate all new datapoints in real time as they get logged to the project.\n\nToday, to augment online evaluators, we are releasing **offline evaluators** as the second part of our evaluations framework.\n\nOffline evaluators provide the ability to test your prompt engineering efforts rigorously in development and CI. Offline evaluators test the performance of your model configs against a pre-defined suite of **testcases** - much like unit testing in traditional programming.\n\nWith this framework, you can use test-driven development practices to iterate and improve your model configs, while monitoring for regressions in CI.\n\nTo learn more about how to use online and offline evaluators, check out the [Evaluate your model](/docs/guides/evaluate-your-model) section of our guides.\n\n![](file:c2a297f9-ef12-475e-b8cf-8a1a2ace7048)\n",
    '## Improved error handling\n\nWe\'ve unified how errors returned by model providers are handled and enabled error monitoring using [eval functions](/docs/guides/evaluate-your-model).\n\nA common production pain point we see is that hosted SOTA language models can still be flaky at times, especially at real scale. With this release, Humanloop can help users better understand the extent of the problem and guide them to different models choices to improve reliability.\n\n### Unified errors\n\nOur users integrate the Humanloop `/chat` and `/completion` API endpoints as a unified interface into all the popular model providers including OpenAI, Anthropic, Azure, Cohere, etc. Their Humanloop projects can then be used to manage model experimentation, versioning, evaluation and deployment.\n\nErrors returned by these endpoints may be raised by the model provider\'s system. With this release we\'ve updated our API to map all the error behaviours from different model providers to a unified set of [error response codes](/api-reference/errors#http-error-codes).\n\nWe\'ve also extended our error responses to include more details of the error with fields for `type`, `message`, `code` and `origin`. The `origin` field indicates if the error originated from one of the integrated model providers systems, or directly from Humanloop.\n\nFor example, for our `/chat ` endpoint where we attempt to call OpenAI with an invalid setting for `max_tokens`, the message returned is that raised by OpenAI and the origin is set to OpenAI.\n\n```json\n{\n  "type": "unprocessable_entity_error",\n  "message": "This model\'s maximum context length is 4097 tokens. However, you requested 10000012 tokens (12 in the messages, 10000000 in the completion). Please reduce the length of the messages or completion.",\n  "code": 422,\n  "origin": "OpenAI"\n}\n```\n\n### Monitor model reliability with evals\n\nWith this release, all errors returned from the different model providers are now persisted with the corresponding input data as datapoints on Humanloop. Furthermore this error data is made available to use within [evaluation functions](/docs/guides/evaluate-your-model).\n\nYou can now turn on the **Errors** eval function, which tracks overall error rates of the different model variations in your project. Or you can customise this template to track more specific error behaviour.\n\n<img src="file:2abf7ebc-ecfb-4b6a-b24f-3f15bacd0428" alt="Errors evaluation function template now available" />\n',
    '## OpenAI functions in Playground\n\nWe\'ve added support for [OpenAI functions](https://platform.openai.com/docs/guides/gpt/function-calling) to our playground!\n\nThis builds on our [API support](https://docs.humanloop.com/changelog/openai-functions-as-tools) and allows you to easily experiment with OpenAI functions within our playground UI.\n\nOpenAI functions are implemented as [tools](https://docs.humanloop.com/docs/setup-semantic-search) on Humanloop. Tools follow the same universal [json-schema](https://json-schema.org/) definition as OpenAI functions. You can now define tools as part of your model configuration in the playground. These tools are sent as OpenAI functions when running the OpenAI chat models that support function calling.\n\nThe model can choose to return a JSON object containing the arguments needed to call a function. This object is displayed as a special assistant message within the playground. You can then provide the result of the call in a message back to the model to consider, which simulates the function calling workflow.\n\n### Use tools in Playground\n\nTake the following steps to use tools for function calling in the playground:\n\n1. **Find tools:** Navigate to the playground and locate the `Tools` section. This is where you\'ll be able to manage your tool definitions.\n\n![](file:51a973bc-6de7-4d79-9c2b-2b6cb7afc329)\n\n2. **Create a new tool:** Click on the "Add Tool" button. There are two options in the dropdown: create a new tool or to start with one of our examples. You define your tool using the [json-schema](https://json-schema.org/) syntax. This represents the function definition sent to OpenAI.\n\n![](file:c5bf7e40-beed-4279-ba21-c76e453cf268)\n\n3. **Edit a tool:** To edit an existing tool, simply click on the tool in the Tools section and make the necessary changes to its json-schema definition. This will result in a new model configuration.\n\n![](file:a6e200f3-6541-406f-b4b3-330a7c345f39)\n\n4. **Run a model with tools:** Once you\'ve defined your tools, you can run the model by pressing the "Run" button.\n   1. If the model chooses to call a function, an assistant message will be displayed with the corresponding tool name and arguments to use.\n   2. A subsequent `Tool` message is then displayed to simulate sending the results of the call back to the model to consider.\n\n![](file:e56e8f9e-e577-4a4d-821e-f9562076dab8)\n\n5. **Save your model config with tools** by using the **Save** button. Model configs with tools defined can then deployed to [environments](/docs/guides/deploy-to-an-environment) as normal.\n\n### Coming soon\n\nProvide the runtime for your tool under the existing pre-defined [Tools section ](https://app.humanloop.com/tools) of your organization on Humanloop.\n',
    '## Llama 2\n\nWe\'ve added support for Llama 2!\n\nYou can now select `llama70b-v2` from the model dropdown in the Playground and Editor. You don\'t currently need to provide an API key or any other special configuration to get Llama 2 access via Humanloop. \n\n<img src="file:5edfcd56-fe24-446a-af59-38c92c298d93" alt="Llama 2 is available in Playground and Editor for all Humanloop users." />\n\n\nRead more about the latest version of Llama [here](https://ai.meta.com/llama/) and in the [original announcement](https://about.fb.com/news/2023/07/llama-2/).\n',
    "## Claude 2\n\nWe've added support for Anthropic's latest model Claude 2.0!\n\nRead more about the latest Claude [here](https://www.anthropic.com/index/claude-2).\n",
    '## Evaluators\n\nWe\'ve added **Evaluators** to Humanloop in beta! \n\nEvaluators allow you to quantitatively define what constitutes a good or bad output from your models. Once set up, you can configure an Evaluators to run automatically across all new datapoints as they appear in your project; or, you can simply run it manually on selected datapoints from the **Data** tab. \n\nWe\'re going to be adding lots more functionality to this feature in the coming weeks, so check back for more!\n\n### Create an Evaluator\n\nIf you\'ve been given access to the feature, you\'ll see a new **Evaluations** tab in the Humanloop app. To create your first evaluation function, select **+ New Evaluator**. In the dialog, you\'ll be presented with a library of example Evaluators, or you can start from scratch.\n\n<img src="file:f855530f-6971-43aa-ac74-04250fcdfbfa" alt="We offer a library of example Evaluators to get you started." />\n\n\nWe\'ll pick **Valid JSON** for this guide.\n\n<img src="file:752b1ca1-98cc-427a-a5b0-d59d8d250ace" alt="Evaluator editor." />\n\n\nIn the editor, provide details of your function\'s name, description and return type. In the code editor, you can provide a function which accepts a `datapoint` argument and should return a value of the chosen type.\n\nCurrently, the available return types for an Evaluators are `number` and `boolean`. You should ensure that your function returns the expected data type - an error will be raised at runtime if not.\n\n#### The `Datapoint` argument\n\nThe `datapoint` passed into your function will be a Python `dict` with the following structure.\n\n```python\n{\n    "id":"data_XXXX",          # Datapoint id\n    "model_config": {...},     # Model config used to generate the datapoint\n    "inputs": {...},           # Model inputs (interpolated into the prompt)\n    "output": "...",           # Generated output from the model\n    "provider_latency": 0.6,   # Provider latency in seconds\n    "metadata": {...},         # Additional metadata attached to the logged datapoint\n    "created_at": "...",       # Creation timestamp\n    "feedback": [...]          # Array of feedback provided on the datapoint\n}\n```\n\nTo inspect datapoint dictionaries in more detail, click **Random selection** in the debug console at the bottom of the window. This will load a random set of five datapoints from your project, exactly as they will be passed into the Evaluation Function. \n\n<img src="file:15181291-4895-412a-9789-36b5c2388592" alt="The debug console - load datapoints to inspect the argument passed into Evaluators." />\n\n\nFor this demo, we\'ve created a prompt which asks the model to produce valid JSON as its output. The Evaluator uses a simple `json.loads` call to determine whether the output is validly formed JSON - if this call raises an exception, it means that the output is not valid JSON, and we return `False`.\n\n```python\nimport json\n    \ndef check_valid_json(datapoint):\n    try:\n        return json.loads(datapoint["output"]) is not None\n    except:\n        return False\n```\n\n#### Debugging\n\nOnce you have drafted a Python function, try clicking the run button next to one of the debug datapoints in the debug console. You should shortly see the result of executing your function on that datapoint in the table.\n\n<img src="file:5d9166d8-f201-44ea-97e4-beaaabe858b1" alt="A `True` result from executing the **Valid JSON** Evaluators on the datapoint. " />\n\n\nIf your Evaluator misbehaves, either by being invalid Python code, raising an unhandled exception or returning the wrong type, an error will appear in the result column. You can hover this error to see more details about what went wrong - the exception string is displayed in the tooltip. \n\nOnce you\'re happy with your Evaluator, click **Create** in the bottom left of the dialog.\n\n### Activate / Deactivate an Evaluator\n\nYour Evaluators are available across all your projects. When you visit the **Evaluations** tab from a specific project, you\'ll see all Evaluators available in your organisation.\n\nEach Evaluator has a toggle. If you toggle the Evaluator **on**, it will run on every new datapoint that gets logged to **that** project. (Switch to another project and you\'ll see that the Evaluator is not yet toggled on if you haven\'t chosen to do so).\n\nYou can deactivate an Evaluator for a project by toggling it back off at any time.\n\n### Aggregations and Graphs\n\nAt the top of the **Dashboard** tab, you\'ll see new charts for each activated Evaluation Function. These display aggregated Evaluation results through time for datapoints in the project. \n\nAt the bottom of the **Dashboard** tab is a table of all the model configs in your project. That table will display a column for each activated Evaluator in the project. The data displayed in this column is an aggregation of all the Evaluation Results (by model config) for each Evaluator. This allows you to assess the relative performance of your models.\n\n<img src="file:92828868-e318-41a2-b876-8408d4283543" alt="Evaluation Results through time, by model config. In this example, one of the model configs is not producing Valid JSON outputs, while the other is about 99% of the time." />\n\n\n#### Aggregation\n\nFor the purposes of both the charts and the model configs table, aggregations work as follows for the different return types of Evaluators:\n\n- `Boolean`: percentage returning `True` of the total number of evaluated datapoints\n- `Number`: average value across all evaluated datapoints\n\n### Data logs\n\nIn the **Data** tab, you\'ll also see that a column is visible for each activated Evaluator, indicating the result of running the function on each datapoint.\n\n<img src="file:b96cca6c-4b96-4914-a70b-9a6d070fe343" alt="The **Data** tab for a project, showing the **Valid JSON** Evaluation Results for a set of datapoints." />\n\n\nFrom this tab, you can choose to re-run an Evaluator on a selection of datapoints. Either use the menu at the far right of a single datapoint, or select multiple datapoints and choose **Run evals** from the **Actions** menu in the top right. \n\n### Available Modules\n\nThe following Python modules are available to be imported in your Evaluation Function:\n\n- `math`\n- `random`\n- `datetime`\n- `json` (useful for validating JSON grammar as per the example above)\n- `jsonschema` (useful for more fine-grained validation of JSON output - see the in-app example)\n- `sqlglot` (useful for validating SQL query grammar)\n- `requests` (useful to make further LLM calls as part of your evaluation - see the in-app example for a suggestion of how to get started).\n\nLet us know if you would like to see more modules available.\n',
    '## Chain LLM calls\n\nWe\'ve introduced sessions to Humanloop, allowing you to link multiple calls together when building a chain or agent.\n\nUsing sessions with your LLM calls helps you troubleshoot and improve your chains and agents.\n\n<img src="file:18a0ccfc-f1d2-49bc-ba8d-1c880870e72e" alt="Trace of an Agent\'s steps logged as a session" />\n\n### Adding a datapoint to a session\n\nTo log your LLM calls to a session, you just need to define a unique identifier for the session and pass it into your Humanloop calls with `session_reference_id`.\n\nFor example, using `uuid4()` to generate this ID,\n\n```python\nimport uuid\nsession_reference_id = str(uuid.uuid4())\n\nresponse = humanloop.complete(\n    project="sessions_example_assistant",\n    model_config={\n        "prompt_template": "Question: {{user_request}}\\nGoogle result: {{google_answer}}\\nAnswer:\\n",\n        "model": "text-davinci-002",\n        "temperature": 0,\n    },\n    inputs={"user_request": user_request, "google_answer": google_answer},\n    session_reference_id=session_reference_id,\n)\n```\n\nSimilarly, our other methods such as `humanloop.complete_deployed()`, `humanloop.chat()`, and `humanloop.log()` etc. support `session_reference_id`.\n\nIf you\'re using our API directly, you can pass `session_reference_id` within the request body in your `POST /v4/completion` etc. endpoints.\n\n### Further details\n\nFor a more detailed walkthrough on how to use `session_reference_id`, check out [our guide](/docs/guides/logging-session-traces) that runs through how to record datapoints to a session in an example script.\n',
    '## Introducing Tools\n\nToday we’re announcing Tools as a part of Humanloop.\n\nTools allow you to connect an LLM to any API and to an array of data sources to give it extra capabilities and access to private data. Under your organization settings on Humanloop you can now configure and manage tools in a central place.\n\nRead more on [our blog](https://humanloop.com/blog/announcing-tools) and see an example of setting up a [tool for semantic search](/docs/guides/set-up-semantic-search).\n\n## OpenAI functions API\n\nWe\'ve updated our APIs to support [OpenAI function calling](https://platform.openai.com/docsgpt/function-calling).\n\nOpenAI functions are now supported as tools on Humanloop. This allows you to pass tool definitions as part of the model configuration when calling our `chat` and `log` endpoints. For the latest OpenAI models `gpt-3.5-turbo-0613` and `gpt-4-0613` the model can then choose to output a JSON object containing arguments to call these tools.\n\nThis unlocks getting more reliable structured data back from the model and makes it easier to create useful agents.\n\n### Recap on OpenAI functions\n\nAs described in the [OpenAI documentation](https://platform.openai.com/docsgpt/function-calling), the basic steps for using functions are:\n\n1. Call one of the models `gpt-3.5-turbo-0613` and `gpt-4-0613` with a user query and a set of function definitions described using the universal [json-schema](https://json-schema.org/) syntax.\n2. The model can then choose to call one of the functions provided. If it does, a stringified JSON object adhering to your json schema definition will be returned.\n3. You can then parse the string into JSON in your code and call the chosen function with the provided arguments (**NB:** the model may hallucinate or return invalid json, be sure to consider these scenarios in your code).\n4. Finally call the model again by appending the function response as a new message. The model can then use this information to respond to the original use query.\n\nOpenAI have provided a simple example in their docs for a `get_current_weather` function that we will show how to adapt to use with Humanloop:\n\n```python\nimport openai\nimport json\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit="fahrenheit"):\n    """Get the current weather in a given location"""\n    weather_info = {\n        "location": location,\n        "temperature": "72",\n        "unit": unit,\n        "forecast": ["sunny", "windy"],\n    }\n    return json.dumps(weather_info)\n\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{"role": "user", "content": "What\'s the weather like in Boston?"}]\n    functions = [\n        {\n            "name": "get_current_weather",\n            "description": "Get the current weather in a given location",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "The city and state, e.g. San Francisco, CA",\n                    },\n                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},\n                },\n                "required": ["location"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model="gpt-3.5-turbo-0613",\n        messages=messages,\n        functions=functions,\n        function_call="auto",  # auto is default, but we\'ll be explicit\n    )\n    response_message = response["choices"][0]["message"]\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get("function_call"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            "get_current_weather": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message["function_call"]["name"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message["function_call"]["arguments"])\n        function_response = fuction_to_call(\n            location=function_args.get("location"),\n            unit=function_args.get("unit"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant\'s reply\n        messages.append(\n            {\n                "role": "function",\n                "name": function_name,\n                "content": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo-0613",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n        return second_response\n\n\nprint(run_conversation())\n```\n\n### Using with Humanloop tools\n\nOpenAI functions are treated as tools on Humanloop. Tools conveniently follow the same universal json-schema definition as OpenAI functions.\n\nWe\'ve expanded the definition of our model configuration to also include tool definitions. Historically the model config is made up of the chat template, choice of base model and any hyper-parameters that change the behaviour of the model.\n\nIn the cases of OpenAIs `gpt-3.5-turbo-0613` and `gpt-4-0613` models, any tools defined as part of the model config are passed through as functions for the model to use.\n\nYou can now specify these tools when using the Humanloop chat endpoint (as a replacement for OpenAI\'s ChatCompletion), or when using the Humanloop log endpoint in addition to the OpenAI calls:\n\n#### Chat endpoint\n\nWe show here how to update the `run_conversation()` method from the OpenAI example to instead use the Humanloop chat endpoint with tools:\n\n```python\nfrom humanloop import Humanloop\n\nhl = Humanloop(\n  \t# get your API key here: https://app.humanloop.com/account/api-keys\n    api_key="YOUR_API_KEY",\n)\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{"role": "user", "content": "What\'s the weather like in Boston?"}]\n    # functions are referred to as tools on Humanloop, but follows the same schema\n\t\ttools = [\n        {\n            "name": "get_current_weather",\n            "description": "Get the current weather in a given location",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "The city and state, e.g. San Francisco, CA",\n                    },\n                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},\n                },\n                "required": ["location"],\n            },\n        }\n    ]\n    response = hl.chat(\n      project="Assistant",\n      model_config={\n        "model": "gpt-3.5-turbo-0613",\n      \t"tools": tools\n      },\n      messages=messages\n    )\n    response = response.body.data[0]\n\n    # Step 2: check if GPT wanted to call a tool\n    if response.get("tool_call"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            "get_current_weather": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message["function_call"]["name"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response["tool_call"]["arguments"])\n        function_response = fuction_to_call(\n            location=function_args.get("location"),\n            unit=function_args.get("unit"),\n        )\n\n        # Step 4: send the response back to the model\n        messages.append(response_message)\n        messages.append(\n            {\n                "role": "tool",\n                "name": function_name,\n                "content": function_response,\n            }\n        )\n        second_response = hl.chat(\n          project="Assistant",\n          model_config={\n            "model": "gpt-3.5-turbo-0613",\n            "tools": tools\n          },\n          messages=messages\n        )\n        return second_response\n```\n\nAfter running this snippet, the model configuration recorded on your project in Humanloop will now track what tools were provided to the model and the logged datapoints will provide details of the tool called to inspect:\n\n![](file:51d021d2-1958-44e5-9f71-a4c0e1206707)\n\n#### Log endpoint\n\nAlternatively, you can also use the explicit Humanloop log alongside your existing OpenAI calls to achieve the same result:\n\n```python\nfrom humanloop import Humanloop\n\nhl = Humanloop(\n  \t# get your API key here: https://app.humanloop.com/account/api-keys\n    api_key="YOUR_API_KEY",\n)\n\ndef run_conversation():\n    # Step 1: send the conversation and available functions to GPT\n    messages = [{"role": "user", "content": "What\'s the weather like in Boston?"}]\n    functions = [\n        {\n            "name": "get_current_weather",\n            "description": "Get the current weather in a given location",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "The city and state, e.g. San Francisco, CA",\n                    },\n                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},\n                },\n                "required": ["location"],\n            },\n        }\n    ]\n    response = openai.ChatCompletion.create(\n        model="gpt-3.5-turbo-0613",\n        messages=messages,\n        functions=functions,\n        function_call="auto",  # auto is default, but we\'ll be explicit\n    )\n    response_message = response["choices"][0]["message"]\n\n\t\t# log the result to humanloop\n    log_response = hl.log(\n       project="Assistant",\n          model_config={\n            "model": "gpt-3.5-turbo-0613",\n            "tools": tools,\n          },\n          messages=messages,\n      \t\ttool_call=response_message.get("function_call")\n    )\n\n    # Step 2: check if GPT wanted to call a function\n    if response_message.get("function_call"):\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            "get_current_weather": get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = response_message["function_call"]["name"]\n        fuction_to_call = available_functions[function_name]\n        function_args = json.loads(response_message["function_call"]["arguments"])\n        function_response = fuction_to_call(\n            location=function_args.get("location"),\n            unit=function_args.get("unit"),\n        )\n\n        # Step 4: send the info on the function call and function response to GPT\n        messages.append(response_message)  # extend conversation with assistant\'s reply\n        messages.append(\n            {\n                "role": "function",\n                "name": function_name,\n                "content": function_response,\n            }\n        )  # extend conversation with function response\n        second_response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo-0613",\n            messages=messages,\n        )  # get a new response from GPT where it can see the function response\n\n        log_response = hl.log(\n          project="Assistant",\n          model_config={\n                  "model": "gpt-3.5-turbo-0613",\n                  "tools": tools,\n          },\n          messages=messages,\n          output=second_response["choices"][0]["message"]["content"],\n    )\n    return second_response\n\n\nprint(run_conversation())\n```\n\n### Coming soon\n\nSupport for defining tools in the playground!\n',
    '## Deployment environments\n\nWe\'ve added support for environments to your deployments in Humanloop!\n\nThis enables you to deploy your model configurations to specific environments. You\'ll no longer have to duplicate your projects to manage the deployment workflow between testing and production. With environments, you\'ll have the control required to manage the full LLM deployment lifecycle.\n\n### Enabling environments for your organisation\n\nEvery organisation automatically receives a default production environment. For any of your existing projects that had active deployments define, these have been automatically migrated over to use the default environment with no change in behaviour for the APIs.\n\nYou can create additional environments with custom names by visiting your organisation\'s [environments page](https://app.humanloop.com/account/environments).\n\n#### Creating an environment\n\nEnter a custom name in the create environment dialog. Names have a constraint in that they must be unique within an organisation.\n\n![](file:56fd44d0-2e84-4913-9e51-f6501f22bfa2)\n\nThe environments you define for your organisation will be available for each project and can be viewed in the project dashboard once created.\n\n![](file:21d62e1d-d4d4-4f09-b63e-a1e9b3091301)\n\n#### The default environment\n\nBy default, the production environment is marked as the Default environment. This means that all API calls targeting the "Active Deployment," such as [Get Active Config](/api-reference/projects/getactiveconfig) or [Chat Deployed](/api-reference/chats/createdeployed) will use this environment.\n\n<Warning> \nRenaming environments will take immediate effect, so ensure that this change is planned and does not disrupt your production workflows.\n</Warning>\n\n### Using environments\n\nOnce created on the environments page, environments can be used for each project and are visible in the respective project dashboards.\n\nYou can deploy directly to a specific environment by selecting it in the **Deployments** section.\n\n![](file:533f6e55-2f43-459c-aba7-843359922185)\n\nAlternatively, you can deploy to multiple environments simultaneously by deploying a Model Config from either the Editor or the Model Configs table.\n\n### Using environments via API\n\n![](file:b5d22c5e-0253-4424-8580-1e29dbfde7ac)\n\nFor v4.0 API endpoints that support Active Deployments, such as [Get Active Config](/api-reference/projects/getactiveconfig) or [Chat Deployed](/api-reference/chats/createdeployed), you can now optionally point to a model configuration deployed in a specific environment by including an optional additional `environment` field.\n\nYou can find this information in our v4.0 API Documentation or within the environment card in the Project Dashboard under the "Use API" option.\n\nClicking on the "Use API" option will provide code snippets that demonstrate the usage of the `environment` variable in practice.\n\n![](file:744c76b7-da51-44ba-add4-8bc9fea66cde)\n',
    '## Improved Python SDK streaming response\n\nWe\'ve improved our Python SDK\'s streaming response to contain the datapoint ID. Using the ID, you can now provide feedback to datapoints created through streaming.\n\nThe `humanloop.chat_stream()` and `humanloop.complete_stream()` methods now yield a dictionary with `output` and `id`.\n\n```python\n{\'output\': \'...\', \'id\': \'data_...\'}\n```\n\nInstall the updated SDK with\n\n```shell\npip install --upgrade humanloop\n```\n\n### Example snippet\n\n```\nimport asyncio\nfrom humanloop import Humanloop\n\nhumanloop = Humanloop(\n    api_key="YOUR_API_KEY",\n    openai_api_key="YOUR_OPENAI_API_KEY",\n)\n\nasync def main():\n    response = await humanloop.chat_stream(\n        project="sdk-example",\n        messages=[\n            {\n                "role": "user",\n                "content": "Explain asynchronous programming.",\n            }\n        ],\n        model_config={\n            "model": "gpt-3.5-turbo",\n            "max_tokens": -1,\n            "temperature": 0.7,\n            "chat_template": [\n                {\n                    "role": "system",\n                    "content": "You are a helpful assistant who replies in the style of {{persona}}.",\n                },\n            ],\n        },\n        inputs={\n            "persona": "the pirate Blackbeard",\n        },\n    )\n    async for token in response.content:\n        print(token)  # E.g. {\'output\': \'Ah\', \'id\': \'data_oun7034jMNpb0uBnb9uYx\'}\n\nasyncio.run(main())\n```\n\n## OpenAI Azure support\n\nWe\'ve just added support for Azure deployments of OpenAI models to Humanloop!\n\nThis update adds the ability to target Microsoft Azure deployments of OpenAI models to the playground and your projects. To set this up, visit your [organization\'s settings](https://app.humanloop.com/account/api-keys).\n\n### Enabling Azure OpenAI for your organization\n\nAs a prerequisite, you will need to already be setup with Azure OpenAI Service. See the [Azure OpenAI docs](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal) for more details. At the time of writing, access is granted by application only.\n\n![](file:2d98f1f6-c481-4495-8c8d-9ec0cfc73f73)\n\nClick the Setup button and provide your Azure OpenAI endpoint and API key.\n\nYour endpoint can be found in the Keys & Endpoint section when examining your resource from the Azure portal. Alternatively, you can find the value in Azure OpenAI Studio > Playground > Code View. An example endpoint is: docs-test-001.openai.azure.com.\n\nYour API keys can also be found in the Keys & Endpoint section when examining your resource from the Azure portal. You can use either KEY1 or KEY2.\n\n### Working with Azure OpenAI models\n\nOnce you\'ve successfully enabled Azure OpenAI for your organization, you\'ll be able to access it through the [playground](https://app.humanloop.com/playground) and in your projects in exactly the same way as your existing OpenAI and/or Anthropic models.\n\n<img src="file:8f8ed1a2-33a5-472f-bade-aa7f9449231a" />\n\n### REST API and Python / TypeScript support\n\nAs with other model providers, once you\'ve set up an Azure OpenAI-backed model config, you can call it with the Humanloop [REST API or our SDKs](/docs/api-reference/sdks).\n\n```typescript\nimport { Humanloop } from "humanloop";\n\nconst humanloop = new Humanloop({\n  apiKey: "API_KEY",\n});\n\nconst chatResponse = await humanloop.chat({\n  project: "project_example",\n  messages: [\n    {\n      role: "user",\n      content: "Write me a song",\n    },\n  ],\n  provider_api_keys: {\n    openai_azure: OPENAI_AZURE_API_KEY,\n    openai_azure_endpoint: OPENAI_AZURE_ENDPOINT,\n  },\n  model_config: {\n    model: "my-azure-deployed-gpt-4",\n    temperature: 1,\n  },\n});\n\nconsole.log(chatResponse);\n```\n\nIn the `model_config.model` field, provide the name of the model that you deployed from the Azure portal (see note below for important naming conventions when setting up your deployment in the Azure portal).\n\nThe request will use the stored organization level key and endpoint you configured above, unless you override this on a per-request basis by passing both the endpoint and API key in the `provider_api_keys` field, as shown in the example above.\n\n### Note: Naming Model Deployments\n\nWhen you deploy a model through the Azure portal, you\'ll have the ability to provide your deployment with a unique name. For instance, if you choose to deploy an instance of `gpt-35-turbo` in your OpenAI Service, you may choose to give this an arbitrary name like `my-orgs-llm-model`.\n\nIn order to use all Humanloop features with your Azure model deployment, you must ensure that your deployments are named either with an unmodified base model name like `gpt-35-turbo`, or the base model name with a custom prefix like `my-org-gpt-35-turbo`. If your model deployments use arbitrary names which do not prefix a base model name, you may find that certain features such as setting `max_tokens=-1` in your model configs fail to work as expected.\n',
    '## Project Editor\n\nWe’ve introduced an Editor within each project to help you make it easier to to change prompts and bring in project specific data. \n\n<img src="file:26445b9d-65a8-46a6-aa2d-ad492b2a1134" alt="The Editor will load up the currently active model config, and will save the generations in the project’s data table." />\n\nYou can now also bring datapoints directly to the Editor. Select any datapoints you want to bring to Editor (also through `x` shortcut) and you can choose to open them in Editor (or `e` shortcut) \n\n<img src="file:9c1f130d-7866-453f-a2a7-8993e0fe9f37" alt="Press `e` while selecting a datapoint to bring it into Editor" />\n\nWe think this workflow significantly improves the workflow to go from interesting datapoint to improved model config. As always, let us know if you have other feedback.\n',
    '## Cohere\n\nWe\'ve just added support for Cohere to Humanloop!\n\n<img src="file:ba760438-9c9e-459b-b09a-6bd610d0ca73" />\n\nThis update adds Cohere models to the playground and your projects - just add your Cohere API key in your [organization\'s settings](https://app.humanloop.com/account/api-keys). As with other providers, each user in your organization can also set a personal override API key, stored locally in the browser, for use in Cohere requests from the Playground.\n\n### Enabling Cohere for your organization\n\n<img src="file:34982bc2-6548-45cb-89bc-69e725b2f022" alt="Add your Cohere API key to your organization settings to start using Cohere models with Humanloop." />\n\n### Working with Cohere models\n\nOnce you\'ve successfully enabled Cohere for your organization, you\'ll be able to access it through the [playground](https://app.humanloop.com/playground) and in your projects, in exactly the same way as your existing OpenAI and/or Anthropic models.\n\n<img src="file:915b0365-88af-4650-a807-13c4215ab76b" />\n\n### REST API and Python / TypeScript support\n\nAs with other model providers, once you\'ve set up a Cohere-backed model config, you can call it with the Humanloop [REST API or our SDKs](/docs/api-reference/sdks).\n\n```typescript\nimport { Humanloop } from "humanloop";\n\nconst humanloop = new Humanloop({\n  apiKey: "API_KEY",\n});\n\nconst chatResponse = await humanloop.chat({\n  project: "project_example",\n  messages: [\n    {\n      role: "user",\n      content: "Write me a song",\n    },\n  ],\n  provider_api_keys: {\n    cohere: COHERE_API_KEY,\n  },\n  model_config: {\n    model: "command",\n    temperature: 1,\n  },\n});\n\nconsole.log(chatResponse);\n```\n\nIf you don\'t provide a Cohere API key under the `provider_api_keys` field, the request will fall back on the stored organization level key you configured above.\n',
    '## Improved Python SDK\n\nWe\'ve just released a new version of our Python SDK supporting our v4 API!\n\nThis brings support for:\n\n- 💬 Chat mode `humanloop.chat(...)`\n- 📥 Streaming support `humanloop.chat_stream(...)`\n- 🕟 Async methods `humanloop.acomplete(...)`\n\n[https://pypi.org/project/humanloop/](https://pypi.org/project/humanloop/)\n\n### Installation\n\n`pip install --upgrade humanloop`\n\n### Example usage\n\n```python\ncomplete_response = humanloop.complete(\n  project="sdk-example",\n  inputs={\n    "text": "Llamas that are well-socialized and trained to halter and lead after weaning and are very friendly and pleasant to be around. They are extremely curious and most will approach people easily. However, llamas that are bottle-fed or over-socialized and over-handled as youth will become extremely difficult to handle when mature, when they will begin to treat humans as they treat each other, which is characterized by bouts of spitting, kicking and neck wrestling.[33]",\n  },\n  model_config={\n    "model": "gpt-3.5-turbo",\n    "max_tokens": -1,\n    "temperature": 0.7,\n    "prompt_template": "Summarize this for a second-grade student:\\n\\nText:\\n{{text}}\\n\\nSummary:\\n",\n  },\n  stream=False,\n)\npprint(complete_response)\npprint(complete_response.project_id)\npprint(complete_response.data[0])\npprint(complete_response.provider_responses)\n```\n\n### Migration from `0.3.x`\n\nFor those coming from an older SDK version, this introduces some breaking changes. A brief highlight of the changes:\n\n- The client initialization step of `hl.init(...)` is now `humanloop = Humanloop(...)`.\n  - Previously `provider_api_keys` could be provided in `hl.init(...)`. They should now be provided when constructing `Humanloop(...)` client.\n  - ```python\n    humanloop = Humanloop(\n        api_key="YOUR_API_KEY",\n        openai_api_key="YOUR_OPENAI_API_KEY",\n        anthropic_api_key="YOUR_ANTHROPIC_API_KEY",\n    )\n    ```\n- `hl.generate(...)`\'s various call signatures have now been split into individual methods for clarity. The main ones are:\n  - `humanloop.complete(project, model_config={...}, ...)` for a completion with the specified model config parameters.\n  - `humanloop.complete_deployed(project, ...)` for a completion with the project\'s active deployment.\n',
    '## TypeScript SDK\n\nWe now have a fully typed TypeScript SDK to make working with Humanloop even easier.\n\n[https://www.npmjs.com/package/humanloop](https://www.npmjs.com/package/humanloop)\n\nYou can use this with your JavaScript, TypeScript or Node projects.\n\n**Installation**\n\n```shell\nnpm i humanloop\n```\n\n**Example usage**\n\n```typescript\nimport { Humanloop } from "humanloop"\n\nconst humanloop = new Humanloop({\n  // Defining the base path is optional and defaults to https://api.humanloop.com/v3\n  // basePath: "https://api.humanloop.com/v3",\n  apiKey: \'API_KEY\',\n})\n\n\nconst chatResponse = await humanloop.chat({\n  "project": "project_example",\n  "messages": [\n    {\n      "role": "user",\n      "content": "Write me a song",\n    }\n  ],\n  "provider_api_keys": {\n    "openai": OPENAI_API_KEY\n  },\n  "model_config": {\n    "model": "gpt-4",\n    "temperature": 1,\n  },\n})\n\nconsole.log(chatResponse)\n```\n',
    '## Keyboard shortcuts and datapoint links\n\n<img src="file:a772c69d-3581-4e02-9316-9bc926609700" />\n\nWe’ve added keyboard shortcuts to the datapoint viewer \n\n`g` for good  \n`b` for bad\n\nand `j` /` k` for next/prev\n\nThis should help you for quickly annotating data within your team.\n\nYou can also link to specific datapoint in the URL now as well.\n',
    "## ChatGPT support\n\nChatGPT is here! It's called 'gpt-3.5-turbo'. Try it out today in playground and on the generate endpoint.\n\nFaster and 10x cheaper than text-davinci-003.\n\n<img src=\"file:8f0bf06d-9d83-4e73-9d8a-93af21039810\" />\n",
    '## Faster datapoints table loading\n\nInitial datapoints table is now twice as fast to load! And it will continue to get faster.\n\n## Ability to open datapoint in playground\n\nAdded a way to go from the datapoint drawer to the playground with that datapoint loaded. Very convenient for trying tweaks to a model config or understanding an issue, without copy pasting.\n\n<div style="position: relative; padding-bottom: 76.37906647807637%; height: 0;">\n  <iframe src="https://www.loom.com/embed/edc690d4c9294dda9f90a939e0d83091" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>\n</div>\n\n## Markdown view and completed prompt templates\n\nWe’ve added a tab to the datapoint drawer so you can see the prompt template filled in with the inputs and output.\n\nWe’ve also button in the top right hand corner (or press `M`)  to toggle on/off viewing the text as markdown.\n\n<div style="position: relative; padding-bottom: 67.75407779171894%; height: 0;">\n  <iframe src="https://www.loom.com/embed/3db8842975dc4dcaa25b7ec079c57463" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe>\n</div>\n',
];

const anchorIds = [
    "update-logs-api",
    "search-files-by-path",
    "updated-gemini-15-models",
    "custom-attributes-for-files",
    "improved-popover-ui",
    "evaluate-uncommitted-versions",
    "human-evaluator-upgrades",
    "evaluations-improvements",
    "more-code-evaluator-packages",
    "openai-structured-outputs",
    "improved-code-evaluator-debugging",
    "select-multiple-versions-when-creating-an-evaluation",
    "faster-log-queries",
    "gpt-4o-mini-support",
    "enhanced-code-evaluators",
    "gemini-15-flash-support",
    "committing-and-deploying-ux-improvements",
    "claude-35-sonnet-support",
    "prompt-and-tool-version-drawer-in-evaluation-reports",
    "status-of-human-evaluators",
    "faster-evaluations",
    "evaluation-comparison-reports",
    "azure-model-updates",
    "improved-logs-filtering",
    "monitoring-with-deployed-evaluators",
    "gpt-4o",
    "logs-for-evaluators",
    "improved-evaluator-management",
    "log-drawer-in-editor",
    "groq-support-beta",
    "llama-3",
    "anthropic-tool-support-beta",
    "cost-tokens-and-latency",
    "cohere-command-r",
    "dataset-files--versions",
    "mixtral-8x7b",
    "surfacing-uncommitted-versions",
    "improved-navigation--sidebar",
    "claude-3",
    "new-tool-creation-flow",
    "online-evaluators-for-monitoring-tools",
    "prompt-version-authorship",
    "filterable-and-sortable-evaluations-overview",
    "projects-rename-and-file-creation-flow",
    "control-logging-level",
    "add-evaluators-to-existing-runs",
    "improved-evaluation-run-launcher",
    "faster-offline-evaluations",
    "evaluation-api-enhancements",
    "human-evaluators",
    "chat-sessions-in-editor",
    "environment-logs",
    "improved-evaluator-ui",
    "tool-linking",
    "improved-log-table-ui",
    "improved-rbacs",
    "quality-of-life-app-improvements",
    "claude-21",
    "parallel-tool-calling",
    "llm-evaluators",
    "evaluation-comparison-charts",
    "comparison-mode-in-editor",
    "improved-evaluation-runs",
    "improved-logs-drawer",
    "local-editor-history",
    "project-folders",
    "datasets",
    "get-api-tool",
    "evaluations-improvements",
    "organization-slug-in-urls",
    "allow-trusted-email-domains",
    "editor---insert-new-message-within-existing-chat",
    "claude-instant-12",
    "offline-evaluations-with-testsets",
    "improved-error-handling",
    "openai-functions-in-playground",
    "llama-2",
    "claude-2",
    "evaluators",
    "chain-llm-calls",
    "introducing-tools",
    "deployment-environments",
    "improved-python-sdk-streaming-response",
    "project-editor",
    "cohere",
    "improved-python-sdk",
    "typescript-sdk",
    "keyboard-shortcuts-and-datapoint-links",
    "chatgpt-support",
    "faster-datapoints-table-loading",
];

describe("parseMarkdownPageToAnchorTag", () => {
    it("should parse markdown and retrieve proper anchor tag", () => {
        humanloopMarkdown.map((markdown, idx) => expect(parseMarkdownPageToAnchorTag(markdown)).toBe(anchorIds[idx]));
    });
});
