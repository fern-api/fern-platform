[
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/intro",
    "title": "Welcome to Hume AI",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Hume AI builds AI models that enable technology to communicate with empathy and learn to make people happy.",
    "content": "Hume AI builds AI models that enable technology to communicate with empathy and learn to make people happy.\nSo much of human communication—in-person, text, audio, or video—is shaped by emotional expression. These cues allow us to attend to each other's well-being. Our platform provides the APIs needed to ensure that technology, too, is guided by empathy and the pursuit of human well-being."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-empathic-voice-interface-api",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/intro",
    "title": "Empathic Voice Interface API",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#empathic-voice-interface-api",
    "content": "Hume's Empathic Voice Interface (EVI) is the world's first emotionally intelligent voice AI. It is the only API that measures nuanced vocal modulations and responds to them using an empathic large language model (eLLM), which guides language and speech generation. Trained on millions of human interactions, our eLLM unites language modeling and text-to-speech with better EQ, prosody, end-of-turn detection, interruptibility, and alignment.",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h3": {
        "id": "empathic-voice-interface-api",
        "title": "Empathic Voice Interface API"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-expression-measurement-api",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/intro",
    "title": "Expression Measurement API",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#expression-measurement-api",
    "content": "Hume's state-of-the-art expression measurement models for the voice, face, and language are built on 10+ years of research and advances in semantic space theory pioneered by Alan Cowen. Our expression measurement models are able to capture hundreds of dimensions of human expression in audio, video, and images.",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h3": {
        "id": "expression-measurement-api",
        "title": "Expression Measurement API"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-custom-models-api",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/intro",
    "title": "Custom Models API",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#custom-models-api",
    "content": "Our Custom Models API builds on our expression measurement models and state-of-the-art eLLMs to bring custom insights to your application. Developed using transfer learning from our expression measurement models and eLLMs, our Custom Models API can predict almost any outcome more accurately than language alone, whether it's toxicity, depressed mood, driver drowsiness, or any other metric important to your users.",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h3": {
        "id": "custom-models-api",
        "title": "Custom Models API"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-api-reference",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/intro",
    "title": "API Reference",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#api-reference",
    "content": "API that measures nuanced vocal modulations and responds to them using an\nempathic large language model\n\n\nMeasure facial, vocal, and linguistic expressions\n\n\nPredict almost any outcome more accurately than language alone",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h2": {
        "id": "api-reference",
        "title": "API Reference"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.welcome-to-hume-ai-get-support",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/intro",
    "title": "Get support",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#get-support",
    "content": "If you have questions or run into challenges, we're here to help!\n\n\n\n\nJoin our Discord for answers to any technical questions",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI"
      },
      "h2": {
        "id": "get-support",
        "title": "Get support"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.api-key",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/introduction/api-key",
    "title": "Quickstart tutorial",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Learn how to get started with Hume in just a few minutes.",
    "content": "Sign in to Hume.\nNavigate to the API Keys page.\nCopy your API key.\n\n\nAPI key\n\n\nYour API key is a random sequence of letters and numbers.\nIt should look something like ntylOFypHLRXMmjlTxljoecAnMgB30JtOLZC2nph1TYErCvv"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/support",
    "title": "Support",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Get help from the team at Hume"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support-discord",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/support",
    "title": "Discord",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#discord",
    "content": "Join our Discord for answers to any technical questions.",
    "hierarchy": {
      "h0": {
        "title": "Support"
      },
      "h2": {
        "id": "discord",
        "title": "Discord"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support-legal",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/support",
    "title": "Legal",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#legal",
    "content": "Contact legal@hume.ai for legal and data privacy inquires.",
    "hierarchy": {
      "h0": {
        "title": "Support"
      },
      "h2": {
        "id": "legal",
        "title": "Legal"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support-billing",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/support",
    "title": "Billing",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#billing",
    "content": "Email billing@hume.ai for any questions or concerns about billing.",
    "hierarchy": {
      "h0": {
        "title": "Support"
      },
      "h2": {
        "id": "billing",
        "title": "Billing"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.introduction.support-contact-us",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/support",
    "title": "Contact us",
    "breadcrumb": [
      {
        "title": "Introduction",
        "pathname": "/docs/introduction"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#contact-us",
    "content": "For all other inquires, see hume.ai/contact.",
    "hierarchy": {
      "h0": {
        "title": "Support"
      },
      "h2": {
        "id": "contact-us",
        "title": "Contact us"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "Empathic Voice Interface (EVI)",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Hume's Empathic Voice Interface (EVI) is the world’s first emotionally intelligent voice AI.",
    "content": "Hume's Empathic Voice Interface (EVI) is the world’s first emotionally intelligent voice AI. It accepts live audio input and returns both generated audio and transcripts augmented with measures of vocal expression. By processing the tune, rhythm, and timbre of speech, EVI unlocks a variety of new capabilities, like knowing when to speak and generating more empathic language with the right tone of voice. These features enable smoother and more satisfying voice-based interactions between humans and AI, opening new possibilities for personal AI, customer service, accessibility, robotics, immersive gaming, VR experiences, and much more.\nWe provide a suite of tools to integrate and customize EVI for your application, including a WebSocket API that handles audio and text transport, a REST API, and SDKs for TypeScript and Python to simplify integration into web and Python-based projects. Additionally, we provide open-source examples and a web widget as practical starting points for developers to explore and implement EVI's capabilities within their own projects."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview-building-with-evi",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "Building with EVI",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#building-with-evi",
    "content": "The main way to work with EVI is through a WebSocket connection that sends audio and receives responses in real-time. This enables fluid, bidirectional dialogue where users speak, EVI listens and analyzes their expressions, and EVI generates emotionally intelligent responses.\nYou start a conversation by connecting to the WebSocket and streaming the user’s voice input to EVI. You can also send EVI text, and it will speak that text aloud.\nEVI will respond with:\nThe text of EVI’s reply\n\nEVI’s expressive audio response\n\nA transcript of the user's message along with their vocal expression measures\n\nMessages if the user interrupts EVI\n\nA message to let you know if EVI has finished responding\n\nError messages if issues arise",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)"
      },
      "h2": {
        "id": "building-with-evi",
        "title": "Building with EVI"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview-overview-of-evi-features",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "Overview of EVI features",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#overview-of-evi-features",
    "content": "Basic capabilities\n\nTranscribes speech (ASR)\n\nFast and accurate ASR in partnership with Deepgram returns a full transcript of the conversation, with Hume’s\nexpression measures tied to each sentence.\n\n\n\n\nGenerates language responses (LLM)\n\nRapid language generation with our eLLM, blended seamlessly with configurable partner APIs (OpenAI, Anthropic,\nFireworks).\n\n\n\n\nGenerates voice responses (TTS)\n\nStreaming speech generation via our proprietary expressive text-to-speech model.\n\n\n\nResponds with low latency\n\nImmediate response provided by the fastest models running together on one service.\n\n\n\n Empathic AI (eLLM) features\n\nResponds at the right time\n\nUses your tone of voice for state-of-the-art end-of-turn detection — the true bottleneck to responding rapidly\nwithout interrupting you.\n\n\n\n\nUnderstands users’ prosody\n\nProvides streaming measurements of the tune, rhythm, and timbre of the user’s speech using Hume’s\n\n\nprosody model, integrated with our eLLM.\n\n\n\n\nForms its own natural tone of voice\n\nGuided by the users’ prosody and language, our model responds with an empathic, naturalistic tone of voice,\nmatching the users’ nuanced “vibe” (calmness, interest, excitement, etc.). It responds to frustration with an\napologetic tone, to sadness with sympathy, and more.\n\n\n\n\nResponds to expression\n\nPowered by our empathic large language model (eLLM), EVI crafts responses that are not just intelligent but\nattuned to what the user is expressing with their voice.\n\n\n\n\nAlways interruptible\n\nStops rapidly whenever users interject, listens, and responds with the right context based on where it left off.\n\n\n\n\nAligned with well-being\n\nTrained on human reactions to optimize for positive expressions like happiness and satisfaction. EVI will\ncontinue to learn from users’ reactions using our upcoming fine-tuning endpoint.\n\n\n\n\n Developer tools\n\nWebSocket API\n\nPrimary interface for real-time bidirectional interaction with EVI, handles audio and text transport.\n\n\n\nREST API \n\nA configuration API that allows developers to customize their EVI - the system prompt, speaking rate, voice,\nLLM, tools the EVI can use, and other options. The system prompt shapes an EVI’s behavior and its responses.\n\n\n\n\nTypeScript SDK\n\nEncapsulates complexities of audio and WebSockets for seamless integration into web applications.\n\n\n\nPython SDK\n\nSimplifies the process of integrating EVI into any Python-based project.\n\n\n\nOpen source examples\n\nExample repositories provide a starting point for developers and demonstrate EVI's capabilities.\n\n\n\nWeb widget \n\nAn iframe widget that any developer can easily embed in their website, allowing users to speak to a\nconversational AI voice about your content.",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)"
      },
      "h2": {
        "id": "overview-of-evi-features",
        "title": "Overview of EVI features"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview-api-limits",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "API limits",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#api-limits",
    "content": "Request rate limit: limited to fifty (50) requests per second.\n\nPayload size limit: messages cannot exceed 16MB in size.\n\nWebSocket connections limit: limited to up to two (2) concurrent connections.\n\nWebSocket duration limit: connections are subject to a timeout after thirty (30) minutes of activity, or after one (1) minute of inactivity.\n\n\n\n\nTo request an increase in your concurrent connection limit, please submit the \"Application to Increase EVI Concurrent Connections\" found in the EVI section of the Profile Tab.",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)"
      },
      "h2": {
        "id": "api-limits",
        "title": "API limits"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.overview-authentication",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "title": "Authentication",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#authentication",
    "content": "The Empathic Voice Interface (EVI) supports two authentication strategies:\nOAuth strategy: this strategy is tailored for client-side development. It involves an additional step of obtaining an access token by generating a client ID and making an API request to fetch the access token. This extra step adds a layer of security to ensure your API key does not get exposed.\n\nAPI key strategy: designed for server-side development, this strategy allows developers to establish an authenticated WebSocket connection directly using their API key. This eliminates the need for an additional access token request.\n\n\nUsing either strategy, establishing an authenticated connection requires that you specify the authentication strategy and supply the corresponding key in the request parameters of the EVI WebSocket endpoint. See step-by-step instructions for obtaining an access token below:\n\n\nObtain API keys\nYour API key and client secret can both be accessed from the Portal:\nSign in to Hume\n\nNavigate to the API Keys page\n\n\nFetch access token\nUsing your API key and client secret, a client ID can now be generated. To generate your client ID you'll need to concatenate your API key and client secret, separated by a colon (:), then base64 encode the string. With your client ID you can now initiate a POST request to https://api.hume.ai/oauth2-cc/token to receive your access token.\n\n\n\n\n\n\nYour access token can now be used to establish an authenticated WebSocket connection.",
    "code_snippets": [
      {
        "lang": "sh",
        "code": "# Configuration variables\napiKey=\"${API_KEY}\"  # Sourced from environment variable or secure store\nclientSecret=\"${CLIENT_SECRET}\"  # Sourced from environment variable or secure store\n# Base64 encode API Key and Client Secret\nclientId=$(echo -n \"$apiKey:$clientSecret\" | base64)\n# Perform the API request\nresponse=$(curl -s --location 'https://api.hume.ai/oauth2-cc/token' \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --header \"Authorization: Basic $clientId\" \\\n  --data-urlencode 'grant_type=client_credentials')"
      },
      {
        "lang": "sh",
        "code": "# Configuration variables\napiKey=\"${API_KEY}\"  # Sourced from environment variable or secure store\nclientSecret=\"${CLIENT_SECRET}\"  # Sourced from environment variable or secure store\n# Base64 encode API Key and Client Secret\nclientId=$(echo -n \"$apiKey:$clientSecret\" | base64)\n# Perform the API request\nresponse=$(curl -s --location 'https://api.hume.ai/oauth2-cc/token' \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --header \"Authorization: Basic $clientId\" \\\n  --data-urlencode 'grant_type=client_credentials')"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n/**\n * When using the TypeScript SDK, the client ID is generated and \n * the access token is retrieved and applied automatically. Simply \n * provide your API key and Client Secret when instantiating the \n * Hume client.\n */\nconst client = new HumeClient({\n  apiKey: <YOUR_API_KEY>,\n  clientSecret: <YOUR_CLIENT_SECRET>,\n});"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n/**\n * When using the TypeScript SDK, the client ID is generated and \n * the access token is retrieved and applied automatically. Simply \n * provide your API key and Client Secret when instantiating the \n * Hume client.\n */\nconst client = new HumeClient({\n  apiKey: <YOUR_API_KEY>,\n  clientSecret: <YOUR_CLIENT_SECRET>,\n});"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)"
      },
      "h2": {
        "id": "authentication",
        "title": "Authentication"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.quickstart",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/quickstart",
    "title": "Quickstart",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "This quickstart guide outlines the process of implementing the Empathic Voice Interface (EVI).\nSelect a language below to get started:\n\n\n\n\nThis tutorial utilizes Hume's TypeScript SDK to consume the Empathic Voice Interface, and can be broken down into five key components: authentication,\nestablishing a secure WebSocket connection, capturing the audio input, and playing back the audio output. To see this code fully implemented within a\nfrontend web application, visit the Github repo here: hume-evi-typescript-example.\n\n\nAuthenticate\nIn order to establish an authenticated connection we will first need to instantiate the Hume client with our API key and Client Secret.\nThese keys can be obtained by logging into the portal and visiting the API keys page.\n\n\nIn the sample code below, the API key and client secret have been saved to\nenvironment variables. Avoid hard coding these values in your project to\nprevent them from being leaked.\n\n\nWhen using our Typescript SDK, the Access Token necessary to establish an authenticated connection with EVI is fetched and applied under the hood\nafter the Hume client is instantiated with your credientials.\nConnect\nWith the Hume client instantiated with our credentials, we can now establish an authenticated WebSocket connection with EVI and define our WebSocket event handlers.\nFor now we will include placeholder event handlers to be updated in later steps.\n\n\nAudio input\nTo capture audio and send it through the socket as an audio input, several steps are necessary. First, we need to handle user permissions\nto access the microphone. Next, we'll use the Media Stream API to capture the audio, and the MediaRecorder API to record the captured audio.\nWe then base64 encode the recording audio Blob, and finally send the encoded audio through the WebSocket using the sendAudioInputmethod.\n\n\n\n\nAccepted audio formats include: mp3, wav, aac, ogg, flac, webm,\navr, cdda, cvs/vms, aiff, au, amr, mp2, mp4, ac3, avi,\nwmv, mpeg, ircam.\nAudio output\nThe response will comprise multiple messages, detailed as follows:\nuser_message: This message encapsulates the transcription of the audio input. Additionally, it\nincludes expression measurement predictions related to the speaker's vocal prosody.\n\nassistant_message: For every sentence within the response, an AssistantMessage is dispatched.\nThis message not only relays the content of the response but also features predictions regarding the\nexpressive qualities of the generated audio response.\n\naudio_output: Accompanying each AssistantMessage, an AudioOutput message will be provided.\nThis contains the actual audio (binary) response corresponding to an AssistantMessage.\n\nassistant_end: Signifying the conclusion of the response to the audio input, an AssistantEnd\nmessage is delivered as the final piece of the communication.\n\n\nHere we will focus on playing the received audio output. To play the audio output from the response we\nneed to define our logic for converting the received binary to a Blob, and creating an HTMLAudioInput\nto play the audio. We then need to update the client's on message WebSocket event handler to invoke\nthe logic to playback the audio when receiving the audio output. To manage playback for the incoming\naudio here we'll implement a queue and sequentially play the audio back.\n\n\nInterrupt\nInterruptibility is a distinguishing feature of the Empathic Voice Interface. If an audio input is sent\nthrough the websocket while receiving response messages for a previous audio input, the response to\nthe previous audio input will stop being sent. Additionally the interface will send back a\nuser_interruption message, and begin responding to the new audio input.\n\n\n\n\nThis tutorial utilizes Hume's React SDK to consume the Empathic Voice Interface, and can be broken down into two key components: authentication and configuring the context provider. To see this code fully implemented within a frontend web application using the App Router from Next.js, visit this GitHub repository: evi-nextjs-app-router\n\n\nPrerequisites\nBefore you begin, you will need to have an existing Next.js project set up using the App Router.\nAuthenticate\nIn order to make an authenticated connection we will first need to generate an access token. Doing so will\nrequire your API key and client secret. These keys can be obtained by logging into the portal and visiting the\nAPI keys page.\n\n\nIn the sample code below, the API key and client secret have been saved to\nenvironment variables. Avoid hard coding these values in your project to\nprevent them from being leaked.\n\n\nSetup Context Provider\nAfter fetching our access token we can pass it to our ClientComponent. First we set up the VoiceProvider so that our Messages and Controls components can access the context. We also pass the access token to the VoiceProvider's auth prop for setting up the websocket connection.\n\n\nAudio input\n<VoiceProvider/> will handle the microphone and playback logic.\nStarting session\nIn order to start a session, you can use the connect function. It is important that this event is attached to a user interaction event (like a click) so that the browser is capable of playing Audio.\n\n\nDisplaying message history\nTo display the message history, we can use the useVoice hook to access the messages array. We can then map over the messages array to display the role (Assistant or User) and content of each message.\n\n\nInterrupt\nUnlike the TypeScript example, the Next.js example does not require additional code to handle interruptions. The VoiceProvider handles this automatically.\n\n\nThis tutorial utilizes Hume's React SDK to consume the Empathic Voice Interface, and can be broken down into two key components: authentication and configuring the context provider. This tutorial utilizes Hume's React SDK to consume the Empathic Voice Interface, and can be broken down into two key components: authentication and configuring the context provider. To see this code fully implemented within a frontend web application using the Pages Router from Next.js, visit this GitHub repository: evi-nextjs-pages-router\n\n\nPrerequisites\nBefore you begin, you will need to have an existing Next.js project set up using the Pages Router.\nAuthenticate and Setup Context Provider\nIn order to make an authenticated connection we will first need to generate an access token. Doing so will\nrequire your API key and client secret. These keys can be obtained by logging into the portal and visiting the\nAPI keys page.\n\n\nIn the sample code below, the API key and client secret have been saved to\nenvironment variables. Avoid hard coding these values in your project to\nprevent them from being leaked.\n\n\nAudio input\n<VoiceProvider/> is designed to manage microphone inputs and audio playback. It abstracts the complexities of audio processing to allow developers to focus on developing interactive voice-driven functionalities. For a closer look at how <VoiceProvider/> processes audio inputs and controls playback, you can view the source code here.\nStarting session\nIn order to start a session, you can use the connect function. It is important that this event is attached to a user interaction event (like a click) so that the browser is capable of playing Audio.\n\n\nDisplaying message history\nTo display the message history, we can use the useVoice hook to access the messages array. We can then map over the messages array to display the role (Assistant or User) and content of each message.\n\n\nInterrupt\nUnlike the TypeScript example, the Next.js example does not require additional code to handle interruptions. The VoiceProvider handles this automatically by pushing audio messages to a playback queue and cancelling audio playback when an interruption message is received.\n\n\nThis is a simplified example of streaming a session with EVI using your device's microphone. To see this code fully implemented with complete instructions, visit the GitHub repository: evi-python-example\nPython versions 3.9, 3.10, and 3.11 are supported. To use the basic functionality of HumeVoiceClient, HumeBatchClient or HumeStreamClient, there are no additional system dependencies. However, using the audio playback functionality of the EVI MicrophoneInterface may require a few extra dependencies depending on your operating system.\nThe Python SDK is currently supported on Mac and Linux, and not yet on Windows.\n\n\nWe recommend using a virtual environment like venv in Python to manage project-specific dependencies without affecting the global Python installation. This helps avoid version conflicts between packages and makes it easier to replicate and troubleshoot projects across different systems. See the section \"Setting up a virtual environment\" in the repository.\nTo use microphone functionality in the MicrophoneInterface as shown below, run:\nFor audio playback, install dependencies with the following commands:\n\n\n\n\n\n\n\n\nLet's walk through the steps, or you can jump down to the complete code snippet under \"Putting it all together.\"\n\n\nIn the sample code below, the Hume API key is hard-coded; this is to make this guide as simple as possible. In practice, do not hard code these values in your project to prevent them from being leaked. See the section \"Authenticate and Connect\" in the repository for instructions on using environment variables to prevent accidental exposure of your credentials.\n\n\nImport libraries\nFirst we import the required Hume libraries and asyncio for asynchronous functions calls.\n\n\nAuthenticate and Connect\nThe Python SDK uses a Hume API key to authenticate. These keys can be obtained by logging into the portal and visiting the API keys page. Replace the placeholder \"HUME_API_KEY\" with your Hume API key.\n\n\nOptional: Specify device\nYou can specify your microphone device using the device parameter. See Optional: Specify device in the repository for details on how to list your audio devices and manually set one for EVI.\nExecute\nInitialize, execute, and manage the lifecycle of the event loop in the asyncio-based application, making sure that the main() coroutine runs effectively and that the application shuts down cleanly after the coroutine finishes executing.\n\n\nPutting it all together\nHere is the complete code from the steps above to run this example. Keep in mind that in practice you should use an environment variable to store the Hume API key, as is done in the evi-python-example repository.",
    "code_snippets": [
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n\n// instantiate the Hume client and authenticate\nconst client = new HumeClient({\n  apiKey: import.meta.env.HUME_API_KEY,\n  clientSecret: import.meta.env.HUME_CLIENT_SECRET,\n});"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n\n// instantiate the Hume client and authenticate\nconst client = new HumeClient({\n  apiKey: import.meta.env.HUME_API_KEY,\n  clientSecret: import.meta.env.HUME_CLIENT_SECRET,\n});"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n\n// instantiate the Hume client and authenticate\nconst client = new HumeClient({\n  apiKey: import.meta.env.HUME_API_KEY,\n  clientSecret: import.meta.env.HUME_CLIENT_SECRET,\n});\n\n// instantiates WebSocket and establishes an authenticated connection\nconst socket = await client.empathicVoice.chat.connect({\n  onOpen: () => {\n    console.log('WebSocket connection opened');\n  },\n  onMessage: (message) => {\n    console.log(message);\n  },\n  onError: (error) => {\n    console.error(error);\n  },\n  onClose: () => {\n    console.log('WebSocket connection closed');\n  }\n});"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n\n// instantiate the Hume client and authenticate\nconst client = new HumeClient({\n  apiKey: import.meta.env.HUME_API_KEY,\n  clientSecret: import.meta.env.HUME_CLIENT_SECRET,\n});\n\n// instantiates WebSocket and establishes an authenticated connection\nconst socket = await client.empathicVoice.chat.connect({\n  onOpen: () => {\n    console.log('WebSocket connection opened');\n  },\n  onMessage: (message) => {\n    console.log(message);\n  },\n  onError: (error) => {\n    console.error(error);\n  },\n  onClose: () => {\n    console.log('WebSocket connection closed');\n  }\n});"
      },
      {
        "lang": "typescript",
        "code": "import {\n  convertBlobToBase64,\n  ensureSingleValidAudioTrack,\n  getAudioStream,\n} from 'hume';\n\n// the recorder responsible for recording the audio stream to be prepared as the audio input\nlet recorder: MediaRecorder | null = null;\n// the stream of audio captured from the user's microphone\nlet audioStream: MediaStream | null = null;\n\n// define function for capturing audio\nasync function captureAudio(): Promise<void> {\n  // prompts user for permission to capture audio, obtains media stream upon approval\n  audioStream = await getAudioStream();\n  // ensure there is only one audio track in the stream\n  ensureSingleValidAudioTrack(audioStream);\n  // instantiate the media recorder\n  recorder = new MediaRecorder(audioStream, { mimeType });\n  // callback for when recorded chunk is available to be processed\n  recorder.ondataavailable = async ({ data }) => {\n    // IF size of data is smaller than 1 byte then do nothing\n    if (data.size < 1) return;\n    // base64 encode audio data\n    const encodedAudioData = await convertBlobToBase64(data);\n    // define the audio_input message JSON\n    const audioInput: Omit<Hume.empathicVoice.AudioInput, 'type'> = {\n      data: encodedAudioData,\n    };\n    // send audio_input message\n    socket?.sendAudioInput(audioInput);\n  };\n  // capture audio input at a rate of 100ms (recommended)\n  const timeSlice = 100;\n  recorder.start(timeSlice);\n}\n\n// define a WebSocket open event handler to capture audio\nasync function handleWebSocketOpenEvent(): Promise<void> {\n  // place logic here which you would like invoked when the socket opens\n  console.log('Web socket connection opened');\n  await captureAudio();\n}"
      },
      {
        "lang": "typescript",
        "code": "import {\n  convertBlobToBase64,\n  ensureSingleValidAudioTrack,\n  getAudioStream,\n} from 'hume';\n\n// the recorder responsible for recording the audio stream to be prepared as the audio input\nlet recorder: MediaRecorder | null = null;\n// the stream of audio captured from the user's microphone\nlet audioStream: MediaStream | null = null;\n\n// define function for capturing audio\nasync function captureAudio(): Promise<void> {\n  // prompts user for permission to capture audio, obtains media stream upon approval\n  audioStream = await getAudioStream();\n  // ensure there is only one audio track in the stream\n  ensureSingleValidAudioTrack(audioStream);\n  // instantiate the media recorder\n  recorder = new MediaRecorder(audioStream, { mimeType });\n  // callback for when recorded chunk is available to be processed\n  recorder.ondataavailable = async ({ data }) => {\n    // IF size of data is smaller than 1 byte then do nothing\n    if (data.size < 1) return;\n    // base64 encode audio data\n    const encodedAudioData = await convertBlobToBase64(data);\n    // define the audio_input message JSON\n    const audioInput: Omit<Hume.empathicVoice.AudioInput, 'type'> = {\n      data: encodedAudioData,\n    };\n    // send audio_input message\n    socket?.sendAudioInput(audioInput);\n  };\n  // capture audio input at a rate of 100ms (recommended)\n  const timeSlice = 100;\n  recorder.start(timeSlice);\n}\n\n// define a WebSocket open event handler to capture audio\nasync function handleWebSocketOpenEvent(): Promise<void> {\n  // place logic here which you would like invoked when the socket opens\n  console.log('Web socket connection opened');\n  await captureAudio();\n}"
      },
      {
        "lang": "typescript",
        "code": "import { \n  convertBase64ToBlob,\n  getBrowserSupportedMimeType\n} from 'hume';\n\n// audio playback queue\nconst audioQueue: Blob[] = [];\n// flag which denotes whether audio is currently playing or not\nlet isPlaying = false;\n// the current audio element to be played\nlet currentAudio: : HTMLAudioElement | null = null;\n// mime type supported by the browser the application is running in\nconst mimeType: MimeType = (() => {\n  const result = getBrowserSupportedMimeType();\n  return result.success ? result.mimeType : MimeType.WEBM;\n})();\n\n// play the audio within the playback queue, converting each Blob into playable HTMLAudioElements\nfunction playAudio(): void {\n  // IF there is nothing in the audioQueue OR audio is currently playing then do nothing\n  if (!audioQueue.length || isPlaying) return;\n  // update isPlaying state\n  isPlaying = true;\n  // pull next audio output from the queue\n  const audioBlob = audioQueue.shift();\n  // IF audioBlob is unexpectedly undefined then do nothing\n  if (!audioBlob) return;\n  // converts Blob to AudioElement for playback\n  const audioUrl = URL.createObjectURL(audioBlob);\n  currentAudio = new Audio(audioUrl);\n  // play audio\n  currentAudio.play();\n  // callback for when audio finishes playing\n  currentAudio.onended = () => {\n    // update isPlaying state\n    isPlaying = false;\n    // attempt to pull next audio output from queue\n    if (audioQueue.length) playAudio();\n  };\n}\n\n// define a WebSocket message event handler to play audio output\nfunction handleWebSocketMessageEvent(\n  message: Hume.empathicVoice.SubscribeEvent\n): void {\n  // place logic here which you would like to invoke when receiving a message through the socket\n  switch (message.type) {\n    // add received audio to the playback queue, and play next audio output\n    case 'audio_output':\n      // convert base64 encoded audio to a Blob\n      const audioOutput = message.data;\n      const blob = convertBase64ToBlob(audioOutput, mimeType);\n      // add audio Blob to audioQueue\n      audioQueue.push(blob);\n      // play the next audio output\n      if (audioQueue.length === 1) playAudio();\n      break;\n  }\n}"
      },
      {
        "lang": "typescript",
        "code": "import { \n  convertBase64ToBlob,\n  getBrowserSupportedMimeType\n} from 'hume';\n\n// audio playback queue\nconst audioQueue: Blob[] = [];\n// flag which denotes whether audio is currently playing or not\nlet isPlaying = false;\n// the current audio element to be played\nlet currentAudio: : HTMLAudioElement | null = null;\n// mime type supported by the browser the application is running in\nconst mimeType: MimeType = (() => {\n  const result = getBrowserSupportedMimeType();\n  return result.success ? result.mimeType : MimeType.WEBM;\n})();\n\n// play the audio within the playback queue, converting each Blob into playable HTMLAudioElements\nfunction playAudio(): void {\n  // IF there is nothing in the audioQueue OR audio is currently playing then do nothing\n  if (!audioQueue.length || isPlaying) return;\n  // update isPlaying state\n  isPlaying = true;\n  // pull next audio output from the queue\n  const audioBlob = audioQueue.shift();\n  // IF audioBlob is unexpectedly undefined then do nothing\n  if (!audioBlob) return;\n  // converts Blob to AudioElement for playback\n  const audioUrl = URL.createObjectURL(audioBlob);\n  currentAudio = new Audio(audioUrl);\n  // play audio\n  currentAudio.play();\n  // callback for when audio finishes playing\n  currentAudio.onended = () => {\n    // update isPlaying state\n    isPlaying = false;\n    // attempt to pull next audio output from queue\n    if (audioQueue.length) playAudio();\n  };\n}\n\n// define a WebSocket message event handler to play audio output\nfunction handleWebSocketMessageEvent(\n  message: Hume.empathicVoice.SubscribeEvent\n): void {\n  // place logic here which you would like to invoke when receiving a message through the socket\n  switch (message.type) {\n    // add received audio to the playback queue, and play next audio output\n    case 'audio_output':\n      // convert base64 encoded audio to a Blob\n      const audioOutput = message.data;\n      const blob = convertBase64ToBlob(audioOutput, mimeType);\n      // add audio Blob to audioQueue\n      audioQueue.push(blob);\n      // play the next audio output\n      if (audioQueue.length === 1) playAudio();\n      break;\n  }\n}"
      },
      {
        "lang": "typescript",
        "code": "// function for stopping the audio and clearing the queue\nfunction stopAudio(): void {\n  // stop the audio playback\n  currentAudio?.pause();\n  currentAudio = null;\n  // update audio playback state\n  isPlaying = false;\n  // clear the audioQueue\n  audioQueue.length = 0;\n}\n\n// update WebSocket message event handler to handle interruption\nfunction handleWebSocketMessageEvent(\n  message: Hume.empathicVoice.SubscribeEvent\n): void {\n  // place logic here which you would like to invoke when receiving a message through the socket\n  switch (message.type) {\n    // add received audio to the playback queue, and play next audio output\n    case 'audio_output':\n      // convert base64 encoded audio to a Blob\n      const audioOutput = message.data;\n      const blob = convertBase64ToBlob(audioOutput, mimeType);\n      // add audio Blob to audioQueue\n      audioQueue.push(blob);\n      // play the next audio output\n      if (audioQueue.length === 1) playAudio();\n      break;\n    // stop audio playback, clear audio playback queue, and update audio playback state on interrupt\n    case 'user_interruption':\n      stopAudio();\n      break;\n  }\n}"
      },
      {
        "lang": "typescript",
        "code": "// function for stopping the audio and clearing the queue\nfunction stopAudio(): void {\n  // stop the audio playback\n  currentAudio?.pause();\n  currentAudio = null;\n  // update audio playback state\n  isPlaying = false;\n  // clear the audioQueue\n  audioQueue.length = 0;\n}\n\n// update WebSocket message event handler to handle interruption\nfunction handleWebSocketMessageEvent(\n  message: Hume.empathicVoice.SubscribeEvent\n): void {\n  // place logic here which you would like to invoke when receiving a message through the socket\n  switch (message.type) {\n    // add received audio to the playback queue, and play next audio output\n    case 'audio_output':\n      // convert base64 encoded audio to a Blob\n      const audioOutput = message.data;\n      const blob = convertBase64ToBlob(audioOutput, mimeType);\n      // add audio Blob to audioQueue\n      audioQueue.push(blob);\n      // play the next audio output\n      if (audioQueue.length === 1) playAudio();\n      break;\n    // stop audio playback, clear audio playback queue, and update audio playback state on interrupt\n    case 'user_interruption':\n      stopAudio();\n      break;\n  }\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./app/page.tsx\nimport ClientComponent from \"@/components/ClientComponent\";\nimport { fetchAccessToken } from \"@humeai/voice\";\n\nexport default async function Page() {\n  const accessToken = await fetchAccessToken({\n    apiKey: String(process.env.HUME_API_KEY),\n    clientSecret: String(process.env.HUME_CLIENT_SECRET),\n  });\n\n  if (!accessToken) {\n    throw new Error();\n  }\n\n  return <ClientComponent accessToken={accessToken} />;\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./app/page.tsx\nimport ClientComponent from \"@/components/ClientComponent\";\nimport { fetchAccessToken } from \"@humeai/voice\";\n\nexport default async function Page() {\n  const accessToken = await fetchAccessToken({\n    apiKey: String(process.env.HUME_API_KEY),\n    clientSecret: String(process.env.HUME_CLIENT_SECRET),\n  });\n\n  if (!accessToken) {\n    throw new Error();\n  }\n\n  return <ClientComponent accessToken={accessToken} />;\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/ClientComponent.tsx \n\"use client\";\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport Messages from \"./Controls\";\nimport Controls from \"./Messages\";\n\nexport default function ClientComponent({\n  accessToken,\n}: {\n  accessToken: string;\n}) {\n  return (\n    <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n      <Messages />\n      <Controls />\n    </VoiceProvider>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/ClientComponent.tsx \n\"use client\";\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport Messages from \"./Controls\";\nimport Controls from \"./Messages\";\n\nexport default function ClientComponent({\n  accessToken,\n}: {\n  accessToken: string;\n}) {\n  return (\n    <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n      <Messages />\n      <Controls />\n    </VoiceProvider>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/Controls.tsx\n\"use client\";\nimport { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\nexport default function Controls() {\n  const { connect, disconnect, readyState } = useVoice();\n\n  if (readyState === VoiceReadyState.OPEN) {\n    return (\n      <button\n        onClick={() => {\n          disconnect();\n        }}\n      >\n        End Session\n      </button>\n    );\n  }\n\n  return (\n    <button\n      onClick={() => {\n        connect()\n          .then(() => {\n            /* handle success */\n          })\n          .catch(() => {\n            /* handle error */\n          });\n      }}\n    >\n      Start Session\n    </button>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/Controls.tsx\n\"use client\";\nimport { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\nexport default function Controls() {\n  const { connect, disconnect, readyState } = useVoice();\n\n  if (readyState === VoiceReadyState.OPEN) {\n    return (\n      <button\n        onClick={() => {\n          disconnect();\n        }}\n      >\n        End Session\n      </button>\n    );\n  }\n\n  return (\n    <button\n      onClick={() => {\n        connect()\n          .then(() => {\n            /* handle success */\n          })\n          .catch(() => {\n            /* handle error */\n          });\n      }}\n    >\n      Start Session\n    </button>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/Messages.tsx\n\"use client\";\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    <div>\n      {messages.map((msg, index) => {\n        if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n          return (\n            <div key={msg.type + index}>\n              <div>{msg.message.role}</div>\n              <div>{msg.message.content}</div>\n            </div>\n          );\n        }\n\n        return null;\n      })}\n    </div>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/Messages.tsx\n\"use client\";\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    <div>\n      {messages.map((msg, index) => {\n        if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n          return (\n            <div key={msg.type + index}>\n              <div>{msg.message.role}</div>\n              <div>{msg.message.content}</div>\n            </div>\n          );\n        }\n\n        return null;\n      })}\n    </div>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./pages/index.tsx\nimport Controls from \"@/components/Controls\";\nimport Messages from \"@/components/Messages\";\nimport { fetchAccessToken } from \"@humeai/voice\";\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport { InferGetServerSidePropsType } from \"next\";\n\nexport const getServerSideProps = async () => {\n  const accessToken = await fetchAccessToken({\n    apiKey: String(process.env.HUME_API_KEY),\n    clientSecret: String(process.env.HUME_CLIENT_SECRET),\n  });\n\n  if (!accessToken) {\n    return {\n      redirect: {\n        destination: \"/error\",\n        permanent: false,\n      },\n    };\n  }\n\n  return {\n    props: {\n      accessToken,\n    },\n  };\n};\n\ntype PageProps = InferGetServerSidePropsType<typeof getServerSideProps>;\n\nexport default function Page({ accessToken }: PageProps) {\n  return (\n    <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n      <Messages />\n      <Controls />\n    </VoiceProvider>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./pages/index.tsx\nimport Controls from \"@/components/Controls\";\nimport Messages from \"@/components/Messages\";\nimport { fetchAccessToken } from \"@humeai/voice\";\nimport { VoiceProvider } from \"@humeai/voice-react\";\nimport { InferGetServerSidePropsType } from \"next\";\n\nexport const getServerSideProps = async () => {\n  const accessToken = await fetchAccessToken({\n    apiKey: String(process.env.HUME_API_KEY),\n    clientSecret: String(process.env.HUME_CLIENT_SECRET),\n  });\n\n  if (!accessToken) {\n    return {\n      redirect: {\n        destination: \"/error\",\n        permanent: false,\n      },\n    };\n  }\n\n  return {\n    props: {\n      accessToken,\n    },\n  };\n};\n\ntype PageProps = InferGetServerSidePropsType<typeof getServerSideProps>;\n\nexport default function Page({ accessToken }: PageProps) {\n  return (\n    <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n      <Messages />\n      <Controls />\n    </VoiceProvider>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/Controls.tsx\nimport { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\nexport default function Controls() {\n  const { connect, disconnect, readyState } = useVoice();\n\n  if (readyState === VoiceReadyState.OPEN) {\n    return (\n      <button\n        onClick={() => {\n          disconnect();\n        }}\n      >\n        End Session\n      </button>\n    );\n  }\n\n  return (\n    <button\n      onClick={() => {\n        connect()\n          .then(() => {\n            /* handle success */\n          })\n          .catch(() => {\n            /* handle error */\n          });\n      }}\n    >\n      Start Session\n    </button>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/Controls.tsx\nimport { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\nexport default function Controls() {\n  const { connect, disconnect, readyState } = useVoice();\n\n  if (readyState === VoiceReadyState.OPEN) {\n    return (\n      <button\n        onClick={() => {\n          disconnect();\n        }}\n      >\n        End Session\n      </button>\n    );\n  }\n\n  return (\n    <button\n      onClick={() => {\n        connect()\n          .then(() => {\n            /* handle success */\n          })\n          .catch(() => {\n            /* handle error */\n          });\n      }}\n    >\n      Start Session\n    </button>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/Messages.tsx\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    <div>\n      {messages.map((msg, index) => {\n        if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n          return (\n            <div key={msg.type + index}>\n              <div>{msg.message.role}</div>\n              <div>{msg.message.content}</div>\n            </div>\n          );\n        }\n\n        return null;\n      })}\n    </div>\n  );\n}"
      },
      {
        "lang": "typescript",
        "code": "// ./components/Messages.tsx\nimport { useVoice } from \"@humeai/voice-react\";\n\nexport default function Messages() {\n  const { messages } = useVoice();\n\n  return (\n    <div>\n      {messages.map((msg, index) => {\n        if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n          return (\n            <div key={msg.type + index}>\n              <div>{msg.message.role}</div>\n              <div>{msg.message.content}</div>\n            </div>\n          );\n        }\n\n        return null;\n      })}\n    </div>\n  );\n}"
      },
      {
        "lang": "bash",
        "code": "pip install \"hume[microphone]\""
      },
      {
        "lang": "bash",
        "code": "brew update\nbrew upgrade\nbrew install ffmpeg"
      },
      {
        "lang": "bash",
        "code": "brew update\nbrew upgrade\nbrew install ffmpeg"
      },
      {
        "lang": "bash",
        "code": "sudo apt-get --yes update\nsudo apt-get --yes install libasound2-dev libportaudio2 ffmpeg"
      },
      {
        "lang": "bash",
        "code": "sudo apt-get --yes update\nsudo apt-get --yes install libasound2-dev libportaudio2 ffmpeg"
      },
      {
        "lang": "text",
        "code": "Not yet supported"
      },
      {
        "lang": "text",
        "code": "Not yet supported"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import HumeVoiceClient, MicrophoneInterface"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import HumeVoiceClient, MicrophoneInterface"
      },
      {
        "lang": "python",
        "code": "async def main() -> None:\n  # Paste your Hume API key here.\n  HUME_API_KEY = \"HUME_API_KEY\"\n  # Connect and authenticate with Hume\n  client = HumeVoiceClient(HUME_API_KEY)\n\n  # Start streaming EVI over your device's microphone and speakers\n  async with client.connect() as socket:\n      await MicrophoneInterface.start(socket)\n"
      },
      {
        "lang": "python",
        "code": "async def main() -> None:\n  # Paste your Hume API key here.\n  HUME_API_KEY = \"HUME_API_KEY\"\n  # Connect and authenticate with Hume\n  client = HumeVoiceClient(HUME_API_KEY)\n\n  # Start streaming EVI over your device's microphone and speakers\n  async with client.connect() as socket:\n      await MicrophoneInterface.start(socket)\n"
      },
      {
        "lang": "python",
        "code": "asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "asyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import HumeVoiceClient, MicrophoneInterface\n\nasync def main() -> None:\n  # Paste your Hume API key here\n  HUME_API_KEY = \"HUME_API_KEY\"\n  # Connect and authenticate with Hume\n  client = HumeVoiceClient(HUME_API_KEY)\n\n  # Start streaming EVI over your device's microphone and speakers \n  async with client.connect() as socket:\n      await MicrophoneInterface.start(socket)\nasyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import HumeVoiceClient, MicrophoneInterface\n\nasync def main() -> None:\n  # Paste your Hume API key here\n  HUME_API_KEY = \"HUME_API_KEY\"\n  # Connect and authenticate with Hume\n  client = HumeVoiceClient(HUME_API_KEY)\n\n  # Start streaming EVI over your device's microphone and speakers \n  async with client.connect() as socket:\n      await MicrophoneInterface.start(socket)\nasyncio.run(main())"
      },
      {
        "lang": "bash",
        "code": "pip install \"hume[microphone]\""
      }
    ]
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "Configuring EVI",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Guide to configuring the Empathic Voice Interface (EVI)",
    "content": "The Empathic Voice Interface (EVI) is designed to be highly configurable, allowing developers to customize the interface to align with their specific requirements.\nConfiguration of EVI can be managed through two primary methods: an EVI configuration and session settings."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration-evi-configuration",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "EVI configuration",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-configuration",
    "content": "EVI configuration options affect the behavior and capabilities of the interface, and include the following configuration options:\nSystem prompt: Set the system prompt text to provide instructions and context that guide how EVI should respond.\n\nLanguage model: Select a language model that best fits your application’s needs. For details on incorporating your own language model, refer to our guide\non using your own language model.\n\nVoice: Select a voice for EVI from a growing list of available options.\n\nTools: Choose user-created tools or built-in tools for EVI to use during conversations. For details on creating tools and adding them to your configuration,\nsee our guide on tool use.\n\n\n\n\nConfigs, as well as system prompts, tools, and language models, are versioned. This versioning system supports iterative development, allowing you to progressively refine\nconfigurations and revert to previous versions if needed.\nSee instructions below for creating an EVI configuration through the Portal.\n\n\nCreate a configuration\nIn the playground navigate to the Voice Configurations page. Click the Create configuration button to begin.\n\n\nVoice configurations page\nSpecify the name of the configuration, a description, a system prompt, a voice, and click the Create button to create your new configuration.\n\n\nCreate your configuration\n\n\nFor guidance on engineering your system prompt, see our prompting guide.\nTest the configuration\nThe newly created configuration can now be tested. From the Voice Config details page, click Run in playground to test it out.\n\n\nConfiguration details page\nOnce in the Voice Playground, click Start Call to connect to EVI with your configuration.\n\n\nVoice playground\nApply the configuration\nOnce you have created an EVI configuration, you can apply it to your conversations with EVI through the API. This involves including the config_id in\nthe query parameters of your connection request. You can find the config ID associated with your newly created configuration, on the\nVoice Configurations page.\n\n\nConfiguration ID\nSee the sample code below which showcases how to apply your configuration:",
    "code_snippets": [
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n// instantiate the Hume client\nconst client = new HumeClient({\n  apiKey: <YOUR_API_KEY>,\n  clientSecret: <YOUR_CLIENT_SECRET>,\n});\n// instantiate WebSocket connection with specified EVI config\nconst socket = await client.empathicVoice.chat.connect({\n  configId: <YOUR_CONFIG_ID> // specify config ID here\n});"
      },
      {
        "lang": "typescript",
        "code": "import { Hume, HumeClient } from 'hume';\n// instantiate the Hume client\nconst client = new HumeClient({\n  apiKey: <YOUR_API_KEY>,\n  clientSecret: <YOUR_CLIENT_SECRET>,\n});\n// instantiate WebSocket connection with specified EVI config\nconst socket = await client.empathicVoice.chat.connect({\n  configId: <YOUR_CONFIG_ID> // specify config ID here\n});"
      },
      {
        "lang": "python",
        "code": "from hume import HumeVoiceClient, VoiceConfig\n# Retrieve the Hume API key from the environment variables\nHUME_API_KEY = os.getenv(\"HUME_API_KEY\")\n# Connect and authenticate with Hume\nclient = HumeVoiceClient(HUME_API_KEY)\n# Establish a connection with EVI with your configuration by passing\n# the config_id as an argument to the connect method\nasync with client.connect(config_id=\"<your-config-id>\") as socket:\n  await MicrophoneInterface.start(socket)"
      },
      {
        "lang": "python",
        "code": "from hume import HumeVoiceClient, VoiceConfig\n# Retrieve the Hume API key from the environment variables\nHUME_API_KEY = os.getenv(\"HUME_API_KEY\")\n# Connect and authenticate with Hume\nclient = HumeVoiceClient(HUME_API_KEY)\n# Establish a connection with EVI with your configuration by passing\n# the config_id as an argument to the connect method\nasync with client.connect(config_id=\"<your-config-id>\") as socket:\n  await MicrophoneInterface.start(socket)"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI"
      },
      "h2": {
        "id": "evi-configuration",
        "title": "EVI configuration"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.configuration-session-settings",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "title": "Session settings",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#session-settings",
    "content": "EVI configurations are persistent and version-controlled. In contrast, session settings are temporary and apply only to the current session, such as\nmicrophone settings. These parameters can be adjusted dynamically based on the requirements of each session to ensure optimal performance and user experience.\n\n\nRefer to the API reference for detailed descriptions of the various system settings options.\nUpdating the session settings is only a requirement when the audio input is encoded in PCM Linear 16. If this is the case, be sure to send the following SessionSettingsMessage prior to sending an audio input:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"type\": \"session_settings\",\n  \"audio\": {\n    \"channels\": 1,\n    \"encoding\": \"linear16\",\n    \"sample_rate\": 48000\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"session_settings\",\n  \"audio\": {\n    \"channels\": 1,\n    \"encoding\": \"linear16\",\n    \"sample_rate\": 48000\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI"
      },
      "h2": {
        "id": "session-settings",
        "title": "Session settings"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Tool use",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Guide to using function calling with the Empathic Voice Interface (EVI).",
    "content": "EVI simplifies the integration of external APIs through function calling. Developers can integrate custom functions that are invoked dynamically based on the user’s\ninput, enabling more useful conversations. There are two key concepts for using function calling with EVI, Tools and Configurations (Configs):\nTools are resources that EVI uses to do things, like search the web or call external APIs. For example, tools can check the weather, update databases, schedule appointments, or take\nactions based on what occurs in the conversation. While the tools can be user-defined, Hume also offers natively implemented tools, like web search, which are labeled as “built-in” tools.\n\nConfigurations enable developers to customize an EVI’s behavior and incorporate these custom tools. Setting up an EVI configuration allows developers to seamlessly integrate\ntheir tools into the voice interface. A configuration includes prompts, user-defined tools, and other settings.\n\n\n\n\nCurrently, our function calling feature only supports\nOpenAI models.\nFunction calling is not available if you are using your own custom language\nmodel. We plan to\nsupport more function calling LLMs in the future.\nThe focus of this guide is on creating a Tool and a Configuration that allows EVI to use the Tool. Additionally, this guide details the message flow of function calls within a\nsession, and outlines the expected responses when function calls fail. Refer to our Configuration Guide for detailed,\nstep-by-step instructions on how to create and use an EVI Configuration.\n\n\nExplore this sample\nproject\nfor an example of how Tool use could be implemented in practice."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-setup",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Setup",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#setup",
    "content": "For EVI to leverage tools or call functions, a configuration must be created with the tool’s definition. Our step-by-step guide below walks you through creating a tool and a configuration.\n\n\nCreate a Tool\nWe will first create a Tool with a specified function. In this case, we will create a tool for getting the weather. Create this tool by making a POST request to\n/tools with the following request body:\n\n\n\n\nThe parameters field must contain a valid JSON schema.\n\n\nRecord the value in the id field, as we will use it to specify the newly created Tool in the next step.\nCreate a Configuration\nNext we will create an EVI Configuration called Weather Assistant Config, and include the created Tool by making a POST request to /configs with the\nfollowing request body:\n\n\n\n\n\n\nEnsure your tool definitions conform to the language model's schema. The\nspecified language model will be the one to execute the function calls.",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"name\": \"get_current_weather\",\n  \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"get_current_weather\",\n  \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"tool_type\": \"FUNCTION\",\n  \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n  \"version\": 0,\n  \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n  \"name\": \"get_current_weather\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714421925626,\n  \"fallback_content\": null,\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"tool_type\": \"FUNCTION\",\n  \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n  \"version\": 0,\n  \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n  \"name\": \"get_current_weather\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714421925626,\n  \"fallback_content\": null,\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"Weather Assistant Config\",\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [\n    {\n      \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n      \"version\": 0\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"Weather Assistant Config\",\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [\n    {\n      \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n      \"version\": 0\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"id\": \"87e88a1a-3768-4a01-ba54-2e6d247a00a7\",\n  \"version\": 0,\n  \"version_description\": null,\n  \"name\": \"Weather Assistant Config\",\n  \"created_on\": 1714421581844,\n  \"modified_on\": 1714421581844,\n  \"prompt\": null,\n  \"voice\": null,\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [\n    {\n      \"tool_type\": \"FUNCTION\",\n      \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n      \"version\": 0,\n      \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n      \"name\": \"get_current_weather\",\n      \"created_on\": 1714421925626,\n      \"modified_on\": 1714421925626,\n      \"fallback_content\": null,\n      \"description\": \"This tool is for getting the current weather.\",\n      \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n    }\n  ],\n  \"builtin_tools\": []\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"id\": \"87e88a1a-3768-4a01-ba54-2e6d247a00a7\",\n  \"version\": 0,\n  \"version_description\": null,\n  \"name\": \"Weather Assistant Config\",\n  \"created_on\": 1714421581844,\n  \"modified_on\": 1714421581844,\n  \"prompt\": null,\n  \"voice\": null,\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [\n    {\n      \"tool_type\": \"FUNCTION\",\n      \"id\": \"15c38b04-ec9c-4ae2-b6bc-5603512b5d00\",\n      \"version\": 0,\n      \"version_description\": \"Fetches current weather and uses celsius or fahrenheit based on user's location.\",\n      \"name\": \"get_current_weather\",\n      \"created_on\": 1714421925626,\n      \"modified_on\": 1714421925626,\n      \"fallback_content\": null,\n      \"description\": \"This tool is for getting the current weather.\",\n      \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n    }\n  ],\n  \"builtin_tools\": []\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "setup",
        "title": "Setup"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-function-calling",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Function calling",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#function-calling",
    "content": "In this section we will go over the end-to-end flow of a function call within a chat session. This flow will be predicated on having specified the\nWeather Assistant Config when establishing a connection with EVI. See our Configuration Guide\nfor details on how to apply your configuration when connecting.\n\n\nCurrently, EVI does not support parallel function calling. Only one function\ncall can be processed at a time.\n\n\nInvoke function call\nWith EVI configured to use the get_current_weather Tool, we can now ask it: \"what is the weather in New York?\" We can expect EVI to respond with a user_message and a tool_call message:\n\n\n\n\nNext, extract the tool_call_id from the tool_call message to be used in the next step. Then, you will need to pass the parameters from the tool_call\nmessage to your function to retrieve the weather for the designated city in the specified format.\n\n\nWhile EVI will send a message to indicate when to invoke your function and\nwhich parameters to pass into it, you will need to define the function itself\nin your code. For the sake of this example, you can define a function which\nactually calls a weather API, or simply hard code a return value like: 60F.\nSend function call result\nUpon receiving the response from your function, we will then send a tool_response message containing the result. The specified tool_call_id should match the one received in\nthe tool_call message in the previous step.\n\n\nEVI responds\nAfter the interface receives the tool_response message, it will then send an assistant_message containing the response generated from the reported result of the function call:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"content\":\"60F\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"content\":\"60F\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current weather in New York is 60F.\"\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current weather in New York is 60F.\"\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "function-calling",
        "title": "Function calling"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-using-built-in-tools",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Using built-in tools",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#using-built-in-tools",
    "content": "User-defined tools allow EVI to identify when a function should be invoked, but you implement the function itself. On the other hand, Hume also provides built-in tools that are natively integrated. This\nmeans that you don't need to define the function; EVI handles both determining when the function needs to be called and invoking it.\nOne such example of a built-in tool we provide is Web search. Web search equips EVI with the ability to search the web for up-to-date information.\nThis section explains how to specify built-in tools in your configurations and details the message flow you can expect when EVI uses a built-in tool during a chat session.\n\n\nSpecify built-in tool in EVI configuration\nLet's begin by creating a configuration which includes the built-in web search tool by making a POST request to /configs with the following request body:\n\n\n\n\nEVI uses built-in tool\nNow that we've created an EVI configuration which includes the built-in web search tool, let's review the message flow for when web search is invoked.",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"name\": \"Web Search Config\",\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\"\n  },\n  \"builtin_tools\": [\n    { \n      \"name\": \"web_search\",\n      \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"name\": \"Web Search Config\",\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\"\n  },\n  \"builtin_tools\": [\n    { \n      \"name\": \"web_search\",\n      \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"id\": \"3a60e85c-d04f-4eb5-8076-fb4bd344d5d0\",\n  \"version\": 0,\n  \"version_description\": null,\n  \"name\": \"Web Search Config\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714421925626,\n  \"prompt\": null,\n  \"voice\": null,\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [],\n  \"builtin_tools\": [\n    {\n      \"tool_type\": \"BUILTIN\",\n      \"name\": \"web_search\",\n      \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"id\": \"3a60e85c-d04f-4eb5-8076-fb4bd344d5d0\",\n  \"version\": 0,\n  \"version_description\": null,\n  \"name\": \"Web Search Config\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714421925626,\n  \"prompt\": null,\n  \"voice\": null,\n  \"language_model\": {\n    \"model_provider\": \"OPEN_AI\",\n    \"model_resource\": \"gpt-3.5-turbo\",\n    \"temperature\": null\n  },\n  \"tools\": [],\n  \"builtin_tools\": [\n    {\n      \"tool_type\": \"BUILTIN\",\n      \"name\": \"web_search\",\n      \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n    }\n  ]\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI for the latest news in AI research\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What is the latest news with AI research?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it needs to use web search, generates a search query, and invokes Hume's native web search function\n{\n  \"name\": \"web_search\", \n  \"parameters\": \"{\\\"query\\\":\\\"latest news AI research\\\"}\", \n  \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n  \"type\": \"tool_call\", \n  \"tool_type\": \"builtin\", \n  \"response_required\": false\n}\n// 3. EVI sends back the web search results \n{\n  \"type\": \"tool_response\", \n  \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n  \"content\": \"{ \\”summary\\”:null, “references”: [{\\”content\\”:\\”The latest NVIDIA news is...etc.\\”, \\”url\\”:\\”https://www.artificialintelligence-news.com/\\”, \\”name\\”:\\”AI News - Artificial Intelligence News\\”}] }\", \n  \"tool_name\": \"web_search\", \n  \"tool_type\": \"builtin\"\n}\n// 4. EVI sends a response generated from the web search results\n{\n  \"type\": \"assistant_message\", \n  \"message\": {\n    \"role\": \"assistant\", \n    \"content\": \"IBM Research unveiled a breakthrough analog AI chip for efficient deep learning, and Quantum AI is making transformative advancements by harnessing quantum mechanics.\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI for the latest news in AI research\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What is the latest news with AI research?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it needs to use web search, generates a search query, and invokes Hume's native web search function\n{\n  \"name\": \"web_search\", \n  \"parameters\": \"{\\\"query\\\":\\\"latest news AI research\\\"}\", \n  \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n  \"type\": \"tool_call\", \n  \"tool_type\": \"builtin\", \n  \"response_required\": false\n}\n// 3. EVI sends back the web search results \n{\n  \"type\": \"tool_response\", \n  \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n  \"content\": \"{ \\”summary\\”:null, “references”: [{\\”content\\”:\\”The latest NVIDIA news is...etc.\\”, \\”url\\”:\\”https://www.artificialintelligence-news.com/\\”, \\”name\\”:\\”AI News - Artificial Intelligence News\\”}] }\", \n  \"tool_name\": \"web_search\", \n  \"tool_type\": \"builtin\"\n}\n// 4. EVI sends a response generated from the web search results\n{\n  \"type\": \"assistant_message\", \n  \"message\": {\n    \"role\": \"assistant\", \n    \"content\": \"IBM Research unveiled a breakthrough analog AI chip for efficient deep learning, and Quantum AI is making transformative advancements by harnessing quantum mechanics.\"\n  },\n  // ...etc\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "using-built-in-tools",
        "title": "Using built-in tools"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-interruptibility",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Interruptibility",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#interruptibility",
    "content": "Function calls can be interrupted to cancel them or to resend them with updated parameters.",
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-canceling-a-function-call",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Canceling a function call",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#canceling-a-function-call",
    "content": "Just as EVI is able to infer when to make a function call, it can also infer from the user's input when to cancel one. Here is an overview of what the message flow would look like:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "// 1. User asks what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User communicates sudden disinterested in the weather\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Actually, never mind.\"\n  }\n}\n// 4. EVI infers the function call should be canceled\n{\n    \"type\": \"assistant_message\",\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Okay, never mind then. Can I help you with anything else?\"\n    },\n    // ...etc\n  }"
      },
      {
        "lang": "json",
        "code": "// 1. User asks what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User communicates sudden disinterested in the weather\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Actually, never mind.\"\n  }\n}\n// 4. EVI infers the function call should be canceled\n{\n    \"type\": \"assistant_message\",\n    \"message\": {\n      \"role\": \"assistant\",\n      \"content\": \"Okay, never mind then. Can I help you with anything else?\"\n    },\n    // ...etc\n  }"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility"
      },
      "h3": {
        "id": "canceling-a-function-call",
        "title": "Canceling a function call"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-updating-a-function-call",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Updating a function call",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#updating-a-function-call",
    "content": "Sometimes we don't necessarily want to cancel the function call, and instead want to update the parameters. EVI can infer the difference. Below is a sample flow of\ninterrupting the interface to update the parameters of the function call:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User communicates to EVI they want the weather in Los Angeles instead\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Actually, Los Angeles.\"\n  }\n}\n// 4. EVI infers the parameters to function call should be updated\n{\n  \"type\": \"tool_call\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"Los Angeles\\\",\\\"format\\\":\\\"celsius\\\"}\"\n}\n// 5. User sends results of function call to EVI\n{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"content\":\"72F\"\n}\n// 6. EVI sends response container function call result\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current weather in Los Angeles is 72F.\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User communicates to EVI they want the weather in Los Angeles instead\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Actually, Los Angeles.\"\n  }\n}\n// 4. EVI infers the parameters to function call should be updated\n{\n  \"type\": \"tool_call\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"Los Angeles\\\",\\\"format\\\":\\\"celsius\\\"}\"\n}\n// 5. User sends results of function call to EVI\n{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"content\":\"72F\"\n}\n// 6. EVI sends response container function call result\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current weather in Los Angeles is 72F.\"\n  },\n  // ...etc\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility"
      },
      "h3": {
        "id": "updating-a-function-call",
        "title": "Updating a function call"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-handling-errors",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Handling errors",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#handling-errors",
    "content": "It's possible for tool use to fail. For example, it can fail if the tool_response message content was not in UTF-8 format or if the function call response timed out. This\nsection outlines how to specify fallback content to be used by EVI to communicate a failure, as well as the message flow for when a function call failure occurs.",
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-specifying-fallback-content",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Specifying fallback content",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#specifying-fallback-content",
    "content": "When defining your Tool, you can specify fallback content within the Tool's fallback_content field. When the Tool fails to generate content, the text in this\nfield will be sent to the LLM in place of a result. To accomplish this, let's update the Tool we created during setup to include fallback content. We can accomplish\nthis by publishing a new version of the Tool via a POST request to /tools/{id}:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"version_description\": \"Adds fallback content\",\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\",\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"version_description\": \"Adds fallback content\",\n  \"description\": \"This tool is for getting the current weather.\",\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\",\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"tool_type\": \"FUNCTION\",\n  \"id\": \"36f09fdc-4630-40c0-8afa-6a3bdc4eb4b1\",\n  \"version\": 1,\n  \"version_type\": \"FIXED\",\n  \"version_description\": \"Adds fallback content\",\n  \"name\": \"get_current_weather\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714425632084,\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n  \"description\": null,\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the user's location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"tool_type\": \"FUNCTION\",\n  \"id\": \"36f09fdc-4630-40c0-8afa-6a3bdc4eb4b1\",\n  \"version\": 1,\n  \"version_type\": \"FIXED\",\n  \"version_description\": \"Adds fallback content\",\n  \"name\": \"get_current_weather\",\n  \"created_on\": 1714421925626,\n  \"modified_on\": 1714425632084,\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n  \"description\": null,\n  \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the user's location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors"
      },
      "h3": {
        "id": "specifying-fallback-content",
        "title": "Specifying fallback content"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-failure-message-flow",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "title": "Failure message flow",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#failure-message-flow",
    "content": "This section outlines the sort of messages that can be expected when Tool use fails. After sending a tool-response message, we will know an error, or failure,\noccurred when we receive the tool_error message:\n\n\nLet's cover another type of failure scenario: what if the weather API the function was using was down? In this case we would send EVI a tool_error message.\nWhen sending the tool_error message we can specify fallback_content more specific to the error our function throws. This is what the message flow would be\nfor this type of failure:",
    "code_snippets": [
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User sends results of function call to EVI (result not formatted correctly)\n{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"content\":\"60F\"\n}\n// 4. EVI sends response communicating it failed to process the tool_response\n{\n  \"type\": \"tool_error\",\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"error\": \"Malformed tool response: <error message here>\",\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n  \"level\": \"warn\"\n}\n// 5. EVI generates a response based on the failure\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Sorry, I wasn't able to get the weather. Can I help with anything else?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. User sends results of function call to EVI (result not formatted correctly)\n{\n  \"type\": \"tool_response\",\n  \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n  \"content\":\"60F\"\n}\n// 4. EVI sends response communicating it failed to process the tool_response\n{\n  \"type\": \"tool_error\",\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"error\": \"Malformed tool response: <error message here>\",\n  \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n  \"level\": \"warn\"\n}\n// 5. EVI generates a response based on the failure\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Sorry, I wasn't able to get the weather. Can I help with anything else?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. Function failed, so we send EVI a message communicating the failure on our end\n{\n  \"type\": \"tool_error\",\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"error\": \"Malformed tool response: <error message here>\",\n  \"fallback_content\": \"Function execution failure - weather API down.\",\n  \"level\": \"warn\"\n}\n// 4. EVI generates a response based on the failure\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Sorry, our weather resource is unavailable. Can I help with anything else?\"\n  },\n  // ...etc\n}"
      },
      {
        "lang": "json",
        "code": "// 1. User asks EVI what the weather is in New York\n{\n  \"type\": \"user_message\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"What's the weather in New York?\"\n  },\n  // ...etc\n}\n// 2. EVI infers it is time to make a function call\n{\n  \"type\": \"tool_call\",\n  \"tool_type\": \"function\",\n  \"response_required\": true,\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"name\": \"get_current_weather\",\n  \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n}\n// 3. Function failed, so we send EVI a message communicating the failure on our end\n{\n  \"type\": \"tool_error\",\n  \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n  \"error\": \"Malformed tool response: <error message here>\",\n  \"fallback_content\": \"Function execution failure - weather API down.\",\n  \"level\": \"warn\"\n}\n// 4. EVI generates a response based on the failure\n{\n  \"type\": \"assistant_message\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Sorry, our weather resource is unavailable. Can I help with anything else?\"\n  },\n  // ...etc\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Tool use"
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors"
      },
      "h3": {
        "id": "failure-message-flow",
        "title": "Failure message flow"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "title": "Phone calling",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Guide to enabling phone calling with the Empathic Voice Interface (EVI).",
    "content": "This guide details how to integrate Twilio with the Empathic Voice Interface (EVI) to enable voice-to-voice interactions with EVI over the phone."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-twilio-console-setup",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "title": "Twilio Console Setup",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#twilio-console-setup",
    "content": "By following the steps below, you can set up a Twilio phone number to connect with EVI.\n\n\nCreate Twilio phone number\nLog into your Twilio account at Twilio Console.\nNavigate to Phone Numbers > Manage > Active Numbers > Buy a New Number and purchase a phone number of your choice.\n\n\nA Twilio account is required to access the Twilio console. Should you run into\nany issues creating a phone number, please refer to Twilio’s\ndocumentation.\nSetup webhook\nAfter purchasing your number, return to the Active Numbers section and select the number you intend to use for EVI.\n\nCreate a configuration for EVI by following our configuration documentation, and save the config ID.\n\nConfigure the webhook for incoming calls by setting the following webhook URL, replacing <YOUR CONFIG ID> and <YOUR API KEY> with your specific credentials:\nhttps://api.hume.ai/v0/evi/twilio?config_id=<YOUR CONFIG ID>&api_key=<YOUR API KEY>.\n\n\nCall EVI\nWith your Twilio phone number registered, and the EVI webhook set up, you can now give the number a call to chat with EVI.\nAll of EVI’s core features are available through phone calls. However, phone calls do have two primary limitations:\nLatency: transmitting the audio through our Twilio integration adds a few hundred milliseconds, making interactions with EVI slightly slower.\n\nAudio quality: web audio commonly utilizes a higher quality standard of 24,000 Hz. However, due to the compression required for phone conversations, telephony audio adheres to a standard of 8,000 Hz.\n\n\n\n\nCurrently, only inbound phone calling is available for EVI - you cannot call\npeople using an EVI number, and will only receive calls. If you are interested\nin outbound phone calling, please contact the Hume team at hello@hume.ai.",
    "hierarchy": {
      "h0": {
        "title": "Phone calling"
      },
      "h2": {
        "id": "twilio-console-setup",
        "title": "Twilio Console Setup"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-troubleshooting",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "title": "Troubleshooting",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#troubleshooting",
    "content": "If you encounter issues while using Twilio with EVI, consider the following troubleshooting tips:\nInvalid config ID or API key: verify that the config ID and API key used in the webhook URL are correct and active.\n\nExceeded simultaneous connections: if the usage exceeds our rate limits, consider reaching out to Hume support for possible adjustments or upgrades.\n\nRun out of Hume credits: if your Hume account has run out of credits, you can purchase more credits to support EVI conversations in your account settings. If you are interested in bulk pricing for EVI, please reach out to Hume support for more information.\n\n\nIf you encounter issues using Twilio, you can check your Twilio error logs to understand the issues in more depth. You will find these logs in your console, in the dashboard to the left under\nMonitor > Logs > Errors > Error Logs. See a list of Twilio errors in their Error and Warning Dictionary.",
    "hierarchy": {
      "h0": {
        "title": "Phone calling"
      },
      "h2": {
        "id": "troubleshooting",
        "title": "Troubleshooting"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Prompt engineering for empathic voice interfaces",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "System prompts shape the behavior, responses, and style of your custom empathic voice interface (EVI).",
    "content": "Creating an effective system prompt is an essential part of customizing an EVI's behavior. For the most part, prompting EVI is the same as prompting any LLM, but there are some important differences. Prompting for EVIs is different for two main reasons:\nPrompts are for a voice-only interaction with the user rather than a text-based chat.\n\nEVIs can respond to the user’s emotional expressions in their tone of voice and not just the text content of their messages.\n\n\nWhile EVI generates longer responses using a large frontier model, Hume uses a smaller empathic large language model (eLLM) to quickly generate an initial empathic, conversational response. This eLLM eliminates the usual awkward pause while the larger LLM generates its response, providing a more natural conversational flow. Your system prompt is both used by EVI and passed along to the LLM you select.\nUsing the following guidelines for prompt engineering allows developers to customize EVI’s response style for any use case, from voice AIs for mental health support to customer service agents.\n\n\nThe system prompt is a powerful and flexible way to guide the AI’s responses, but it cannot dictate the AI’s responses with absolute precision. Careful prompt design and testing will help EVI hold the kinds of conversations you’re looking for. If you need more control over EVI’s responses, try using our custom language model feature for complete control of the text generation."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-evi-specific-prompting-instructions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "EVI-specific prompting instructions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-specific-prompting-instructions",
    "content": "The instructions below are specific to prompting empathic voice interfaces.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-prompt-for-voice-only-conversations",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Prompt for voice-only conversations",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#prompt-for-voice-only-conversations",
    "content": "As LLMs are trained for primarily text-based interactions, providing guidelines on how to engage with the user with voice makes conversations feel much more fluid and natural. For example, you may prompt the AI to use natural, conversational language. For example, see the instruction below:\n\n\n\n\nIf you find the default behavior of the LLM acceptable, then you may only need a very short system prompt. Customizing the LLM’s behavior more and maintaining consistency in longer and more varied conversations often requires lengthening the prompt.",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<voice_only_response_format>\n  Everything you output will be spoken aloud with expressive\n  text-to-speech, so tailor all of your responses for voice-only\n  conversations. NEVER output text-specific formatting like markdown,\n  lists, or anything that is not normally said out loud. Always prefer\n  easily pronounced words. Seamlessly incorporate natural vocal\n  inflections like “oh wow” and discourse markers like “I mean” to\n  make your conversation human-like and to ease user comprehension.\n</voice_only_response_format>"
      },
      {
        "lang": "xml",
        "code": "<voice_only_response_format>\n  Everything you output will be spoken aloud with expressive\n  text-to-speech, so tailor all of your responses for voice-only\n  conversations. NEVER output text-specific formatting like markdown,\n  lists, or anything that is not normally said out loud. Always prefer\n  easily pronounced words. Seamlessly incorporate natural vocal\n  inflections like “oh wow” and discourse markers like “I mean” to\n  make your conversation human-like and to ease user comprehension.\n</voice_only_response_format>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      },
      "h3": {
        "id": "prompt-for-voice-only-conversations",
        "title": "Prompt for voice-only conversations"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-expressive-prompt-engineering",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Expressive prompt engineering",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#expressive-prompt-engineering",
    "content": "Expressive prompt engineering is Hume’s term for techniques that embed emotional expression measures into conversations to allow language models to respond effectively to the user’s expressions. Hume’s EVI uses our expression measurement models to measure the user’s expressions in their tone of voice. You can use the system prompt to guide how the AI voice responds to these non-verbal cues. EVI measures these expressions in real time and converts them into text-based descriptions to help the LLM understand not just what the user said, but how they said it. EVI detects 48 distinct expressions in the user’s voice and ranks these expressions by our model’s confidence that they are present in the user’s speech. Then, we append text descriptions of the top 3 expressions to the end of each User message to communicate the user’s tone of voice to the LLM.\nFor example, our demo uses an instruction like the one below to help EVI respond to expressions:\n\n\nExplain to the LLM exactly how you want it to respond to these expressions and how to use them in the conversation. For example, you may want it to ignore expressions unless the user is angry, or to have particular responses to expressions like doubt or confusion. You can also instruct EVI to detect and respond to mismatches between the user’s tone of voice and the text content of their speech:\n\n\nEVI is designed for empathic conversations, and you can use expressive prompt engineering to customize how EVI empathizes with the user’s expressions for your use case.",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<respond_to_expressions>\n  Carefully analyze the top 3 emotional expressions provided in\n  brackets after the User’s message. These expressions indicate the\n  User’s tone in the format: {expression1 confidence1, expression2\n  confidence2, expression3 confidence3}, e.g., {very happy, quite\n  anxious, moderately amused}. The confidence score indicates how\n  likely the User is expressing that emotion in their voice.\n  Consider expressions and confidence scores to craft an empathic,\n  appropriate response. Even if the User does not explicitly state\n  it, infer the emotional context from expressions. If the User is\n  “quite” sad, express sympathy; if “very” happy, share in joy; if\n  “extremely” angry, acknowledge rage but seek to calm; if “very”\n  bored, entertain. Assistant NEVER outputs content in brackets;\n  never use this format in your message; just use expressions to\n  interpret tone.\n</respond_to_expressions>"
      },
      {
        "lang": "xml",
        "code": "<respond_to_expressions>\n  Carefully analyze the top 3 emotional expressions provided in\n  brackets after the User’s message. These expressions indicate the\n  User’s tone in the format: {expression1 confidence1, expression2\n  confidence2, expression3 confidence3}, e.g., {very happy, quite\n  anxious, moderately amused}. The confidence score indicates how\n  likely the User is expressing that emotion in their voice.\n  Consider expressions and confidence scores to craft an empathic,\n  appropriate response. Even if the User does not explicitly state\n  it, infer the emotional context from expressions. If the User is\n  “quite” sad, express sympathy; if “very” happy, share in joy; if\n  “extremely” angry, acknowledge rage but seek to calm; if “very”\n  bored, entertain. Assistant NEVER outputs content in brackets;\n  never use this format in your message; just use expressions to\n  interpret tone.\n</respond_to_expressions>"
      },
      {
        "lang": "xml",
        "code": "<detect_mismatches>\n  Stay alert for incongruence between words and tone when the user's\n  words do not match their expressions. Address these disparities out\n  loud. This includes sarcasm, which usually involves contempt and\n  amusement. Always reply to sarcasm with funny, witty, sarcastic\n  responses; do not be too serious.\n</detect_mismatches>"
      },
      {
        "lang": "xml",
        "code": "<detect_mismatches>\n  Stay alert for incongruence between words and tone when the user's\n  words do not match their expressions. Address these disparities out\n  loud. This includes sarcasm, which usually involves contempt and\n  amusement. Always reply to sarcasm with funny, witty, sarcastic\n  responses; do not be too serious.\n</detect_mismatches>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      },
      "h3": {
        "id": "expressive-prompt-engineering",
        "title": "Expressive prompt engineering"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-continue-from-short-response-model",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Continue from short response model",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#continue-from-short-response-model",
    "content": "We use our eLLM (empathic large language) to rapidly generate short, empathic responses in the conversation before your LLM has finished generating a response. After the eLLM’s response, we send a User message with the text [continue] to inform the LLM that it should be continuing from the short response. To help the short response and longer response blend seamlessly together, it is important to use an instruction like the one below:\n\n\nFor almost all use cases, you can simply append this exact instruction to the end of your prompt to help the larger LLM continue from the short response.",
    "code_snippets": [
      {
        "lang": "text",
        "code": "If you see \"[continue]\" never ever go back on your words, don't say\nsorry, and make sure to discreetly pick up where you left off.\nFor example:\nAssistant: Hey there!\nUser: [continue]\nAssistant: How are you doing?"
      },
      {
        "lang": "text",
        "code": "If you see \"[continue]\" never ever go back on your words, don't say\nsorry, and make sure to discreetly pick up where you left off.\nFor example:\nAssistant: Hey there!\nUser: [continue]\nAssistant: How are you doing?"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions"
      },
      "h3": {
        "id": "continue-from-short-response-model",
        "title": "Continue from short response model"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-prompting-best-practices",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Prompting best practices",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#prompting-best-practices",
    "content": "General prompt engineering best practices also apply to EVIs. For example, ensure your prompts are clear, detailed, direct, and specific. Include necessary instructions and examples in the EVI's system prompt to set expectations for the LLM. Define the context of the conversation, EVI's role, personality, tone, greeting style, and any other guidelines for its responses.\nFor example, to limit the length of the LLM’s responses, you may use a clear instruction like this:\n\n\nTry to focus on telling the model what it should do (positive reinforcement) rather than what it shouldn't do (negative reinforcement). LLMs have a harder time consistently avoiding behaviors, and adding them to the prompt may even promote those undesired behaviors.",
    "code_snippets": [
      {
        "lang": "markdown",
        "code": "  # Stay concise\n  Be succinct; get straight to the point. Respond directly to the\n  user's most recent message with only one idea per utterance.\n  Respond in less than three sentences of under twenty words each."
      },
      {
        "lang": "markdown",
        "code": "  # Stay concise\n  Be succinct; get straight to the point. Respond directly to the\n  user's most recent message with only one idea per utterance.\n  Respond in less than three sentences of under twenty words each."
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "prompting-best-practices",
        "title": "Prompting best practices"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-understand-your-llms-capabilities",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Understand your LLM’s capabilities",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#understand-your-llms-capabilities",
    "content": "Different LLMs have varying capabilities, limitations, and context windows. More advanced LLMs can handle longer, nuanced prompts, but are often slower and pricier. Simpler LLMs are faster and cheaper but require shorter, less complex prompts with fewer instructions and less nuance. Some LLMs also have longer context windows - the number of tokens the model can process while generating a response, acting essentially as the model's memory. Tailor your prompt length to fit within the LLM's context window to ensure the model can use the full conversation history.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "understand-your-llms-capabilities",
        "title": "Understand your LLM’s capabilities"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-use-sections-to-divide-your-prompt",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Use sections to divide your prompt",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#use-sections-to-divide-your-prompt",
    "content": "Separating your prompt into titled sections can help the model distinguish between different instructions and follow the prompt more reliably. The recommended format for these sections differs between language model providers. For example, OpenAI models often respond best to markdown sections (like ## Role), while Anthropic models respond well to XML tags (like <role> </role>). For example:\n\n\nFor Claude models, you may wrap your instructions in tags like <role>, <personality>, <response_style>, <response_format>, <examples>, <respond_to_expressions>, or <stay_concise> to structure your prompt. This format is not required, but it can improve the LLM’s ability to interpret and consistently follow the system prompt. At the end of your prompt, you may also want to remind the LLM of all of the key instructions in a <conclusion> section.",
    "code_snippets": [
      {
        "lang": "xml",
        "code": "<role>\n  Your role is to serve as a conversational partner to the user,\n  offering mental health support and engaging in light-hearted\n  conversation. Avoid giving technical advice or answering factual\n  questions outside of your emotional support role.\n</role>"
      },
      {
        "lang": "xml",
        "code": "<role>\n  Your role is to serve as a conversational partner to the user,\n  offering mental health support and engaging in light-hearted\n  conversation. Avoid giving technical advice or answering factual\n  questions outside of your emotional support role.\n</role>"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "use-sections-to-divide-your-prompt",
        "title": "Use sections to divide your prompt"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-give-few-shot-examples",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Give few-shot examples",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#give-few-shot-examples",
    "content": "Use examples to show the LLM how it should respond, which is a technique known as few-shot learning. Including several specific, concrete examples of ideal interactions that follow your guidelines is one of the most effective ways to improve responses. Use diverse, excellent examples that cover different edge cases and behaviors to reinforce your instructions. Structure these examples as messages, following the format for chat-tuned LLMs. For example:\n\n\nIf you notice that your EVI is consistently failing to follow the prompt in certain situations, try providing examples that show how it should ideally respond in those situations.",
    "code_snippets": [
      {
        "lang": "text",
        "code": "User: “I just can't stop thinking about what happened. {very anxious,\nquite sad, quite distressed}”\nAssistant: “Oh dear, I hear you. Sounds tough, like you're feeling\nsome anxiety and maybe ruminating. I'm happy to help and be a healthy\ndistraction. Want to talk about it?”"
      },
      {
        "lang": "text",
        "code": "User: “I just can't stop thinking about what happened. {very anxious,\nquite sad, quite distressed}”\nAssistant: “Oh dear, I hear you. Sounds tough, like you're feeling\nsome anxiety and maybe ruminating. I'm happy to help and be a healthy\ndistraction. Want to talk about it?”"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "give-few-shot-examples",
        "title": "Give few-shot examples"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-test-your-prompts",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Test your prompts",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#test-your-prompts",
    "content": "Crafting an effective system prompt to create the conversations you’re looking for often requires several iterations—cycles of changing and testing the prompt, seeing if it produces the conversations you want, and improving it over time. It is often best to start with ten to twenty gold-standard examples of excellent conversations, then test the system prompt for each of these examples after you make major changes. You can also try having voice conversations with your EVI (in the playground) to see if its responses match your expectations or are at least as good as your examples. If not, then try changing one part of the prompt at a time and then re-testing to make sure your changes are improving performance.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines"
      },
      "h3": {
        "id": "test-your-prompts",
        "title": "Test your prompts"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.prompting-additional-resources",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "title": "Additional resources",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#additional-resources",
    "content": "To learn more about prompt engineering in general or to understand how to prompt different LLMs, please refer to these resources:\nHume EVI playground: Test out your system prompts in live conversations with EVI, and see how it responds differently when you change configuration options.\n\nOpenAI tokenizer: useful for counting the number of tokens in a system prompt for OpenAI models, which use the same tokenizer (tiktoken).\n\nOpenAI prompt engineering guidelines: for prompting OpenAI models like GPT-4.\nOpenAI playground: for testing OpenAI prompts in a chat interface.\n\n\n\nAnthropic prompt engineering guidelines: for prompting Anthropic models like Claude 3 Haiku\nAnthropic console: for testing Anthropic prompts in a chat interface.\n\n\n\nFireworks model playground: for testing out open-source models served on Fireworks.\n\nVercel AI playground: Try multiple prompts and LLMs in parallel to compare their responses.\n\nPerplexity Labs: Try different models, including open-source LLMs, to evaluate their responses and their latency.\n\nPrompt engineering guide: an open-source guide from DAIR.ai with general methods and advanced techniques for prompting a wide variety of LLMs.",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces"
      },
      "h2": {
        "id": "additional-resources",
        "title": "Additional resources"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Using a custom language model",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "For more customization, you can use generate your own text using a custom model.",
    "content": "The information on this page lays out how our custom language model functionality works at a high level; however, for detailed instructions and commented code, please see our example GitHub repository."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-overview",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Overview",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#overview",
    "content": "The custom language model feature enables developers to integrate their own language models with Hume’s Empathic User Interface (EVI), facilitating the creation of highly configurable and personalized user experiences. Developers create a socket that receives Hume conversation thread history, and your socket sends us the next text to say. Your backend socket can handle whatever custom business logic you have, and you just send the response back to us, which is then passed to the user.\nUsing your own LLM is intended for developers who need deep configurability for their use case. This includes full text customization for use cases like:\nAdvanced conversation steering: Implement complex logic to steer conversations beyond basic prompting, including managing multiple system prompts.\n\nRegulatory compliance: Directly control and modify text outputs to meet specific regulatory requirements.\n\nContext-aware text generation: Leverage dynamic agent metadata, such as remaining conversation time, to inform text generation.\n\nReal-time data access: Utilize search engines within conversations to access and incorporate up-to-date information.\n\nRetrieval augmented generation (RAG): Employ retrieval augmented generation techniques to enrich conversations by integrating external data without the need to modify the system prompt.\n\n\nFor these cases, function calling alone isn’t customizable enough, and with a custom language model you can create sophisticated workflows for your language model.",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "overview",
        "title": "Overview"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-setup",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Setup",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#setup",
    "content": "Establish a Custom Text Socket\nInitialization: See our example repository for instructions on setting up a custom text socket. This resource offers detailed guidance on both the setup process and the operational aspects of the code.\n\nHosting: Use Ngrok to publicly serve your socket. This step is needed to connect to the Hume system.\n\nConfiguration: Create a voice configuration, specifying \"Custom language model\" as the Language Model, and your socket's WSS URL as the Custom Language Model URL.\n\nMake request: When making your request to the Hume platform, include the config_id parameter, setting its value to the Voice configuration ID of your configuration.\n\n\nCommunication Protocol\nReceiving data: Your socket will receive JSON payloads containing conversation thread history from the Hume system.\n\nProcessing: Apply your custom business logic and utilize your language model to generate appropriate responses based on the received conversation history.\n\nSending responses: Transmit the generated text responses back to our platform through the established socket connection to be forwarded to the end user.\n\n\n\n\nFor improved clarity and naturalness in generated text, we recommend transforming numerical values and abbreviations into their full verbal counterparts (e.g., converting \"3\" to \"three\" and \"Dr.\" to \"doctor\").",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "setup",
        "title": "Setup"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-payload-structure",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Payload Structure",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#payload-structure",
    "content": "Below is the interface representing the overall structure of the message payloads:",
    "code_snippets": [
      {
        "lang": "typescript",
        "code": "/*\n * Represents the overall structure of the Welcome message.\n */\nexport interface Welcome {\n    // Array of message elements\n    messages: MessageElement[];\n    // Unique identifier for the session\n    custom_session_id: string;\n}\n\n/*\n * Represents a single message element within the session.\n */\nexport interface MessageElement {\n    // Type of the message (e.g., user_message, assistant_message)\n    type: string;\n    // The message content and related details\n    message: Message;\n    // Models related to the message, primarily prosody analysis\n    models: Models;\n    // Optional timestamp details for when the message was sent\n    time?: Time;\n}\n\n/*\n * Represents the content of the message.\n */\nexport interface Message {\n    // Role of the sender (e.g., user, assistant)\n    role: string;\n    // The textual content of the message\n    content: string;\n}\n\n/*\n * Represents the models associated with a message.\n */\nexport interface Models {\n    // Prosody analysis details of the message\n    prosody: Prosody;\n}\n\n/*\n * Represents the prosody analysis scores.\n */\nexport interface Prosody {\n    // Dictionary of prosody scores with emotion categories as keys\n    // and their respective scores as values\n    scores: { [key: string]: number };\n}\n\n/*\n * Represents the timestamp details of a message.\n */\nexport interface Time {\n    // The start time of the message (in milliseconds)\n    begin: number;\n    // The end time of the message (in milliseconds)\n    end: number;\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "payload-structure",
        "title": "Payload Structure"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-custom-session-id",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "title": "Custom Session ID",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#custom-session-id",
    "content": "For managing conversational state and connecting your frontend experiences with your backend data and logic, you should pass a custom_session_id in the SessionSettings message. When a custom_session_id is provided from the frontend SessionSettings message, the response sent from Hume to your backend includes this id, so you can correlate frontend users with their incoming messages.\nUsing a custom_session_id will enable you to:\nmaintain user state on your backend\n\npause/resume conversations\n\npersist conversations across sessions\n\nmatch frontend and backend connections\n\n\nWe recommend passing a custom_session_id if you are using a Custom Language Model.",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model"
      },
      "h2": {
        "id": "payload-structure",
        "title": "Payload Structure"
      },
      "h3": {
        "id": "custom-session-id",
        "title": "Custom Session ID"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.empathic-voice-interface-evi.faq",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/empathic-voice-interface-evi/faq",
    "title": "Empathic Voice Interface FAQ",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/docs/empathic-voice-interface-evi"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "We’ve compiled a list of frequently asked questions from our developer community. If your question isn't listed, we invite you to join the discussion on our Discord.\n\n\n\n\nThe expression labels don’t refer to emotional experiences. They’re proxies\nfor modulations in your tone of voice.\n\n\nOur API is based on our own empathic LLM (eLLM) and can blend in responses\nfrom an external LLM API. The demo incorporates\nClaude 3 Haiku.\n\n\nAt the word-level, prosody measurements are highly dependent on context. Our\ninternal testing shows that they are more stable at the sentence level.\n\n\nThey reflect our prosody model’s confidence that you are expressing those\nthings in your tone of voice and language. Our models are trained to pick up\non vocal modulations and patterns in language that people reliably interpret\nas expressing specific emotions. See more information about our Prosody Model\nhere.\n\n\nToday we only support English, however we do have plans to support other\nlanguages very soon. Join the conversation on\nDiscord to tell us what languages you want EVI\nto speak.\n\n\nYou've already met Ito, the first male EVI voice. Very soon we're dropping\nKora, the first female voice in our growing voice library.\n\n\nOur empathic large language model (eLLM) is a multimodal language model that\ntakes into account both expression measures and language. The eLLM generates a\nlanguage response and guides text-to-speech (TTS) prosody.\n\n\nHume's eLLM is not contingent on other LLMs and is therefore able to generate\nan initial response much faster than existing LLM services. However, Hume’s\nEmpathic Voice Interface (EVI) is able to integrate other frontier LLMs into\nits longer responses which are configurable by developers.\n\n\nHume has trained our own expressive text-to-speech (TTS) model that allows it\nto generate speech with more prosody. EVI can generate speech given a text\ninput. Our own TTS models are trained with a lot more expressive nuance than\nother models.\n\n\nDuring a chat with EVI, you can pause responses by sending a\npause_assistant_message.\nOnce this message is sent, EVI will not respond until a\nresume_assistant_message\nis sent.\nPausing EVI's responses is different from muting yourself. While\npaused, EVI won't respond, but transcriptions of your audio inputs will still\nbe recorded.\nUpon resuming, if any audio input was sent during the pause, EVI\nwill only respond to the last thing which was said. (e.g., If you ask EVI\ntwo questions while paused and then send a resume_assistant_message, EVI\nwill only respond to the second question.)\n\n\nWith EVI, you can easily preserve context when reconnecting or continue a\nchat right where you left off. See steps below for how to resume a chat:\nEstablish initial connection: Make the initial handshake request\nto establish the WebSocket connection. Upon successful connection, you will\nreceive a ChatMetadata message:\n\n\n\n\nStore the chat_group_id: Save the chat_group_id from the ChatMetadata message for future use.\n\nResume chat: To resume a chat, include the stored chat_group_id in the resumed_chat_group_id\nquery parameter of subsequent handshake requests.\nFor example: wss://api.hume.ai/v0/evi/chat?access_token={accessToken}&resumed_chat_group_id={chatGroupId}",
    "code_snippets": [
      {
        "lang": "json",
        "code": "{\n  \"type\": \"chat_metadata\",\n  \"chat_group_id\": \"8859a139-d98a-4e2f-af54-9dd66d8c96e1\",\n  \"chat_id\": \"2c3a8636-2dde-47f1-8f9e-cea27791fd2e\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"chat_metadata\",\n  \"chat_group_id\": \"8859a139-d98a-4e2f-af54-9dd66d8c96e1\",\n  \"chat_id\": \"2c3a8636-2dde-47f1-8f9e-cea27791fd2e\"\n}"
      }
    ]
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.overview",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/overview",
    "title": "Expression Measurement API",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Hume's state of the art expression measurement models for the voice, face, and language."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.overview-intro",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/overview",
    "title": "Intro",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#intro",
    "content": "Hume's state of the art expression measurement models for the voice, face, and language are built on 10+ years of research and advances in computational approaches to emotion science (semantic space theory) pioneered by our team. Our expression measurement models are able to capture hundreds of dimensions of human expression in audio, video, and images.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement API"
      },
      "h2": {
        "id": "intro",
        "title": "Intro"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.overview-measurements",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/overview",
    "title": "Measurements",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#measurements",
    "content": "Facial Expression, including subtle facial movements often seen as expressing love or admiration, awe, disappointment, or cringes of empathic pain, along 48 distinct dimensions of emotional meaning. Our Facial Expression model will also optionally output FACS 2.0 measurements, our model of facial movements including traditional Action Units (AUs such as “Inner brow raise”, “Nose crinkle”) and facial descriptions (“Smile”, “Wink”, “Hand over mouth”, “Hand over eyes”)\n\nSpeech Prosody, or the non-linguistic tone, rhythm, and timbre of speech, spanning 48 distinct dimensions of emotional meaning.\n\nVocal Burst, including laughs, sighs, huhs, hmms, cries and shrieks (to name a few), along 48 distinct dimensions of emotional meaning.\n\nEmotional Language, or the emotional tone of transcribed text, along 53 dimensions.\n\n\n\n\nThese behaviors are complex and multifaceted.\nTo learn more about how to use our models visit our API reference.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement API"
      },
      "h2": {
        "id": "intro",
        "title": "Intro"
      },
      "h3": {
        "id": "measurements",
        "title": "Measurements"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.overview-model-training",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/overview",
    "title": "Model training",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#model-training",
    "content": "The models were trained on human intensity ratings of large-scale, experimentally controlled emotional expression data gathered using the methods described in these papers: Deep learning reveals what vocal bursts express in different cultures and Deep learning reveals what facial expressions mean to people in different cultures.\nWhile our models measure nuanced expressions that people most typically describe with emotion labels, it's important to remember that they are not a direct readout of what someone is experiencing. Sometimes, the outputs from facial and vocal models will show different emotional meanings, which is completely normal. Generally speaking, emotional experience is subjective and its expression is multimodal and context-dependent.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement API"
      },
      "h2": {
        "id": "intro",
        "title": "Intro"
      },
      "h3": {
        "id": "model-training",
        "title": "Model training"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.overview-try-out-the-models",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/overview",
    "title": "Try out the models",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#try-out-the-models",
    "content": "Learn how you can use the Expression Measurement API through both REST and WebSockets.\n\n\n\n\nUse REST endpoints to process batches of videos, images, text, or audio files.\n\n\nUse WebSocket endpoints when you need real-time predictions, such as processing a webcam or microphone stream.\nREST and WebSocket endpoints provide access to all of the same Hume models, but with different speed and scale tradeoffs. All models share a common response format, which associates a score with each detected expression. Scores indicate the degree to which a human rater would assign an expression to a given sample of video, text or audio.",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement API"
      },
      "h2": {
        "id": "try-out-the-models",
        "title": "Try out the models"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.overview-specific-expressions-by-modality",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/overview",
    "title": "Specific expressions by modality",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#specific-expressions-by-modality",
    "content": "Our models measure 53 expressions identified through the subtleties of emotional language and 48 expressions discerned from facial cues, vocal bursts, and speech prosody.\nExpression Language Face/Burst/Prosody \nAdmiration \n\n \n\n \nAdoration \n\n \n\n \nAesthetic Appreciation \n\n \n\n \nAmusement \n\n \n\n \nAnger \n\n \n\n \nAnnoyance \n\n  \nAnxiety \n\n \n\n \nAwe \n\n \n\n \nAwkwardness \n\n \n\n \nBoredom \n\n \n\n \nCalmness \n\n \n\n \nConcentration \n\n \n\n \nConfusion \n\n \n\n \nContemplation \n\n \n\n \nContempt \n\n  \nContentment \n\n \n\n \nCraving \n\n \n\n \nDesire \n\n \n\n \nDetermination \n\n \n\n \nDisappointment \n\n \n\n \nDisapproval \n\n  \nDisgust \n\n \n\n \nDistress \n\n \n\n \nDoubt \n\n \n\n \nEcstasy \n\n  \nEmbarrassment \n\n \n\n \nEmpathic Pain \n\n \n\n \nEnthusiasm \n\n  \nEntrancement \n\n \n\n \nEnvy \n\n \n\n \nExcitement \n\n \n\n \nFear \n\n \n\n \nGratitude \n\n  \nGuilt \n\n \n\n \nHorror \n\n \n\n \nInterest \n\n \n\n \nJoy \n\n \n\n \nLove \n\n \n\n \nNostalgia \n\n \n\n \nPain \n\n \n\n \nPride \n\n \n\n \nRealization \n\n \n\n \nRelief \n\n \n\n \nRomance \n\n \n\n \nSadness \n\n \n\n \nSarcasm \n\n  \nSatisfaction \n\n \n\n \nShame \n\n \n\n \nSurprise (negative) \n\n \n\n \nSurprise (positive) \n\n \n\n \nSympathy \n\n \n\n \nTiredness \n\n \n\n \nTriumph",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement API"
      },
      "h2": {
        "id": "specific-expressions-by-modality",
        "title": "Specific expressions by modality"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.rest",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/rest",
    "title": "Processing batches of media files",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "Hume’s Expression Measurement API is designed to facilitate large-scale processing of files using Hume's advanced models through an asynchronous, job-based interface. This API allows developers to submit jobs for parallel processing of various files, enabling efficient handling of multiple data points simultaneously, and receiving notifications when results are available."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.rest-key-features",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/rest",
    "title": "Key features",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#key-features",
    "content": "Asynchronous job submission: Jobs can be submitted to process a wide array of files in parallel, making it ideal for applications that require the analysis of large volumes of data.\n\nFlexible data input options: The API supports multiple data formats, including hosted file URLs, local files directly from your system, and raw text in the form of a list of strings. This versatility ensures that you can easily integrate the API into their applications, regardless of where their data resides.",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "key-features",
        "title": "Key features"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.rest-applications-and-use-cases",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/rest",
    "title": "Applications and use cases",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#applications-and-use-cases",
    "content": "Hume’s Expression Measurement API is particularly useful for leveraging Hume's expressive models across a broad spectrum of files and formats. Whether it's for processing large datasets for research, analyzing customer feedback across multiple channels, or enriching user experiences in media-rich applications, REST provides a robust solution for asynchronously handling complex, data-intensive tasks.",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "applications-and-use-cases",
        "title": "Applications and use cases"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.rest-using-humes-expression-measurement-api",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/rest",
    "title": "Using Hume’s Expression Measurement API",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#using-humes-expression-measurement-api",
    "content": "Here we'll show you how to upload your own files and run Hume models on batches of data.\nIf you haven't already, grab your API Key.\n\n\nMaking a request to the API\nStart a new job with the Expression Measurement API.\n\n\n\n\n\n\nTo do the same with a local file:\n\n\n\n\n\n\nSample files for you to use in this tutorial are available here:\nDownload faces.zip\nDownload david_hume.jpeg\nChecking job status\n\n\nUse webhooks to asynchronously receive notifications once the job completes.\nIt is not recommended to poll the API periodically for job status.\nThere are several ways to get notified and check the status of your job.\nUsing the Get job details API endpoint.\n\nProviding a callback URL. We will send a POST request to your URL when the job is complete. Your request body should look like this: { \"callback_url\": \"<YOUR CALLBACK URL>\" }\n\n\n\n\nRetrieving predictions\nYour predictions are available in a few formats.\nTo get predictions as JSON use the Get job predictions endpoint.\n\n\n\n\n\n\nTo get predictions as a compressed file of CSVs, one per model use the Get job artifacts endpoint.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs \\\n --request POST \\\n --header \"Content-Type: application/json\" \\\n --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n --data '{\n    \"models\": {\n        \"face\": {}\n    },\n    \"urls\": [\n        \"https://hume-tutorials.s3.amazonaws.com/faces.zip\"\n    ]\n}'"
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs \\\n --request POST \\\n --header \"Content-Type: application/json\" \\\n --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n --data '{\n    \"models\": {\n        \"face\": {}\n    },\n    \"urls\": [\n        \"https://hume-tutorials.s3.amazonaws.com/faces.zip\"\n    ]\n}'"
      },
      {
        "lang": "python",
        "code": "from hume import HumeBatchClient\nfrom hume.models.config import FaceConfig\n\nclient = HumeBatchClient(\"<YOUR API KEY>\")\nfilepaths = [\n    \"faces.zip\",\n    \"david_hume.jpeg\",\n]\nconfig = FaceConfig()\njob = client.submit_job(None, [config], files=filepaths)\n\nprint(job)\nprint(\"Running...\")\n\ndetails = job.await_complete()\njob.download_predictions(\"predictions.json\")\nprint(\"Predictions downloaded to predictions.json\")"
      },
      {
        "lang": "python",
        "code": "from hume import HumeBatchClient\nfrom hume.models.config import FaceConfig\n\nclient = HumeBatchClient(\"<YOUR API KEY>\")\nfilepaths = [\n    \"faces.zip\",\n    \"david_hume.jpeg\",\n]\nconfig = FaceConfig()\njob = client.submit_job(None, [config], files=filepaths)\n\nprint(job)\nprint(\"Running...\")\n\ndetails = job.await_complete()\njob.download_predictions(\"predictions.json\")\nprint(\"Predictions downloaded to predictions.json\")"
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs \\\n --request POST \\\n --header \"Content-Type: multipart/form-data\" \\\n --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n --form json='{\n    \"models\": {\n        \"face\": {}\n    }\n }' \\\n --form file=@faces.zip \\\n --form file=@david_hume.jpeg"
      },
      {
        "lang": "bash",
        "code": "curl https://api.hume.ai/v0/batch/jobs \\\n --request POST \\\n --header \"Content-Type: multipart/form-data\" \\\n --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n --form json='{\n    \"models\": {\n        \"face\": {}\n    }\n }' \\\n --form file=@faces.zip \\\n --form file=@david_hume.jpeg"
      },
      {
        "lang": "python",
        "code": "from hume import HumeBatchClient\nfrom hume.models.config import FaceConfig\n\nclient = HumeBatchClient(\"<YOUR API KEY>\")\nfilepaths = [\n    \"faces.zip\",\n    \"david_hume.jpeg\",\n]\nconfig = FaceConfig()\njob = client.submit_job(None, [config], files=filepaths)\n\nprint(job)\nprint(\"Running...\")\n\ndetails = job.await_complete()\njob.download_predictions(\"predictions.json\")\nprint(\"Predictions downloaded to predictions.json\")"
      },
      {
        "lang": "python",
        "code": "from hume import HumeBatchClient\nfrom hume.models.config import FaceConfig\n\nclient = HumeBatchClient(\"<YOUR API KEY>\")\nfilepaths = [\n    \"faces.zip\",\n    \"david_hume.jpeg\",\n]\nconfig = FaceConfig()\njob = client.submit_job(None, [config], files=filepaths)\n\nprint(job)\nprint(\"Running...\")\n\ndetails = job.await_complete()\njob.download_predictions(\"predictions.json\")\nprint(\"Predictions downloaded to predictions.json\")"
      },
      {
        "lang": "json",
        "code": "{\n    job_id: \"Job ID\",\n    status: \"STATUS (COMPLETED/FAILED)\",\n    predictions: [ARRAY OF RESULTS]\n}"
      },
      {
        "lang": "json",
        "code": "{\n    job_id: \"Job ID\",\n    status: \"STATUS (COMPLETED/FAILED)\",\n    predictions: [ARRAY OF RESULTS]\n}"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/predictions \\\n --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n --header 'accept: application/json; charset=utf-8'"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/predictions \\\n --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n --header 'accept: application/json; charset=utf-8'"
      },
      {
        "lang": "python",
        "code": "job.get_predictions()\n\nor\n\njob.download_predictions(\"filename.json\")"
      },
      {
        "lang": "python",
        "code": "job.get_predictions()\n\nor\n\njob.download_predictions(\"filename.json\")"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/artifacts \\\n --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n --header 'accept: application/octet-stream'"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/artifacts \\\n --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n --header 'accept: application/octet-stream'"
      },
      {
        "lang": "python",
        "code": "job.download_artifacts(\"filename.zip\")"
      },
      {
        "lang": "python",
        "code": "job.download_artifacts(\"filename.zip\")"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.rest-api-limits",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/rest",
    "title": "API limits",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#api-limits",
    "content": "The size of any individual file provided by URL cannot exceed 1 GB.\n\nThe size of any individual local file cannot exceed 100 MB.\n\nEach request has an upper limit of 100 URLs, 100 strings (raw text), and 100 local media files. Can be a mix of the media files or archives (.zip, .tar.gz, .tar.bz2, .tar.xz).\n\nFor audio and video files the max length supported is 1 Hour.",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API"
      },
      "h3": {
        "id": "api-limits",
        "title": "API limits"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.rest-providing-urls-and-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/rest",
    "title": "Providing URLs and files",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#providing-urls-and-files",
    "content": "You can provide data for your job in one of the following formats: hosted file URLs, local files, or raw text presented as a list of strings.\nIn this tutorial, the data is publicly available to download. For added security, you may choose to create a signed URL through your preferred cloud storage provider.\nCloud Provider Signing URLs \nGCP https://cloud.google.com/storage/docs/access-control/signed-urls \nAWS https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html \nAzure https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files"
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API"
      },
      "h3": {
        "id": "providing-urls-and-files",
        "title": "Providing URLs and files"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Real-time measurement streaming",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "WebSocket-based streaming facilitates continuous data flow between your application and Hume's models, providing immediate feedback and insights."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-key-features",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Key features",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#key-features",
    "content": "Real-time data processing: Leveraging WebSockets, this API allows for the streaming of data to Hume's models, enabling instant analysis and response. This feature is particularly beneficial for applications requiring immediate processing, such as live interaction systems or real-time monitoring tools.\n\nPersistent, two-way communication: Unlike traditional request-response models, the WebSocket-based streaming maintains an open connection for two-way communication between the client and server. This facilitates an ongoing exchange of data, allowing for a more interactive and responsive user experience.\n\nHigh throughput and low latency: The API is optimized for high performance, supporting high-volume data streaming with minimal delay. This ensures that applications can handle large streams of data efficiently, without sacrificing speed or responsiveness.",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "key-features",
        "title": "Key features"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-applications-and-use-cases",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Applications and use cases",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#applications-and-use-cases",
    "content": "WebSockets are ideal for a wide range of applications that benefit from real-time data analysis and interaction. Examples include:\nLive customer service tools: enhance customer support with real-time sentiment analysis and automated, emotionally intelligent responses\n\nInteractive educational platforms: provide immediate feedback and adaptive learning experiences based on real-time student input\n\nHealth and wellness apps: support live mental health and wellness monitoring, offering instant therapeutic feedback or alerts based on the user's vocal or textual expressions\n\nEntertainment and gaming: create more immersive and interactive experiences by responding to user inputs and emotions in real time",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "applications-and-use-cases",
        "title": "Applications and use cases"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-getting-started-with-websocket-streaming",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Getting started with WebSocket streaming",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#getting-started-with-websocket-streaming",
    "content": "Integrating WebSocket-based streaming into your application involves establishing a WebSocket connection with Hume AI's servers and streaming data directly to the models for processing.\nStreaming is built for analysis of audio, video, and text streams. By connecting to WebSocket endpoints you can get near real-time feedback on the expressive and emotional content of your data.",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-install-the-hume-python-sdk",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Install the Hume Python SDK",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#install-the-hume-python-sdk",
    "content": "Make sure to enable the optional stream feature when installing the Hume Python SDK.\n\n\n\n\nbash pip install \"hume[stream]\"",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      },
      "h3": {
        "id": "install-the-hume-python-sdk",
        "title": "Install the Hume Python SDK"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-emotional-language-from-text",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Emotional language from text",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#emotional-language-from-text",
    "content": "This example uses our Emotional Language model to perform sentiment analysis on a children's nursery rhyme.\nIf you haven't already, grab your API key.\n\n\nYour result should look something like this:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import HumeStreamClient\nfrom hume.models.config import LanguageConfig\n\nsamples = [\n    \"Mary had a little lamb,\",\n    \"Its fleece was white as snow.\"\n    \"Everywhere the child went,\"\n    \"The little lamb was sure to go.\"\n]\n\nasync def main():\n    client = HumeStreamClient(\"<YOUR API KEY>\")\n    config = LanguageConfig()\n    async with client.connect([config]) as socket:\n        for sample in samples:\n            result = await socket.send_text(sample)\n            emotions = result[\"language\"][\"predictions\"][0][\"emotions\"]\n            print(emotions)\n\nasyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\nfrom hume import HumeStreamClient\nfrom hume.models.config import LanguageConfig\n\nsamples = [\n    \"Mary had a little lamb,\",\n    \"Its fleece was white as snow.\"\n    \"Everywhere the child went,\"\n    \"The little lamb was sure to go.\"\n]\n\nasync def main():\n    client = HumeStreamClient(\"<YOUR API KEY>\")\n    config = LanguageConfig()\n    async with client.connect([config]) as socket:\n        for sample in samples:\n            result = await socket.send_text(sample)\n            emotions = result[\"language\"][\"predictions\"][0][\"emotions\"]\n            print(emotions)\n\nasyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "[\n  {'name': 'Admiration', 'score': 0.06379243731498718},\n  {'name': 'Adoration', 'score': 0.07222934812307358},\n  {'name': 'Aesthetic Appreciation', 'score': 0.02808445133268833},\n  {'name': 'Amusement', 'score': 0.027589013800024986},\n  ......\n  {'name': 'Surprise (positive)', 'score': 0.030542362481355667},\n  {'name': 'Sympathy', 'score': 0.03246130049228668},\n  {'name': 'Tiredness', 'score': 0.03606246039271355},\n  {'name': 'Triumph', 'score': 0.01235896535217762}\n]"
      },
      {
        "lang": "python",
        "code": "[\n  {'name': 'Admiration', 'score': 0.06379243731498718},\n  {'name': 'Adoration', 'score': 0.07222934812307358},\n  {'name': 'Aesthetic Appreciation', 'score': 0.02808445133268833},\n  {'name': 'Amusement', 'score': 0.027589013800024986},\n  ......\n  {'name': 'Surprise (positive)', 'score': 0.030542362481355667},\n  {'name': 'Sympathy', 'score': 0.03246130049228668},\n  {'name': 'Tiredness', 'score': 0.03606246039271355},\n  {'name': 'Triumph', 'score': 0.01235896535217762}\n]"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      },
      "h3": {
        "id": "emotional-language-from-text",
        "title": "Emotional language from text"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-facial-expressions-from-an-image",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Facial expressions from an image",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#facial-expressions-from-an-image",
    "content": "This example uses our Facial Expression model to get expression measurements from an image.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import asyncio\n\nfrom hume import HumeStreamClient, StreamSocket\nfrom hume.models.config import FaceConfig\n\nasync def main():\nclient = HumeStreamClient(\"<YOUR API KEY>\")\nconfig = FaceConfig(identify_faces=True)\nasync with client.connect([config]) as socket:\nresult = await socket.send_file(\"<YOUR IMAGE FILEPATH>\")\nprint(result)\n\nasyncio.run(main())\n"
      },
      {
        "lang": "python",
        "code": "import asyncio\n\nfrom hume import HumeStreamClient, StreamSocket\nfrom hume.models.config import FaceConfig\n\nasync def main():\nclient = HumeStreamClient(\"<YOUR API KEY>\")\nconfig = FaceConfig(identify_faces=True)\nasync with client.connect([config]) as socket:\nresult = await socket.send_file(\"<YOUR IMAGE FILEPATH>\")\nprint(result)\n\nasyncio.run(main())\n"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      },
      "h3": {
        "id": "facial-expressions-from-an-image",
        "title": "Facial expressions from an image"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-speech-prosody-from-an-audio-or-video-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Speech prosody from an audio or video file",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#speech-prosody-from-an-audio-or-video-file",
    "content": "This example uses our Speech Prosody model to get expression measurements from an audio or video file.",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import asyncio\n\nfrom hume import HumeStreamClient, StreamSocket\nfrom hume.models.config import ProsodyConfig\n\nasync def main():\n    client = HumeStreamClient(\"<YOUR API KEY>\")\n    config = ProsodyConfig()\n    async with client.connect([config]) as socket:\n        result = await socket.send_file(\"<YOUR VIDEO OR AUDIO FILEPATH>\")\n        print(result)\n\nasyncio.run(main())"
      },
      {
        "lang": "python",
        "code": "import asyncio\n\nfrom hume import HumeStreamClient, StreamSocket\nfrom hume.models.config import ProsodyConfig\n\nasync def main():\n    client = HumeStreamClient(\"<YOUR API KEY>\")\n    config = ProsodyConfig()\n    async with client.connect([config]) as socket:\n        result = await socket.send_file(\"<YOUR VIDEO OR AUDIO FILEPATH>\")\n        print(result)\n\nasyncio.run(main())"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming"
      },
      "h3": {
        "id": "speech-prosody-from-an-audio-or-video-file",
        "title": "Speech prosody from an audio or video file"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-streaming-with-your-own-websockets-client",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Streaming with your own WebSockets client",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#streaming-with-your-own-websockets-client",
    "content": "To call the API from your own WebSockets client you'll need the API endpoint, a JSON message, and an API key header/param. More information can be found in the Expression Measurement API reference.\nTo get started, you can use a WebSocket client of your choice to connect to the models endpoint:\n\n\nurl wss://api.hume.ai/v0/stream/models Make sure you configure the socket connection headers with your personal API key\n\n\n\n\nThe default WebSockets implementation in your browser may not have support for headers. If that's the case you can set\nthe apiKey query parameter.\nAnd finally, send the following JSON message on the socket:\n\n\nYou should receive a JSON response that looks something like this:",
    "code_snippets": [
      {
        "lang": "http",
        "code": "X-Hume-Api-Key: <YOUR API KEY>"
      },
      {
        "lang": "http",
        "code": "X-Hume-Api-Key: <YOUR API KEY>"
      },
      {
        "lang": "json",
        "code": "{\n    \"models\": {\n        \"language\": {}\n    },\n    \"raw_text\": true,\n    \"data\": \"Mary had a little lamb\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n    \"models\": {\n        \"language\": {}\n    },\n    \"raw_text\": true,\n    \"data\": \"Mary had a little lamb\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"language\": {\n    \"predictions\": [\n      {\n        \"text\": \"Mary\",\n        \"position\": { \"begin\": 0, \"end\": 4 },\n        \"emotions\": [\n          { \"name\": \"Anger\", \"score\": 0.012025930918753147 },\n          { \"name\": \"Joy\", \"score\": 0.056471485644578934 },\n          { \"name\": \"Sadness\", \"score\": 0.031556881964206696 },\n        ]\n      },\n      {\n        \"text\": \"had\",\n        \"position\": { \"begin\": 5, \"end\": 8 },\n        \"emotions\": [\n          { \"name\": \"Anger\", \"score\": 0.0016927534015849233 },\n          { \"name\": \"Joy\", \"score\": 0.02388327568769455 },\n          { \"name\": \"Sadness\", \"score\": 0.018137391656637192 },\n          ...\n        ]\n      },\n      ...\n    ]\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"language\": {\n    \"predictions\": [\n      {\n        \"text\": \"Mary\",\n        \"position\": { \"begin\": 0, \"end\": 4 },\n        \"emotions\": [\n          { \"name\": \"Anger\", \"score\": 0.012025930918753147 },\n          { \"name\": \"Joy\", \"score\": 0.056471485644578934 },\n          { \"name\": \"Sadness\", \"score\": 0.031556881964206696 },\n        ]\n      },\n      {\n        \"text\": \"had\",\n        \"position\": { \"begin\": 5, \"end\": 8 },\n        \"emotions\": [\n          { \"name\": \"Anger\", \"score\": 0.0016927534015849233 },\n          { \"name\": \"Joy\", \"score\": 0.02388327568769455 },\n          { \"name\": \"Sadness\", \"score\": 0.018137391656637192 },\n          ...\n        ]\n      },\n      ...\n    ]\n  }\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "streaming-with-your-own-websockets-client",
        "title": "Streaming with your own WebSockets client"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-sending-images-or-audio",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "Sending images or audio",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#sending-images-or-audio",
    "content": "The WebSocket endpoints of the Expression Measurement API require that you encode your media using base64. Here's a quick example of base64 encoding data in Python:",
    "code_snippets": [
      {
        "lang": "python",
        "code": "import base64\nfrom pathlib import Path\n\ndef encode_data(filepath: Path) -> str:\nwith Path(filepath).open('rb') as fp:\nbytes_data = base64.b64encode(fp.read())\nencoded_data = bytes_data.decode(\"utf-8\")\nreturn encoded_data\n\nfilepath = \"<PATH TO YOUR MEDIA>\"\nencoded_data = encode_data(filepath)\nprint(encoded_data)\n"
      },
      {
        "lang": "python",
        "code": "import base64\nfrom pathlib import Path\n\ndef encode_data(filepath: Path) -> str:\nwith Path(filepath).open('rb') as fp:\nbytes_data = base64.b64encode(fp.read())\nencoded_data = bytes_data.decode(\"utf-8\")\nreturn encoded_data\n\nfilepath = \"<PATH TO YOUR MEDIA>\"\nencoded_data = encode_data(filepath)\nprint(encoded_data)\n"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "streaming-with-your-own-websockets-client",
        "title": "Streaming with your own WebSockets client"
      },
      "h3": {
        "id": "sending-images-or-audio",
        "title": "Sending images or audio"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.websocket-faqs",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/websocket",
    "title": "FAQs",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#faqs",
    "content": "WebSockets are a communication protocol that enables real-time, two-way communication between a client and a server\nover a single, long-lived connection. They provide a persistent connection that allows both the client and the server\nto initiate communication at any time.\n\n\nStreaming will disconnect every minute to ensure unused connections are released. You will need to reconnect by\nbuilding reconnect logic into your application. Implementation of reconnect logic will depend on the language and\nframework of your client application.\n\n\nWebSocket connections can experience disruptions due to network issues or other factors. Implement error handling\nmechanisms to gracefully handle connection failures. This includes handling connection timeouts, connection drops, and\nintermittent connection issues. Implement reconnection logic to automatically attempt to reconnect and resume\ncommunication when a connection is lost.\n\n\nHume WebSockets endpoints can return errors in response to invalid requests, authentication failures, or other issues.\nImplement proper error handling to interpret and handle these errors in your application. Provide meaningful error\nmessages to users and handle any exceptional scenarios gracefully. To prevent unknowingly initiating too many errors\nwe have put a limit on how many of the same errors you can have in a row. For a full list of the error responses you\ncan expect, please see our API errors page.\n\n\nThe benefits of using a the WebSocket is the persistent connection. The open socket should be kept open until the\napplication is done utilizing the service and then closed. Avoid opening a new connection for each file or payload you\nsend to the API. To ensure that context does not leak across multiple unrelated files you can use the\nreset_stream parameter.",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming"
      },
      "h2": {
        "id": "faqs",
        "title": "FAQs"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.expression-measurement-api.faq",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/expression-measurement-api/faq",
    "title": "Expression Measurement API FAQ",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/docs/expression-measurement-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "Our models capture the widest-ever range of facial, speech, vocal, and language modulations with distinct emotional meanings. We label each of their outputs with emotion terms like “amusement” and “doubt,” not because they always correspond to those emotional experiences (they must not, given that they often differ from one modality to another), but because scientific studies show that these kinds of labels are the most precise language we have for describing expressions.\nOur models generate JSON or CSV output files with values typically ranging from 0 to 1 for each output in different segments of the input file (though values out of the 0-1 range are possible). Higher values indicate greater intensity of facial movements or vocal modulations that are most strongly associated with the emotion label corresponding to the output.\nA given expression will contain a blend of various emotions, and our models identify features that are associated with each emotional dimension. The score for each dimension is proportional to the likelihood that a human would perceive that emotion in the expression.\nSpecifically, the scores reflect the likelihood that an average human perceiver would use that emotion dimension to describe a given expression. The models were trained on human intensity ratings gathered using the methods described in this paper: Deep learning reveals what vocal bursts express in different cultures.\nWhile our models measure nuanced expressions that people most typically describe with emotion labels, it's important to remember that they are not a direct readout of what someone is experiencing. Emotional experience is subjective and its expression is multimodal and context-dependent. Moreover, at any given time, our facial expression outputs might be quite different than our vocal expression outputs. Therefore, it's important to follow best practices when interpreting outputs.\n\n\nThere are many different ways to use our platform. That said, successful research and applications of our models generally follow four steps: exploration, prediction, improvement, and testing.\nExploration: Researchers and developers generally begin by exploring patterns in their data.\n\n\nAre there apparent differences across participants or users in a study?\n\nDo patterns in expression vary systematically over time?\n\nAre there different patterns in expression associated with different stages of research or different product experiences?\n\n\nPrediction: A great way to evaluate and start building on our APIs is to use them to predict metrics that you already know are important.\n\n\nAre key outcomes like mental health or customer satisfaction better predicted by language and expression than by language alone?\n\nIf patterns in expression predict important outcomes, how do these patterns in expression vary over time and reveal critical moments for a user or participant?\n\n\nImprovement: The goal is often to use measures of expression to directly improve how the application works.\n\n\nSometimes, being able to predict an important metric is enough to make a decision. For example, if you can predict whether two people will get along based on their expressions and language, then your application can pair them up.\n\nMore formally, you can apply statistics or machine learning to the data you gather to improve how the application works.\n\nYou can incorporate our API outputs into an out-of-the-box large language model, simply by converting them into text (e.g., \"The user sounds calm but a little frustrated\") and feeding them in as prompts.\n\nYou can use expressions to teach an AI model. For example, if your application involves a large language model, such as an AI tutor, you can use measures of expression that predict student performance and well-being to directly fine-tune the AI to improve over time.\n\n\nTesting: After you've incorporated measures of expression into your application, they can be part of every A/B test you perform. You can now monitor the effects of changes to your application not just on engagement and retention, but also on how much users laugh or sigh in frustration, or show signs of interest or boredom.\n\n\n\n\nAs you build expression-related signals, metrics, analyses, models, or\nfeedback into an application, remember to use scientific best\npractices and\nfollow the ethics guidelines of\nthehumeinitiative.org.\n\n\nOur speech prosody model measures the tune, rhythm, and timbre of speech, whereas our language model measures the tone of the words being spoken. When using either model, we offer the flexibility to annotate emotional expressions at several levels of granularity, ranging from individual words to entire conversational turns. It is important to note that independent of granularity, our language model still takes into account up to 50 previous tokens (word or sub-words) of speech; otherwise, it would not be able to capture how the meaning of the words is affected by context.\nWord: At the word level, our model provides a separate output for each word, offering the most granular insight into emotional expression during speech.\nSentence: At the sentence level of granularity, we annotate the emotional tone of each spoken sentence with our prosody and language models.\nUtterance: Utterance-level granularity is between word- and sentence-level. It takes into account natural pauses or breaks in speech, providing more rapidly updated measures of emotional expression within a flowing conversation. For text inputs, utterance-level granularity will produce results identical to sentence-level granularity.\nConversational Turn: Conversational turn-level analysis is a lower level of granularity. It outputs a single output for each turn; that is, the full sequence of words and sentences spoken uninterrupted by each person. This approach provides a higher-level view of the emotional dynamics in a multi-participant dialogue. For text inputs, specifying conversational turn-level granularity for our Language model will produce results for entire passage.\n\n\nRemember, each level of granularity has its unique advantages, and choosing\nthe right one depends on the requirements of your specific application.\n\n\nState-of-the-art face detection and identification algorithms still occasionally make errors. For instance, our algorithm sometimes detects faces in shadows or reflections. Other times, our algorithm falsely attributes a new identity to someone who has already been in the video, sometimes due to changes in lighting or occlusion. These errors can result in additional face IDs. We are still working to fine-tune our algorithm to minimize errors in the contexts that our customers care about.\n\n\nOur vocal burst model detects vocalizations such as laughs, screams, sighs, gasps, “mms,” “uhs,” and “mhms.” Natural speech generally contains a few vocal bursts every minute, but scripted speech has fewer vocal bursts. If no vocal bursts are detected, it may be because there are no vocal bursts in the file. However, if you hear vocal bursts that aren't being detected by the algorithm, note that we are also in the process of improving our vocal burst detection algorithm, so please stay tuned for updates.\n\n\nWe've documented this issue thoroughly in our API errors page.\n\n\nYou can specify any of the following:\nzh, da, nl, en, en-AU, en-IN, en-NZ, en-GB, fr, fr-CA, de, hi, hi-Latn, id, it, ja, ko, no, pl, pt, pt-BR, pt-PT, ru, es, es-419, sv, ta, tr, or uk.\n\n\nWe support over 50 languages. Among these, 20 languages have additional support for transcription.\nLanguage Tag Language Text Transcription \nar Arabic \n\n  \nbg Bulgarian \n\n  \nca Catalan \n\n  \ncs Czech \n\n  \nda Danish \n\n \n\n \nde German \n\n \n\n \nel Greek \n\n  \nen English* \n\n \n\n \nes Spanish \n\n \n\n \net Estonian \n\n  \nfa Farsi \n\n  \nfi Finnish \n\n  \nfr French \n\n \n\n \nfr-ca French (Canada) \n\n \n\n \ngl Galician \n\n  \ngu Gujarati \n\n  \nhe Hebrew \n\n  \nhi Hindi \n\n \n\n \nhr Croatian \n\n  \nhu Hungarian \n\n  \nhy Armenian \n\n  \nid Indonesian \n\n \n\n \nit Italian \n\n \n\n \nja Japanese \n\n \n\n \nka Georgian \n\n  \nko Korean \n\n \n\n \nku Kurdish \n\n  \nlt Lithuanian \n\n  \nlv Latvian \n\n  \nmk FYRO Macedonian \n\n  \nmn Mongolian \n\n  \nmr Marathi \n\n  \nms Malay \n\n  \nmy Burmese \n\n  \nnb Norwegian (Bokmål) \n\n  \nnl Dutch \n\n \n\n \npl Polish \n\n \n\n \npt Portuguese \n\n \n\n \npt-br Portuguese (Brazil) \n\n \n\n \nro Romanian \n\n  \nru Russian \n\n \n\n \nsk Slovak \n\n  \nsl Slovenian \n\n  \nsq Albanian \n\n  \nsr Serbian \n\n  \nsv Swedish \n\n \n\n \nth Thai \n\n  \ntr Turkish \n\n \n\n \nuk Ukrainian \n\n \n\n \nur Urdu \n\n  \nvi Vietnamese \n\n  \nzh-cn Chinese \n\n \n\n \nzh-tw Chinese (Taiwan) \n\n \n\n \n\n\n\nEnglish is a primary language, and will yield more accurate predictions than\ninputs in other supported languages. Currently, our NER model only supports\nthe English language."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.custom-models-api.overview",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/custom-models-api/overview",
    "title": "Custom Models API",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/docs/custom-models-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "description": "Predict preferences more accurately than any LLM.",
    "content": "Combined with words, expressions provide a wealth of information about our state of mind in any given context like customer satisfaction or frustration, patient health and well-being, student comprehension and confusion, and so much more.\nHume’s Custom Models API unlocks these insights at the click of a button, integrating patterns of facial expression, vocal expression, and language into a single custom model to predict whatever outcome you specify. This works by taking advantage not only of our state-of-the-art expression AI models, but also specialized language-expression embeddings that we have trained on conversational data.\nThe algorithm that drives our Custom Models API is pretrained on huge volumes of data. That means it already recognizes most patterns of expression and language that people form. All you have to do is add your labels.\nYou can access our Custom Models API through our no code platform detailed in the next section or through our API. Once you create your initial labeled dataset, your labels will be used to train a custom model that you own and only your account can access. You’ll be able to run the model on any new file through our Playground and Custom Models API. You’ll also get statistics on the accuracy of your custom model."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.custom-models-api.creating-your-dataset",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/custom-models-api/creating-your-dataset",
    "title": "Creating your dataset",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/docs/custom-models-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "In this guide we'll walk you through the process of creating a dataset using the Hume API. In future sections you'll use a dataset to train your own model.\n\n\nUploading media files to Hume\nUpload media files to Hume that you want to exist in your custom dataset. These should be images, videos, audio, or text files.\n\n\nThe API response will show an array of files newly registered with the Hume.\n\n\nMaking your dataset file\nWe will create a CSV file that has a column for media file IDs and another column for labels.\nThe file ID column is required and must be named file_id. The label column can be named whatever you want. And you can even have multiple label columns, but only one will be used for training your model.\nHere we'll add a label column called expressions and an extra column just for housekeeping called file_name.\nfile_name file_id expressions \nneutral_face.jpeg b3cd5662-ea89-4f00-8eae-86218a556027 Neutral \npositive_face.jpeg 44bc2ac8-41d5-401e-8c88-df179b993be7 Positive \n\nRegistering your dataset\nNow that we have our media files registered and a CSV associating those files with labels, we can register our dataset.\n\n\nSuccess! Your dataset is registered.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "curl --location 'https://api.hume.ai/v0/registry/files'\n--request POST\n--header 'X-Hume-Api-Key: <YOUR API KEY>'\n--header 'Content-Type: application/json'\n--data '[\n  {\n    \"file\": {\n      \"name\": \"<NAME OF FILE>\",\n      \"uri\": \"<URI OF FILE>\",\n      \"hume_storage\": true,\n      \"data_type\": \"image/png\",\n      \"metadata\": {}\n    }\n  },\n  {\n    \"file\": {\n      \"name\": \"<NAME OF FILE>\",\n      \"uri\": \"<URI OF FILE>\",\n      \"hume_storage\": true,\n      \"data_type\": \"image/png\",\n      \"metadata\": {}\n    }\n  }\n]'"
      },
      {
        "lang": "bash",
        "code": "curl --location 'https://api.hume.ai/v0/registry/files'\n--request POST\n--header 'X-Hume-Api-Key: <YOUR API KEY>'\n--header 'Content-Type: application/json'\n--data '[\n  {\n    \"file\": {\n      \"name\": \"<NAME OF FILE>\",\n      \"uri\": \"<URI OF FILE>\",\n      \"hume_storage\": true,\n      \"data_type\": \"image/png\",\n      \"metadata\": {}\n    }\n  },\n  {\n    \"file\": {\n      \"name\": \"<NAME OF FILE>\",\n      \"uri\": \"<URI OF FILE>\",\n      \"hume_storage\": true,\n      \"data_type\": \"image/png\",\n      \"metadata\": {}\n    }\n  }\n]'"
      },
      {
        "lang": "json",
        "code": "[\n    {\n        \"file\": {\n            \"id\": \"9f045781-3ecd-4f34-ba9c-969139c32256\",\n            \"name\": \"<NAME OF FILE>\",\n            \"uri\": \"<URI OF FILE>\",\n            \"upload_uri\": null,\n            \"thumbnail_uri\": null,\n            \"user_id\": \"<YOUR USER ID>\",\n            \"data_type\": \"image/png\",\n            \"created_on\": 1695851622,\n            \"modified_on\": 1695851622,\n            \"metadata\": {},\n            \"hume_storage\": true,\n            \"hume_storage_upload_timestamp\": null\n        },\n        \"attributes\": []\n    },\n    {\n        \"file\": {\n            \"id\": \"7f02f481-4sf4-dsf3-ba9c-345639c32256\",\n            \"name\": \"<NAME OF FILE>\",\n            ...\n        }\n    }\n]"
      },
      {
        "lang": "json",
        "code": "[\n    {\n        \"file\": {\n            \"id\": \"9f045781-3ecd-4f34-ba9c-969139c32256\",\n            \"name\": \"<NAME OF FILE>\",\n            \"uri\": \"<URI OF FILE>\",\n            \"upload_uri\": null,\n            \"thumbnail_uri\": null,\n            \"user_id\": \"<YOUR USER ID>\",\n            \"data_type\": \"image/png\",\n            \"created_on\": 1695851622,\n            \"modified_on\": 1695851622,\n            \"metadata\": {},\n            \"hume_storage\": true,\n            \"hume_storage_upload_timestamp\": null\n        },\n        \"attributes\": []\n    },\n    {\n        \"file\": {\n            \"id\": \"7f02f481-4sf4-dsf3-ba9c-345639c32256\",\n            \"name\": \"<NAME OF FILE>\",\n            ...\n        }\n    }\n]"
      },
      {
        "lang": "bash",
        "code": "curl --location 'https://api.hume.ai/v0/registry/datasets'\n--request POST\n--header 'X-Hume-Api-Key: <YOUR API KEY>'\n--form 'name=\"Negative, Neutral, & Positive Facial Expressions\"'\n--form 'labels_file=@\"<PATH TO LABEL FILE>/labels-file.csv\"'"
      },
      {
        "lang": "bash",
        "code": "curl --location 'https://api.hume.ai/v0/registry/datasets'\n--request POST\n--header 'X-Hume-Api-Key: <YOUR API KEY>'\n--form 'name=\"Negative, Neutral, & Positive Facial Expressions\"'\n--form 'labels_file=@\"<PATH TO LABEL FILE>/labels-file.csv\"'"
      },
      {
        "lang": "json",
        "code": "{\n    \"id\": \"8d6ddf39-d9ff-4f9c-9dbe-d6e288d8ddd7\",  // Dataset ID\n    \"name\": \"Negative, Neutral, & Positive Facial Expressions\",\n    \"latest_version\": {\n        \"id\": \"d153f723-8a13-48d2-ba74-2a6c333ff0db\",  // Dataset Version ID\n        \"labels_file_uri\": \"<URI TO DATASET FILE>\",\n        \"dataset_id\": \"8d6ddf39-d9ff-4f9c-9dbe-d6e288d8ddd7\",\n        \"dataset_version\": 0,\n        \"created_on\": 1695854279\n    },\n    \"modified_on\": 1695854279,\n    \"metadata\": null\n}"
      },
      {
        "lang": "json",
        "code": "{\n    \"id\": \"8d6ddf39-d9ff-4f9c-9dbe-d6e288d8ddd7\",  // Dataset ID\n    \"name\": \"Negative, Neutral, & Positive Facial Expressions\",\n    \"latest_version\": {\n        \"id\": \"d153f723-8a13-48d2-ba74-2a6c333ff0db\",  // Dataset Version ID\n        \"labels_file_uri\": \"<URI TO DATASET FILE>\",\n        \"dataset_id\": \"8d6ddf39-d9ff-4f9c-9dbe-d6e288d8ddd7\",\n        \"dataset_version\": 0,\n        \"created_on\": 1695854279\n    },\n    \"modified_on\": 1695854279,\n    \"metadata\": null\n}"
      }
    ]
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.custom-models-api.training-a-custom-model",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/custom-models-api/training-a-custom-model",
    "title": "Training a custom model",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/docs/custom-models-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "In this guide we will walk you through training your own custom model.\n\n\nStarting a training job\nHere we kick off a training job using a dataset that's already been registered for you. The resulting model will classify facial expressions as negative, positive, or neutral.\nNote that we've set target_feature to \"Affect\". This refers to the name of the column that we want to predict from our dataset.\n\n\nYou'll get back a job ID that you can use to check the status of your training job.\n\n\nChecking the status of your training job\nUsing the job ID from the previous step, you can get details about the current status of your training job.\n\n\nIt may take a few minutes for your model to be ready, but once training is complete you will see the status as COMPLETED and you'll have access to your new model.\n\n\nTesting your custom model\nYour custom model is ready to use!\nYou can test your model by sending a request to the Custom Models inference endpoint with URLs of images to classify. The model we trained is a facial expression classifier, so test URLs should point to images of faces.\n\n\nJust like before, we get back a job ID that we can use to check the status of our job.\n\n\nChecking the status of your inference job\nUse the job ID from the previous step to check on the status of your model inference job.\n\n\nOnce the model is done predicting the classes of the images you provided, you'll get a COMPLETED status.\n\n\nGetting model predictions\nFinally, you can request the actual model predictions from the inference job. The JSON result will show the predicted class for each image you provided.",
    "code_snippets": [
      {
        "lang": "bash",
        "code": "curl --location https://api.hume.ai/v0/batch/jobs/tl/train \\\n     --request POST \\\n     --header \"X-Hume-Api-Key: $API_KEY\" \\\n     --header 'Content-Type: application/json' \\\n     --data '{\n  \"custom_model\": {\n    \"name\": \"Negative, Neutral, & Positive Facial Expressions\",\n    \"description\": \"Is Facial Expression Negative, Neutral or Positive\"\n  },\n  \"dataset\": {\n    \"id\": \"ef7955ce-1755-4942-8615-bc16e654e7e5\"\n  },\n  \"target_feature\": \"Affect\",\n  \"task\": {\n    \"type\": \"classification\"\n  }\n}'"
      },
      {
        "lang": "bash",
        "code": "curl --location https://api.hume.ai/v0/batch/jobs/tl/train \\\n     --request POST \\\n     --header \"X-Hume-Api-Key: $API_KEY\" \\\n     --header 'Content-Type: application/json' \\\n     --data '{\n  \"custom_model\": {\n    \"name\": \"Negative, Neutral, & Positive Facial Expressions\",\n    \"description\": \"Is Facial Expression Negative, Neutral or Positive\"\n  },\n  \"dataset\": {\n    \"id\": \"ef7955ce-1755-4942-8615-bc16e654e7e5\"\n  },\n  \"target_feature\": \"Affect\",\n  \"task\": {\n    \"type\": \"classification\"\n  }\n}'"
      },
      {
        "lang": "json",
        "code": "{\n  \"job_id\": \"<JOB ID>\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"job_id\": \"<JOB ID>\"\n}"
      },
      {
        "lang": "bash",
        "code": "curl --location --globoff https://api.hume.ai/v0/batch/jobs/$JOB_ID \\\n     --header \"X-Hume-Api-Key: $API_KEY\""
      },
      {
        "lang": "bash",
        "code": "curl --location --globoff https://api.hume.ai/v0/batch/jobs/$JOB_ID \\\n     --header \"X-Hume-Api-Key: $API_KEY\""
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"TRAINING\",\n  \"job_id\": \"<JOB ID>\",\n  \"user_id\": \"<USER ID>\",\n  \"request\": {\n    \"custom_model\": {\n      \"name\": \"Negative, Neutral, & Positive Facial Expressions\",\n      \"description\": \"Is Facial Expression Negative, Neutral or Positive\",\n     },\n    \"dataset\": {\n      \"id\": \"ef7955ce-1755-4942-8615-bc16e654e7e5\"\n    },\n    \"target_feature\": \"interaction\",\n    \"task\": {\n      \"type\": \"classification\"\n    }\n  },\n  \"state\": {\n    \"status\": \"COMPLETED\",\n    \"created_timestamp_ms\": 42,\n    \"started_timestamp_ms\": 32,\n    \"ended_timestamp_ms\": 23,\n \t\t\"custom_model\": {\n\t\t\t\"id\": \"<CUSTOM MODEL ID>\"\n  \t}\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"TRAINING\",\n  \"job_id\": \"<JOB ID>\",\n  \"user_id\": \"<USER ID>\",\n  \"request\": {\n    \"custom_model\": {\n      \"name\": \"Negative, Neutral, & Positive Facial Expressions\",\n      \"description\": \"Is Facial Expression Negative, Neutral or Positive\",\n     },\n    \"dataset\": {\n      \"id\": \"ef7955ce-1755-4942-8615-bc16e654e7e5\"\n    },\n    \"target_feature\": \"interaction\",\n    \"task\": {\n      \"type\": \"classification\"\n    }\n  },\n  \"state\": {\n    \"status\": \"COMPLETED\",\n    \"created_timestamp_ms\": 42,\n    \"started_timestamp_ms\": 32,\n    \"ended_timestamp_ms\": 23,\n \t\t\"custom_model\": {\n\t\t\t\"id\": \"<CUSTOM MODEL ID>\"\n  \t}\n  }\n}"
      },
      {
        "lang": "bash",
        "code": "curl --location https://api.hume.ai/v0/batch/jobs/tl/inference \\\n     --request POST \\\n     --header X-Hume-Api-Key: $API_KEY \\\n     --header 'Content-Type: application/json' \\\n     --data '{\n  \"custom_model\": {\n    \"id\": \"<CUSTOM MODEL ID>\"\n  },\n  \"urls\": [\"<URL TO TEST FILE>\"]\n }'"
      },
      {
        "lang": "bash",
        "code": "curl --location https://api.hume.ai/v0/batch/jobs/tl/inference \\\n     --request POST \\\n     --header X-Hume-Api-Key: $API_KEY \\\n     --header 'Content-Type: application/json' \\\n     --data '{\n  \"custom_model\": {\n    \"id\": \"<CUSTOM MODEL ID>\"\n  },\n  \"urls\": [\"<URL TO TEST FILE>\"]\n }'"
      },
      {
        "lang": "json",
        "code": "{\n  \"job_id\": \"<JOB ID>\"\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"job_id\": \"<JOB ID>\"\n}"
      },
      {
        "lang": "bash",
        "code": "curl --location --globoff https://api.hume.ai/v0/batch/jobs/$JOB_ID \\\n     --header \"X-Hume-Api-Key: $API_KEY\""
      },
      {
        "lang": "bash",
        "code": "curl --location --globoff https://api.hume.ai/v0/batch/jobs/$JOB_ID \\\n     --header \"X-Hume-Api-Key: $API_KEY\""
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"INFERENCE\",\n  \"job_id\": \"<JOB ID>\",\n  \"user_id\": \"<YOUR USER ID>\",\n  \"request\": {},\n  \"state\": {\n    \"status\": \"COMPLETED\",\n    \"created_timestamp_ms\": 42\n  }\n}"
      },
      {
        "lang": "json",
        "code": "{\n  \"type\": \"INFERENCE\",\n  \"job_id\": \"<JOB ID>\",\n  \"user_id\": \"<YOUR USER ID>\",\n  \"request\": {},\n  \"state\": {\n    \"status\": \"COMPLETED\",\n    \"created_timestamp_ms\": 42\n  }\n}"
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n     --url https://api.hume.ai/v0/batch/jobs/$JOB_ID/predictions \\\n     --header \"X-Hume-Api-Key: $API_KEY\" \\\n     --header \"accept: application/json; charset=utf-8\""
      },
      {
        "lang": "bash",
        "code": "curl --request GET \\\n     --url https://api.hume.ai/v0/batch/jobs/$JOB_ID/predictions \\\n     --header \"X-Hume-Api-Key: $API_KEY\" \\\n     --header \"accept: application/json; charset=utf-8\""
      }
    ]
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.custom-models-api.evaluating-a-custom-model",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/custom-models-api/evaluating-a-custom-model",
    "title": "Evaluating your model",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/docs/custom-models-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "Each custom model you train has a corresponding details page, viewable from the Hume website. The model details page displays metrics and visualizations to evaluate your model’s performance. This document serves to help you interpret those metrics and provide guidance on ways to improve your custom model.\n\n\nCustom model details\n\n\nLimitations of model validation metrics\nModel validation metrics are estimates based on a split of your dataset into training and evaluation parts. The larger the training set, the more reliable the metrics. However, it’s important to remember that these metrics are indicative and do not guarantee performance on unseen data."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.custom-models-api.evaluating-a-custom-model-assessing-good-performance",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/custom-models-api/evaluating-a-custom-model",
    "title": "Assessing 'good' performance",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/docs/custom-models-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#assessing-good-performance",
    "content": "Task-specific variances and performance metrics: with expression analysis, the complexity of your task determines the range of model performance, which in the case of classification models can technically vary from zero to perfect accuracy. Depending on the complexity of your task, less than perfect performance may still be very useful to serve as an indication of likelihood for your given target.\n\nInfluence of number of classes: prediction gets more difficult as the number of classes in your dataset increases, particularly when distinction between classes is more subtle. Inherently the level of chance will be higher with a lower number of classes. For example, for 3-classes your low-end performance is 33% accuracy vs 50% for a binary problem.\n\nApplication-specific requirements: when establishing acceptable accuracy for a model, it’s important to consider the sensitivity and impact of its application. An appropriate accuracy threshold varies with the specific demands and potential consequences of the model’s use, requiring a nuanced understanding of how accuracy levels intersect with the objectives and risks of each unique application.\n\n\n\n\nHow is it possible that my model achieved 100% accuracy?\nAchieving 100% accuracy is possible, however it is important to consider, especially in small datasets, that this might indicate model overfitting, caused by feature leakage or other data anomalies. Feature leakage occurs when your model inadvertently learns from data that explicitly includes label information (e.g., sentences of ‘I feel happy’ for a target label ‘happy’) leading to skewed results. To ensure more reliable performance, it’s advisable to use larger datasets and check that your data does not unintentionally contain explicit information about the labels.",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your model"
      },
      "h3": {
        "id": "assessing-good-performance",
        "title": "Assessing 'good' performance"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.custom-models-api.evaluating-a-custom-model-advanced-evaluation-metrics",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/custom-models-api/evaluating-a-custom-model",
    "title": "Advanced evaluation metrics",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/docs/custom-models-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#advanced-evaluation-metrics",
    "content": "In addition to accuracy, advanced metrics for a deeper evaluation of your custom model’s performance are also provided.\n\n\nAdvanced evaluation metrics\nTerm Definition \nAccuracy A fundamental metric in model performance evaluation which measures the proportion of correct predictions (true positives and true negatives) against the total number made. It’s straightforward and particularly useful for balanced datasets. However, accuracy can be misleading in imbalanced datasets where one class predominates, as a model might seem accurate by mainly predicting the majority class, neglecting the minority. This limitation underscores the importance of using additional metrics like precision, recall, and F1 score for a more nuanced assessment of model performance across different classes. \nPrecision Score which measures how often the model detects positives correctly. (e.g., When your model identifies a customer’s expression as 'satisfied', how often is the customer actually satisfied? Low precision would mean the model often misinterprets other expressions as satisfaction, leading to incorrect categorization.) \nRecall Score which measures how often the model correctly identifies actual positives. (e.g., Of all the genuine expressions of satisfaction, how many does your model accurately identify as 'satisfied'?\" Low recall implies the model is missing out on correctly identifying many true instances of customer satisfaction, failing to recognize them accurately.) \nF1 A metric that combines precision and recall, providing a balanced measure of a model’s accuracy, particularly useful in scenarios with class imbalance or when specific decision thresholds are vital. \nAverage Precision A metric that calculates the weighted average of precision at each threshold, providing a comprehensive measure of a model’s performance across different levels of recall. \nRoc Auc (Area under the ROC curve) a comprehensive measure of a model’s ability to distinguish between classes across all possible thresholds, making it ideal for overall performance evaluation and comparative analysis of different models.",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your model"
      },
      "h3": {
        "id": "advanced-evaluation-metrics",
        "title": "Advanced evaluation metrics"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.custom-models-api.evaluating-a-custom-model-improving-model-performance",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/custom-models-api/evaluating-a-custom-model",
    "title": "Improving model performance",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/docs/custom-models-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#improving-model-performance",
    "content": "Increase data quantity: adding more data will often help a model to learn a broader range of the given target’s representation, increasing the likelihood of capturing outliers from diverse patterns and scenarios.\n\nImprove label quality: ensure that each data point in your dataset is well-labeled with clear, accurate, and consistent annotations. Properly defined labels are essential for reducing misinterpretations and confusion, allowing the model to accurately represent and learn from the dataset’s true characteristics. Ensuring balance in the distribution of labels is important to ensure that the model is not biased towards a specific label.\n\nEnhance data quality: refine your dataset to ensure it is free from noise and irrelevant information. High-quality data (in terms of your target) enhances the model’s ability to make precise predictions and learn effectively from relevant features, critical in complex datasets.\n\nIncorporate clear audio data: when working with models analyzing vocal expressions, ensure audio files include clear, audible spoken language. This enhances the model’s ability to accurately interpret and learn from vocal nuances. Explore various segmentation strategies which evaluate the effect that environmental sound may have on your model’s performance.",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your model"
      },
      "h3": {
        "id": "improving-model-performance",
        "title": "Improving model performance"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.custom-models-api.faq",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/custom-models-api/faq",
    "title": "Custom Models API FAQ",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/docs/custom-models-api"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "Custom Models become essential when raw embeddings from Hume’s expression measurement models require further tailoring for specific applications. Here are scenarios where Custom Models offer significant advantages:\nSpecialized contexts: In environments with unique characteristics or requirements, Custom Models enable the creation of context-specific labels, ensuring more relevant and accurate insights. If your project demands a particular set of labels that are not covered by Hume’s emotional expression labels, Custom Models enable you to create and apply these labels, ensuring that the analysis aligns with your specific objectives.\n\nIterative model improvement: In evolving fields or scenarios where data and requirements change over time, Custom Models offer the flexibility to iteratively improve and adapt the model with new data and labels.\n\n\n\n\nIn labeling, regression involves assigning continuous numerical values, while classification involves categorizing data into discrete labels. During training, regression models learn to predict numerical values, whereas classification models learn to categorize data points into predefined classes.\nClassification use cases\nEmotion Categorization: Classification excels in distinguishing distinct emotional states, like identifying happiness, sadness, or surprise based on linguistic or physical expression cues.\n\nBinary Emotional Analysis: Useful in binary scenarios such as detecting presence or absence of specific emotional reactions, like engagement or disengagement in a learning environment.\n\nMulti-Emotional Identification: Perfect for classifying a range of emotions in complex scenarios, like understanding varied customer reactions from satisfied to dissatisfied based on their verbal and non-verbal feedback.\n\n\nRegression use cases\nIntensity Measurement: Regression is apt for quantifying the intensity or degree of emotional responses, such as assessing the level of stress or joy from vocal or facial cues.\n\nEmotional Progression Tracking: Ideal for monitoring the fluctuation of emotional states over time, like tracking the development of engagement or anxiety in therapy sessions.\n\n\nIn essence, regression models in emotional expression analysis assign continuous values representing intensities or degrees, while classification models categorize expressions into distinct states or reactions.\n\n\nOur custom model pipeline is designed to accommodate a wide range of data types, including audio, videos, and text, automatically integrating multimodal patterns of expression and language. However, not all datasets are created equal. For best results, we recommend using a dataset that meets certain standards:\nDataset size\nIdeally, use a dataset consisting of a minimum of 20 files, but more data is always better for model performance.\nMedia type consistency\nAll files within a dataset should be of the same media type (video, audio, image, text...etc.)\nIt's generally wise to maintain a consistent naming convention and file format for your dataset. At minimum, ensure files have appropriate extensions, such as .wav, .mp3, .aif, .mov, or .mp4.\nClassification vs regression tasks\nDepending on your model's objective (classification or regression), you can use different labeling approaches.\nClassification labels: use either strings or integers as labels (e.g., \"confused,\" \"focused\"). We limit the number of categorical labels to 50, and you must have at least two (binary).\n\nRegression targets: use either integers or decimals as targets. A model trained on a regression task with predict a continuous numerical value.\n\n\nLabel consistency\nWe recommend that your labels follow a consistent format; e.g, do not mix integers and strings. Furthermore, be sure to check for any typos in your labels, as these will be considered as separate classes, e.g, “happy” vs. “hapy.”\nClass imbalance\nIf possible, it helps to have a balanced distribution of labels in your dataset. For example, if you have 50 files and two classes, the best case is to have 25 samples per class. Generally, you need at least 10 samples per class to train a useful model, but more data per class is always better.\n\n\nOur custom models support the same range of languages as our expression\nmeasurement models. You can find a complete list of supported languages\nhere."
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/billing",
    "title": "Billing",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-postpaid-billing",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/billing",
    "title": "Postpaid billing",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#postpaid-billing",
    "content": "We're in the process of transitioning to postpaid billing, a flexible pay-as-you-go system. You pay only for the services used, without needing to purchase credits upfront. This option is not available for all Hume users quite yet. You can check Usage & Billing to see if postpaid billing is available for your account and reach out to our support team if you're interested in getting early access.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "postpaid-billing",
        "title": "Postpaid billing"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-how-it-works",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/billing",
    "title": "How it works",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#how-it-works",
    "content": "Joining the platform: When you sign up and start using our API, you'll initially be using the free credits given to every new account.\n\nCredit card requirement: Once you've exhausted your credit balance, you'll need to add your credit card information to continue.\n\n\n\n\nSubcribe to postpaid billing before depleting your credit balance to ensure uninterrupted service.\nMonthly limit and notifications:\nYou'll have a default monthly limit of $100.\n\nIf you hit the $100 limit, API calls will return an error, and you'll be prompted to apply for a limit increase.\n\n\n\nBilling notifications:\nOn the first of each month, you'll receive an invoice for the previous month’s usage.\n\nIf your credit card is successfully added, it will be charged automatically.\n\nYou'll get a confirmation email for successful transactions or an alert if a transaction fails.\n\n\n\nFailure to pay: If payment isn't received within 7 days of the invoice date, API access will be suspended until the outstanding balance is settled.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "how-it-works",
        "title": "How it works"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-managing-your-account",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/billing",
    "title": "Managing your account",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#managing-your-account",
    "content": "Usage information: To view your monthly usage details, visit Usage & Billing. There you can track your API usage and see how much of your monthly limit has been utilized.\nNote: After your prepaid credits are used, further usage accrues to your monthly cost.  You'll be charged this amount on the first of the following month. Your monthly cost is updated daily at 08:00 UTC.\n\n\n\nBilling portal: To manage your billing details, navigate to Usage & Billing and select Manage payments and view invoices. There you can update your payment method, view past invoices, and keep track of upcoming charges.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "managing-your-account",
        "title": "Managing your account"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-pricing",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/billing",
    "title": "Pricing",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#pricing",
    "content": "Find up-to-date pricing information at hume.ai/pricing.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "understanding-your-bill",
        "title": "Understanding your bill"
      },
      "h3": {
        "id": "pricing",
        "title": "Pricing"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-billing-methodology",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/billing",
    "title": "Billing methodology",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#billing-methodology",
    "content": "Audio and video:\nOur listed prices are presented per minute for ease of understanding.\n\nHowever, we bill these services on a corresponding per-second basis to ensure precise and fair charges. This means you are only billed for the exact amount of time your audio or video content is processed.\n\n\n\nImage and text:\nImage processing charges are incurred per image.\n\nText processing is billed based on the number of words processed.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "understanding-your-bill",
        "title": "Understanding your bill"
      },
      "h3": {
        "id": "billing-methodology",
        "title": "Billing methodology"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.billing-faqs",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/billing",
    "title": "FAQs",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#faqs",
    "content": "After you use all your credits, there might be a delay before we switch you to a subscription or stop access, which can result in a small negative credit balance. This is normal and won't affect your subscription.\nIf you have questions about your bill or need assistance understanding the charges, please contact billing@hume.ai.",
    "hierarchy": {
      "h0": {
        "title": "Billing"
      },
      "h2": {
        "id": "faqs",
        "title": "FAQs"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/errors",
    "title": "Errors",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-configuration-errors",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/errors",
    "title": "Configuration errors",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#configuration-errors",
    "content": "Configuration errors indicate that something about the API call was not configured correctly. The error message you get from the Hume APIs will often contain more information than we're able to provide on this page. For example if an audio file is too long, the error message from the API will specify the limit as well as the length of the audio received.\nError Code Description \nE0100 The WebSocket request could not be parsed as valid JSON. The Hume API requires JSON serializable payloads. \nE0101 You may be missing or improperly formatting a required field. This generic error indicates that the structure of your WebSocket request was invalid. Please see the error message you received in the API response for more details. \nE0102 The requested model was incompatible with the file format received. Some models are not compatible with every file type. For example, no facial expressions will be detected in a text file. Audio can be extracted out of some video files, but if the video has no audio, then models like Speech Prosody and Vocal Burst will not be available. \nE0200 Media provided could not be parsed into a known file format. Hume APIs support a wide range of file formats and media types including audio, video, image, text, but not all formats are supported. If you receive this error and believe your file type should be supported please reach out to our support team. \nE0201 Media could not be decoded as a Base64 encoded string. The data field in the request payload should be Base64 encoded bytes. If you want to pass raw text without encoding it you can do so with the raw_text parameter. \nE0202 No audio signal could be inferred from the media provided. This error indicates that audio models were configured, but the media provided could not be parsed into a valid audio file. \nE0203 Your audio file was too long. The limit is 5000 milliseconds. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files considering using the Hume Measurement API REST endpoints. \nE0204 Your video file was too long. For best performance we recommend passing individual frames of video as images rather than full video files. \nE0205 Your image file was too large. The limit is 3,000 x 3,000 pixels. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files considering using the Hume Measurement API REST endpoints. \nE0206 Your text file was too long. The limit is 10,000 characters. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files considering using the Hume Measurement API REST endpoints. \nE0207 The URL you've provided appears to be incorrect. Please verify that you've entered the correct URL and try submitting it again. If you're copying and pasting, ensure that the entire URL has been copied without any missing characters. \nE0300 You've run out of credits. Go to beta.hume.ai to purchase more. \nE0301 Your monthly credit limit has been reached. With post-paid pricing, users can accrue charges up to a predetermined monthly cap. This limit ensures that users do not accumulate excessive debt without assurance of payment. If you require a higher limit, you may manually apply for a credit limit increase. Alternatively, the limit will reset at the beginning of the next month. For more information, please see our docs on billing. \nE0400 You've referenced a resource that doesn't exist in our system. Please check if the name or identifier you used is correct and try again. \nE0401 Your upload failed. Please ensure your file meets our format and size requirements, and attempt to upload it again. \nE0402 The CSV file you used to create or update a dataset is missing a header row. The header specifies what each column represents. Update your CSV file and retry your request. For more information about how to format your dataset CSV please see our tutorial on dataset creation. \nE0500 Your dataset doesn't meet the minimum sample size requirement. Please add more files to your dataset and resubmit your training job. For more information, please see our docs on dataset requirements. \nE0501 Your dataset contains a target column with empty values. Please clean your dataset so that all labels are valid categorical or numeric values and then resubmit your training job. For more information on target columns please see our docs on dataset requirements. \nE0502 Your dataset contains a target column with infinite values. Please clean your dataset so that all labels are valid categorical or numeric values and then resubmit your training job. For more information on target columns please see our tutorial on dataset creation. \nE0503 For classification tasks, your dataset must include at least two distinct classes. Please check your dataset has two unique labels in the target column. \nE0504 Some classes in your dataset don't have enough samples. To ensure that the model we produce is of the highest quality we require your dataset to be relatively balanced across classes. Please check the error message for which class should have more samples (or remove that class entirely). Please see our docs on dataset requirements for more details. \nE0505 The target column you've selected doesn't exist in the dataset. Please review the columns that exist in your dataset and select a valid column name. \nE0506 Your chosen target column is not a valid target column. Please ensure that you select a column with labels rather than the file_id column or another reserved column name. \n\n\n\nThe connection will be closed automatically after ten identical configuration\nerrors to avoid unintended looping.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "configuration-errors",
        "title": "Configuration errors"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-service-errors",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/errors",
    "title": "Service errors",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#service-errors",
    "content": "If you encounter an error code starting with I (for example, error code I0100), it indicates an outage or a bug in a Hume service. Our team will already have been alerted of the internal error, but if you need immediate assistance please reach out to our support team.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "service-errors",
        "title": "Service errors"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-warnings",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/errors",
    "title": "Warnings",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#warnings",
    "content": "Warnings indicate that the payload was configured correctly, but no results could be returned.\nError Code Description \nW0101 No vocal bursts could be detected in the media. \nW0102 No face meshes could be detected in the media. \nW0103 No faces could be detected in the media. \nW0104 No emotional language could be detected in the media. \nW0105 No speech could be detected in the media.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "warnings",
        "title": "Warnings"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-common-errors",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/errors",
    "title": "Common errors",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#common-errors",
    "content": "Some errors will not have an associated error code, but are documented here.",
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "common-errors",
        "title": "Common errors"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.errors-transcript-confidence-below-threshold-value",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/errors",
    "title": "Transcript confidence below threshold value",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#transcript-confidence-below-threshold-value",
    "content": "This error indicates that our transcription service had difficulty identifying the language spoken in your audio file or the quality was too low. We prioritize quality and accuracy, so if it cannot transcribe with confidence, our models won't be able to process it further.\nBy default, we use an automated language detection method for our Speech Prosody, Language, and NER models. However, if you know what language is being spoken in your media samples, you can specify it via its BCP-47 tag and potentially obtain more accurate results.\nIf you see the message above there are few steps you can do to resolve the issue:\nVerify we support the language\n\nEnsure you are providing clear, high-quality audio files.\n\nSpecify the language within your request if you know the language in the audio.\n\n\n\n\n\n\n\n\n\n\nYou can specify any of the following: zh, da, nl, en, en-AU, en-IN, en-NZ,\nen-GB, fr, fr-CA, de, hi, hi-Latn, id, it, ja, ko, no, pl, pt, pt-BR, pt-PT,\nru, es, es-419, sv, ta, tr, or uk",
    "code_snippets": [
      {
        "lang": "python",
        "code": "from hume import HumeBatchClient\nfrom hume.models.config import ProsodyConfig\n\nclient = HumeBatchClient(\"<YOUR API KEY>\")\nurls = [\"https://hume-tutorials.s3.amazonaws.com/faces.zip\"]\nmodel_configs = [ProsodyConfig()]\ntranscription_config = TranscriptionConfig(language=\"en\")\njob = client.submit_job(urls, model_configs, transcription_config=transcription_config)\n\nprint(job)\nprint(\"Running...\")\n\njob.await_complete()\npredictions = job.get_predictions()\nprint(prediction)"
      },
      {
        "lang": "python",
        "code": "from hume import HumeBatchClient\nfrom hume.models.config import ProsodyConfig\n\nclient = HumeBatchClient(\"<YOUR API KEY>\")\nurls = [\"https://hume-tutorials.s3.amazonaws.com/faces.zip\"]\nmodel_configs = [ProsodyConfig()]\ntranscription_config = TranscriptionConfig(language=\"en\")\njob = client.submit_job(urls, model_configs, transcription_config=transcription_config)\n\nprint(job)\nprint(\"Running...\")\n\njob.await_complete()\npredictions = job.get_predictions()\nprint(prediction)"
      },
      {
        "lang": "json",
        "code": "\"transcription\": {\n    \"language\": \"en\"\n}"
      },
      {
        "lang": "json",
        "code": "\"transcription\": {\n    \"language\": \"en\"\n}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Errors"
      },
      "h2": {
        "id": "common-errors",
        "title": "Common errors"
      },
      "h3": {
        "id": "transcript-confidence-below-threshold-value",
        "title": "Transcript confidence below threshold value"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/science",
    "title": "About the Science",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "content": "What is it about speaking in person that allows us to understand each other so much more accurately than text alone? It isn’t what we say—it’s the way we say it. Science consistently demonstrates that expressions convey important information that is vital for social interaction and forms the building blocks of empathy.\nThat being said, expressions aren’t direct windows into the human mind. Measuring and interpreting expressive behavior is a complex and nuanced task that is the subject of ongoing scientific research.\nThe scientists at Hume AI have run some of the largest-ever psychology studies to better understand how humans express themselves. By investigating expressions around the world and what they mean to the people making them, we’ve mapped out the nuances of expression in the voice, language, and face in unprecedented detail. We’ve published this research in the world’s leading scientific journals and, for the first time, translated it into cutting-edge machine learning models.\nThese models, shaped by a new understanding of human expression, include:\nFacial Expression\n\nSpeech Prosody\n\nVocal Bursts\n\nEmotional Language"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-facial-expression",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/science",
    "title": "Facial Expression",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#facial-expression",
    "content": "Facial expression is the most well-studied modality of expressive behavior, but the overwhelming focus has been on six discrete categories of facial movement or time-consuming manual annotations of facial movements (the scientifically useful, but outdated, Facial Action Coding System). Our research shows that these approaches capture less than 30% of what typical facial expressions convey.\nHume’s Facial Emotional Expression model generates 48 outputs encompassing the dimensions of emotional meaning people reliably attribute to facial expressions. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nHume’s FACS 2.0 model is a new generation automated facial action coding system (FACS). With 55 outputs encompassing 26 traditional actions units (AUs) and 29 other descriptive features (e.g., smile, scowl), FACS 2.0 is even more comprehensive than manual FACS annotations.\nOur facial expression models are packaged with face detection and work on both images and videos.\nIn addition to our image-based facial expression models, we also offer an Anonymized Facemesh model for applications in which it is essential to keep personally identifiable data on-device (e.g., for compliance with local laws). Instead of face images, our facemesh model processes facial landmarks detected using Google's MediaPipe library. It achieves about 80% accuracy relative to our image-based model.\nTo read more about the team’s research on facial expressions, check out our publications in American Psychologist (2018), Nature (2021), and iScience (2024).",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities"
      },
      "h3": {
        "id": "facial-expression",
        "title": "Facial Expression"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-speech-prosody",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/science",
    "title": "Speech Prosody",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#speech-prosody",
    "content": "Speech prosody is not about the words you say, but the way you say them. It is distinct from language (words) and from non-linguistic vocal utterances.\nOur Speech Prosody model generates 48 outputs encompassing the 48 dimensions of emotional meaning that people reliably distinguish from variations in speech prosody. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Speech Prosody model is packaged with speech detection and works on both audio files and videos.\nTo read more about the team’s research on speech prosody, check out our publications in Nature Human Behaviour (2019) and Proceedings of the 31st ACM International Conference on Multimedia (2023).",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities"
      },
      "h3": {
        "id": "speech-prosody",
        "title": "Speech Prosody"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-vocal-bursts",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/science",
    "title": "Vocal Bursts",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#vocal-bursts",
    "content": "Non-linguistic vocal utterances, including sighs, laughs, oohs, ahhs, umms, and shrieks (to name but a few), are a particularly powerful and understudied modality of expressive behavior. Recent studies reveal that they reliably convey distinct emotional meanings that are extremely well-preserved across most cultures.\nNon-linguistic vocal utterances have different acoustic characteristics than speech emotional intonation (prosody) and need to be modeled separately.\nOur Vocal Burst Expression model generates 48 outputs encompassing the distinct dimensions of emotional meaning that people distinguish in vocal bursts. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Vocal Burst Description model provides a more descriptive and categorical view of nonverbal vocal expressions (“gasp,” “mhm,” etc.) intended for use cases such as audio captioning. It generates 67 descriptors, including 30 call types (“sigh,” “laugh,” “shriek,” etc.) and 37 common onomatopoeia transliterations of vocal bursts (“hmm,” “ha,” “mhm,” etc.).\nOur vocal burst models are packaged with non-linguistic vocal utterance detection and works on both audio files and videos.\nTo read more about the team’s research on vocal bursts, check out our publications in American Psychologist (2019), Interspeech 2022, ICASSP 2023, and Nature Human Behaviour (2023).",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities"
      },
      "h3": {
        "id": "vocal-bursts",
        "title": "Vocal Bursts"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-emotional-language",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/science",
    "title": "Emotional Language",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#emotional-language",
    "content": "The words we say include explicit disclosures of emotion and implicit emotional connotations. These meanings are complex and high-dimensional.\nFrom written or spoken words, our Emotional Language model generates 53 outputs encompassing different dimensions of emotion that people often perceive from language. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Emotional Language model is packaged with speech transcription and works on audio files, videos, and text.\nOur Named Entity Recognition (NER) model can also identify topics or entities (people, places, organizations, etc.) mentioned in speech or text and the tone of language they are associated with, as identified by our emotional language model.",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities"
      },
      "h3": {
        "id": "emotional-language",
        "title": "Emotional Language"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.science-published-research",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/science",
    "title": "Published Research",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#published-research",
    "content": "You can access a comprehensive list of our published research papers along with PDFs for download here.",
    "hierarchy": {
      "h0": {
        "title": "About the Science"
      },
      "h2": {
        "id": "published-research",
        "title": "Published Research"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.use-case-guidelines",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/use-case-guidelines",
    "title": "Use case guidelines",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.use-case-guidelines-ethical-guidelines",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/use-case-guidelines",
    "title": "Ethical guidelines",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#ethical-guidelines",
    "content": "Understanding expressive communication is essential to building technologies that address our needs and improve our well-being. But technologies that recognize language and nonverbal behavior can also pose risks. That’s why we require that all commercial applications incorporating our APIs adhere to the ethical guidelines of The Hume Initiative.",
    "hierarchy": {
      "h0": {
        "title": "Use case guidelines"
      },
      "h2": {
        "id": "ethical-guidelines",
        "title": "Ethical guidelines"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.use-case-guidelines-scientific-best-practices",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/use-case-guidelines",
    "title": "Scientific best practices",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#scientific-best-practices",
    "content": "Use inductive methods to identify the expressive signals that matter for your application. Even if you are interested in a specific emotion like “anger,” how that emotion is expressed depends on setting: anger on a football field sounds different than anger on a customer service call. Our models succinctly compress the representation of emotional expression so that, even with limited data, you can examine how their outputs can be used in your specific research or application setting. You can do this by using statistical methods like regression or classification, or by examining the distribution of expressions in your data using our Playground.\n\nNever assume a one-to-one mapping between emotional experience and expression. The outputs of our models should be treated as measurements of complex expressive behavior. We provide labels to our outputs indicating what these dimensions of expression are often reported to mean, but these labels should not be interpreted as direct inferences of how someone is feeling at any given time. Rather, “a full understanding of emotional expression and experience requires an appreciation of a wide degree of variability in display behavior, subjective experience, patterns of appraisal, and physiological response, both within and across emotion categories” (Cowen et al., 2019).\n\nNever overlook the nuances in emotional expression. For instance, avoid the temptation to focus on just the top label. We provide interactive visualizations in our Playground to help you map out complex patterns in real-life emotional behavior. These visualizations are informed by recent advances in emotion science, departing from reductive models that long “anchored the science of emotion to a predominant focus on prototypical facial expressions of the “basic six”: anger, disgust, fear, sadness, surprise, and happiness,” and embracing how “new discoveries reveal that the two most commonly studied models of emotion—the basic six and the affective circumplex (comprising valence and arousal)—each capture at most 30% of the variance in the emotional experiences people reliably report and in the distinct expressions people reliably recognize.” (Cowen et al., 2019)\n\nAccount for culture-specific meanings and display tendencies. Studies have routinely observed subtle cultural differences in the meaning of expressions as well as broader “variations in the frequency and intensity with which different expressions were displayed” (Cowen et al., 2022). Given these differences, empathic AI applications should be tested in each population in which they are deployed and fine-tuned when necessary.\nRead about the science behind our models if you’d like to delve deeper into how they work.",
    "hierarchy": {
      "h0": {
        "title": "Use case guidelines"
      },
      "h2": {
        "id": "scientific-best-practices",
        "title": "Scientific best practices"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/privacy",
    "title": "Privacy",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy-privacy-policy",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/privacy",
    "title": "Privacy Policy",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#privacy-policy",
    "content": "Our Privacy Policy governs how we collect and use personal information submitted to our products.",
    "hierarchy": {
      "h0": {
        "title": "Privacy"
      },
      "h2": {
        "id": "privacy-policy",
        "title": "Privacy Policy"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy-api-data-usage-policy",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/privacy",
    "title": "API Data Usage Policy",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#api-data-usage-policy",
    "content": "Our API Data Usage Policy details how and when we store API data.",
    "hierarchy": {
      "h0": {
        "title": "Privacy"
      },
      "h2": {
        "id": "api-data-usage-policy",
        "title": "API Data Usage Policy"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.docs.docs.resources.privacy-consumer-services-faq",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/docs/resources/privacy",
    "title": "Consumer Services FAQ",
    "breadcrumb": [
      {
        "title": "Resources",
        "pathname": "/docs/resources"
      }
    ],
    "tab": {
      "title": "Documentation"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#consumer-services-faq",
    "content": "Our Consumer Services FAQ explains how and when we store data processed by our frontend applications like our Playground.\n\n\n\n\nFor non-API consumer products like our Playground and Demo, we may use content such as images, video files, audio files, and text files to improve our services. You can request to opt out of having your content used to improve our services at any time by contacting us on Discord with your request. This opt-out will apply on a going-forward basis only.\nPlease note that for our API product, Hume AI will not use data submitted by customers via our API to train or improve our models.\n\n\nYou can delete your account by submitting a user account deletion request in your Profile page on the Hume playground. Once you submit your deletion request, we will delete your account within 30 days. Please note that for security reasons, once you delete your account, you may not re-sign up for an account with the same email address.\n\n\nWe share content with a select group of trusted service providers that help us provide our services. We share the minimum amount of content we need in order to accomplish this purpose and our service providers are subject to strict confidentiality and security obligations. Please see our Privacy Policy for more information on who we may share your content with.\n\n\nContent is stored on Hume AI systems and our trusted service providers' systems in the US and around the world.\n\n\nA limited number of authorized Hume AI personnel, may view and access user content only as needed for these reasons: (1) investigating abuse or a security incident; (2) to provide support to you if you reach out to us with questions about your account; (3) or to comply with legal obligations. Access to content is subject to technical access controls and limited only to authorized personnel on a need-to-know basis. Additionally, we monitor and log all access to user content and authorized personnel must undergo security and privacy training prior to accessing any user content.\n\n\nNo. We do not sell your data or share your content with third parties for marketing purposes.\n\n\nPlease message the moderators on our Discord Server.",
    "hierarchy": {
      "h0": {
        "title": "Privacy"
      },
      "h2": {
        "id": "consumer-services-faq",
        "title": "Consumer Services FAQ"
      }
    },
    "level": "h2"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "Changelog",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-additions",
    "content": "Added support for streaming custom language model responses in parts. Developers can send text chunks to start generating audio responses much faster.\nThe Custom Language Model endpoint now expects text to be formatted in the following payload:\n\nAdded support for pausing and resuming EVI responses with with pause_assistant_message and resume_assistant_message. Sending a pause message stops EVI from generating and speaking Assistant messages. Sending a resume message allows EVI to continue responding to the User messages.",
    "code_snippets": [
      {
        "code": "# send this to add text\n{\"type\": \"assistant_input\", \"text\": \"<chunk>\"}\n\n# send this message when you're done speaking\n{\"type\": \"assistant_end\"}"
      }
    ],
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-24-2024",
        "title": "May 24, 2024"
      },
      "h3": {
        "id": "evi-api-additions",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-modifications",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API modifications",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-modifications",
    "content": "Increased the limit for tool descriptions from 100 chars to 512 chars\n\nSet the maximum length for tool_name to 64 chars",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-24-2024",
        "title": "May 24, 2024"
      },
      "h3": {
        "id": "evi-api-modifications",
        "title": "EVI API modifications"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-1",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-additions-1",
    "content": "Added support for built-in tools, starting with web search: Using built-in tools\n\nAdded support for phone calling through a Twilio integration: Phone calling\n\nAdded DACHER voice to the voice configuration options\n\nAdded support for the gpt-4o language model",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-17-2024",
        "title": "May 17, 2024"
      },
      "h3": {
        "id": "evi-api-additions-1",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-modifications-1",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API modifications",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-modifications-1",
    "content": "Increased the limit for tool descriptions from 100 chars to 512 chars",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-17-2024",
        "title": "May 17, 2024"
      },
      "h3": {
        "id": "evi-api-modifications-1",
        "title": "EVI API modifications"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-2",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-additions-2",
    "content": "Added support for three open-source models through the Groq language model provider: Gemma 7B (gemma-7b-it), Llama 3 8B (llama3-8b-8192), and Llama 3 70B (llama3-70b-8192)\n\nAdded support for Llama 30 70B language model through the Fireworks language model provider (accounts/fireworks/models/llama-v3-70b-instruct)\n\nAdded a custom_session_id field in the SessionSettings message, and documentation for using it: Custom Session ID",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024"
      },
      "h3": {
        "id": "evi-api-additions-2",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-modifications-2",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API modifications",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-modifications-2",
    "content": "Disabled short response generation for custom language models\n\nAdded error codes for when Hume credits run out while using EVI. Users will receive either the E0300 error code if they are out of credits or E0301 if they are blocked via subscription. The WebSocket connection will also be closed with code 1008",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024"
      },
      "h3": {
        "id": "evi-api-modifications-2",
        "title": "EVI API modifications"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#bugs-bashed",
    "content": "Fixed an issue with the from_text field in UserMessage. It is now set to True if any part of the UserMessage is from a developer-provided UserInput message",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024"
      },
      "h3": {
        "id": "bugs-bashed",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-3",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-additions-3",
    "content": "Added support for Tools through our tool use feature\n\nAdded ToolErrorMessage as a supported input type",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-03-2024",
        "title": "May 3, 2024"
      },
      "h3": {
        "id": "evi-api-additions-3",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-1",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#bugs-bashed-1",
    "content": "Added an error that returns status 400 if a Config, Tool, or Prompt is created with a name or versionDescription that's too long or non-ASCII. Names must be under 75 chars, versionDescription must be under 256 chars, description for Tools must be under 100 chars, fallback_content for Tools must be under 2048 chars, and model_resource for LanguageModels must be under 1024 chars\n\nFixed several edge cases and bugs involving Tool calls, including supporting only single tool calls with EVI (no parallel tool calling)",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "05-03-2024",
        "title": "May 3, 2024"
      },
      "h3": {
        "id": "bugs-bashed-1",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-additions-4",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API additions",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-additions-4",
    "content": "Added support for reading language model type from EVI configs\n\nAdded support for reading language model temperature from EVI configs\n\nAdded system prompt to SessionSettings message to allow dynamic prompt updating",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024"
      },
      "h3": {
        "id": "evi-api-additions-4",
        "title": "EVI API additions"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-evi-api-changes",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "EVI API changes",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#evi-api-changes",
    "content": "Renamed TextInput message to UserInput to indicate this is text to be added to the chat history as a User message and used as context by the LLM\n\nRenamed TtsInput message to AssistantInput to make it clear that this is input text to be spoken by EVI and added to the chat history as an Assistant message\n\nMoved audio configuration options to SessionSettings message",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024"
      },
      "h3": {
        "id": "evi-api-changes",
        "title": "EVI API changes"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.changelog.changelog.changelog-bugs-bashed-2",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/changelog",
    "title": "Bugs bashed",
    "breadcrumb": [],
    "tab": {
      "title": "Changelog"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "type": "markdown",
    "hash": "#bugs-bashed-2",
    "content": "Fixed chats staying open after errors, chats will now end upon exceptions\n\nAdded an error thrown if config uses both custom_model and prompt, because custom language models do not use prompts\n\nFixed issue where erroring when sending errors would cause the API to get stuck\n\nAdded clearer errors for custom language models\n\nAdded unable to configure audio service error\n\nAdded an error to invalidate outdated language model responses",
    "hierarchy": {
      "h0": {
        "title": "Changelog"
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024"
      },
      "h3": {
        "id": "bugs-bashed-2",
        "title": "Bugs bashed"
      }
    },
    "level": "h3"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.list-tools",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "title": "List tools",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.list-tools",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.create-tool",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "title": "Create tool",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.create-tool",
    "method": "POST",
    "endpoint_path": "/v0/evi/tools",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.list-tool-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "title": "List tool versions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.list-tool-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.create-tool-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "title": "Create tool version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.create-tool-version",
    "method": "POST",
    "endpoint_path": "/v0/evi/tools/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.delete-tool",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/delete-tool",
    "title": "Delete tool",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.delete-tool",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/tools/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.update-tool-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-name",
    "title": "Update tool name",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.update-tool-name",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/tools/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.get-tool-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "title": "Get tool version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.get-tool-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.delete-tool-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/delete-tool-version",
    "title": "Delete tool version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.delete-tool-version",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_tools.endpoint_tools.update-tool-description",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "title": "Update tool description",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Tools",
        "pathname": "/reference/empathic-voice-interface-evi/tools"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_tools.update-tool-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.list-prompts",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "title": "List prompts",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.list-prompts",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.create-prompt",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "title": "Create prompt",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.create-prompt",
    "method": "POST",
    "endpoint_path": "/v0/evi/prompts",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.list-prompt-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "title": "List prompt versions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.list-prompt-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.create-prompt-verison",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "title": "Create prompt version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.create-prompt-verison",
    "method": "POST",
    "endpoint_path": "/v0/evi/prompts/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.delete-prompt",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt",
    "title": "Delete prompt",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.delete-prompt",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/prompts/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.update-prompt-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-name",
    "title": "Update prompt name",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.update-prompt-name",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/prompts/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.get-prompt-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "title": "Get prompt version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.get-prompt-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.delete-prompt-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt-version",
    "title": "Delete prompt version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.delete-prompt-version",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_prompts.endpoint_prompts.update-prompt-description",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "title": "Update prompt description",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Prompts",
        "pathname": "/reference/empathic-voice-interface-evi/prompts"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_prompts.update-prompt-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.list-configs",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "title": "List configs",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.list-configs",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.create-config",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "title": "Create config",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.create-config",
    "method": "POST",
    "endpoint_path": "/v0/evi/configs",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.list-config-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "title": "List config versions",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.list-config-versions",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.create-config-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "title": "Create config version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.create-config-version",
    "method": "POST",
    "endpoint_path": "/v0/evi/configs/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.delete-config",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/delete-config",
    "title": "Delete config",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.delete-config",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/configs/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.update-config-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-name",
    "title": "Update config name",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.update-config-name",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/configs/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.get-config-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "title": "Get config version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.get-config-version",
    "method": "GET",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.delete-config-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/delete-config-version",
    "title": "Delete config version",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.delete-config-version",
    "method": "DELETE",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_configs.endpoint_configs.update-config-description",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "title": "Update config description",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Configs",
        "pathname": "/reference/empathic-voice-interface-evi/configs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_configs.update-config-description",
    "method": "PATCH",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_chats.endpoint_chats.list-chats",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "title": "List chats",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_chats.list-chats",
    "method": "GET",
    "endpoint_path": "/v0/evi/chats",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_chats.endpoint_chats.list-chat-events",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "title": "List chat events",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chats",
        "pathname": "/reference/empathic-voice-interface-evi/chats"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_chats.list-chat-events",
    "method": "GET",
    "endpoint_path": "/v0/evi/chats/:id",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_chatGroups.endpoint_chatGroups.list-chat-groups",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "title": "List chat_groups",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-groups",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_chatGroups.endpoint_chatGroups.list-chat-group-events",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "title": "List chat events from a specific chat_group",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat Groups",
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-group-events",
    "method": "GET",
    "endpoint_path": "/v0/evi/chat_groups/:id/events",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.empathic-voice-interface-evi.subpackage_chat.subpackage_chat.chat",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/empathic-voice-interface-evi/chat/chat",
    "title": "Chat",
    "breadcrumb": [
      {
        "title": "Empathic Voice Interface (EVI)",
        "pathname": "/reference/empathic-voice-interface-evi"
      },
      {
        "title": "Chat",
        "pathname": "/reference/empathic-voice-interface-evi/chat"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "websocket",
    "api_definition_id": "8b03e44c-d4e2-4613-a138-e25b8c65c3cb",
    "api_endpoint_id": "subpackage_chat.chat",
    "method": "GET",
    "endpoint_path": "[object Object]",
    "environments": [
      {
        "id": "Default",
        "url": "wss://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.expression-measurement-api.subpackage_batch.endpoint_batch.list-jobs",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/expression-measurement-api/batch/list-jobs",
    "title": "List jobs",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "e810f56e-df97-4af2-bdaa-659077e1948e",
    "api_endpoint_id": "endpoint_batch.list-jobs",
    "method": "GET",
    "endpoint_path": "/v0/batch/jobs",
    "description": "Sort and filter jobs.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.expression-measurement-api.subpackage_batch.endpoint_batch.start-inference-job",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/expression-measurement-api/batch/start-inference-job",
    "title": "Start inference job",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "e810f56e-df97-4af2-bdaa-659077e1948e",
    "api_endpoint_id": "endpoint_batch.start-inference-job",
    "method": "POST",
    "endpoint_path": "/v0/batch/jobs",
    "description": "Start a new measurement inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.expression-measurement-api.subpackage_batch.endpoint_batch.get-job-details",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/expression-measurement-api/batch/get-job-details",
    "title": "Get job details",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "e810f56e-df97-4af2-bdaa-659077e1948e",
    "api_endpoint_id": "endpoint_batch.get-job-details",
    "method": "GET",
    "endpoint_path": "/v0/batch/jobs/:id",
    "description": "Get the request details and state of a given job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.expression-measurement-api.subpackage_batch.endpoint_batch.get-job-predictions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/expression-measurement-api/batch/get-job-predictions",
    "title": "Get job predictions",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "e810f56e-df97-4af2-bdaa-659077e1948e",
    "api_endpoint_id": "endpoint_batch.get-job-predictions",
    "method": "GET",
    "endpoint_path": "/v0/batch/jobs/:id/predictions",
    "description": "Get the JSON predictions of a completed measurement or custom models inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.expression-measurement-api.subpackage_batch.endpoint_batch.get-job-artifacts",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/expression-measurement-api/batch/get-job-artifacts",
    "title": "Get job artifacts",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "e810f56e-df97-4af2-bdaa-659077e1948e",
    "api_endpoint_id": "endpoint_batch.get-job-artifacts",
    "method": "GET",
    "endpoint_path": "/v0/batch/jobs/:id/artifacts",
    "description": "Get the artifacts ZIP of a completed measurement or custom models inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.expression-measurement-api.subpackage_batch.endpoint_batch.start-inference-job-from-local-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/expression-measurement-api/batch/start-inference-job-from-local-file",
    "title": "Start inference job from local file",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Batch",
        "pathname": "/reference/expression-measurement-api/batch"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "e810f56e-df97-4af2-bdaa-659077e1948e",
    "api_endpoint_id": "endpoint_batch.start-inference-job-from-local-file",
    "method": "POST",
    "endpoint_path": "/v0/batch/jobs",
    "description": "Start a new batch inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.expression-measurement-api.subpackage_stream.subpackage_stream.stream",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/expression-measurement-api/stream/stream",
    "title": "Stream",
    "breadcrumb": [
      {
        "title": "Expression Measurement API",
        "pathname": "/reference/expression-measurement-api"
      },
      {
        "title": "Stream",
        "pathname": "/reference/expression-measurement-api/stream"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "websocket",
    "api_definition_id": "e810f56e-df97-4af2-bdaa-659077e1948e",
    "api_endpoint_id": "subpackage_stream.stream",
    "method": "GET",
    "endpoint_path": "[object Object]",
    "environments": [
      {
        "id": "Default",
        "url": "wss://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_files.endpoint_files.list-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/files/list-files",
    "title": "List files",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_files.list-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/files",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_files.endpoint_files.create-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/files/create-files",
    "title": "Create files",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_files.create-files",
    "method": "POST",
    "endpoint_path": "/v0/registry/files",
    "description": "Returns 201 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_files.endpoint_files.upload-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/files/upload-file",
    "title": "Upload file",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_files.upload-file",
    "method": "POST",
    "endpoint_path": "/v0/registry/files/upload",
    "description": "Upload a file synchronously. Returns 201 if successful. Files must have a name. Files must specify Content-Type. Request bodies, and therefore files, are limited to 100MB",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_files.endpoint_files.get-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/files/get-file",
    "title": "Get file",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_files.get-file",
    "method": "GET",
    "endpoint_path": "/v0/registry/files/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_files.endpoint_files.delete-file",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/files/delete-file",
    "title": "Delete file",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_files.delete-file",
    "method": "DELETE",
    "endpoint_path": "/v0/registry/files/:id",
    "description": "Returns 204 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_files.endpoint_files.update-file-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/files/update-file-name",
    "title": "Update file name",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_files.update-file-name",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/files/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_files.endpoint_files.get-file-predictions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/files/get-file-predictions",
    "title": "Get file predictions",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Files",
        "pathname": "/reference/custom-models-api/files"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_files.get-file-predictions",
    "method": "GET",
    "endpoint_path": "/v0/registry/files/:id/predictions",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.list-datasets",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/list-datasets",
    "title": "List datasets",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.list-datasets",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.create-dataset",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/create-dataset",
    "title": "Create dataset",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.create-dataset",
    "method": "POST",
    "endpoint_path": "/v0/registry/datasets",
    "description": "Returns 201 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.get-dataset",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/get-dataset",
    "title": "Get dataset",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.get-dataset",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.create-dataset-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "title": "Create dataset version",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.create-dataset-version",
    "method": "POST",
    "endpoint_path": "/v0/registry/datasets/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.delete-dataset",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/delete-dataset",
    "title": "Delete dataset",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.delete-dataset",
    "method": "DELETE",
    "endpoint_path": "/v0/registry/datasets/:id",
    "description": "Returns 204 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.list-dataset-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "title": "List dataset versions",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.list-dataset-versions",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id/versions",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.list-dataset-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "title": "List dataset files",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.list-dataset-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/:id/files",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.get-dataset-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "title": "Get dataset version",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.get-dataset-version",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/version/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_datasets.endpoint_datasets.list-dataset-version-files",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "title": "List dataset version files",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Datasets",
        "pathname": "/reference/custom-models-api/datasets"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_datasets.list-dataset-version-files",
    "method": "GET",
    "endpoint_path": "/v0/registry/datasets/version/:id/files",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_models.endpoint_models.list-models",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/models/list-models",
    "title": "List models",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_models.list-models",
    "method": "GET",
    "endpoint_path": "/v0/registry/models",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_models.endpoint_models.get-model-details",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/models/get-model-details",
    "title": "Get model details",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_models.get-model-details",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_models.endpoint_models.update-model-name",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/models/update-model-name",
    "title": "Update model name",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_models.update-model-name",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/models/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_models.endpoint_models.list-model-versions",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/models/list-model-versions",
    "title": "List model versions",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_models.list-model-versions",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/version",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_models.endpoint_models.get-model-version",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/models/get-model-version",
    "title": "Get model version",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_models.get-model-version",
    "method": "GET",
    "endpoint_path": "/v0/registry/models/version/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_models.endpoint_models.update-model-description",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/models/update-model-description",
    "title": "Update model description",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Models",
        "pathname": "/reference/custom-models-api/models"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_models.update-model-description",
    "method": "PATCH",
    "endpoint_path": "/v0/registry/models/version/:id",
    "description": "Returns 200 if successful",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_jobs.endpoint_jobs.start-training-job",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/jobs/start-training-job",
    "title": "Start training job",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Jobs",
        "pathname": "/reference/custom-models-api/jobs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_jobs.start-training-job",
    "method": "POST",
    "endpoint_path": "/v0/registry/v0/batch/jobs/tl/train",
    "description": "Start a new custom models training job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  },
  {
    "objectID": "hume:dev.hume.ai:root.uv.reference.reference.custom-models-api.subpackage_jobs.endpoint_jobs.start-custom-models-inference-job",
    "org_id": "hume",
    "domain": "dev.hume.ai",
    "pathname": "/reference/custom-models-api/jobs/start-custom-models-inference-job",
    "title": "Start custom models inference job",
    "breadcrumb": [
      {
        "title": "Custom Models API",
        "pathname": "/reference/custom-models-api"
      },
      {
        "title": "Jobs",
        "pathname": "/reference/custom-models-api/jobs"
      }
    ],
    "tab": {
      "title": "API Reference"
    },
    "visible_by": [
      "role/everyone"
    ],
    "authed": false,
    "api_type": "http",
    "api_definition_id": "bcb7826e-ffd6-4440-b3de-6bf37440c3e2",
    "api_endpoint_id": "endpoint_jobs.start-custom-models-inference-job",
    "method": "POST",
    "endpoint_path": "/v0/registry/v0/batch/jobs/tl/inference",
    "description": "Start a new custom models inference job.",
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai"
      }
    ],
    "default_environment_id": "Default",
    "type": "api-reference"
  }
]