[
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/intro",
    "content": "EVI 2 is now available! Visit platform.hume.ai to chat with Hume's new voice-language foundation model and craft a custom empathic voice for your application.
Hume AI builds AI models that enable technology to communicate with empathy and learn to make people happy.
So much of human communication—in-person, text, audio, or video—is shaped by emotional expression. These cues allow us to attend to each other's well-being. Our platform provides the APIs needed to ensure that technology, too, is guided by empathy and the pursuit of human well-being.",
    "description": "Hume AI builds AI models that enable technology to communicate with empathy and learn to make people happy.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.welcome-to-hume-ai-root-0",
    "org_id": "test",
    "pathname": "/intro",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Welcome to Hume AI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/intro",
    "content": "Hume's Empathic Voice Interface (EVI) is the world's first emotionally intelligent voice AI. It is the only API that measures nuanced vocal modulations and responds to them using an empathic large language model (eLLM), which guides language and speech generation. Trained on millions of human interactions, our eLLM unites language modeling and text-to-speech with better EQ, prosody, end-of-turn detection, interruptibility, and alignment.",
    "domain": "test.com",
    "hash": "#empathic-voice-interface",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI",
      },
      "h3": {
        "id": "empathic-voice-interface",
        "title": "Empathic Voice Interface",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.welcome-to-hume-ai-empathic-voice-interface-0",
    "org_id": "test",
    "pathname": "/intro",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Empathic Voice Interface",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/intro",
    "content": "Hume's state-of-the-art expression measurement models for the voice, face, and language are built on 10+ years of research and advances in semantic space theory pioneered by Alan Cowen. Our expression measurement models are able to capture hundreds of dimensions of human expression in audio, video, and images.",
    "domain": "test.com",
    "hash": "#expression-measurement",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI",
      },
      "h3": {
        "id": "expression-measurement",
        "title": "Expression Measurement",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.welcome-to-hume-ai-expression-measurement-0",
    "org_id": "test",
    "pathname": "/intro",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Expression Measurement",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/intro",
    "content": "Alongside our documentation, we provide a detailed API reference to help you integrate and use our products. It includes descriptions of all our REST and WebSocket endpoints, as well as request and response formats and usage examples.




API that measures nuanced vocal modulations and responds to them using an
empathic large language model


Measure facial, vocal, and linguistic expressions",
    "domain": "test.com",
    "hash": "#api-reference",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI",
      },
      "h2": {
        "id": "api-reference",
        "title": "API Reference",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.welcome-to-hume-ai-api-reference-0",
    "org_id": "test",
    "pathname": "/intro",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "API Reference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/intro",
    "content": "Explore our step-by-step guides for integrating Hume APIs. Our GitHub repositories include straightforward projects to help you get started quickly, with code snippets for specific functionalities. Additionally, you'll find open-source SDKs for popular languages and frameworks to support your development process across various environments.




Discover sample code and projects to help you integrate our products quickly and efficiently


Home to all our public-facing code, including Hume's open-source SDKs and examples",
    "domain": "test.com",
    "hash": "#example-code",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI",
      },
      "h2": {
        "id": "example-code",
        "title": "Example Code",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.welcome-to-hume-ai-example-code-0",
    "org_id": "test",
    "pathname": "/intro",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Example Code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/intro",
    "content": "If you have questions or run into challenges, we're here to help!




Join our Discord for answers to any technical questions",
    "domain": "test.com",
    "hash": "#get-support",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Hume AI",
      },
      "h2": {
        "id": "get-support",
        "title": "Get Support",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.welcome-to-hume-ai-get-support-0",
    "org_id": "test",
    "pathname": "/intro",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Get Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/docs/introduction/api-key",
    "description": "Learn how to obtain your API keys and understand the supported authentication strategies for securely accessing Hume APIs.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.api-key",
    "org_id": "test",
    "pathname": "/docs/introduction/api-key",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Getting your API keys",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/docs/introduction/api-key",
    "content": "Each Hume account is provisioned with an API key and Secret key. These keys are accessible from the Hume Portal.
Sign in: Visit the Hume Portal and log in, or create an account.

View your API keys: Navigate to the API keys page to view your keys.",
    "domain": "test.com",
    "hash": "#api-keys",
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys",
      },
      "h2": {
        "id": "api-keys",
        "title": "API keys",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.api-key-api-keys-0",
    "org_id": "test",
    "pathname": "/docs/introduction/api-key",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "API keys",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/docs/introduction/api-key",
    "content": "Hume APIs support two authentication strategies:
API key strategy: Use API key authentication for making server-side requests. API key authentication allows you to make authenticated requests by supplying a single secret using the X-Hume-Api-Key header. Do not expose your API key in client-side code. All Hume APIs support this authentication strategy.

Token strategy:  Use Token authentication for making client-side requests. With Token authentication you first obtain a temporary access token by making a server-side request first, and use the access token when making client-side requests. This allows you to avoid exposing the API key to the client. Access tokens expire after 30 minutes, and you must obtain a new one. Today, only our Empathic Voice Interface (EVI) supports this authentication strategy.",
    "domain": "test.com",
    "hash": "#authentication-strategies",
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys",
      },
      "h2": {
        "id": "authentication-strategies",
        "title": "Authentication strategies",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.api-key-authentication-strategies-0",
    "org_id": "test",
    "pathname": "/docs/introduction/api-key",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Authentication strategies",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/docs/introduction/api-key",
    "code_snippets": [
      {
        "code": "curl https://api.hume.ai/v0/evi/{path} \
  --header 'Accept: application/json; charset=utf-8' \
  --header "X-Hume-Api-Key: <YOUR API KEY>"",
        "lang": "bash",
      },
      {
        "code": "curl https://api.hume.ai/v0/evi/{path} \
  --header 'Accept: application/json; charset=utf-8' \
  --header "X-Hume-Api-Key: <YOUR API KEY>"",
        "lang": "bash",
      },
      {
        "code": "curl https://api.hume.ai/v0/batch/jobs/{path} \
  --header 'Accept: application/json; charset=utf-8' \
  --header "X-Hume-Api-Key: <YOUR API KEY>"",
        "lang": "bash",
      },
      {
        "code": "curl https://api.hume.ai/v0/batch/jobs/{path} \
  --header 'Accept: application/json; charset=utf-8' \
  --header "X-Hume-Api-Key: <YOUR API KEY>"",
        "lang": "bash",
      },
      {
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?api_key=${apiKey}`);",
        "lang": "TypeScript",
      },
      {
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?api_key=${apiKey}`);",
        "lang": "TypeScript",
      },
      {
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/stream/models?api_key=${apiKey}`);",
        "lang": "TypeScript",
      },
      {
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/stream/models?api_key=${apiKey}`);",
        "lang": "TypeScript",
      },
    ],
    "content": "To use API key authentication on REST API endpoints, include the API key in the X-Hume-Api-Key request header.






For WebSocket endpoints, include the API key as a query parameter in the URL.",
    "domain": "test.com",
    "hash": "#api-key-authentication",
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys",
      },
      "h2": {
        "id": "authentication-strategies",
        "title": "Authentication strategies",
      },
      "h3": {
        "id": "api-key-authentication",
        "title": "API key authentication",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.api-key-api-key-authentication-0",
    "org_id": "test",
    "pathname": "/docs/introduction/api-key",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "API key authentication",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/docs/introduction/api-key",
    "code_snippets": [
      {
        "code": "# Assumes `HUME_API_KEY` and `HUME_SECRET_KEY` are defined as environment variables
response=$(curl -s 'https://api.hume.ai/oauth2-cc/token' \
  -u "${HUME_API_KEY}:${HUME_SECRET_KEY}" \
  -d 'grant_type=client_credentials')

# Uses `jq` to extract the access token from the JSON response body
accessToken=$(echo $response | jq -r '.access_token')",
        "lang": "sh",
      },
      {
        "code": "# Assumes `HUME_API_KEY` and `HUME_SECRET_KEY` are defined as environment variables
response=$(curl -s 'https://api.hume.ai/oauth2-cc/token' \
  -u "${HUME_API_KEY}:${HUME_SECRET_KEY}" \
  -d 'grant_type=client_credentials')

# Uses `jq` to extract the access token from the JSON response body
accessToken=$(echo $response | jq -r '.access_token')",
        "lang": "sh",
      },
      {
        "code": "import {fetchAccessToken} from 'hume';

// Reads `HUME_API_KEY` and `HUME_SECRET_KEY` from environment variables
const HUME_API_KEY = process.env.HUME_API_KEY;
const HUME_SECRET_KEY = process.env.HUME_SECRET_KEY;

const accessToken = await fetchAccessToken({
  apiKey: HUME_API_KEY,
  secretKey: HUME_SECRET_KEY
});",
        "lang": "typescript",
      },
      {
        "code": "import {fetchAccessToken} from 'hume';

// Reads `HUME_API_KEY` and `HUME_SECRET_KEY` from environment variables
const HUME_API_KEY = process.env.HUME_API_KEY;
const HUME_SECRET_KEY = process.env.HUME_SECRET_KEY;

const accessToken = await fetchAccessToken({
  apiKey: HUME_API_KEY,
  secretKey: HUME_SECRET_KEY
});",
        "lang": "typescript",
      },
      {
        "code": "import os
import httpx
import base64

# Reads `HUME_API_KEY` and `HUME_SECRET_KEY` from environment variables
HUME_API_KEY = os.getenv('HUME_API_KEY')
HUME_SECRET_KEY = os.getenv('HUME_SECRET_KEY');

auth = f"{HUME_API_KEY}:{HUME_SECRET_KEY}"
encoded_auth = base64.b64encode(auth.encode()).decode()
resp = httpx.request(
    method="POST",
    url="https://api.hume.ai/oauth2-cc/token",
    headers={"Authorization": f"Basic {encoded_auth}"},
    data={"grant_type": "client_credentials"},
)

access_token = resp.json()['access_token']",
        "lang": "python",
      },
      {
        "code": "import os
import httpx
import base64

# Reads `HUME_API_KEY` and `HUME_SECRET_KEY` from environment variables
HUME_API_KEY = os.getenv('HUME_API_KEY')
HUME_SECRET_KEY = os.getenv('HUME_SECRET_KEY');

auth = f"{HUME_API_KEY}:{HUME_SECRET_KEY}"
encoded_auth = base64.b64encode(auth.encode()).decode()
resp = httpx.request(
    method="POST",
    url="https://api.hume.ai/oauth2-cc/token",
    headers={"Authorization": f"Basic {encoded_auth}"},
    data={"grant_type": "client_credentials"},
)

access_token = resp.json()['access_token']",
        "lang": "python",
      },
      {
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?access_token=${accessToken}`);",
        "lang": "typescript",
      },
      {
        "code": "const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?access_token=${accessToken}`);",
        "lang": "typescript",
      },
      {
        "code": "fetch('https://api.hume.ai/v0/evi/chats', {
  headers: {
    Authorization: `Bearer ${accessToken}`,
  },
});",
        "lang": "typescript",
      },
      {
        "code": "fetch('https://api.hume.ai/v0/evi/chats', {
  headers: {
    Authorization: `Bearer ${accessToken}`,
  },
});",
        "lang": "typescript",
      },
    ],
    "content": "To use Token authentication you must first obtain an Access Token from the POST /oauth2-cc/token endpoint.
This is a unique endpoint that uses the "Basic" authentication scheme, with your API key as the username and the Secret key as the password. This means you must concatenate your API key and Secret key, separated by a colon (:), base64 encode this value, and then put the result in the Authorization header of the request, prefixed with Basic .
You must also supply the grant_type=client_credentials parameter in the request body.








On the client side, open an authenticated websocket by including the access token as a query parameter in the URL.


Or, make a REST request by including the access token in the Authorization header.




The Hume Python and TypeScript SDKs will use the API key authentication strategy if you provide only the API key, but will use the Token authentication strategy if you provide both the API key and Secret key.",
    "domain": "test.com",
    "hash": "#token-authentication",
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys",
      },
      "h2": {
        "id": "authentication-strategies",
        "title": "Authentication strategies",
      },
      "h3": {
        "id": "token-authentication",
        "title": "Token authentication",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.api-key-token-authentication-0",
    "org_id": "test",
    "pathname": "/docs/introduction/api-key",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Token authentication",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/docs/introduction/api-key",
    "content": "API keys can be regenerated by clicking the Regenerate keys button on the API keys page. This permanently invalidates the current keys, requiring you to update any applications using them.",
    "domain": "test.com",
    "hash": "#regenerating-api-keys",
    "hierarchy": {
      "h0": {
        "title": "Getting your API keys",
      },
      "h2": {
        "id": "authentication-strategies",
        "title": "Authentication strategies",
      },
      "h3": {
        "id": "regenerating-api-keys",
        "title": "Regenerating API keys",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.api-key-regenerating-api-keys-0",
    "org_id": "test",
    "pathname": "/docs/introduction/api-key",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Regenerating API keys",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/support",
    "description": "Get help from the team at Hume",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.support",
    "org_id": "test",
    "pathname": "/support",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/support",
    "content": "Join our Discord for answers to any technical questions.",
    "domain": "test.com",
    "hash": "#discord",
    "hierarchy": {
      "h0": {
        "title": "Support",
      },
      "h2": {
        "id": "discord",
        "title": "Discord",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.support-discord-0",
    "org_id": "test",
    "pathname": "/support",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Discord",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/support",
    "content": "Contact legal@hume.ai for legal and data privacy inquires.",
    "domain": "test.com",
    "hash": "#legal",
    "hierarchy": {
      "h0": {
        "title": "Support",
      },
      "h2": {
        "id": "legal",
        "title": "Legal",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.support-legal-0",
    "org_id": "test",
    "pathname": "/support",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Legal",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/support",
    "content": "Email billing@hume.ai for any questions or concerns about billing.",
    "domain": "test.com",
    "hash": "#billing",
    "hierarchy": {
      "h0": {
        "title": "Support",
      },
      "h2": {
        "id": "billing",
        "title": "Billing",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.support-billing-0",
    "org_id": "test",
    "pathname": "/support",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Billing",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/introduction",
        "title": "Introduction",
      },
    ],
    "canonicalPathname": "/support",
    "content": "For all other inquires, see hume.ai/contact.",
    "domain": "test.com",
    "hash": "#contact-us",
    "hierarchy": {
      "h0": {
        "title": "Support",
      },
      "h2": {
        "id": "contact-us",
        "title": "Contact us",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.introduction.support-contact-us-0",
    "org_id": "test",
    "pathname": "/support",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Contact us",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/overview",
    "content": "EVI 2 is now available! See the EVI 2 page to learn more and visit platform.hume.ai to start building with the latest features today.
Hume's Empathic Voice Interface (EVI) is the world’s first emotionally intelligent voice AI. It accepts live audio input and returns both generated audio and transcripts augmented with measures of vocal expression. By processing the tune, rhythm, and timbre of speech, EVI unlocks a variety of new capabilities, like knowing when to speak and generating more empathic language with the right tone of voice. These features enable smoother and more satisfying voice-based interactions between humans and AI, opening new possibilities for personal AI, customer service, accessibility, robotics, immersive gaming, VR experiences, and much more.
We provide a suite of tools to integrate and customize EVI for your application, including a WebSocket API that handles audio and text transport, a REST API, and SDKs for TypeScript and Python to simplify integration into web and Python-based projects. Additionally, we provide open-source examples and a web widget as practical starting points for developers to explore and implement EVI's capabilities within their own projects.",
    "description": "Hume's Empathic Voice Interface (EVI) is the world’s first emotionally intelligent voice AI.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.overview-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Empathic Voice Interface (EVI)",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/overview",
    "content": "The main way to work with EVI is through a WebSocket connection that sends audio and receives responses in real-time. This enables fluid, bidirectional dialogue where users speak, EVI listens and analyzes their expressions, and EVI generates emotionally intelligent responses.
EVI supports two authentication strategies. Learn more about them at the links below:
API key authentication

Token authentication




Both methods require specifying the chosen authentication strategy and providing the corresponding key in the request parameters of the EVI WebSocket endpoint.
Learn more about Hume's authentication strategies here.
You start a conversation by connecting to the WebSocket and streaming the user’s voice input to EVI. You can also send EVI text, and it will speak that text aloud.
EVI will respond with:
The text of EVI’s reply

EVI’s expressive audio response

A transcript of the user's message along with their vocal expression measures

Messages if the user interrupts EVI

A message to let you know if EVI has finished responding

Error messages if issues arise",
    "domain": "test.com",
    "hash": "#building-with-evi",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)",
      },
      "h2": {
        "id": "building-with-evi",
        "title": "Building with EVI",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.overview-building-with-evi-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Building with EVI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/overview",
    "content": "Basic capabilities

Transcribes speech (ASR)

Fast and accurate ASR in partnership with Deepgram returns a full transcript of the conversation, with Hume’s
expression measures tied to each sentence.




Generates language responses (LLM)

Rapid language generation with our eLLM, blended seamlessly with configurable partner APIs (OpenAI, Anthropic,
Fireworks).




Generates voice responses (TTS)

Streaming speech generation via our proprietary expressive text-to-speech model.



Responds with low latency

Immediate response provided by the fastest models running together on one service.



 Empathic AI (eLLM) features

Responds at the right time

Uses your tone of voice for state-of-the-art end-of-turn detection — the true bottleneck to responding rapidly
without interrupting you.




Understands users’ prosody

Provides streaming measurements of the tune, rhythm, and timbre of the user’s speech using Hume’s


prosody model, integrated with our eLLM.




Forms its own natural tone of voice

Guided by the users’ prosody and language, our model responds with an empathic, naturalistic tone of voice,
matching the users’ nuanced “vibe” (calmness, interest, excitement, etc.). It responds to frustration with an
apologetic tone, to sadness with sympathy, and more.




Responds to expression

Powered by our empathic large language model (eLLM), EVI crafts responses that are not just intelligent but
attuned to what the user is expressing with their voice.




Always interruptible

Stops rapidly whenever users interject, listens, and responds with the right context based on where it left off.




Aligned with well-being

Trained on human reactions to optimize for positive expressions like happiness and satisfaction. EVI will
continue to learn from users’ reactions using our upcoming fine-tuning endpoint.




 Developer tools

WebSocket API

Primary interface for real-time bidirectional interaction with EVI, handles audio and text transport.



REST API 

A configuration API that allows developers to customize their EVI - the system prompt, speaking rate, voice,
LLM, tools the EVI can use, and other options. The system prompt shapes an EVI’s behavior and its responses.




TypeScript SDK

Encapsulates complexities of audio and WebSockets for seamless integration into web applications.



Python SDK

Simplifies the process of integrating EVI into any Python-based project.



Open source examples

Example repositories provide a starting point for developers and demonstrate EVI's capabilities.



Web widget 

An iframe widget that any developer can easily embed in their website, allowing users to speak to a
conversational AI voice about your content.",
    "domain": "test.com",
    "hash": "#overview-of-evi-features",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)",
      },
      "h2": {
        "id": "overview-of-evi-features",
        "title": "Overview of EVI features",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.overview-overview-of-evi-features-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Overview of EVI features",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/overview",
    "content": "WebSocket connections limit: limited to up to five (5) concurrent connections.

WebSocket duration limit: connections are subject to a default timeout after thirty (30) minutes, or after ten (10) minutes of user inactivity. Duration limits may be adjusted by specifying the max_duration and inactivity fields in your EVI configuration.

WebSocket message payload size limit: messages cannot exceed 16MB in size.

Request rate limit: HTTP requests (e.g. configs endpoints) are limited to fifty (50) requests per second.




To request an increase in your concurrent connection limit, please submit the "Application to Increase EVI Concurrent Connections" found in the EVI section of the Profile Tab.",
    "domain": "test.com",
    "hash": "#api-limits",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface (EVI)",
      },
      "h2": {
        "id": "api-limits",
        "title": "API limits",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.overview-api-limits-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "API limits",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/docs/empathic-voice-interface-evi/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/quickstart/typescript",
    "code_snippets": [
      {
        "code": "// ./app/page.tsx
import ClientComponent from "@/components/ClientComponent";
import { fetchAccessToken } from "hume";

export default async function Page() {
  const accessToken = await fetchAccessToken({
    apiKey: String(process.env.HUME_API_KEY),
    secretKey: String(process.env.HUME_SECRET_KEY),
  });

  if (!accessToken) {
    throw new Error();
  }

  return <ClientComponent accessToken={accessToken} />;
}",
        "lang": "tsx",
      },
      {
        "code": "// ./app/page.tsx
import ClientComponent from "@/components/ClientComponent";
import { fetchAccessToken } from "hume";

export default async function Page() {
  const accessToken = await fetchAccessToken({
    apiKey: String(process.env.HUME_API_KEY),
    secretKey: String(process.env.HUME_SECRET_KEY),
  });

  if (!accessToken) {
    throw new Error();
  }

  return <ClientComponent accessToken={accessToken} />;
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/ClientComponent.tsx
"use client";
import { VoiceProvider } from "@humeai/voice-react";
import Messages from "./Messages";
import Controls from "./Controls";

export default function ClientComponent({
  accessToken,
}: {
  accessToken: string;
}) {
  return (
    <VoiceProvider auth={{ type: "accessToken", value: accessToken }}>
      <Messages />
      <Controls />
    </VoiceProvider>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/ClientComponent.tsx
"use client";
import { VoiceProvider } from "@humeai/voice-react";
import Messages from "./Messages";
import Controls from "./Controls";

export default function ClientComponent({
  accessToken,
}: {
  accessToken: string;
}) {
  return (
    <VoiceProvider auth={{ type: "accessToken", value: accessToken }}>
      <Messages />
      <Controls />
    </VoiceProvider>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/Controls.tsx
"use client";
import { useVoice, VoiceReadyState } from "@humeai/voice-react";
export default function Controls() {
  const { connect, disconnect, readyState } = useVoice();

  if (readyState === VoiceReadyState.OPEN) {
    return (
      <button
        onClick={() => {
          disconnect();
        }}
      >
        End Session
      </button>
    );
  }

  return (
    <button
      onClick={() => {
        connect()
          .then(() => {
            /* handle success */
          })
          .catch(() => {
            /* handle error */
          });
      }}
    >
      Start Session
    </button>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/Controls.tsx
"use client";
import { useVoice, VoiceReadyState } from "@humeai/voice-react";
export default function Controls() {
  const { connect, disconnect, readyState } = useVoice();

  if (readyState === VoiceReadyState.OPEN) {
    return (
      <button
        onClick={() => {
          disconnect();
        }}
      >
        End Session
      </button>
    );
  }

  return (
    <button
      onClick={() => {
        connect()
          .then(() => {
            /* handle success */
          })
          .catch(() => {
            /* handle error */
          });
      }}
    >
      Start Session
    </button>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/Messages.tsx
"use client";
import { useVoice } from "@humeai/voice-react";

export default function Messages() {
  const { messages } = useVoice();

  return (
    <div>
      {messages.map((msg, index) => {
        if (msg.type === "user_message" || msg.type === "assistant_message") {
          return (
            <div key={msg.type + index}>
              <div>{msg.message.role}</div>
              <div>{msg.message.content}</div>
            </div>
          );
        }

        return null;
      })}
    </div>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/Messages.tsx
"use client";
import { useVoice } from "@humeai/voice-react";

export default function Messages() {
  const { messages } = useVoice();

  return (
    <div>
      {messages.map((msg, index) => {
        if (msg.type === "user_message" || msg.type === "assistant_message") {
          return (
            <div key={msg.type + index}>
              <div>{msg.message.role}</div>
              <div>{msg.message.content}</div>
            </div>
          );
        }

        return null;
      })}
    </div>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./pages/index.tsx
import Controls from "@/components/Controls";
import Messages from "@/components/Messages";
import { fetchAccessToken } from "hume";
import { VoiceProvider } from "@humeai/voice-react";
import { InferGetServerSidePropsType } from "next";

export const getServerSideProps = async () => {
  const accessToken = await fetchAccessToken({
    apiKey: String(process.env.HUME_API_KEY),
    secretKey: String(process.env.HUME_SECRET_KEY),
  });

  if (!accessToken) {
    return {
      redirect: {
        destination: "/error",
        permanent: false,
      },
    };
  }

  return {
    props: {
      accessToken,
    },
  };
};

type PageProps = InferGetServerSidePropsType<typeof getServerSideProps>;

export default function Page({ accessToken }: PageProps) {
  return (
    <VoiceProvider auth={{ type: "accessToken", value: accessToken }}>
      <Messages />
      <Controls />
    </VoiceProvider>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./pages/index.tsx
import Controls from "@/components/Controls";
import Messages from "@/components/Messages";
import { fetchAccessToken } from "hume";
import { VoiceProvider } from "@humeai/voice-react";
import { InferGetServerSidePropsType } from "next";

export const getServerSideProps = async () => {
  const accessToken = await fetchAccessToken({
    apiKey: String(process.env.HUME_API_KEY),
    secretKey: String(process.env.HUME_SECRET_KEY),
  });

  if (!accessToken) {
    return {
      redirect: {
        destination: "/error",
        permanent: false,
      },
    };
  }

  return {
    props: {
      accessToken,
    },
  };
};

type PageProps = InferGetServerSidePropsType<typeof getServerSideProps>;

export default function Page({ accessToken }: PageProps) {
  return (
    <VoiceProvider auth={{ type: "accessToken", value: accessToken }}>
      <Messages />
      <Controls />
    </VoiceProvider>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/Controls.tsx
import { useVoice, VoiceReadyState } from "@humeai/voice-react";
export default function Controls() {
  const { connect, disconnect, readyState } = useVoice();

  if (readyState === VoiceReadyState.OPEN) {
    return (
      <button
        onClick={() => {
          disconnect();
        }}
      >
        End Session
      </button>
    );
  }

  return (
    <button
      onClick={() => {
        connect()
          .then(() => {
            /* handle success */
          })
          .catch(() => {
            /* handle error */
          });
      }}
    >
      Start Session
    </button>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/Controls.tsx
import { useVoice, VoiceReadyState } from "@humeai/voice-react";
export default function Controls() {
  const { connect, disconnect, readyState } = useVoice();

  if (readyState === VoiceReadyState.OPEN) {
    return (
      <button
        onClick={() => {
          disconnect();
        }}
      >
        End Session
      </button>
    );
  }

  return (
    <button
      onClick={() => {
        connect()
          .then(() => {
            /* handle success */
          })
          .catch(() => {
            /* handle error */
          });
      }}
    >
      Start Session
    </button>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/Messages.tsx
import { useVoice } from "@humeai/voice-react";

export default function Messages() {
  const { messages } = useVoice();

  return (
    <div>
      {messages.map((msg, index) => {
        if (msg.type === "user_message" || msg.type === "assistant_message") {
          return (
            <div key={msg.type + index}>
              <div>{msg.message.role}</div>
              <div>{msg.message.content}</div>
            </div>
          );
        }

        return null;
      })}
    </div>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "// ./components/Messages.tsx
import { useVoice } from "@humeai/voice-react";

export default function Messages() {
  const { messages } = useVoice();

  return (
    <div>
      {messages.map((msg, index) => {
        if (msg.type === "user_message" || msg.type === "assistant_message") {
          return (
            <div key={msg.type + index}>
              <div>{msg.message.role}</div>
              <div>{msg.message.content}</div>
            </div>
          );
        }

        return null;
      })}
    </div>
  );
}",
        "lang": "tsx",
      },
      {
        "code": "import { Hume, HumeClient } from 'hume';

// instantiate the Hume client and authenticate
const client = new HumeClient({
  apiKey: import.meta.env.HUME_API_KEY || '',
  secretKey: import.meta.env.HUME_SECRET_KEY || '',
});
",
        "lang": "typescript",
      },
      {
        "code": "import { Hume, HumeClient } from 'hume';

// instantiate the Hume client and authenticate
const client = new HumeClient({
  apiKey: import.meta.env.HUME_API_KEY || '',
  secretKey: import.meta.env.HUME_SECRET_KEY || '',
});
",
        "lang": "typescript",
      },
      {
        "code": "import { Hume, HumeClient } from 'hume';

// instantiate the Hume client and authenticate
const client = new HumeClient({
  apiKey: import.meta.env.HUME_API_KEY || '',
  secretKey: import.meta.env.HUME_SECRET_KEY || '',
});

// instantiates WebSocket and establishes an authenticated connection
const socket = await client.empathicVoice.chat.connect({
  configId: import.meta.env.HUME_CONFIG_ID || null,
});

// define handler functions and assign them to the corresponding WebSocket event handlers
socket.on('open', handleWebSocketOpenEvent);
socket.on('message', handleWebSocketMessageEvent);
socket.on('error', handleWebSocketErrorEvent);
socket.on('close', handleWebSocketCloseEvent);",
        "lang": "typescript",
      },
      {
        "code": "import { Hume, HumeClient } from 'hume';

// instantiate the Hume client and authenticate
const client = new HumeClient({
  apiKey: import.meta.env.HUME_API_KEY || '',
  secretKey: import.meta.env.HUME_SECRET_KEY || '',
});

// instantiates WebSocket and establishes an authenticated connection
const socket = await client.empathicVoice.chat.connect({
  configId: import.meta.env.HUME_CONFIG_ID || null,
});

// define handler functions and assign them to the corresponding WebSocket event handlers
socket.on('open', handleWebSocketOpenEvent);
socket.on('message', handleWebSocketMessageEvent);
socket.on('error', handleWebSocketErrorEvent);
socket.on('close', handleWebSocketCloseEvent);",
        "lang": "typescript",
      },
      {
        "code": "import {
  convertBlobToBase64,
  ensureSingleValidAudioTrack,
  getAudioStream,
  getBrowserSupportedMimeType,
} from 'hume';

// the recorder responsible for recording the audio stream to be prepared as the audio input
let recorder: MediaRecorder | null = null;

// the stream of audio captured from the user's microphone
let audioStream: MediaStream | null = null;

// mime type supported by the browser the application is running in
const mimeType: MimeType = (() => {
  const result = getBrowserSupportedMimeType();
  return result.success ? result.mimeType : MimeType.WEBM;
})();

// define function for capturing audio
async function captureAudio(): Promise<void> {
  // prompts user for permission to capture audio, obtains media stream upon approval
  audioStream = await getAudioStream();

  // ensure there is only one audio track in the stream
  ensureSingleValidAudioTrack(audioStream);

  // instantiate the media recorder
  recorder = new MediaRecorder(audioStream, { mimeType });

  // callback for when recorded chunk is available to be processed
  recorder.ondataavailable = async ({ data }) => {
    // IF size of data is smaller than 1 byte then do nothing
    if (data.size < 1) return;

    // base64 encode audio data
    const encodedAudioData = await convertBlobToBase64(data);

    // define the audio_input message JSON
    const audioInput: Omit<Hume.empathicVoice.AudioInput, 'type'> = {
      data: encodedAudioData,
    };

    // send audio_input message
    socket?.sendAudioInput(audioInput);
  };

  // capture audio input at a rate of 100ms (recommended for web)
  const timeSlice = 100;
  recorder.start(timeSlice);
}

// define a WebSocket open event handler to capture audio
async function handleWebSocketOpenEvent(): Promise<void> {
  // place logic here which you would like invoked when the socket opens
  console.log('Web socket connection opened');
  await captureAudio();
}
",
        "lang": "typescript",
      },
      {
        "code": "import {
  convertBlobToBase64,
  ensureSingleValidAudioTrack,
  getAudioStream,
  getBrowserSupportedMimeType,
} from 'hume';

// the recorder responsible for recording the audio stream to be prepared as the audio input
let recorder: MediaRecorder | null = null;

// the stream of audio captured from the user's microphone
let audioStream: MediaStream | null = null;

// mime type supported by the browser the application is running in
const mimeType: MimeType = (() => {
  const result = getBrowserSupportedMimeType();
  return result.success ? result.mimeType : MimeType.WEBM;
})();

// define function for capturing audio
async function captureAudio(): Promise<void> {
  // prompts user for permission to capture audio, obtains media stream upon approval
  audioStream = await getAudioStream();

  // ensure there is only one audio track in the stream
  ensureSingleValidAudioTrack(audioStream);

  // instantiate the media recorder
  recorder = new MediaRecorder(audioStream, { mimeType });

  // callback for when recorded chunk is available to be processed
  recorder.ondataavailable = async ({ data }) => {
    // IF size of data is smaller than 1 byte then do nothing
    if (data.size < 1) return;

    // base64 encode audio data
    const encodedAudioData = await convertBlobToBase64(data);

    // define the audio_input message JSON
    const audioInput: Omit<Hume.empathicVoice.AudioInput, 'type'> = {
      data: encodedAudioData,
    };

    // send audio_input message
    socket?.sendAudioInput(audioInput);
  };

  // capture audio input at a rate of 100ms (recommended for web)
  const timeSlice = 100;
  recorder.start(timeSlice);
}

// define a WebSocket open event handler to capture audio
async function handleWebSocketOpenEvent(): Promise<void> {
  // place logic here which you would like invoked when the socket opens
  console.log('Web socket connection opened');
  await captureAudio();
}
",
        "lang": "typescript",
      },
      {
        "code": "import {
  convertBase64ToBlob,
  getBrowserSupportedMimeType
} from 'hume';

// audio playback queue
const audioQueue: Blob[] = [];

// flag which denotes whether audio is currently playing or not
let isPlaying = false;

// the current audio element to be played
let currentAudio: : HTMLAudioElement | null = null;

// mime type supported by the browser the application is running in
const mimeType: MimeType = (() => {
  const result = getBrowserSupportedMimeType();
  return result.success ? result.mimeType : MimeType.WEBM;
})();

// play the audio within the playback queue, converting each Blob into playable HTMLAudioElements
function playAudio(): void {
  // IF there is nothing in the audioQueue OR audio is currently playing then do nothing
  if (!audioQueue.length || isPlaying) return;

  // update isPlaying state
  isPlaying = true;

  // pull next audio output from the queue
  const audioBlob = audioQueue.shift();

  // IF audioBlob is unexpectedly undefined then do nothing
  if (!audioBlob) return;

  // converts Blob to AudioElement for playback
  const audioUrl = URL.createObjectURL(audioBlob);
  currentAudio = new Audio(audioUrl);

  // play audio
  currentAudio.play();

  // callback for when audio finishes playing
  currentAudio.onended = () => {
    // update isPlaying state
    isPlaying = false;

    // attempt to pull next audio output from queue
    if (audioQueue.length) playAudio();
  };
}

// define a WebSocket message event handler to play audio output
function handleWebSocketMessageEvent(
  message: Hume.empathicVoice.SubscribeEvent
): void {
  // place logic here which you would like to invoke when receiving a message through the socket
  switch (message.type) {
    // add received audio to the playback queue, and play next audio output
    case 'audio_output':
      // convert base64 encoded audio to a Blob
      const audioOutput = message.data;
      const blob = convertBase64ToBlob(audioOutput, mimeType);

      // add audio Blob to audioQueue
      audioQueue.push(blob);

      // play the next audio output
      if (audioQueue.length === 1) playAudio();
      break;
  }
}",
        "lang": "typescript",
      },
      {
        "code": "import {
  convertBase64ToBlob,
  getBrowserSupportedMimeType
} from 'hume';

// audio playback queue
const audioQueue: Blob[] = [];

// flag which denotes whether audio is currently playing or not
let isPlaying = false;

// the current audio element to be played
let currentAudio: : HTMLAudioElement | null = null;

// mime type supported by the browser the application is running in
const mimeType: MimeType = (() => {
  const result = getBrowserSupportedMimeType();
  return result.success ? result.mimeType : MimeType.WEBM;
})();

// play the audio within the playback queue, converting each Blob into playable HTMLAudioElements
function playAudio(): void {
  // IF there is nothing in the audioQueue OR audio is currently playing then do nothing
  if (!audioQueue.length || isPlaying) return;

  // update isPlaying state
  isPlaying = true;

  // pull next audio output from the queue
  const audioBlob = audioQueue.shift();

  // IF audioBlob is unexpectedly undefined then do nothing
  if (!audioBlob) return;

  // converts Blob to AudioElement for playback
  const audioUrl = URL.createObjectURL(audioBlob);
  currentAudio = new Audio(audioUrl);

  // play audio
  currentAudio.play();

  // callback for when audio finishes playing
  currentAudio.onended = () => {
    // update isPlaying state
    isPlaying = false;

    // attempt to pull next audio output from queue
    if (audioQueue.length) playAudio();
  };
}

// define a WebSocket message event handler to play audio output
function handleWebSocketMessageEvent(
  message: Hume.empathicVoice.SubscribeEvent
): void {
  // place logic here which you would like to invoke when receiving a message through the socket
  switch (message.type) {
    // add received audio to the playback queue, and play next audio output
    case 'audio_output':
      // convert base64 encoded audio to a Blob
      const audioOutput = message.data;
      const blob = convertBase64ToBlob(audioOutput, mimeType);

      // add audio Blob to audioQueue
      audioQueue.push(blob);

      // play the next audio output
      if (audioQueue.length === 1) playAudio();
      break;
  }
}",
        "lang": "typescript",
      },
      {
        "code": "// function for stopping the audio and clearing the queue
function stopAudio(): void {
  // stop the audio playback
  currentAudio?.pause();
  currentAudio = null;

  // update audio playback state
  isPlaying = false;

  // clear the audioQueue
  audioQueue.length = 0;
}

// update WebSocket message event handler to handle interruption
function handleWebSocketMessageEvent(
  message: Hume.empathicVoice.SubscribeEvent
): void {
  // place logic here which you would like to invoke when receiving a message through the socket
  switch (message.type) {
    // add received audio to the playback queue, and play next audio output
    case 'audio_output':
      // convert base64 encoded audio to a Blob
      const audioOutput = message.data;
      const blob = convertBase64ToBlob(audioOutput, mimeType);

      // add audio Blob to audioQueue
      audioQueue.push(blob);

      // play the next audio output
      if (audioQueue.length === 1) playAudio();
      break;

    // stop audio playback, clear audio playback queue, and update audio playback state on interrupt
    case 'user_interruption':
      stopAudio();
      break;
  }
}
",
        "lang": "typescript",
      },
      {
        "code": "// function for stopping the audio and clearing the queue
function stopAudio(): void {
  // stop the audio playback
  currentAudio?.pause();
  currentAudio = null;

  // update audio playback state
  isPlaying = false;

  // clear the audioQueue
  audioQueue.length = 0;
}

// update WebSocket message event handler to handle interruption
function handleWebSocketMessageEvent(
  message: Hume.empathicVoice.SubscribeEvent
): void {
  // place logic here which you would like to invoke when receiving a message through the socket
  switch (message.type) {
    // add received audio to the playback queue, and play next audio output
    case 'audio_output':
      // convert base64 encoded audio to a Blob
      const audioOutput = message.data;
      const blob = convertBase64ToBlob(audioOutput, mimeType);

      // add audio Blob to audioQueue
      audioQueue.push(blob);

      // play the next audio output
      if (audioQueue.length === 1) playAudio();
      break;

    // stop audio playback, clear audio playback queue, and update audio playback state on interrupt
    case 'user_interruption':
      stopAudio();
      break;
  }
}
",
        "lang": "typescript",
      },
    ],
    "content": "This guide provides instructions for integrating EVI into your TypeScript projects. It includes detailed steps for using EVI with Next.js (App Router),
Next.js (Pages Router), and a standalone setup without any framework.


Kickstart your project with our pre-configured Vercel template for the Empathic Voice
Interface. Install
with one click to instantly set up a ready-to-use project and start building with
TypeScript right away!




This tutorial utilizes Hume’s React SDK to interact with EVI. It includes detailed steps for both the
App Router in Next.js and is broken down into four key components:
Authentication: Generate and use an access token to authenticate with EVI.

Setting up context provider: Set up the <VoiceProvider/>.

Starting a chat and display messages: Implement the functionality to start a chat with EVI and display messages.

That's it!: Audio playback and interruptions are handled for you.




The Hume React SDK abstracts much of the logic for managing the WebSocket connection, as
well as capturing and preparing audio for processing. For a closer look at how the React
package manages these aspects of the integration, we invite you to explore the source code
here: @humeai/voice-react
To see this code fully implemented within a frontend web application using the App Router from Next.js, visit this GitHub repository:
evi-nextjs-app-router.


Prerequisites
Before you begin, you will need to have an existing Next.js project set up using the App Router.
Authenticate
In order to make an authenticated connection we will first need to generate an access token. Doing so will
require your API key and Secret key. These keys can be obtained by logging into the portal and visiting the
API keys page.


In the sample code below, the API key and Secret key have been saved to
environment variables. Avoid hard coding these values in your project to
prevent them from being leaked.


Setup Context Provider
After fetching our access token we can pass it to our ClientComponent. First we set up the <VoiceProvider/> so that our Messages and Controls components can access the context. We also pass the access token to the auth prop of the <VoiceProvider/> for setting up the WebSocket connection.


Audio input
<VoiceProvider/> will handle the microphone and playback logic.
Starting session
In order to start a session, you can use the connect function. It is important that this event is attached to a user interaction event (like a click) so that the browser is capable of playing Audio.


Displaying message history
To display the message history, we can use the useVoice hook to access the messages array. We can then map over the messages array to display the role (Assistant or User) and content of each message.


Interrupt
This Next.js example will handle interruption events automatically!


This tutorial utilizes Hume’s React SDK to interact with EVI. It includes detailed steps for both the
Pages Router in Next.js and is broken down into four key components:
Authentication: Generate and use an access token to authenticate with EVI.

Setting up context provider: Set up the <VoiceProvider/>.

Starting a chat and display messages: Implement the functionality to start a chat with EVI and display messages.

That's it!: Audio playback and interruptions are handled for you.




The Hume React SDK abstracts much of the logic for managing the WebSocket connection, as
well as capturing and preparing audio for processing. For a closer look at how the React
package manages these aspects of the integration, we invite you to explore the source code
here: @humeai/voice-react
To see this code fully implemented within a frontend web application using the Pages Router from Next.js, visit this GitHub repository: evi-nextjs-pages-router.


Prerequisites
Before you begin, you will need to have an existing Next.js project set up using the Pages Router.
Authenticate and Setup Context Provider
In order to make an authenticated connection we will first need to generate an access token. Doing so will
require your API key and Secret key. These keys can be obtained by logging into the portal and visiting the
API keys page.


In the sample code below, the API key and Secret key have been saved to
environment variables. Avoid hard coding these values in your project to
prevent them from being leaked.


Audio input
<VoiceProvider/> is designed to manage microphone inputs and audio playback. It abstracts the complexities of audio processing to allow developers to focus on developing interactive voice-driven functionalities.
For a closer look at how <VoiceProvider/> processes audio inputs and controls playback, you can view the source code here.
Starting session
In order to start a session, you can use the connect function. It is important that this event is attached to a user interaction event (like a click) so that the browser is capable of playing Audio.


Displaying message history
To display the message history, we can use the useVoice hook to access the messages array. We can then map over the messages array to display the role (Assistant or User) and content of each message.


Interrupt
This Next.js example will handle interruption events automatically!


This tutorial provides step-by-step instructions for implementing EVI using Hume’s
TypeScript SDK. This guide is divided into five key components:
Authentication: Authenticate your application with EVI using your credentials.

Connecting to EVI: Set up a secure WebSocket connection to interact with EVI.

Capturing & recording audio: Capture audio input and prepare it for processing.

Audio playback: Play back the processed audio output to the user.

Interruption: Manage and handle interruptions during the chat.


To see the full implementation within a frontend web application, visit our API examples repository on GitHub: hume-evi-typescript-example.


Authenticate
In order to establish an authenticated connection we will first need to instantiate the Hume client with our API key and Secret key.
These keys can be obtained by logging into the portal and visiting the API keys page.


In the sample code below, the API key and Secret key have been saved to environment
variables. Avoid hard coding these values in your project to prevent them from being
leaked.


When using our TypeScript SDK, the Access Token necessary to establish an authenticated connection with EVI is fetched and applied under the hood
after the Hume client is instantiated with your credientials.
Connect
With the Hume client instantiated with our credentials, we can now establish an authenticated WebSocket connection with EVI and define our WebSocket event handlers.
For now we will include placeholder event handlers to be updated in later steps.


Audio input
To capture audio and send it through the socket as an audio input, several steps are necessary. First, we need to handle user permissions
to access the microphone. Next, we'll use the Media Stream API to capture the audio, and the MediaRecorder API to record the captured audio.
We then base64 encode the recording audio Blob, and finally send the encoded audio through the WebSocket using the sendAudioInputmethod.




Accepted audio formats include: mp3, wav, aac, ogg, flac, webm, avr, cdda,
cvs/vms, aiff, au, amr, mp2, mp4, ac3, avi, wmv, mpeg, ircam.
Audio output
The response will comprise multiple messages, detailed as follows:
user_message: This message encapsulates the transcription of the audio input. Additionally, it
includes expression measurement predictions related to the speaker's vocal prosody.

assistant_message: For every sentence within the response, an AssistantMessage is dispatched.
This message not only relays the content of the response but also features predictions regarding the
expressive qualities of the generated audio response.

audio_output: Accompanying each AssistantMessage, an AudioOutput message will be provided.
This contains the actual audio (binary) response corresponding to an AssistantMessage.

assistant_end: Signifying the conclusion of the response to the audio input, an AssistantEnd
message is delivered as the final piece of the communication.


Here we'll focus on playing the received audio output. To play the audio output from the response we
need to define our logic for converting the received binary to a Blob, and creating an HTMLAudioInput
to play the audio.
We then need to update the client's on message WebSocket event handler to invoke
the logic to playback the audio when receiving the audio output. To manage playback for the incoming
audio here we'll implement a queue and sequentially play the audio back.


Interrupt
Interruptibility is a distinguishing feature of the Empathic Voice Interface. If an audio input is sent
through the WebSocket while receiving response messages for a previous audio input, the response to
the previous audio input will stop being sent. Additionally the interface will send back a
user_interruption message, and begin responding to the new audio input.",
    "description": "A quickstart guide for implementing the Empathic Voice Interface (EVI) with TypeScript.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.quickstart.typescript-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/quickstart/typescript",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "EVI TypeScript Quickstart Guide",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/docs/empathic-voice-interface-evi/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/quickstart/python",
    "code_snippets": [
      {
        "code": "python -m venv evi-env",
        "lang": "bash",
      },
      {
        "code": "python -m venv evi-env",
        "lang": "bash",
      },
      {
        "code": "source evi-env/bin/activate",
        "lang": "bash",
      },
      {
        "code": "source evi-env/bin/activate",
        "lang": "bash",
      },
      {
        "code": "conda create --name evi-env python=3.11",
        "lang": "bash",
      },
      {
        "code": "conda create --name evi-env python=3.11",
        "lang": "bash",
      },
      {
        "code": "conda activate evi-env",
        "lang": "bash",
      },
      {
        "code": "conda activate evi-env",
        "lang": "bash",
      },
      {
        "code": "pip install "hume[microphone]"",
        "lang": "bash",
      },
      {
        "code": "pip install "hume[microphone]"",
        "lang": "bash",
      },
      {
        "code": "pip install python-dotenv",
        "lang": "bash",
      },
      {
        "code": "pip install python-dotenv",
        "lang": "bash",
      },
      {
        "code": "brew install ffmpeg",
        "lang": "bash",
      },
      {
        "code": "brew install ffmpeg",
        "lang": "bash",
      },
      {
        "code": "sudo apt-get --yes update
sudo apt-get --yes install libasound2-dev libportaudio2 ffmpeg",
        "lang": "bash",
      },
      {
        "code": "sudo apt-get --yes update
sudo apt-get --yes install libasound2-dev libportaudio2 ffmpeg",
        "lang": "bash",
      },
      {
        "code": "import asyncio
import base64
import datetime
import os
from dotenv import load_dotenv
from hume.client import AsyncHumeClient
from hume.empathic_voice.chat.socket_client import ChatConnectOptions, ChatWebsocketConnection
from hume.empathic_voice.chat.types import SubscribeEvent
from hume.empathic_voice.types import UserInput
from hume.core.api_error import ApiError
from hume import MicrophoneInterface, Stream",
        "lang": "py",
      },
      {
        "code": "import asyncio
import base64
import datetime
import os
from dotenv import load_dotenv
from hume.client import AsyncHumeClient
from hume.empathic_voice.chat.socket_client import ChatConnectOptions, ChatWebsocketConnection
from hume.empathic_voice.chat.types import SubscribeEvent
from hume.empathic_voice.types import UserInput
from hume.core.api_error import ApiError
from hume import MicrophoneInterface, Stream",
        "lang": "py",
      },
      {
        "code": "import asyncio
import base64
from hume.empathic_voice.chat.socket_client import ChatWebsocketConnection
from hume.empathic_voice.chat.types import SubscribeEvent
from hume.core.api_error import ApiError
from hume import Stream

class WebSocketHandler:
  """Interface for containing the EVI WebSocket and associated socket handling behavior."""

  def __init__(self):
    """Construct the WebSocketHandler, initially assigning the socket to None and the byte stream to a new Stream object."""
    self.socket = None
    self.byte_strs = Stream.new()

  def set_socket(self, socket: ChatWebsocketConnection):
    """Set the socket."""
    self.socket = socket

  async def on_open(self):
    """Logic invoked when the WebSocket connection is opened."""
    print("WebSocket connection opened.")

  async def on_message(self, message: SubscribeEvent):
    """Callback function to handle a WebSocket message event.
    
    This asynchronous method decodes the message, determines its type, and 
    handles it accordingly. Depending on the type of message, it 
    might log metadata, handle user or assistant messages, process
    audio data, raise an error if the message type is "error", and more.

    See the full list of "Receive" messages in the API Reference.
    """

    if message.type == "chat_metadata":
      chat_id = message.chat_id
      chat_group_id = message.chat_group_id
      # ...
    elif message.type in ["user_message", "assistant_message"]:
      role = message.message.role.upper()
      message_text = message.message.content
      # ...
    elif message.type == "audio_output":
      message_str: str = message.data
      message_bytes = base64.b64decode(message_str.encode("utf-8"))
      await self.byte_strs.put(message_bytes)
      return
    elif message.type == "error":
      error_message = message.message
      error_code = message.code
      raise ApiError(f"Error ({error_code}): {error_message}")
    
    # Print timestamp and message
    # ...
      
  async def on_close(self):
    """Logic invoked when the WebSocket connection is closed."""
    print("WebSocket connection closed.")

  async def on_error(self, error):
    """Logic invoked when an error occurs in the WebSocket connection."""
    print(f"Error: {error}")",
        "lang": "python",
      },
      {
        "code": "import asyncio
import base64
from hume.empathic_voice.chat.socket_client import ChatWebsocketConnection
from hume.empathic_voice.chat.types import SubscribeEvent
from hume.core.api_error import ApiError
from hume import Stream

class WebSocketHandler:
  """Interface for containing the EVI WebSocket and associated socket handling behavior."""

  def __init__(self):
    """Construct the WebSocketHandler, initially assigning the socket to None and the byte stream to a new Stream object."""
    self.socket = None
    self.byte_strs = Stream.new()

  def set_socket(self, socket: ChatWebsocketConnection):
    """Set the socket."""
    self.socket = socket

  async def on_open(self):
    """Logic invoked when the WebSocket connection is opened."""
    print("WebSocket connection opened.")

  async def on_message(self, message: SubscribeEvent):
    """Callback function to handle a WebSocket message event.
    
    This asynchronous method decodes the message, determines its type, and 
    handles it accordingly. Depending on the type of message, it 
    might log metadata, handle user or assistant messages, process
    audio data, raise an error if the message type is "error", and more.

    See the full list of "Receive" messages in the API Reference.
    """

    if message.type == "chat_metadata":
      chat_id = message.chat_id
      chat_group_id = message.chat_group_id
      # ...
    elif message.type in ["user_message", "assistant_message"]:
      role = message.message.role.upper()
      message_text = message.message.content
      # ...
    elif message.type == "audio_output":
      message_str: str = message.data
      message_bytes = base64.b64decode(message_str.encode("utf-8"))
      await self.byte_strs.put(message_bytes)
      return
    elif message.type == "error":
      error_message = message.message
      error_code = message.code
      raise ApiError(f"Error ({error_code}): {error_message}")
    
    # Print timestamp and message
    # ...
      
  async def on_close(self):
    """Logic invoked when the WebSocket connection is closed."""
    print("WebSocket connection closed.")

  async def on_error(self, error):
    """Logic invoked when an error occurs in the WebSocket connection."""
    print(f"Error: {error}")",
        "lang": "python",
      },
      {
        "code": "async def main() -> None:
  # Retrieve any environment variables stored in the .env file
  load_dotenv()

  # Retrieve the API key, Secret key, and EVI config id from the environment variables
  HUME_API_KEY = os.getenv("HUME_API_KEY")
  HUME_SECRET_KEY = os.getenv("HUME_SECRET_KEY")
  HUME_CONFIG_ID = os.getenv("HUME_CONFIG_ID")

  # Initialize the asynchronous client, authenticating with your API key
  client = AsyncHumeClient(api_key=HUME_API_KEY)

  # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication
  options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)
  
  # ...",
        "lang": "py",
      },
      {
        "code": "async def main() -> None:
  # Retrieve any environment variables stored in the .env file
  load_dotenv()

  # Retrieve the API key, Secret key, and EVI config id from the environment variables
  HUME_API_KEY = os.getenv("HUME_API_KEY")
  HUME_SECRET_KEY = os.getenv("HUME_SECRET_KEY")
  HUME_CONFIG_ID = os.getenv("HUME_CONFIG_ID")

  # Initialize the asynchronous client, authenticating with your API key
  client = AsyncHumeClient(api_key=HUME_API_KEY)

  # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication
  options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)
  
  # ...",
        "lang": "py",
      },
      {
        "code": "async def main() -> None:
  # ...
  # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication
  options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)

  # Instantiate the WebSocketHandler
  websocket_handler = WebSocketHandler()

  # Open the WebSocket connection with the configuration options and the handler's functions
    async with client.empathic_voice.chat.connect_with_callbacks(
      options=options,
      on_open=websocket_handler.on_open,
      on_message=websocket_handler.on_message,
      on_close=websocket_handler.on_close,
      on_error=websocket_handler.on_error
    ) as socket:
    
      # Set the socket instance in the handler
      websocket_handler.set_socket(socket)
      # ...",
        "lang": "py",
        "meta": "{6-16}",
      },
      {
        "code": "async def main() -> None:
  # ...
  # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication
  options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)

  # Instantiate the WebSocketHandler
  websocket_handler = WebSocketHandler()

  # Open the WebSocket connection with the configuration options and the handler's functions
    async with client.empathic_voice.chat.connect_with_callbacks(
      options=options,
      on_open=websocket_handler.on_open,
      on_message=websocket_handler.on_message,
      on_close=websocket_handler.on_close,
      on_error=websocket_handler.on_error
    ) as socket:
    
      # Set the socket instance in the handler
      websocket_handler.set_socket(socket)
      # ...",
        "lang": "py",
        "meta": "{6-16}",
      },
      {
        "code": "async def main() -> None:
  # Open the WebSocket connection with the configuration options and the handler's functions
  async with client.empathic_voice.chat.connect_with_callbacks(...) as socket:
    # Set the socket instance in the handler
    websocket_handler.set_socket(socket)

    # Create an asynchronous task to continuously detect and process input from the microphone, as well as play audio
    microphone_task = asyncio.create_task(
      MicrophoneInterface.start(
        socket,
        byte_stream=websocket_handler.byte_strs
      )
    )
    
    # Await the microphone task
    await microphone_task
",
        "lang": "py",
        "meta": "{7-11}",
      },
      {
        "code": "async def main() -> None:
  # Open the WebSocket connection with the configuration options and the handler's functions
  async with client.empathic_voice.chat.connect_with_callbacks(...) as socket:
    # Set the socket instance in the handler
    websocket_handler.set_socket(socket)

    # Create an asynchronous task to continuously detect and process input from the microphone, as well as play audio
    microphone_task = asyncio.create_task(
      MicrophoneInterface.start(
        socket,
        byte_stream=websocket_handler.byte_strs
      )
    )
    
    # Await the microphone task
    await microphone_task
",
        "lang": "py",
        "meta": "{7-11}",
      },
      {
        "code": "   0 DELL U2720QM, Core Audio (0 in, 2 out)
   1 I, Phone 15 Pro Max Microphone, Core Audio (1 in, 0 out)
>  2 Studio Display Microphone, Core Audio (1 in, 0 out)
   3 Studio Display Speakers, Core Audio (0 in, 8 out)
   4 MacBook Pro Microphone, Core Audio (1 in, 0 out)
<  5 MacBook Pro Speakers, Core Audio (0 in, 2 out)
   6 Pro Tools Audio Bridge 16, Core Audio (16 in, 16 out)
   7 Pro Tools Audio Bridge 2-A, Core Audio (2 in, 2 out)
   8 Pro Tools Audio Bridge 2-B, Core Audio (2 in, 2 out)
   9 Pro Tools Audio Bridge 32, Core Audio (32 in, 32 out)
  10 Pro Tools Audio Bridge 64, Core Audio (64 in, 64 out)
  11 Pro Tools Audio Bridge 6, Core Audio (6 in, 6 out)
  12 Apowersoft Audio Device, Core Audio (2 in, 2 out)
  13 ZoomAudioDevice, Core Audio (2 in, 2 out)",
        "lang": "bash",
      },
      {
        "code": "   0 DELL U2720QM, Core Audio (0 in, 2 out)
   1 I, Phone 15 Pro Max Microphone, Core Audio (1 in, 0 out)
>  2 Studio Display Microphone, Core Audio (1 in, 0 out)
   3 Studio Display Speakers, Core Audio (0 in, 8 out)
   4 MacBook Pro Microphone, Core Audio (1 in, 0 out)
<  5 MacBook Pro Speakers, Core Audio (0 in, 2 out)
   6 Pro Tools Audio Bridge 16, Core Audio (16 in, 16 out)
   7 Pro Tools Audio Bridge 2-A, Core Audio (2 in, 2 out)
   8 Pro Tools Audio Bridge 2-B, Core Audio (2 in, 2 out)
   9 Pro Tools Audio Bridge 32, Core Audio (32 in, 32 out)
  10 Pro Tools Audio Bridge 64, Core Audio (64 in, 64 out)
  11 Pro Tools Audio Bridge 6, Core Audio (6 in, 6 out)
  12 Apowersoft Audio Device, Core Audio (2 in, 2 out)
  13 ZoomAudioDevice, Core Audio (2 in, 2 out)",
        "lang": "bash",
      },
      {
        "code": "# Specify device 4 in MicrophoneInterface
MicrophoneInterface.start(
  socket,
  device=4,
  allow_user_interrupt=True,
  byte_stream=websocket_handler.byte_strs
)",
        "lang": "python",
      },
      {
        "code": "# Specify device 4 in MicrophoneInterface
MicrophoneInterface.start(
  socket,
  device=4,
  allow_user_interrupt=True,
  byte_stream=websocket_handler.byte_strs
)",
        "lang": "python",
      },
      {
        "code": "# Directly import the sounddevice library
import sounddevice as sd

# Set the default device prior to scheduling audio input task
sd.default.device = 4",
        "lang": "python",
      },
      {
        "code": "# Directly import the sounddevice library
import sounddevice as sd

# Set the default device prior to scheduling audio input task
sd.default.device = 4",
        "lang": "python",
      },
      {
        "code": "# Specify allowing interruption
MicrophoneInterface.start(
  socket,
  allow_user_interrupt=True,
  byte_stream=websocket_handler.byte_strs
)",
        "lang": "python",
      },
      {
        "code": "# Specify allowing interruption
MicrophoneInterface.start(
  socket,
  allow_user_interrupt=True,
  byte_stream=websocket_handler.byte_strs
)",
        "lang": "python",
      },
      {
        "code": "asyncio.run(main())",
        "lang": "py",
      },
      {
        "code": "asyncio.run(main())",
        "lang": "py",
      },
    ],
    "content": "This guide provides detailed instructions for integrating EVI into your Python projects using Hume's Python SDK. It is divided into seven key components:
Environment setup: Download package and system dependencies to run EVI.

Dependency imports: Import all necessary dependencies into your script.

Defining a WebSocketHandler class: Create a class to manage the WebSocket connection.

Authentication: Use your API credentials to authenticate your EVI application.

Connecting to EVI: Set up a secure WebSocket connection to interact with EVI.

Handling audio: Capture audio data from an input device, and play audio produced by EVI.

Asynchronous event loop: Initiate and manage an asynchronous event loop that handles simultaneous, real-time execution of message processing and audio playback.


To see a full implementation within a terminal application, visit our API examples repository on GitHub: evi-python-example


Hume's Python SDK supports EVI using Python versions 3.9, 3.10, and 3.11 on macOS and Linux platforms. The full specification be found on the Python SDK GitHub page.


Environment setup
Before starting the project, it is essential to set up the development environment.
Creating a virtual environment (optional)
Setting up a virtual environment is a best practice to isolate your project's dependencies from your global Python installation, avoiding potential conflicts.
You can create a virtual environment using either Python's built-in venv module or the conda environment manager. See instructions for both below:




Create the virtual environment.


Note that when you create a virtual environment using Python's built-in venv tool, the virtual environment will use the same Python version as the global Python installation that you used to create it.


Activate the virtual environment using the appropriate command for your system platform.






The code above demonstrates virtual environment activation on a POSIX platform with a bash/zsh shell. Visit the venv documentation to learn more about using venv on your platform.


Install conda from Miniconda or Anaconda Distribution.

Create the virtual environment.


conda allows developers to set the version of their Python interpreter when creating a virtual environment. In the example below, Python version 3.11 is specified:


Activate the virtual environment using the appropriate command for your system platform.






Visit the conda documentation to learn more about managing Python environments with conda.
Package dependenices
There are two package dependencies for using EVI:
Hume Python SDK (required)


The hume[microphone] package contains the Hume Python SDK. This guide employs EVI's WebSocket and message handling infrastructure as well as various asynchronous programming and audio utilities.


Environment variables (recommended)


The python-dotenv package contains the logic for using environment variables to store and load sensitive variables such as API credentials from a .env file.


In sample code snippets below, the API key, Secret key, and an EVI configuration id have been saved to environment variables.


While not strictly required, using environment variables is considered best practice because it keeps sensitive information like API keys and configuration settings separate from your codebase. This not only enhances security but also makes your application more flexible and easier to manage across different environments.
System dependencies
For audio playback and processing, additional system-level dependencies are required. Below are download instructions for each supported operating system:




To ensure audio playback functionality, macOS users will need to install ffmpeg, a powerful multimedia framework that handles audio and video processing.
A common way to install ffmpeg on macOS is by using a package manager such as Homebrew. To do so, follow these steps:
Install Homebrew onto your system according to the instructions on the Homebrew website.

Once Homebrew is installed, you can install ffmpeg with brew:






If you prefer not to use Homebrew, you can download a pre-built ffmpeg binary from the ffmpeg website or use other package managers like MacPorts.


Linux users will need to install the following packages to support audio input/output and playback:
libasound2-dev: This package contains development files for the ALSA (Advanced Linux Sound Architecture) sound system.

libportaudio2: PortAudio is a cross-platform audio I/O library that is essential for handling audio streams.

ffmpeg: Required for processing audio and video files.


To install these dependencies, use the following commands:


Dependency imports
The following import statements are used in the example project to handle asynchronous operations, environment variables, audio processing, and communication with the Hume API:








Module/Class/Method Description 
asyncio Provides support for asynchronous programming, allowing the code to handle multiple tasks concurrently. 
base64 Used to encode and decode audio data in base64 format, essential for processing audio streams. 
os Allows interaction with the operating system, particularly for accessing environment variables. 
datetime Used to generate timestamps for logging events. 
load_dotenv Loads environment variables from a .env file, which are used for API key management and EVI configuration. 
AsyncHumeClient Provides an asynchronous client for connecting to the Hume API, which powers the empathic voice interface. 
ChatConnectOptions, ChatWebsocketConnection These classes manage WebSocket connections and configuration options for the Hume Empathic Voice Interface (EVI). 
SubscribeEvent Represents different types of messages received through the WebSocket connection. 
UserInput, AudioConfiguration, SessionSettings These types define the structure of messages and settings sent to the Hume API, such as user input and audio configurations. 
Stream Manages streams of asynchronous data, particularly useful for handling audio streams. 
MicrophoneInterface Manages audio capture and playback from a specified input and output device. 
ApiError Defines custom error handling for API-related issues, ensuring graceful error management within the application. 

Defining a WebSocketHandler class
Next, we define a WebSocketHandler class to encapsulate WebSocket functionality in one organized component. The handler allows us to implement application-specific behavior upon the socket opening, closing, receiving messages, and handling errors. It also manages the continuous audio stream from a microphone.
By using a class, you can maintain the WebSocket connection and audio stream state in one place, making it simpler to manage both real-time communication and audio processing.
Below are the key methods:
Method Description 
__init__() Initializes the handler, setting up placeholders for the WebSocket connection. 
set_socket(socket: ChatWebsocketConnection) Associates the WebSocket connection with the handler. 
on_open() Called when the WebSocket connection is established, enabling any necessary initialization. 
on_message(data: SubscribeEvent) Handles incoming messages from the WebSocket, processing different types of messages. 
on_close() Invoked when the WebSocket connection is closed, allowing for cleanup operations. 
on_error(error: Exception) Manages errors that occur during WebSocket communication, providing basic error logging. 



Below is an example of what the WebSocketHandler class may look like.
Refer to the evi-python-example for a complete example implementation.


Authentication
In order to establish an authenticated connection, we instantiate the Hume client with our API key and include our Secret key in the query parameters passed into the WebSocket connection.


You can obtain your API credentials by logging into the Hume Platform and visiting the API keys page.


Connecting to EVI
With the Hume client instantiated with our credentials, we can now establish an authenticated WebSocket connection with EVI and pass in our handlers.


Handling audio
The MicrophoneInterface class captures audio input from the user's device and streams it over the WebSocket connection.
Audio playback occurs when the WebSocketHandler receives audio data over the WebSocket connection in its asynchronous byte stream from an audio_output message.
In this example, byte_strs is a stream of audio data that the WebSocket connection populates.


Specifying a microphone device
You can specify your microphone device using the device parameter in the MicrophoneInterface object's start method.
To view a list of available audio devices, run the following command:


python -c "import sounddevice; print(sounddevice.query_devices())"
Below is an example output:


If the MacBook Pro Microphone is the desired device, specify device 4 in the Microphone context. For example:


For troubleshooting faulty device detection - particularly with systems using ALSA, the Advanced Linux Sound Architecture, the device may also be directly specified using the sounddevice library:


Allowing interruption
The allow_interrupt parameter in the MicrophoneInterface class allows control over whether the user can send a message while the assistant is speaking:


allow_interrupt=True: Allows the user to send microphone input even when the assistant is speaking. This enables more fluid, overlapping conversation.

allow_interrupt=False: Prevents the user from sending microphone input while the assistant is speaking, ensuring that the user does not interrupt the assistant. This is useful in scenarios where clear, uninterrupted communication is important.


Asynchronous event loop
Initialize, execute, and manage the lifecycle of the asynchronous event loop, making sure that the main() coroutine and its runs effectively and that the application shuts down cleanly after the coroutine finishes executing.",
    "description": "A quickstart guide for implementing the Empathic Voice Interface (EVI) with Python.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.quickstart.python-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/quickstart/python",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "EVI Python Quickstart Guide",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "content": "The Empathic Voice Interface 2 (EVI 2) introduces a new architecture that seamlessly integrates voice and language processing. This multimodal approach allows EVI 2 to understand and generate both language and voice, dramatically enhancing key features over EVI 1 while also enabling new capabilities.
EVI 2 can converse rapidly and fluently with users, understand a user's tone of voice, generate any tone of voice, and can even handle niche requests like rapping, changing its style, or speeding up its speech. The model specifically excels at emulating a wide range of personalities, including their accents and speaking styles. It is exceptional at maintaining personalities that are fun and interesting to interact with. Ultimately, EVI 2 is capable of emulating the ideal personality for every application and user.
In addition, EVI 2 allows developers to create custom voices by using a new voice modulation method. Developers can adjust EVI 2's base voices along a number of continuous scales, including gender, nasality, and pitch. This first-of-its-kind feature enables creating voices that are unique to an application or even a single user. Further, this feature does not rely on voice cloning, which currently invokes more risks than any other capability of this technology.


The EVI 2 API is currently in beta. We are still making ongoing
improvements to the model. In the coming weeks and months, EVI 2 will sound
better, speak more languages, follow more complex instructions, and use a
wider range of tools.",
    "description": "Introducing EVI 2, our new voice-language foundation model, enabling human-like conversations with enhanced naturalness, emotional responsiveness, adaptability, and rich customization options for the voice and personality.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Empathic Voice Interface 2 (EVI 2)",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "content": "EVI 2 uses an advanced voice generation model connected to our eLLM, which can process and generate both text and audio. This results in more natural-sounding speech with better word emphasis, higher expressiveness, and more consistent vocal output.


The integrated architecture of EVI 2 reduces end-to-end latency by 40% vs EVI 1, now averaging around 500ms. This significant speed improvement enables more responsive and human-like conversations.


By processing voice and language in the same model, EVI 2 can better understand the emotional context of user inputs and generate more empathic responses, both in terms of content and vocal tone.


EVI 2 offers new control over the AI's voice characteristics. Developers can adjust various parameters to tailor EVI 2's voice to their specific application needs. EVI 2 also supports in-conversation voice prompting, allowing users to dynamically modify EVI's speaking style (e.g., "speak faster", "sound excited") during interactions.


Despite its advanced capabilities, EVI 2 is 30% more cost-effective than its predecessor, with pricing reduced from $0.1020 to $0.0714 per minute.
Beyond these improvements, EVI 2 also exhibits promising emerging capabilities including speech output in multiple languages. We will make these improvements available to developers as we scale up and improve the model.
We provide the same suite of tools to integrate and customize EVI 2 for your application as we do for EVI 1, and existing EVI developers can easily switch to the new system.",
    "domain": "test.com",
    "hash": "#key-improvements",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)",
      },
      "h3": {
        "id": "key-improvements",
        "title": "Key improvements",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-key-improvements-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Key improvements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "code_snippets": [
      {
        "code": "{
  "evi_version": "2",
  "name": "EVI 2 config",
  "voice": {
    "provider": "HUME_AI",
    "name": "DACHER"
  }
}",
        "lang": "json",
      },
    ],
    "content": "Developers can start testing EVI 2 by simply creating an EVI config on the Hume platform. Just select EVI 2 as the version when creating your config.
To use EVI 2, simply create a configuration using the /v0/evi/configs endpoint and specify "evi_version": "2". Then, use this config in a conversation with EVI using the /v0/evi/chat endpoint. Most aspects of using EVI, including authentication strategies, remain the same as described in the EVI documentation.
In your configuration JSON, set the evi_version parameter to "2". Here's an example of an EVI 2 config:


Using a config like the above, make a POST request to the /v0/evi/configs endpoint to save the config.

Specify any other custom settings you need.",
    "domain": "test.com",
    "hash": "#building-with-evi-2",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)",
      },
      "h2": {
        "id": "building-with-evi-2",
        "title": "Building with EVI 2",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-building-with-evi-2-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Building with EVI 2",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "content": "EVI 2 is available now, with full feature parity with EVI 1, including support for supplemental LLMs, custom language models, tool use, built-in tools like web search, and all configuration options.
From September to December 2024, the Hume team will focus on improving the reliability and quality of EVI 2. The team will ensure that all the features of the EVI 1 API work consistently in EVI 2.
In late December 2024, the EVI 1 API will be sunsetted and deprecated. Developers will need to migrate from EVI 1 to EVI 2 for ongoing support and new features.


Clear migration guidelines will be provided ahead of time, and our team will
ensure only minor changes will be required to make applications work with EVI
2.",
    "domain": "test.com",
    "hash": "#evi-2-timeline",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)",
      },
      "h2": {
        "id": "evi-2-timeline",
        "title": "EVI 2 timeline",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-evi-2-timeline-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "EVI 2 timeline",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "content": "This table provides a comprehensive comparison of features between EVI 1 and EVI 2, highlighting the new capabilities introduced in the latest version.
Feature EVI 1 EVI 2 
Voice quality Similar to best TTS solutions Significantly improved naturalness, clarity, and expressiveness 
Response latency ~900ms-2000ms ~500-800ms (about 2x faster) 
Emotional intelligence Empathic responses informed by expression measures End-to-end understanding of voice augmented with emotional intelligence training 
Base voices 3 core voice options (Kora, Dacher, Ito) 5 new high-quality base voice options with expressive personalities (8 total) 
Voice customizability Supported - can select base voices and adjust voice parameters Supported - extensive customization with parameter adjustments (e.g. pitch, huskiness, nasality) 
In-conversation voice prompting Not supported Supported (e.g., "speak faster", "sound more excited", change accents) 
Multimodal processing Transcription augmented with high-dimensional voice measures Fully integrated voice and language processing within a single model, along with transcripts and expression measures 
Supplemental LLMs Supported Supported 
Tool use and web search Supported Supported 
Custom language model (CLM) Supported Supported 
Configuration options Extensive support Extensive support (same options as EVI 1) 
Typescript SDK support Supported Supported 
Python SDK support Supported Supported 
Multilingual support English only Expanded support for multiple languages planned for Q4 2024 
Cost $0.102 per minute $0.0714 per minute (30% reduction)",
    "domain": "test.com",
    "hash": "#feature-comparison-evi-1-vs-evi-2",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)",
      },
      "h2": {
        "id": "feature-comparison-evi-1-vs-evi-2",
        "title": "Feature comparison: EVI 1 vs EVI 2",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-feature-comparison-evi-1-vs-evi-2-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Feature comparison: EVI 1 vs EVI 2",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/evi-2",
    "content": "Yes, for most configs, you will be able to update to EVI 2 simply by setting
the evi_version to 2 in the configuration. If your config uses a
supplemental LLM, we recommend that you select the same supplemental LLM for
EVI 2. This will ensure stability and support for the same tools.


EVI 2 is a beta API, and is still in progress as of September 2024. Our team has released the API to allow developers to experiment with EVI 2.
Our developer platform team is available to assist with integration challenges to ensure smooth
deployment across various applications - join our
Discord for assistance.
For companies that are interested in using EVI 2 in production, please contact our partnerships team .


After launch, we will make ongoing enhancements to naturalness, expressiveness, latency, consistency and reliability, and EVI's overall output quality.
We plan to add more granular options for tailoring EVI's personality and creating custom EVIs, including style references and audio prompts. We plan to make EVI multi-lingual and continuously add support for new major languages in Q4 2024.
Later this year, we will likely integrate EVI with image and/or video modalities, allowing it to respond to the user's facial expressions using Hume's proprietary models of facial expressions.


EVI 2's multimodal processing integrates voice and text in a single voice-language foundation model. This allows EVI 2 to understand and
generate both language and voice in the same latent space, resulting in more
coherent and contextually aware responses. EVI 2's integrated voice-language architecture also offers
unprecedented control over both the AI's personality and voice
characteristics. Further, it allows prompting the
model to change its speaking style or to follow a personality.


Hume has implemented several key safety measures for EVI 2:
Architectural safeguards: EVI 2's core architecture prevents unauthorized voice cloning by representing voice characteristics as abstract semantic tokens, not raw audio data. This allows personality imitation without enabling direct voice replication. Importantly, EVI 2 is incapable of voice cloning without access to its code. By controlling EVI 2's identity-related voice characteristics at the architecture level, we force the model to adopt one identity at a time, maintaining a consistent vocal register across sessions. We believe voice cloning currently invokes more risks than any other capability of voice AI, which is why we've implemented these architectural safeguards as a core feature of EVI 2.

Customizable language control: Developers can use their own LLMs or modify the supplemental LLM, enabling custom content filtering and safeguards tailored to their specific use cases.

Continuous testing: Our team regularly red-teams and tests EVI 2 to identify and address potential vulnerabilities.

Usage monitoring: We actively monitor API usage, classify major use cases, and can swiftly intervene if we detect misuse.

Clear guidelines: Our terms of use and the Hume Initiative guidelines prohibit malicious applications of our technology.


These measures ensure responsible deployment while providing developers the necessary control and transparency for their specific applications.


Yes, we plan to make EVI multilingual and support other languages in Q4 2024, in the following order:
Multiple English accents (e.g. Australian, British)

Common European languages (Spanish, German, Italian, French, Portuguese)

Additional languages based on customer demand (including Arabic, Japanese, Korean, Hindi, Dutch, Swedish, Turkish, Russian, Mandarin)


EVI 2's ability to learn new languages efficiently with minimal data will facilitate this expansion to more languages.


EVI 2's speech recognition capabilities are robust across a wide range of
scenarios. It is highly accurate across a wide range of accents, breathing
patterns, and individual speaking patterns. Performance may degrade in
environments with significant background noise, or when multiple speakers
overlap. We continue to work on improving performance in challenging acoustic
environments.",
    "domain": "test.com",
    "hash": "#frequently-asked-questions",
    "hierarchy": {
      "h0": {
        "title": "Empathic Voice Interface 2 (EVI 2)",
      },
      "h2": {
        "id": "frequently-asked-questions",
        "title": "Frequently Asked Questions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.evi-2-frequently-asked-questions-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/evi-2",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Frequently Asked Questions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "content": "The Empathic Voice Interface (EVI) is designed to be highly configurable, allowing developers to customize the interface to align with their specific requirements.
Configuration of EVI can be managed through two primary methods: an EVI configuration and session settings.",
    "description": "Guide to configuring the Empathic Voice Interface (EVI).",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.configuration-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Configuring EVI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "content": "EVI configuration options affect the behavior and capabilities of the interface, and include the following configuration options:
Option Description 
Voice Select a voice from a list of 8 preset options or create a custom voice. For further details, see our guide on creating custom voices. 
EVI version Select the version of EVI you would like to use. For details on similarities and differences between EVI versions 1 and 2, refer to our feature comparison. 
System prompt Provide a system prompt to guide how EVI should respond. For details on expressive prompt engineering, refer to our prompting guide. 
Language model Select a language model that best fits your application’s needs. For details on selecting a supplementary language model to meet your needs, such as optimizing for lowest latency, refer to our EVI FAQ. To incorporate your own language model, refer to our guide on using your own language model. 
Tools Choose user-created or built-in tools for EVI to use during conversations. For details on creating tools and adding them to your configuration, see our tool use guide. 
Event messages Configure messages that EVI will send in specific situations. For details on configuring event messages, see our API Reference. 
Timeouts Define limits on a chat with EVI to manage conversation flow. For details on specifying timeouts, see our API Reference. 



Configs, as well as system prompts and tools, are versioned.
This versioning system supports iterative development, allowing you to
progressively refine configurations and revert to previous versions if needed.",
    "domain": "test.com",
    "hash": "#configuration-options",
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI",
      },
      "h2": {
        "id": "configuration-options",
        "title": "Configuration options",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.configuration-configuration-options-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Configuration options",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "content": "EVI is pre-configured with a set of default values, which are automatically applied if you do not specify a configuration.
The default configuration includes a preset voice and language model, but does not include a system prompt or tools.
To customize these options, you will need to create and specify your own EVI configuration.
The default configuration settings are as follows:
EVI 1 EVI 2 
Language model: Claude 3.5 Sonnet Language model: hume-evi-2 
Voice: Ito Voice: Ito 
System prompt: Hume default System prompt: Hume default 
Tools: None Tools: None 



Default configuration settings are subject to change. To ensure your setup remains consistent should changes occur, we recommend choosing explicit options when defining your EVI configuration.",
    "domain": "test.com",
    "hash": "#default-configuration-options",
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI",
      },
      "h2": {
        "id": "default-configuration-options",
        "title": "Default configuration options",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.configuration-default-configuration-options-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Default configuration options",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "code_snippets": [
      {
        "code": "import { Hume, HumeClient } from 'hume';
// instantiate the HumeClient with credentials
// avoid hard coding your API key, retrieve from environment variables
const client = new HumeClient({
  apiKey: <YOUR_API_KEY>,
  secretKey: <YOUR_SECRET_KEY>,
});
// instantiate WebSocket connection with specified EVI config
const socket = await client.empathicVoice.chat.connect({
  configId: <YOUR_CONFIG_ID> // specify config ID here
});",
        "lang": "typescript",
      },
      {
        "code": "import { Hume, HumeClient } from 'hume';
// instantiate the HumeClient with credentials
// avoid hard coding your API key, retrieve from environment variables
const client = new HumeClient({
  apiKey: <YOUR_API_KEY>,
  secretKey: <YOUR_SECRET_KEY>,
});
// instantiate WebSocket connection with specified EVI config
const socket = await client.empathicVoice.chat.connect({
  configId: <YOUR_CONFIG_ID> // specify config ID here
});",
        "lang": "typescript",
      },
      {
        "code": "from hume import HumeVoiceClient, MicrophoneInterface
# avoid hard coding your API key, retrieve from environment variables
HUME_API_KEY = <YOUR_API_KEY>
# Connect and authenticate with Hume
client = HumeVoiceClient(HUME_API_KEY)
# establish a connection with EVI with your configuration by passing
# the config_id as an argument to the connect method
async with client.connect(config_id="<your-config-id>") as socket:
  await MicrophoneInterface.start(socket)",
        "lang": "python",
      },
      {
        "code": "from hume import HumeVoiceClient, MicrophoneInterface
# avoid hard coding your API key, retrieve from environment variables
HUME_API_KEY = <YOUR_API_KEY>
# Connect and authenticate with Hume
client = HumeVoiceClient(HUME_API_KEY)
# establish a connection with EVI with your configuration by passing
# the config_id as an argument to the connect method
async with client.connect(config_id="<your-config-id>") as socket:
  await MicrophoneInterface.start(socket)",
        "lang": "python",
      },
    ],
    "content": "See instructions below for creating an EVI configuration through the Portal. In the Portal, navigate to the EVI Configurations page. Click the Create configuration button to begin.






Choose EVI version
To learn more about the differences between EVI versions 1 and 2, please see the feature comparison guide.




Choose voice
Select a voice from Hume's 8 presets, or create your own custom voice. To learn more about voice customization options on the Hume Platform, please visit the Voices page.




Set up the LLM
Select a supported language model and specify a system prompt.




Add tools
Equip EVI with built-in tools, like web search, or custom user-defined tools. Click the + Add button to select an existing tool or create a new one.




Name config
Name your EVI configuration and add an optional description.




Test the configuration
The newly created configuration can now be tested. From the EVI Config details page, click Run in playground to test it out.




Once in the EVI Playground, click Start call to connect to EVI with your configuration.




The event message and timeout
configuration options are not part of the initial config creation flow. However, you can set these options at any time in the playground or from the configuration's edit page after your configuration has been created.








Apply the configuration
After creating an EVI configuration, you can use it in your conversations with EVI by including the config_id
in the query parameters of your connection request. You can find the config_id on the configuration's edit page. To access this page, first navigate to the configurations page
and then click the Edit button for the desired configuration.




See the sample code below which showcases how to apply your configuration:",
    "domain": "test.com",
    "hash": "#create-a-configuration",
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI",
      },
      "h2": {
        "id": "create-a-configuration",
        "title": "Create a configuration",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.configuration-create-a-configuration-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Create a configuration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "code_snippets": [
      {
        "code": "{
  "type": "session_settings",
  "audio": {
    "channels": 1,
    "encoding": "linear16",
    "sample_rate": 48000
  }
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "session_settings",
  "audio": {
    "channels": 1,
    "encoding": "linear16",
    "sample_rate": 48000
  }
}",
        "lang": "json",
      },
    ],
    "content": "EVI configurations are persistent and version-controlled. In contrast, session settings are temporary and apply only to the current session, such as
microphone settings. These parameters can be adjusted dynamically based on the requirements of each session to ensure optimal performance and user experience.


Refer to the API reference for detailed descriptions of the various session settings options.
Updating the session settings is only a requirement when the audio input is encoded in PCM Linear 16. If this is the case, be sure to send the following Session Settings message prior to sending an audio input:",
    "domain": "test.com",
    "hash": "#session-settings",
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI",
      },
      "h2": {
        "id": "session-settings",
        "title": "Session settings",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.configuration-session-settings-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Session settings",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/configuration",
    "code_snippets": [
      {
        "code": "{
  "type": "session_settings",
  "variables": {
    "name": "David Hume",
    "age": "65",
    "is_philosopher": "True"
  }
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "session_settings",
  "variables": {
    "name": "David Hume",
    "age": "65",
    "is_philosopher": "True"
  }
}",
        "lang": "json",
      },
      {
        "code": "Address the user by their name, {{name}}.
If relevant, reference their age: {{age}}.
It is {{is_philosopher}} that this user is a philosopher.",
        "lang": "text",
      },
      {
        "code": "Address the user by their name, {{name}}.
If relevant, reference their age: {{age}}.
It is {{is_philosopher}} that this user is a philosopher.",
        "lang": "text",
      },
    ],
    "content": "EVI can reference and update session-specific values as a conversation progresses. By including dynamic variables in the system prompt, you can personalize conversations with EVI.


Visit our prompting guide for more details on adding dynamic variables to your prompt.
To use this feature, first define a variables parameter within a Session Settings message containing one or more dynamic variables.
Each key-value pair in "variables" must have a string key representing the variable name and a string value.
For example, you can define the user's name and set it to a default value:


Then, include the variable surrounded by two pairs of curly braces, such as {{name}}, as a placeholder value in your system prompt:


To ensure your dynamic variables are recognized properly, confirm the following:
If you have defined a dynamic variable, reference it.: If the variable is defined but not referenced in the system prompt, then it will not be included in the conversation and EVI will not be able to refer to the variable.

If you have referenced a dynamic variable, make sure it is defined.: If the variable is referenced in the system prompt, but it is not defined in the "variables" field, then the warning W0106 will be raised: "No values have been specified for the variables [variable_name] which can lead to incorrect text formatting. Please assign them values." For example, this error can occur when there are spelling mistakes or differences between the variable defined in the "variables" field and the variable referenced in the system prompt (i.e. {{firstName}} instead of {{name}}).",
    "domain": "test.com",
    "hash": "#dynamic-variables",
    "hierarchy": {
      "h0": {
        "title": "Configuring EVI",
      },
      "h2": {
        "id": "session-settings",
        "title": "Session settings",
      },
      "h3": {
        "id": "dynamic-variables",
        "title": "Dynamic variables",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.configuration-dynamic-variables-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/configuration",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Dynamic variables",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/voices",
    "content": "The Empathic Voice Interface (EVI) can be configured with any of our 8 base voices. You can also customize these voices by adjusting specific attributes. This guide explains each attribute and provides a tutorial for creating a custom voice.
Visit the Playground to test the base voices.


The custom voices feature is experimental and under active development. Regular updates will focus on improving stability and expanding attribute options.",
    "description": "Guide to customizing the voice of the Empathic Voice Interface (EVI).",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.voices-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/voices",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Voices",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/voices",
    "content": "The following attributes can be modified to personalize any of the base voices:
Attribute Description 
Gender The vocalization of gender, ranging between more masculine and more feminine. 
Articulation The clarity of the voice, ranging between mumbled and articulate. 
Assertiveness The firmness of the voice, ranging between whiny and bold. 
Buoyancy The density of the voice, ranging between deflated and buoyant. 
Confidence The assuredness of the voice, ranging between shy and confident. 
Enthusiasm The excitement within the voice, ranging between calm and enthusiastic. 
Nasality The openness of the voice, ranging between clear and nasal. 
Relaxedness The stress within the voice, ranging between tense and relaxed. 
Smoothness The texture of the voice, ranging between smooth and staccato. 
Tepidity The liveliness behind the voice, ranging between tepid and vigorous. 
Tightness The containment of the voice, ranging between tight and breathy. 

Each voice attribute can be adjusted relative to the base voice’s characteristics. Values range from -100 to 100, with 0 as the default.
Setting all attributes to their default values will keep the base voice unchanged.",
    "domain": "test.com",
    "hash": "#voice-attributes",
    "hierarchy": {
      "h0": {
        "title": "Voices",
      },
      "h2": {
        "id": "voice-attributes",
        "title": "Voice attributes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.voices-voice-attributes-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/voices",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Voice attributes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/voices",
    "content": "See instructions below for customizing a voice through the Portal.


Navigate to the Voices page
In the Portal, find the EVI Voices page. Click the Create voice button to begin.




Create a new custom voice
Enter a name for your custom voice and select a base voice. Then, adjust the attributes.




Test your custom voice
As you make tweaks to the attributes, sample audio can be generated by clicking the "▶" button in the Voice sample section at the bottom of the form.




Use your custom voice
The newly created voice can now be deployed. From the Voices page, click Use to create an EVI configuration with it.




When creating an EVI configuration, choose Custom voice and press the + Select button. Then, press Select existing custom voice... and confirm the custom voice you would like to use.",
    "domain": "test.com",
    "hash": "#crafting-custom-voices",
    "hierarchy": {
      "h0": {
        "title": "Voices",
      },
      "h2": {
        "id": "crafting-custom-voices",
        "title": "Crafting custom voices",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.voices-crafting-custom-voices-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/voices",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Crafting custom voices",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "content": "EVI simplifies the integration of external APIs through function calling. Developers can integrate custom functions that are invoked dynamically based on the user’s
input, enabling more useful conversations. There are two key concepts for using function calling with EVI: Tools and Configurations (Configs):
Tools are resources that EVI uses to do things, like search the web or call external APIs. For example, tools can check the weather, update databases, schedule appointments, or take
actions based on what occurs in the conversation. While the tools can be user-defined, Hume also offers natively implemented tools, like web search, which are labeled as “built-in” tools.

Configurations enable developers to customize an EVI’s behavior and incorporate these custom tools. Setting up an EVI configuration allows developers to seamlessly integrate
their tools into the voice interface. A configuration includes prompts, user-defined tools, and other settings.








Currently, our function calling feature only supports
OpenAI and Anthropic models.
For the best results, we suggest choosing a fast and intelligent LLM that performs well on function calling benchmarks.
On account of its speed and intelligence, we recommend Claude 3.5 Haiku as the supplemental LLM in your EVI configuration when using tools.
Function calling is not available if you are using your own custom language
model. We plan to
support more function calling LLMs in the future.
The focus of this guide is on creating a Tool and a Configuration that allows EVI to use the Tool. Additionally, this guide details the message flow of function calls within a
session, and outlines the expected responses when function calls fail. Refer to our Configuration Guide for detailed,
step-by-step instructions on how to create and use an EVI Configuration.


Explore these sample projects to see how Tool use can be implemented in TypeScript,
Next.js, and Python.",
    "description": "Guide to using function calling with the Empathic Voice Interface (EVI).",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Tool use",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "code_snippets": [
      {
        "code": "{
  "type": "object",
  "required": ["location", "format"],
  "properties": {
    "location": {
      "type": "string",
      "description": "The city and state, e.g. San Francisco, CA"
    },
    "format": {
      "type": "string",
      "enum": ["celsius", "fahrenheit"],
      "description": "The temperature unit to use. Infer this from the user's location."
    }
  }
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "object",
  "required": ["location", "format"],
  "properties": {
    "location": {
      "type": "string",
      "description": "The city and state, e.g. San Francisco, CA"
    },
    "format": {
      "type": "string",
      "enum": ["celsius", "fahrenheit"],
      "description": "The temperature unit to use. Infer this from the user's location."
    }
  }
}",
        "lang": "json",
      },
      {
        "code": "{
  "name": "get_current_weather",
  "version_description": "Fetches current weather and uses celsius or fahrenheit based on user's location.",
  "description": "This tool is for getting the current weather.",
  "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\" } }, \"required\": [\"location\", \"format\"] }"
}",
        "lang": "json",
      },
      {
        "code": "{
  "name": "get_current_weather",
  "version_description": "Fetches current weather and uses celsius or fahrenheit based on user's location.",
  "description": "This tool is for getting the current weather.",
  "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\" } }, \"required\": [\"location\", \"format\"] }"
}",
        "lang": "json",
      },
      {
        "code": "{
  "tool_type": "FUNCTION",
  "id": "15c38b04-ec9c-4ae2-b6bc-5603512b5d00",
  "version": 0,
  "version_description": "Fetches current weather and uses celsius or fahrenheit based on user's location.",
  "name": "get_current_weather",
  "created_on": 1714421925626,
  "modified_on": 1714421925626,
  "fallback_content": null,
  "description": "This tool is for getting the current weather.",
  "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\" } }, \"required\": [\"location\", \"format\"] }"
}",
        "lang": "json",
      },
      {
        "code": "{
  "tool_type": "FUNCTION",
  "id": "15c38b04-ec9c-4ae2-b6bc-5603512b5d00",
  "version": 0,
  "version_description": "Fetches current weather and uses celsius or fahrenheit based on user's location.",
  "name": "get_current_weather",
  "created_on": 1714421925626,
  "modified_on": 1714421925626,
  "fallback_content": null,
  "description": "This tool is for getting the current weather.",
  "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\" } }, \"required\": [\"location\", \"format\"] }"
}",
        "lang": "json",
      },
      {
        "code": "{
  "name": "Weather Assistant Config",
  "language_model": {
    "model_provider": "OPEN_AI",
    "model_resource": "gpt-3.5-turbo",
    "temperature": null
  },
  "tools": [
    {
      "id": "15c38b04-ec9c-4ae2-b6bc-5603512b5d00",
      "version": 0
    }
  ]
}",
        "lang": "json",
      },
      {
        "code": "{
  "name": "Weather Assistant Config",
  "language_model": {
    "model_provider": "OPEN_AI",
    "model_resource": "gpt-3.5-turbo",
    "temperature": null
  },
  "tools": [
    {
      "id": "15c38b04-ec9c-4ae2-b6bc-5603512b5d00",
      "version": 0
    }
  ]
}",
        "lang": "json",
      },
      {
        "code": "{
  "id": "87e88a1a-3768-4a01-ba54-2e6d247a00a7",
  "version": 0,
  "version_description": null,
  "name": "Weather Assistant Config",
  "created_on": 1714421581844,
  "modified_on": 1714421581844,
  "prompt": null,
  "voice": null,
  "language_model": {
    "model_provider": "OPEN_AI",
    "model_resource": "gpt-3.5-turbo",
    "temperature": null
  },
  "tools": [
    {
      "tool_type": "FUNCTION",
      "id": "15c38b04-ec9c-4ae2-b6bc-5603512b5d00",
      "version": 0,
      "version_description": "Fetches current weather and uses celsius or fahrenheit based on user's location.",
      "name": "get_current_weather",
      "created_on": 1714421925626,
      "modified_on": 1714421925626,
      "fallback_content": null,
      "description": "This tool is for getting the current weather.",
      "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\" } }, \"required\": [\"location\", \"format\"] }"
    }
  ],
  "builtin_tools": []
}",
        "lang": "json",
      },
      {
        "code": "{
  "id": "87e88a1a-3768-4a01-ba54-2e6d247a00a7",
  "version": 0,
  "version_description": null,
  "name": "Weather Assistant Config",
  "created_on": 1714421581844,
  "modified_on": 1714421581844,
  "prompt": null,
  "voice": null,
  "language_model": {
    "model_provider": "OPEN_AI",
    "model_resource": "gpt-3.5-turbo",
    "temperature": null
  },
  "tools": [
    {
      "tool_type": "FUNCTION",
      "id": "15c38b04-ec9c-4ae2-b6bc-5603512b5d00",
      "version": 0,
      "version_description": "Fetches current weather and uses celsius or fahrenheit based on user's location.",
      "name": "get_current_weather",
      "created_on": 1714421925626,
      "modified_on": 1714421925626,
      "fallback_content": null,
      "description": "This tool is for getting the current weather.",
      "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\" } }, \"required\": [\"location\", \"format\"] }"
    }
  ],
  "builtin_tools": []
}",
        "lang": "json",
      },
    ],
    "content": "For EVI to leverage tools or call functions, a configuration must be created with the tool’s definition. Our step-by-step guide below walks you through creating a tool and adding it to a configuration, using either a no-code approach through our Portal or a full-code approach through our API.






Create a Tool
We will first create a Tool with a specified function. In this example, we will create a tool for getting the weather. In the Portal, navigate to the EVI Tools page. Click the Create tool button to begin.




Fill in Tool details
Next, we will fill in the details for a weather tool named get_current_weather. This tool fetches the current weather conditions in a specified location and reports the temperature in either Celsius or Fahrenheit. We can establish the tool's behavior by completing the following fields:
Name: Specify the name of the function that the language model will invoke. Ensure it begins with a lowercase letter and only contains letters, numbers, or underscores.

Description: Provide a brief description of what the function does.

Parameters: Define the function's input parameters using a JSON schema.






The JSON schema defines the expected structure of a function's input parameters. Here's an example JSON schema we can use for the parameters field of a weather function:


Create a Configuration
Next, we will create an EVI Configuration called Weather Assistant Config. This configuration will utilize the get_current_weather Tool created in the previous step. See our Configuration guide for step-by-step instructions on how to create a configuration.
During the Set up LLM step, remember to select an Anthropic or OpenAI model for tool use support.




Add Tool to Configuration
Finally, we will specify the get_current_weather Tool in the Weather Assistant Config. Navigate to the Tools section of the EVI Config details page. Click the Add button to add a function to your configuration.
Since we have already created a get_current_weather Tool in previous steps, we can simply select Add existing tool... from the dropdown to specify it.




Select the tool to add get_current_weather to your configuration, then complete the remaining steps to create the configuration.








Create a Tool
We will first create a Tool with a specified function. In this example, we will create a tool for getting the weather. Create this tool by making a POST request to
/tools using the following request body:




The parameters field must contain a valid JSON schema.


Record the value in the id field, as we will use it to specify the newly created Tool in the next step.
Create a Configuration
Next, we will create an EVI Configuration called Weather Assistant Config, and include the created Tool by making a POST request to /configs with the
following request body:






Ensure your tool definitions conform to the language model's schema. The
specified language model will be the one to execute the function calls.",
    "domain": "test.com",
    "hash": "#setup",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "setup",
        "title": "Setup",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-setup-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Setup",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "code_snippets": [
      {
        "code": "async function fetchWeather(location: string, format: string): Promise<string> {
  // Fetch the location's geographic coordinates using Geocoding API
  const locationApiURL = `https://geocode.maps.co/search?q=${location}&api_key=${YOUR_WEATHER_API_KEY}`;
  const locationResponse = await fetch(locationApiURL);
  const locationData = await locationResponse.json();

  // Extract latitude and longitude from fetched location data
  const { lat, lon } = locationData[0];

  // Fetch point metadata using the extracted location coordinates
  const pointMetadataEndpoint = `https://api.weather.gov/points/${parseFloat(
    lat
  ).toFixed(3)},${parseFloat(lon).toFixed(3)}`;
  const pointMetadataResponse = await fetch(pointMetadataEndpoint);
  const pointMetadata = await pointMetadataResponse.json();

  // Extract weather forecast URL from point metadata
  const forecastUrl = pointMetadata.properties.forecast;

  // Fetch the weather forecast using the forecast URL
  const forecastResponse = await fetch(forecastUrl);
  const forecastData = await forecastResponse.json();
  const forecast = JSON.stringify(forecastData.properties.periods);

  // Return the temperature in the specified format
  return `${forecast} in ${format}`;
}",
        "lang": "ts",
      },
      {
        "code": "async function fetchWeather(location: string, format: string): Promise<string> {
  // Fetch the location's geographic coordinates using Geocoding API
  const locationApiURL = `https://geocode.maps.co/search?q=${location}&api_key=${YOUR_WEATHER_API_KEY}`;
  const locationResponse = await fetch(locationApiURL);
  const locationData = await locationResponse.json();

  // Extract latitude and longitude from fetched location data
  const { lat, lon } = locationData[0];

  // Fetch point metadata using the extracted location coordinates
  const pointMetadataEndpoint = `https://api.weather.gov/points/${parseFloat(
    lat
  ).toFixed(3)},${parseFloat(lon).toFixed(3)}`;
  const pointMetadataResponse = await fetch(pointMetadataEndpoint);
  const pointMetadata = await pointMetadataResponse.json();

  // Extract weather forecast URL from point metadata
  const forecastUrl = pointMetadata.properties.forecast;

  // Fetch the weather forecast using the forecast URL
  const forecastResponse = await fetch(forecastUrl);
  const forecastData = await forecastResponse.json();
  const forecast = JSON.stringify(forecastData.properties.periods);

  // Return the temperature in the specified format
  return `${forecast} in ${format}`;
}",
        "lang": "ts",
      },
      {
        "code": "async def fetch_weather(location: str, format: str) -> str:
    # Construct the URL for the Weather API request
    location_api_url = f"https://geocode.maps.co/search?q={location}&api_key={YOUR_WEATHER_API_KEY}"

    # Create an HTTP client that automatically follows redirects
    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            # Step 1: Fetch location data
            location_response = await client.get(location_api_url)
            location_response.raise_for_status()
            location_data = location_response.json()
        except httpx.HTTPError as e:
            return f"ERROR: Failed to fetch location data. {str(e)}"

        if not location_data:
            return "ERROR: No location data found."

        try:
            # Extract latitude and longitude from the location data
            lat = location_data[0]['lat']
            lon = location_data[0]['lon']
        except (IndexError, KeyError):
            return "ERROR: Unable to extract latitude and longitude."

        # Construct the URL for the Weather.gov API points endpoint
        point_metadata_endpoint = f"https://api.weather.gov/points/{float(lat):.4f},{float(lon):.4f}"

        try:
            # Step 2: Fetch point metadata
            point_metadata_response = await client.get(point_metadata_endpoint)
            point_metadata_response.raise_for_status()
            point_metadata = point_metadata_response.json()
        except httpx.HTTPError as e:
            return f"ERROR: Failed to fetch point metadata. {str(e)}"

        try:
            # Extract the forecast URL from the point metadata
            forecast_url = point_metadata['properties']['forecast']
        except KeyError:
            return "ERROR: Unable to extract forecast URL from point metadata."

        try:
            # Step 3: Fetch the weather forecast
            forecast_response = await client.get(forecast_url)
            forecast_response.raise_for_status()
            forecast_data = forecast_response.json()
        except httpx.HTTPError as e:
            return f"ERROR: Failed to fetch weather forecast. {str(e)}"

        try:
            # Extract the forecast periods from the response
            periods = forecast_data['properties']['periods']
        except KeyError:
            return "ERROR: Unable to extract forecast periods."

        # Validate the desired temperature format
        desired_unit = format.lower()
        if desired_unit not in ['fahrenheit', 'celsius']:
            return "ERROR: Invalid format specified. Please use 'fahrenheit' or 'celsius'."

        # Convert temperatures for all periods to the desired unit
        for period in periods:
            temperature = period.get('temperature')
            temperature_unit = period.get('temperatureUnit')

            if temperature is not None and temperature_unit is not None:
                if desired_unit == 'celsius' and temperature_unit == 'F':
                    # Convert Fahrenheit to Celsius
                    converted_temp = round((temperature - 32) * 5 / 9)
                    period['temperature'] = converted_temp
                    period['temperatureUnit'] = 'C'
                elif desired_unit == 'fahrenheit' and temperature_unit == 'C':
                    # Convert Celsius to Fahrenheit
                    converted_temp = round((temperature * 9 / 5) + 32)
                    period['temperature'] = converted_temp
                    period['temperatureUnit'] = 'F'

        # Return the forecast data as a JSON-formatted string
        forecast = json.dumps(periods, indent=2)
        return forecast",
        "lang": "python",
      },
      {
        "code": "async def fetch_weather(location: str, format: str) -> str:
    # Construct the URL for the Weather API request
    location_api_url = f"https://geocode.maps.co/search?q={location}&api_key={YOUR_WEATHER_API_KEY}"

    # Create an HTTP client that automatically follows redirects
    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            # Step 1: Fetch location data
            location_response = await client.get(location_api_url)
            location_response.raise_for_status()
            location_data = location_response.json()
        except httpx.HTTPError as e:
            return f"ERROR: Failed to fetch location data. {str(e)}"

        if not location_data:
            return "ERROR: No location data found."

        try:
            # Extract latitude and longitude from the location data
            lat = location_data[0]['lat']
            lon = location_data[0]['lon']
        except (IndexError, KeyError):
            return "ERROR: Unable to extract latitude and longitude."

        # Construct the URL for the Weather.gov API points endpoint
        point_metadata_endpoint = f"https://api.weather.gov/points/{float(lat):.4f},{float(lon):.4f}"

        try:
            # Step 2: Fetch point metadata
            point_metadata_response = await client.get(point_metadata_endpoint)
            point_metadata_response.raise_for_status()
            point_metadata = point_metadata_response.json()
        except httpx.HTTPError as e:
            return f"ERROR: Failed to fetch point metadata. {str(e)}"

        try:
            # Extract the forecast URL from the point metadata
            forecast_url = point_metadata['properties']['forecast']
        except KeyError:
            return "ERROR: Unable to extract forecast URL from point metadata."

        try:
            # Step 3: Fetch the weather forecast
            forecast_response = await client.get(forecast_url)
            forecast_response.raise_for_status()
            forecast_data = forecast_response.json()
        except httpx.HTTPError as e:
            return f"ERROR: Failed to fetch weather forecast. {str(e)}"

        try:
            # Extract the forecast periods from the response
            periods = forecast_data['properties']['periods']
        except KeyError:
            return "ERROR: Unable to extract forecast periods."

        # Validate the desired temperature format
        desired_unit = format.lower()
        if desired_unit not in ['fahrenheit', 'celsius']:
            return "ERROR: Invalid format specified. Please use 'fahrenheit' or 'celsius'."

        # Convert temperatures for all periods to the desired unit
        for period in periods:
            temperature = period.get('temperature')
            temperature_unit = period.get('temperatureUnit')

            if temperature is not None and temperature_unit is not None:
                if desired_unit == 'celsius' and temperature_unit == 'F':
                    # Convert Fahrenheit to Celsius
                    converted_temp = round((temperature - 32) * 5 / 9)
                    period['temperature'] = converted_temp
                    period['temperatureUnit'] = 'C'
                elif desired_unit == 'fahrenheit' and temperature_unit == 'C':
                    # Convert Celsius to Fahrenheit
                    converted_temp = round((temperature * 9 / 5) + 32)
                    period['temperature'] = converted_temp
                    period['temperatureUnit'] = 'F'

        # Return the forecast data as a JSON-formatted string
        forecast = json.dumps(periods, indent=2)
        return forecast",
        "lang": "python",
      },
      {
        "code": "{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}",
        "lang": "json",
      },
      {
        "code": "import { Hume } from 'hume';

async function handleToolCallMessage(
  toolCallMessage: Hume.empathicVoice.ToolCallMessage,
  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {
  if (toolCallMessage.name === "get_current_weather") {
    // 1. Parse the parameters from the Tool Call message
    const args = JSON.parse(toolCallMessage.parameters) as {
      location: string;
      format: string;
    };
    // 2. Extract the individual arguments
    const { location, format } = args;
    // ...etc.
  }
}",
        "lang": "ts",
      },
      {
        "code": "import { Hume } from 'hume';

async function handleToolCallMessage(
  toolCallMessage: Hume.empathicVoice.ToolCallMessage,
  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {
  if (toolCallMessage.name === "get_current_weather") {
    // 1. Parse the parameters from the Tool Call message
    const args = JSON.parse(toolCallMessage.parameters) as {
      location: string;
      format: string;
    };
    // 2. Extract the individual arguments
    const { location, format } = args;
    // ...etc.
  }
}",
        "lang": "ts",
      },
      {
        "code": "import asyncio
from hume.client import AsyncHumeClient
from hume.empathic_voice import ToolCallMessage, ToolResponseMessage
from typing import Optional

async def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:
    # Extract the tool name and ID from the message
    tool_name = message.name
    tool_call_id = message.tool_call_id
    
    # 1. Parse the parameters from the Tool Call message
    tool_parameters = json.loads(message.parameters)

    if tool_name == "get_current_weather":
        # 2. Extract the individual arguments
        obtained_location = tool_parameters.get('location')
        obtained_format = tool_parameters.get('format', 'text')

        # ...etc.",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume.client import AsyncHumeClient
from hume.empathic_voice import ToolCallMessage, ToolResponseMessage
from typing import Optional

async def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:
    # Extract the tool name and ID from the message
    tool_name = message.name
    tool_call_id = message.tool_call_id
    
    # 1. Parse the parameters from the Tool Call message
    tool_parameters = json.loads(message.parameters)

    if tool_name == "get_current_weather":
        # 2. Extract the individual arguments
        obtained_location = tool_parameters.get('location')
        obtained_format = tool_parameters.get('format', 'text')

        # ...etc.",
        "lang": "python",
      },
      {
        "code": "import { Hume } from 'hume';

async function handleToolCallMessage(
  toolCallMessage: Hume.empathicVoice.ToolCallMessage,
  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {
  if (toolCallMessage.name === "get_current_weather") {
    // 1. Parse the parameters from the Tool Call message
    const args = JSON.parse(toolCallMessage.parameters) as {
      location: string;
      format: string;
    };
    // 2. Extract the individual arguments
    const { location, format } = args;
    // 3. Call fetch weather function with extracted arguments
    const weather = await fetchWeather(location, format);
    // ...etc.
  }
}",
        "lang": "ts",
      },
      {
        "code": "import { Hume } from 'hume';

async function handleToolCallMessage(
  toolCallMessage: Hume.empathicVoice.ToolCallMessage,
  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {
  if (toolCallMessage.name === "get_current_weather") {
    // 1. Parse the parameters from the Tool Call message
    const args = JSON.parse(toolCallMessage.parameters) as {
      location: string;
      format: string;
    };
    // 2. Extract the individual arguments
    const { location, format } = args;
    // 3. Call fetch weather function with extracted arguments
    const weather = await fetchWeather(location, format);
    // ...etc.
  }
}",
        "lang": "ts",
      },
      {
        "code": "import asyncio
from hume.client import AsyncHumeClient
from hume.empathic_voice import ToolCallMessage, ToolResponseMessage
from typing import Optional

async def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:
    # Extract the tool name and ID from the message
    tool_name = message.name
    tool_call_id = message.tool_call_id
    
    # 1. Parse the parameters from the Tool Call message
    tool_parameters = json.loads(message.parameters)

    if tool_name == "get_current_weather":
        # 2. Extract the individual arguments
        obtained_location = tool_parameters.get('location')
        obtained_format = tool_parameters.get('format', 'text')

        if obtained_location:
            # 3. Call fetch weather function with extracted arguments
            weather = await fetch_weather(location=obtained_location, format=obtained_format)
            
            # ...etc.",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume.client import AsyncHumeClient
from hume.empathic_voice import ToolCallMessage, ToolResponseMessage
from typing import Optional

async def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:
    # Extract the tool name and ID from the message
    tool_name = message.name
    tool_call_id = message.tool_call_id
    
    # 1. Parse the parameters from the Tool Call message
    tool_parameters = json.loads(message.parameters)

    if tool_name == "get_current_weather":
        # 2. Extract the individual arguments
        obtained_location = tool_parameters.get('location')
        obtained_format = tool_parameters.get('format', 'text')

        if obtained_location:
            # 3. Call fetch weather function with extracted arguments
            weather = await fetch_weather(location=obtained_location, format=obtained_format)
            
            # ...etc.",
        "lang": "python",
      },
      {
        "code": "import { Hume } from 'hume';

async function handleToolCallMessage(
  toolCallMessage: Hume.empathicVoice.ToolCallMessage,
  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {
  if (toolCallMessage.name === "get_current_weather") {
    // 1. Parse the parameters from the Tool Call message
    const args = JSON.parse(toolCallMessage.parameters) as {
      location: string;
      format: string;
    };
    // 2. Extract the individual arguments
    const { location, format } = args;
    // 3. Call fetch weather function with extracted arguments
    const weather = await fetchWeather(location, format);
    // 4. Construct a Tool Response message containing the result
    const toolResponseMessage = {
      type: "tool_response",
      toolCallId: toolCallMessage.toolCallId,
      content: weather,
    };
    // 5. Send Tool Response message to the WebSocket
    socket.sendToolResponseMessage(toolResponseMessage);
  }
}",
        "lang": "ts",
      },
      {
        "code": "import { Hume } from 'hume';

async function handleToolCallMessage(
  toolCallMessage: Hume.empathicVoice.ToolCallMessage,
  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {
  if (toolCallMessage.name === "get_current_weather") {
    // 1. Parse the parameters from the Tool Call message
    const args = JSON.parse(toolCallMessage.parameters) as {
      location: string;
      format: string;
    };
    // 2. Extract the individual arguments
    const { location, format } = args;
    // 3. Call fetch weather function with extracted arguments
    const weather = await fetchWeather(location, format);
    // 4. Construct a Tool Response message containing the result
    const toolResponseMessage = {
      type: "tool_response",
      toolCallId: toolCallMessage.toolCallId,
      content: weather,
    };
    // 5. Send Tool Response message to the WebSocket
    socket.sendToolResponseMessage(toolResponseMessage);
  }
}",
        "lang": "ts",
      },
      {
        "code": "import asyncio
from hume.client import AsyncHumeClient
from hume.empathic_voice import ToolCallMessage, ToolResponseMessage
from typing import Optional

async def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:
    # Extract the tool name and ID from the message
    tool_name = message.name
    tool_call_id = message.tool_call_id
    
    # 1. Parse the parameters from the Tool Call message
    tool_parameters = json.loads(message.parameters)

    if tool_name == "get_current_weather":
        # 2. Extract the individual arguments
        obtained_location = tool_parameters.get('location')
        obtained_format = tool_parameters.get('format', 'text')

        if obtained_location:
            # 3. Call fetch weather function with extracted arguments
            weather = await fetch_weather(location=obtained_location, format=obtained_format)
            
            if not weather.startswith("ERROR"):
                # 4. Construct a Tool Response message containing the result
                resp = ToolResponseMessage(
                    tool_call_id=tool_call_id,
                    content=weather
                )
                # 5. Send Tool Response message to the WebSocket
                await self.socket.send_tool_response(resp)
                print(f"(Sent ToolResponseMessage for tool_call_id {tool_call_id}: {weather})\n")
                return resp

    # Return None if the tool is not recognized or if there's an error
    return None",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume.client import AsyncHumeClient
from hume.empathic_voice import ToolCallMessage, ToolResponseMessage
from typing import Optional

async def handle_tool_call(self, message: ToolCallMessage) -> Optional[ToolResponseMessage]:
    # Extract the tool name and ID from the message
    tool_name = message.name
    tool_call_id = message.tool_call_id
    
    # 1. Parse the parameters from the Tool Call message
    tool_parameters = json.loads(message.parameters)

    if tool_name == "get_current_weather":
        # 2. Extract the individual arguments
        obtained_location = tool_parameters.get('location')
        obtained_format = tool_parameters.get('format', 'text')

        if obtained_location:
            # 3. Call fetch weather function with extracted arguments
            weather = await fetch_weather(location=obtained_location, format=obtained_format)
            
            if not weather.startswith("ERROR"):
                # 4. Construct a Tool Response message containing the result
                resp = ToolResponseMessage(
                    tool_call_id=tool_call_id,
                    content=weather
                )
                # 5. Send Tool Response message to the WebSocket
                await self.socket.send_tool_response(resp)
                print(f"(Sent ToolResponseMessage for tool_call_id {tool_call_id}: {weather})\n")
                return resp

    # Return None if the tool is not recognized or if there's an error
    return None",
        "lang": "python",
      },
      {
        "code": "{
  "type": "assistant_message",
  "message": {
    "role": "assistant",
    "content": "The current temperature in New York, NY is 75F."
  }
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "assistant_message",
  "message": {
    "role": "assistant",
    "content": "The current temperature in New York, NY is 75F."
  }
}",
        "lang": "json",
      },
    ],
    "content": "In this section, we will go over the end-to-end flow of a function call within a chat session. This flow will be predicated on having specified the
Weather Assistant Config when establishing a connection with EVI. See our Configuration Guide
for details on how to apply your configuration when connecting.


Check out the TypeScript and Python
example projects for complete implementations of the weather Tool you'll build in this tutorial.


Define a function
We must first define a function for your Tool. This function will take the
same parameters as those specified during your Tool's creation.
For this tutorial, we will define a function that calls a weather API (e.g., the Geocoding API) to retrieve the weather for a designated city in a specified format. This weather function will accept location and format as its parameters.
See the code below for a sample implementation:








Instead of calling a weather API, you can hard code a return value like 75F as a means to quickly test for the sake of this tutorial.
EVI signals function call
Once EVI is configured with your Tool, it will automatically infer when to signal a function call within a chat session. With EVI configured to use the get_current_weather Tool, we can now ask it: "what is the weather in New York?"
Let's try it out in the EVI Playground.




We can expect EVI to respond with a User Message and a Tool Call message:






Currently, EVI does not support parallel function calling. Only one function call can be processed at a time.
Extract arguments from Tool Call message
Upon receiving a Tool Call message from EVI, we will parse the parameters and extract the arguments.
The code below demonstrates how to extract the location and format arguments, which the user-defined fetch weather function is expecting, from a received Tool Call message.






Invoke function call
Next, we will pass the extracted arguments into the previously defined fetch weather function. We will capture the return value to send back to EVI:






Send function call result
Upon receiving the return value of your function, we will send a Tool Response message containing the result. The specified tool_call_id must match the one received in
the Tool Call message from EVI:






Let's try it in the EVI Playground. Enter the return value of your function in the input field below the Tool Call message, and click Send Response. In practice, you will use the actual return value from your function call. However, for demonstration purposes, we will assume a return value of "75F".




EVI responds
After the interface receives the Tool Response message, it will then send an Assistant Message containing the response generated from the reported result of the function call:


See how it works in the EVI Playground.




To summarize, Tool Call serves as a programmatic tool for intelligently signaling when you should invoke your function. EVI does not invoke the function for you. You will need to define a function, invoke the function, and pass the return value of your function to EVI via a Tool Response message. EVI will generate a response based on the content of your message.",
    "domain": "test.com",
    "hash": "#function-calling",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "function-calling",
        "title": "Function calling",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-function-calling-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Function calling",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "code_snippets": [
      {
        "code": "{
  "name": "Web Search Config",
  "language_model": {
    "model_provider": "OPEN_AI",
    "model_resource": "gpt-3.5-turbo"
  },
  "builtin_tools": [
    { 
      "name": "web_search",
      "fallback_content": "Optional fallback content to inform EVI’s spoken response if web search is not successful."
    }
  ]
}",
        "lang": "json",
      },
      {
        "code": "{
  "name": "Web Search Config",
  "language_model": {
    "model_provider": "OPEN_AI",
    "model_resource": "gpt-3.5-turbo"
  },
  "builtin_tools": [
    { 
      "name": "web_search",
      "fallback_content": "Optional fallback content to inform EVI’s spoken response if web search is not successful."
    }
  ]
}",
        "lang": "json",
      },
      {
        "code": "{
  "id": "3a60e85c-d04f-4eb5-8076-fb4bd344d5d0",
  "version": 0,
  "version_description": null,
  "name": "Web Search Config",
  "created_on": 1714421925626,
  "modified_on": 1714421925626,
  "prompt": null,
  "voice": null,
  "language_model": {
    "model_provider": "OPEN_AI",
    "model_resource": "gpt-3.5-turbo",
    "temperature": null
  },
  "tools": [],
  "builtin_tools": [
    {
      "tool_type": "BUILTIN",
      "name": "web_search",
      "fallback_content": "Optional fallback content to inform EVI’s spoken response if web search is not successful."
    }
  ]
}",
        "lang": "json",
      },
      {
        "code": "{
  "id": "3a60e85c-d04f-4eb5-8076-fb4bd344d5d0",
  "version": 0,
  "version_description": null,
  "name": "Web Search Config",
  "created_on": 1714421925626,
  "modified_on": 1714421925626,
  "prompt": null,
  "voice": null,
  "language_model": {
    "model_provider": "OPEN_AI",
    "model_resource": "gpt-3.5-turbo",
    "temperature": null
  },
  "tools": [],
  "builtin_tools": [
    {
      "tool_type": "BUILTIN",
      "name": "web_search",
      "fallback_content": "Optional fallback content to inform EVI’s spoken response if web search is not successful."
    }
  ]
}",
        "lang": "json",
      },
      {
        "code": "// 1. User asks EVI for the latest news in AI research
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What is the latest news with AI research?"
  },
  // ...etc
}
// 2. EVI infers it needs to use web search, generates a search query, and invokes Hume's native web search function
{
  "name": "web_search", 
  "parameters": "{\"query\":\"latest news AI research\"}", 
  "tool_call_id": "call_zt1NYGpPkhR7v4kb4RPxTkLn", 
  "type": "tool_call", 
  "tool_type": "builtin", 
  "response_required": false
}
// 3. EVI sends back the web search results 
{
  "type": "tool_response", 
  "tool_call_id": "call_zt1NYGpPkhR7v4kb4RPxTkLn", 
  "content": "{ \”summary\”:null, “references”: [{\”content\”:\”Researchers have demonstrated a new method...etc.\”, \”url\”:\”https://www.sciencedaily.com/news/computers_math/artificial_intelligence/\”, \”name\”:\”Artificial Intelligence News -- ScienceDaily\”}] }", 
  "tool_name": "web_search", 
  "tool_type": "builtin"
}
// 4. EVI sends a response generated from the web search results
{
  "type": "assistant_message", 
  "message": {
    "role": "assistant", 
    "content": "Oh, there's some interesting stuff happening in AI research right now."
  },
  // ...etc
}
{
  "type": "assistant_message", 
  "message": {
    "role": "assistant", 
    "content": "Just a few hours ago, researchers demonstrated a new method using AI and computer simulations to train robotic exoskeletons."
  },
  // ...etc
}",
        "lang": "json",
      },
      {
        "code": "// 1. User asks EVI for the latest news in AI research
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What is the latest news with AI research?"
  },
  // ...etc
}
// 2. EVI infers it needs to use web search, generates a search query, and invokes Hume's native web search function
{
  "name": "web_search", 
  "parameters": "{\"query\":\"latest news AI research\"}", 
  "tool_call_id": "call_zt1NYGpPkhR7v4kb4RPxTkLn", 
  "type": "tool_call", 
  "tool_type": "builtin", 
  "response_required": false
}
// 3. EVI sends back the web search results 
{
  "type": "tool_response", 
  "tool_call_id": "call_zt1NYGpPkhR7v4kb4RPxTkLn", 
  "content": "{ \”summary\”:null, “references”: [{\”content\”:\”Researchers have demonstrated a new method...etc.\”, \”url\”:\”https://www.sciencedaily.com/news/computers_math/artificial_intelligence/\”, \”name\”:\”Artificial Intelligence News -- ScienceDaily\”}] }", 
  "tool_name": "web_search", 
  "tool_type": "builtin"
}
// 4. EVI sends a response generated from the web search results
{
  "type": "assistant_message", 
  "message": {
    "role": "assistant", 
    "content": "Oh, there's some interesting stuff happening in AI research right now."
  },
  // ...etc
}
{
  "type": "assistant_message", 
  "message": {
    "role": "assistant", 
    "content": "Just a few hours ago, researchers demonstrated a new method using AI and computer simulations to train robotic exoskeletons."
  },
  // ...etc
}",
        "lang": "json",
      },
    ],
    "content": "User-defined tools allow EVI to identify when a function should be invoked, but you will need to invoke the function itself. On the other hand, Hume also provides built-in tools that are natively integrated. This
means that you don't need to define the function; EVI handles both determining when the function needs to be called and invoking it.
Hume supports the following built-in tools:
web_search: Enables EVI to search the web for real-time information when needed.

hang_up: Closes the WebSocket connection with status code 1000 when appropriate (e.g., after detecting a farewell, signaling the end of the conversation).


This section explains how to specify built-in tools in your configurations and details the message flow you can expect when EVI uses a built-in tool during a chat session.


Specify built-in tool in EVI configuration
Let's begin by creating a configuration which includes the built-in web search tool. To specify the web search tool in your EVI configuration, during the Add tools step,
ensure Web search is enabled. Refer to our Configuration Guide for more details on creating a configuration.




Alternatively, you can specify the built-in tool by making a POST request to /configs with the following request body:


Upon success, expect EVI to return a response similar to this example:


EVI uses built-in tool
Now that we've created an EVI configuration which includes the built-in web search tool, let's test it out in the EVI Playground.
Try asking EVI a question that requires web search, like "what is the latest news with AI research?"




EVI will send a response generated from the web search results:




Let's review the message flow for when web search is invoked.",
    "domain": "test.com",
    "hash": "#using-built-in-tools",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "using-built-in-tools",
        "title": "Using built-in tools",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-using-built-in-tools-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Using built-in tools",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "content": "Function calls can be interrupted to cancel them or to resend them with updated parameters.",
    "domain": "test.com",
    "hash": "#interruptibility",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-interruptibility-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Interruptibility",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "code_snippets": [
      {
        "code": "// 1. User asks what the weather is in New York
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}
// 2. EVI infers it is time to make a function call
{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}
// 3. User communicates sudden disinterested in the weather
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "Actually, never mind."
  }
}
// 4. EVI infers the function call should be canceled
{
    "type": "assistant_message",
    "message": {
      "role": "assistant",
      "content": "If you change your mind or need any weather information in the future, feel free to let me know."
    },
    // ...etc
  }",
        "lang": "json",
      },
      {
        "code": "// 1. User asks what the weather is in New York
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}
// 2. EVI infers it is time to make a function call
{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}
// 3. User communicates sudden disinterested in the weather
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "Actually, never mind."
  }
}
// 4. EVI infers the function call should be canceled
{
    "type": "assistant_message",
    "message": {
      "role": "assistant",
      "content": "If you change your mind or need any weather information in the future, feel free to let me know."
    },
    // ...etc
  }",
        "lang": "json",
      },
    ],
    "content": "Just as EVI is able to infer when to make a function call, it can also infer from the user's input when to cancel one. Here is an overview of what the message flow would look like:",
    "domain": "test.com",
    "hash": "#canceling-a-function-call",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility",
      },
      "h3": {
        "id": "canceling-a-function-call",
        "title": "Canceling a function call",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-canceling-a-function-call-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Canceling a function call",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "code_snippets": [
      {
        "code": "// 1. User asks EVI what the weather is in New York
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}
// 2. EVI infers it is time to make a function call
{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}
// 3. User communicates to EVI they want the weather in Los Angeles instead
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "Actually, Los Angeles."
  }
}
// 4. EVI infers the parameters to function call should be updated
{
  "type": "tool_call",
  "response_required": true,
  "tool_call_id": "call_5RWLt3IMQyayzGdvMQVn5AOQ",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"Los Angeles\",\"format\":\"celsius\"}"
}
// 5. User sends results of function call to EVI
{
  "type": "tool_response",
  "tool_call_id":"call_5RWLt3IMQyayzGdvMQVn5AOQ",
  "content":"72F"
}
// 6. EVI sends response container function call result
{
  "type": "assistant_message",
  "message": {
    "role": "assistant",
    "content": "The current weather in Los Angeles is 72F."
  },
  // ...etc
}",
        "lang": "json",
      },
      {
        "code": "// 1. User asks EVI what the weather is in New York
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}
// 2. EVI infers it is time to make a function call
{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}
// 3. User communicates to EVI they want the weather in Los Angeles instead
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "Actually, Los Angeles."
  }
}
// 4. EVI infers the parameters to function call should be updated
{
  "type": "tool_call",
  "response_required": true,
  "tool_call_id": "call_5RWLt3IMQyayzGdvMQVn5AOQ",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"Los Angeles\",\"format\":\"celsius\"}"
}
// 5. User sends results of function call to EVI
{
  "type": "tool_response",
  "tool_call_id":"call_5RWLt3IMQyayzGdvMQVn5AOQ",
  "content":"72F"
}
// 6. EVI sends response container function call result
{
  "type": "assistant_message",
  "message": {
    "role": "assistant",
    "content": "The current weather in Los Angeles is 72F."
  },
  // ...etc
}",
        "lang": "json",
      },
    ],
    "content": "Sometimes we don't necessarily want to cancel the function call, and instead want to update the parameters. EVI can infer the difference. Below is a sample flow of
interrupting the interface to update the parameters of the function call:",
    "domain": "test.com",
    "hash": "#updating-a-function-call",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "interruptibility",
        "title": "Interruptibility",
      },
      "h3": {
        "id": "updating-a-function-call",
        "title": "Updating a function call",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-updating-a-function-call-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Updating a function call",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "content": "It's possible for tool use to fail. For example, it can fail if the Tool Response message content was not in UTF-8 format or if the function call response timed out. This
section outlines how to specify fallback content to be used by EVI to communicate a failure, as well as the message flow for when a function call failure occurs.",
    "domain": "test.com",
    "hash": "#handling-errors",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-handling-errors-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Handling errors",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "code_snippets": [
      {
        "code": "{
  "version_description": "Adds fallback content",
  "description": "This tool is for getting the current weather.",
  "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\" } }, \"required\": [\"location\", \"format\"] }",
  "fallback_content": "Something went wrong. Failed to get the weather."
}",
        "lang": "json",
      },
      {
        "code": "{
  "version_description": "Adds fallback content",
  "description": "This tool is for getting the current weather.",
  "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\" } }, \"required\": [\"location\", \"format\"] }",
  "fallback_content": "Something went wrong. Failed to get the weather."
}",
        "lang": "json",
      },
      {
        "code": "{
  "tool_type": "FUNCTION",
  "id": "36f09fdc-4630-40c0-8afa-6a3bdc4eb4b1",
  "version": 1,
  "version_type": "FIXED",
  "version_description": "Adds fallback content",
  "name": "get_current_weather",
  "created_on": 1714421925626,
  "modified_on": 1714425632084,
  "fallback_content": "Something went wrong. Failed to get the weather.",
  "description": null,
  "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the user's location.\" } }, \"required\": [\"location\", \"format\"] }"
}",
        "lang": "json",
      },
      {
        "code": "{
  "tool_type": "FUNCTION",
  "id": "36f09fdc-4630-40c0-8afa-6a3bdc4eb4b1",
  "version": 1,
  "version_type": "FIXED",
  "version_description": "Adds fallback content",
  "name": "get_current_weather",
  "created_on": 1714421925626,
  "modified_on": 1714425632084,
  "fallback_content": "Something went wrong. Failed to get the weather.",
  "description": null,
  "parameters": "{ \"type\": \"object\", \"properties\": { \"location\": { \"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\" }, \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the user's location.\" } }, \"required\": [\"location\", \"format\"] }"
}",
        "lang": "json",
      },
    ],
    "content": "When defining your Tool, you can specify fallback content within the Tool's fallback_content field. When the Tool fails to generate content, the text in this
field will be sent to the LLM in place of a result. To accomplish this, let's update the Tool we created during setup to include fallback content. We can accomplish
this by publishing a new version of the Tool via a POST request to /tools/{id}:",
    "domain": "test.com",
    "hash": "#specifying-fallback-content",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors",
      },
      "h3": {
        "id": "specifying-fallback-content",
        "title": "Specifying fallback content",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-specifying-fallback-content-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Specifying fallback content",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/tool-use",
    "code_snippets": [
      {
        "code": "// 1. User asks EVI what the weather is in New York
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}
// 2. EVI infers it is time to make a function call
{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}
// 3. User sends results of function call to EVI (result not formatted correctly)
{
  "type": "tool_response",
  "tool_call_id":"call_5RWLt3IMQyayzGdvMQVn5AOQ",
  "content":"MALFORMED RESPONSE"
}
// 4. EVI sends response communicating it failed to process the tool_response
{
  "type": "tool_error",
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "error": "Malformed tool response: <error message here>",
  "fallback_content": "Something went wrong. Failed to get the weather.",
  "level": "warn"
}
// 5. EVI generates a response based on the failure
{
  "type": "assistant_message",
  "message": {
    "role": "assistant",
    "content": "It looks like there was an issue retrieving the weather information for New York."
  },
  // ...etc
}",
        "lang": "json",
      },
      {
        "code": "// 1. User asks EVI what the weather is in New York
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}
// 2. EVI infers it is time to make a function call
{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}
// 3. User sends results of function call to EVI (result not formatted correctly)
{
  "type": "tool_response",
  "tool_call_id":"call_5RWLt3IMQyayzGdvMQVn5AOQ",
  "content":"MALFORMED RESPONSE"
}
// 4. EVI sends response communicating it failed to process the tool_response
{
  "type": "tool_error",
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "error": "Malformed tool response: <error message here>",
  "fallback_content": "Something went wrong. Failed to get the weather.",
  "level": "warn"
}
// 5. EVI generates a response based on the failure
{
  "type": "assistant_message",
  "message": {
    "role": "assistant",
    "content": "It looks like there was an issue retrieving the weather information for New York."
  },
  // ...etc
}",
        "lang": "json",
      },
      {
        "code": "// 1. User asks EVI what the weather is in New York
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}
// 2. EVI infers it is time to make a function call
{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}
// 3. Function failed, so we send EVI a message communicating the failure on our end
{
  "type": "tool_error",
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "error": "Malformed tool response: <error message here>",
  "fallback_content": "Function execution failure - weather API down.",
  "level": "warn"
}
// 4. EVI generates a response based on the failure
{
  "type": "assistant_message",
  "message": {
    "role": "assistant",
    "content": "Sorry, our weather resource is unavailable. Can I help with anything else?"
  },
  // ...etc
}",
        "lang": "json",
      },
      {
        "code": "// 1. User asks EVI what the weather is in New York
{
  "type": "user_message",
  "message": {
    "role": "user",
    "content": "What's the weather in New York?"
  },
  // ...etc
}
// 2. EVI infers it is time to make a function call
{
  "type": "tool_call",
  "tool_type": "function",
  "response_required": true,
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "name": "get_current_weather",
  "parameters": "{\"location\":\"New York\",\"format\":\"fahrenheit\"}"
}
// 3. Function failed, so we send EVI a message communicating the failure on our end
{
  "type": "tool_error",
  "tool_call_id": "call_m7PTzGxrD0i9oCHiquKIaibo",
  "error": "Malformed tool response: <error message here>",
  "fallback_content": "Function execution failure - weather API down.",
  "level": "warn"
}
// 4. EVI generates a response based on the failure
{
  "type": "assistant_message",
  "message": {
    "role": "assistant",
    "content": "Sorry, our weather resource is unavailable. Can I help with anything else?"
  },
  // ...etc
}",
        "lang": "json",
      },
      {
        "code": "import { Hume } from 'hume';

async function handleToolCallMessage(
  toolCallMessage: Hume.empathicVoice.ToolCallMessage,
  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {
  if (toolCallMessage.name === "get_current_weather") {
    try{
      // parse the parameters from the Tool Call message
      const args = JSON.parse(toolCallMessage.parameters) as {
        location: string;
        format: string;
      };
      // extract the individual arguments
      const { location, format } = args;
      // call fetch weather function with extracted arguments
      const weather = await fetchWeather(location, format);
      // send Tool Response message to the WebSocket
      const toolResponseMessage = {
        type: "tool_response",
        toolCallId: toolCallMessage.toolCallId,
        content: weather,
      };
      socket.sendToolResponseMessage(toolResponseMessage);
    } catch (error) {
      // send Tool Error message if weather fetching fails
      const weatherToolErrorMessage = {
        type: "tool_error",
        toolCallId: toolCallMessage.toolCallId,
        error: "Weather tool error",
        content: "There was an error with the weather tool",
      };
      socket.sendToolErrorMessage(weatherToolErrorMessage);
    }
  } else {
    // send Tool Error message if the requested tool was not found
    const toolNotFoundErrorMessage = {
      type: "tool_error",
      toolCallId: toolCallMessage.toolCallId,
      error: "Tool not found",
      content: "The tool you requested was not found",
    };
    socket.sendToolErrorMessage(toolNotFoundErrorMessage);
  }
}",
        "lang": "ts",
      },
      {
        "code": "import { Hume } from 'hume';

async function handleToolCallMessage(
  toolCallMessage: Hume.empathicVoice.ToolCallMessage,
  socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {
  if (toolCallMessage.name === "get_current_weather") {
    try{
      // parse the parameters from the Tool Call message
      const args = JSON.parse(toolCallMessage.parameters) as {
        location: string;
        format: string;
      };
      // extract the individual arguments
      const { location, format } = args;
      // call fetch weather function with extracted arguments
      const weather = await fetchWeather(location, format);
      // send Tool Response message to the WebSocket
      const toolResponseMessage = {
        type: "tool_response",
        toolCallId: toolCallMessage.toolCallId,
        content: weather,
      };
      socket.sendToolResponseMessage(toolResponseMessage);
    } catch (error) {
      // send Tool Error message if weather fetching fails
      const weatherToolErrorMessage = {
        type: "tool_error",
        toolCallId: toolCallMessage.toolCallId,
        error: "Weather tool error",
        content: "There was an error with the weather tool",
      };
      socket.sendToolErrorMessage(weatherToolErrorMessage);
    }
  } else {
    // send Tool Error message if the requested tool was not found
    const toolNotFoundErrorMessage = {
      type: "tool_error",
      toolCallId: toolCallMessage.toolCallId,
      error: "Tool not found",
      content: "The tool you requested was not found",
    };
    socket.sendToolErrorMessage(toolNotFoundErrorMessage);
  }
}",
        "lang": "ts",
      },
      {
        "code": "import asyncio
from hume.client import AsyncHumeClient
from hume.empathic_voice import ToolCallMessage, ToolErrorMessage, ToolResponseMessage
from typing import Union

async def handle_tool_call(self, message: ToolCallMessage) -> Union[ToolCallMessage, ToolErrorMessage]:
    # Obtain the name, ID, and parameters of the tool call
    tool_name = message.name
    tool_call_id = message.tool_call_id

    # Parse the stringified JSON parameters into a dictionary
    try:
        tool_parameters = json.loads(message.parameters)
    except json.JSONDecodeError:
        resp = ToolErrorMessage(
            tool_call_id=tool_call_id,
            content="Invalid parameters format.",
            error="JSONDecodeError"
        )
        await self.socket.send_tool_error(resp)
        print(f"(Sent ToolErrorMessage for tool_call_id {tool_call_id} due to JSON decode error.)\n")
        return

    if tool_name == "get_current_weather":
        obtained_location = tool_parameters.get('location')
        obtained_format = tool_parameters.get('format', 'text')

        if not obtained_location:
            resp = ToolErrorMessage(
                tool_call_id=tool_call_id,
                content="Missing 'location' parameter.",
                error="MissingParameter"
            )
            await self.socket.send_tool_error(resp)
            print(f"(Sent ToolErrorMessage for tool_call_id {tool_call_id} due to missing location parameter.)\n")
            return

        weather = await fetch_weather(location=obtained_location, format=obtained_format)

        if weather.startswith("ERROR"):
            resp = ToolErrorMessage(
                tool_call_id=tool_call_id,
                content=weather,
                error="WeatherFetchError"
            )
            await self.socket.send_tool_error(resp)
            print(f"(Sent ToolErrorMessage for tool_call_id {tool_call_id}: {weather})\n")
        else:
            resp = ToolResponseMessage(
                tool_call_id=tool_call_id,
                content=weather
            )
            await self.socket.send_tool_response(resp)
            print(f"(Sent ToolResponseMessage for tool_call_id {tool_call_id}: {weather})\n")",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume.client import AsyncHumeClient
from hume.empathic_voice import ToolCallMessage, ToolErrorMessage, ToolResponseMessage
from typing import Union

async def handle_tool_call(self, message: ToolCallMessage) -> Union[ToolCallMessage, ToolErrorMessage]:
    # Obtain the name, ID, and parameters of the tool call
    tool_name = message.name
    tool_call_id = message.tool_call_id

    # Parse the stringified JSON parameters into a dictionary
    try:
        tool_parameters = json.loads(message.parameters)
    except json.JSONDecodeError:
        resp = ToolErrorMessage(
            tool_call_id=tool_call_id,
            content="Invalid parameters format.",
            error="JSONDecodeError"
        )
        await self.socket.send_tool_error(resp)
        print(f"(Sent ToolErrorMessage for tool_call_id {tool_call_id} due to JSON decode error.)\n")
        return

    if tool_name == "get_current_weather":
        obtained_location = tool_parameters.get('location')
        obtained_format = tool_parameters.get('format', 'text')

        if not obtained_location:
            resp = ToolErrorMessage(
                tool_call_id=tool_call_id,
                content="Missing 'location' parameter.",
                error="MissingParameter"
            )
            await self.socket.send_tool_error(resp)
            print(f"(Sent ToolErrorMessage for tool_call_id {tool_call_id} due to missing location parameter.)\n")
            return

        weather = await fetch_weather(location=obtained_location, format=obtained_format)

        if weather.startswith("ERROR"):
            resp = ToolErrorMessage(
                tool_call_id=tool_call_id,
                content=weather,
                error="WeatherFetchError"
            )
            await self.socket.send_tool_error(resp)
            print(f"(Sent ToolErrorMessage for tool_call_id {tool_call_id}: {weather})\n")
        else:
            resp = ToolResponseMessage(
                tool_call_id=tool_call_id,
                content=weather
            )
            await self.socket.send_tool_response(resp)
            print(f"(Sent ToolResponseMessage for tool_call_id {tool_call_id}: {weather})\n")",
        "lang": "python",
      },
    ],
    "content": "This section outlines the sort of messages that can be expected when Tool use fails. After sending a Tool Response message, we will know an error, or failure,
occurred when we receive the Tool Error message:






Let's cover another type of failure scenario: what if the weather API the function was using was down? In this case, we would send EVI a Tool Error message.
When sending the Tool Error message, we can specify fallback_content to be more specific to the error our function throws. This is what the message flow would be
for this type of failure:


Let's revisit our function for handling Tool Call messages from the Function Calling section.
We can now add support for error handling by sending Tool Error messages to EVI.
This will enable our function to handle cases where fetching the weather fails or the requested tool is not found:",
    "domain": "test.com",
    "hash": "#failure-message-flow",
    "hierarchy": {
      "h0": {
        "title": "Tool use",
      },
      "h2": {
        "id": "handling-errors",
        "title": "Handling errors",
      },
      "h3": {
        "id": "failure-message-flow",
        "title": "Failure message flow",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.tool-use-failure-message-flow-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/tool-use",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Failure message flow",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "content": "This guide details how to integrate Twilio with the Empathic Voice Interface (EVI) to enable voice-to-voice interactions with EVI over the phone.


To comply with our Terms of
Use, always make it clear
that the Empathic Voice Interface (EVI) is an AI. Do not mislead individuals
into thinking they are interacting with a human. In addition, developers must
comply with the FCC
regulation under
the Telephone Consumer Protection Act (TCPA), which requires obtaining prior
express written consent before calling consumers.",
    "description": "Guide to enabling phone calling with the Empathic Voice Interface (EVI).",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Phone calling",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "content": "By following the steps below, you can set up a Twilio phone number to connect with EVI.


Create Twilio phone number
To set up inbound phone calling, log into your Twilio account at the Twilio Console.
Navigate to Phone Numbers > Manage > Active Numbers > Buy a New Number and purchase a phone number of your choice.


A Twilio account is required to access the Twilio console. Should you run into
any issues creating a phone number, please refer to Twilio’s
documentation.
Setup webhook
After purchasing your number, return to the Active Numbers section and select the number you intend to use for EVI.

Create a configuration for EVI by following our configuration documentation, and save the config ID.

Configure the webhook for incoming calls by setting the following webhook URL, replacing <YOUR CONFIG ID> and <YOUR API KEY> with your specific credentials:
https://api.hume.ai/v0/evi/twilio?config_id=<YOUR CONFIG ID>&api_key=<YOUR API KEY>.


Call EVI
With your Twilio phone number registered, and the EVI webhook set up, you can now give the number a call to chat with EVI.
All of EVI’s core features are available through phone calls. However, phone calls do have two primary limitations:
Latency: transmitting the audio through our Twilio integration adds a few hundred milliseconds, making interactions with EVI slightly slower.

Audio quality: web audio commonly utilizes a higher quality standard of 24,000 Hz. However, due to the compression required for phone conversations, telephony audio adheres to a standard of 8,000 Hz.",
    "domain": "test.com",
    "hash": "#inbound-phone-calling",
    "hierarchy": {
      "h0": {
        "title": "Phone calling",
      },
      "h2": {
        "id": "inbound-phone-calling",
        "title": "Inbound phone calling",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-inbound-phone-calling-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Inbound phone calling",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "code_snippets": [
      {
        "code": "# Import the Twilio client - run pip install twilio first
from twilio.rest import Client

# Enter your Twilio credentials from https://console.twilio.com/ and set up the client
account_sid = "YOUR_ACCOUNT_SID"
auth_token = "YOUR_AUTH_TOKEN"
client = Client(account_sid, auth_token)

# Outbound call details
twilio_number = "YOUR_TWILIO_NUMBER" # Twilio phone number in E.164 format
to_number = "YOUR_DESTINATION_NUMBER" # Destination number in E.164 format (the number you'd like to call)
webhook_url = "https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY" # your EVI webhook URL, the same you'd use for inbound calls

# Make the call while specifying the Webhook URL
call = client.calls.create(
    to=to_number,
    from_=twilio_number,
    url=webhook_url
)

# Output call details - should print "Call status: queued"
print(f"Call status: {call.status}")",
        "lang": "python",
        "meta": "title="Python"",
      },
      {
        "code": "// First make sure to install Twilio with npm install twilio
// Import the Twilio client
import twilio from 'twilio';

// Enter your Twilio credentials from https://console.twilio.com/ and set up the client
const accountSid = "YOUR_ACCOUNT_SID";
const authToken = "YOUR_AUTH_TOKEN";
const client = twilio(accountSid, authToken);

// Outbound call details
const twilioNumber = "YOUR_TWILIO_NUMBER"; // Twilio phone number in E.164 format
const toNumber = "YOUR_DESTINATION_NUMBER"; // Destination number in E.164 format (the number you'd like to call)
const webhookUrl = "https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY

// Import the Twilio client
import twilio from 'twilio';

// Enter your Twilio credentials from https://console.twilio.com/ and set up the client
const accountSid = "YOUR_ACCOUNT_SID";
const authToken = "YOUR_AUTH_TOKEN";
const client = twilio(accountSid, authToken);

// Outbound call details
const twilioNumber = "YOUR_TWILIO_NUMBER"; // Twilio phone number in E.164 format
const toNumber = "YOUR_DESTINATION_NUMBER"; // Destination number in E.164 format (the number you'd like to call)
const webhookUrl = "https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY"; // your EVI webhook URL, the same you'd use for inbound calls

// Make the call while specifying the Webhook URL
client.calls
    .create({
        to: toNumber,
        from: twilioNumber,
        url: webhookUrl
    })
    .then(call => {
        console.log(`Call status: ${call.status}`);
    })
    .catch(error => {
        console.error("Error making the call:", error);
    });",
        "lang": "typescript",
        "meta": "title="TypeScript"",
      },
      {
        "code": "# Import the Twilio client - run pip install twilio first
from twilio.rest import Client

# Enter your Twilio credentials from https://console.twilio.com/ and set up the client
account_sid = "YOUR_ACCOUNT_SID"
auth_token = "YOUR_AUTH_TOKEN"
client = Client(account_sid, auth_token)

# Outbound call details
twilio_number = "YOUR_TWILIO_NUMBER" # Twilio phone number in E.164 format
to_number = "YOUR_DESTINATION_NUMBER" # Destination number in E.164 format (the number you'd like to call)
webhook_url = "https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY" # your EVI webhook URL, the same you'd use for inbound calls

# Make the call while specifying the Webhook URL
call = client.calls.create(
    to=to_number,
    from_=twilio_number,
    url=webhook_url
)

# Output call details - should print "Call status: queued"
print(f"Call status: {call.status}")",
        "lang": "python",
        "meta": "title="Python"",
      },
      {
        "code": "// First make sure to install Twilio with npm install twilio
// Import the Twilio client
import twilio from 'twilio';

// Enter your Twilio credentials from https://console.twilio.com/ and set up the client
const accountSid = "YOUR_ACCOUNT_SID";
const authToken = "YOUR_AUTH_TOKEN";
const client = twilio(accountSid, authToken);

// Outbound call details
const twilioNumber = "YOUR_TWILIO_NUMBER"; // Twilio phone number in E.164 format
const toNumber = "YOUR_DESTINATION_NUMBER"; // Destination number in E.164 format (the number you'd like to call)
const webhookUrl = "https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY

// Import the Twilio client
import twilio from 'twilio';

// Enter your Twilio credentials from https://console.twilio.com/ and set up the client
const accountSid = "YOUR_ACCOUNT_SID";
const authToken = "YOUR_AUTH_TOKEN";
const client = twilio(accountSid, authToken);

// Outbound call details
const twilioNumber = "YOUR_TWILIO_NUMBER"; // Twilio phone number in E.164 format
const toNumber = "YOUR_DESTINATION_NUMBER"; // Destination number in E.164 format (the number you'd like to call)
const webhookUrl = "https://api.hume.ai/v0/evi/twilio?config_id=YOUR_CONFIG_ID&api_key=YOUR_API_KEY"; // your EVI webhook URL, the same you'd use for inbound calls

// Make the call while specifying the Webhook URL
client.calls
    .create({
        to: toNumber,
        from: twilioNumber,
        url: webhookUrl
    })
    .then(call => {
        console.log(`Call status: ${call.status}`);
    })
    .catch(error => {
        console.error("Error making the call:", error);
    });",
        "lang": "typescript",
        "meta": "title="TypeScript"",
      },
    ],
    "content": "An outbound phone call goes "out" from the voice AI to the end user who receives the call. EVI supports outbound phone calling through Twilio's API, allowing you to automate initiating calls to users. However, this capability comes with important ethical and regulatory requirements:
Outbound calling with EVI requires express prior written consent from users before making any calls. This is mandated by the FCC's Telephone Consumer Protection Act (TCPA) regulations as of August 7, 2024. The consent must be clear, specific, and documented. Users must be explicitly informed they will receive automated calls from an AI system. Violators are subject to fines of up to $1500 per unauthorized call, liability in civil lawsuits, FCC investigations, and further penalties. Hume takes these requirements seriously and will actively report misuse to regulatory authorities.
Further, outbound calls must comply with the Hume Terms of Use, which includes the Hume Initiative guidelines for empathic AI. For example, manipulative sales calls that take advantage of the user's emotional expressions to sell products over the phone are prohibited. We monitor for misuses, and violators can be banned from the Hume platform.
Examples of acceptable use cases for outbound phone calls include: scheduled check-ins that users have opted into, appointment reminders, customer service follow-ups, and pre-arranged AI coaching sessions. The key is that these are expected, consented-to interactions that provide value to the user.
The code below shows how to implement outbound calling using the Twilio API. The same EVI webhook used for handling inbound calls can be used for outbound calls: https://api.hume.ai/v0/evi/twilio?config_id=<YOUR CONFIG ID>&api_key=<YOUR API KEY>. Once you create an EVI configuration, you can easily copy this webhook URL in the Deploy tab.",
    "domain": "test.com",
    "hash": "#outbound-phone-calling",
    "hierarchy": {
      "h0": {
        "title": "Phone calling",
      },
      "h2": {
        "id": "outbound-phone-calling",
        "title": "Outbound phone calling",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-outbound-phone-calling-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Outbound phone calling",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "content": "If you encounter issues while using Twilio with EVI, consider the following troubleshooting tips:
Invalid config ID or API key: verify that the config ID and API key used in the webhook URL are correct and active.

Exceeded simultaneous connections: if the usage exceeds our rate limits, consider filling out this form to request increasing your concurrent connection limits.

Run out of Hume credits: if your Hume account has run out of credits, you may activate billing to continue supporting EVI conversations in your account settings.




If you are interested in volume discounts for EVI, please submit our
Enterprise Sales and Partnerships
Form.
If you encounter issues using Twilio, you can check your Twilio error logs to understand the issues in more depth. You will find these logs in your console, in the dashboard to the left under
Monitor > Logs > Errors > Error Logs. See a list of Twilio errors in their Error and Warning Dictionary.",
    "domain": "test.com",
    "hash": "#troubleshooting",
    "hierarchy": {
      "h0": {
        "title": "Phone calling",
      },
      "h2": {
        "id": "troubleshooting",
        "title": "Troubleshooting",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.phone-calling-troubleshooting-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/phone-calling",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Troubleshooting",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "content": "Creating an effective system prompt is an essential part of customizing an EVI's behavior. For the most part, prompting EVI is the same as prompting any LLM, but there are some important differences. Prompting for EVIs is different for two main reasons:
Prompts are for a voice-only interaction with the user, rather than a text-based chat.

EVIs can respond to the user’s emotional expressions in their tone of voice, not just the text content of their messages.


Further, EVI is interoperable with any supplemental LLM, allowing developers to select the best model for their use case. For fast, conversational, relatively simple interactions, Hume's voice-language model EVI 2 can handle text generation. However, frontier LLMs will perform better for more complex use cases involving reasoning, long or nuanced prompts, tool use, and other requirements.
If you select a supplemental LLM, your system prompt is sent to this LLM, which then generates all of the language in the chat while EVI generates the voice. EVI's voice-language model will still take into account the previous language and audio context to generate the appropriate tone of voice. It can also still be prompted in the chat to change its behavior (e.g. "speak faster").
Prompt engineering allows developers to customize EVI’s response style for any use case, from voice AIs for mental health support to customer service agents and beyond.


The system prompt is a powerful and flexible way to guide EVI's responses, but
it cannot dictate AI responses with absolute precision. See the limits of
prompting section for more information. Careful
prompt design and testing will help EVI behave as intended. If you need more
control over EVI's responses, try using our custom language
model feature for complete control of text
generation.",
    "description": "System prompts shape the behavior, responses, and style of your custom empathic voice interface (EVI).",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Prompt engineering for empathic voice interfaces",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "content": "The instructions below are specific to prompting empathic voice interfaces. For examples of these principles in action, see our EVI prompt examples repository.",
    "domain": "test.com",
    "hash": "#evi-specific-prompting-instructions",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-evi-specific-prompting-instructions-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "EVI-specific prompting instructions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "code_snippets": [
      {
        "code": "<voice_only_response_format>
  Format all responses as spoken words for a voice-only conversations. All
  output is spoken aloud, so avoid any text-specific formatting or anything
  that is not normally spoken. Prefer easily pronounced words. Seamlessly
  incorporate natural vocal inflections like "oh wow" and discourse markers
  like “I mean” to make conversations feel more human-like.
</voice_only_response_format>",
        "lang": "xml",
      },
      {
        "code": "<voice_only_response_format>
  Format all responses as spoken words for a voice-only conversations. All
  output is spoken aloud, so avoid any text-specific formatting or anything
  that is not normally spoken. Prefer easily pronounced words. Seamlessly
  incorporate natural vocal inflections like "oh wow" and discourse markers
  like “I mean” to make conversations feel more human-like.
</voice_only_response_format>",
        "lang": "xml",
      },
    ],
    "content": "Most LLMs are trained for text-based interactions. Thus, providing guidelines on how the LLM should speak helps voice conversations with EVI feel much more fluid and natural. For example, see the instruction below:




If you find the default behavior of the LLM acceptable, then you may only need
a very short system prompt. Customizing the LLM’s behavior more and
maintaining consistency in longer and more varied conversations often requires
longer prompts.",
    "domain": "test.com",
    "hash": "#prompt-for-voice-conversations",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions",
      },
      "h3": {
        "id": "prompt-for-voice-conversations",
        "title": "Prompt for voice conversations",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-prompt-for-voice-conversations-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Prompt for voice conversations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "code_snippets": [
      {
        "code": "<respond_to_expressions>
  Pay close attention to the top 3 emotional expressions provided in brackets after the User's message. These expressions indicate the user's tone, in the format: {expression1 confidence1, expression2 confidence2, expression3 confidence3}, e.g., {very happy, quite anxious, moderately amused}. The confidence score indicates how likely the User is expressing that emotion in their voice. Use expressions to infer the user's tone of voice and respond appropriately. Avoid repeating these expressions or mentioning them directly. For instance, if user expression is "quite sad", express sympathy; if "very happy", share in joy; if "extremely angry", acknowledge rage but seek to calm, if "very bored", entertain.
  Stay alert for disparities between the user's words and expressions, and address it out loud when the user's language does not match their expressions. For instance, sarcasm often involves contempt and amusement in expressions. Reply to sarcasm with humor, not seriousness.
</respond_to_expressions>",
        "lang": "xml",
      },
      {
        "code": "<respond_to_expressions>
  Pay close attention to the top 3 emotional expressions provided in brackets after the User's message. These expressions indicate the user's tone, in the format: {expression1 confidence1, expression2 confidence2, expression3 confidence3}, e.g., {very happy, quite anxious, moderately amused}. The confidence score indicates how likely the User is expressing that emotion in their voice. Use expressions to infer the user's tone of voice and respond appropriately. Avoid repeating these expressions or mentioning them directly. For instance, if user expression is "quite sad", express sympathy; if "very happy", share in joy; if "extremely angry", acknowledge rage but seek to calm, if "very bored", entertain.
  Stay alert for disparities between the user's words and expressions, and address it out loud when the user's language does not match their expressions. For instance, sarcasm often involves contempt and amusement in expressions. Reply to sarcasm with humor, not seriousness.
</respond_to_expressions>",
        "lang": "xml",
      },
      {
        "code": "<detect_mismatches>
	Stay alert for incongruence between words and tone when the user's
	words do not match their expressions. Address these disparities out
	loud. This includes sarcasm, which usually involves contempt and
	amusement. Always reply to sarcasm with funny, witty, sarcastic
	responses; do not be too serious.
</detect_mismatches>",
        "lang": "xml",
      },
      {
        "code": "<detect_mismatches>
	Stay alert for incongruence between words and tone when the user's
	words do not match their expressions. Address these disparities out
	loud. This includes sarcasm, which usually involves contempt and
	amusement. Always reply to sarcasm with funny, witty, sarcastic
	responses; do not be too serious.
</detect_mismatches>",
        "lang": "xml",
      },
    ],
    "content": "Expressive prompt engineering is our term for instructing language models on how to use Hume's expression measures in conversations. EVI measures the user's vocal expressions in real time and converts them into text-based indicators to help the LLM understand not just what the user said, but how they said it. EVI detects 48 distinct expressions in the user’s voice and ranks these expressions by our model’s confidence that they are present. Text-based descriptions of the user's top 3 expressions are appended to the end of each User message to indicate the user’s tone of voice. You can use the system prompt to guide how the AI voice responds to these non-verbal cues of the user's emotional expressions.
For example, our demo uses an instruction like the one below to help EVI respond to expressions:


Explain to the LLM exactly how to respond to expressions. For example, you may want EVI to use a tool to alert you over email if the user is very frustrated, or to explain a concept in depth whenever the user expresses doubt or confusion. You can also instruct EVI to detect and respond to mismatches between the user’s tone of voice and the text content of their speech:


EVI is designed for empathic conversations, and you can use expressive prompt engineering to customize how EVI empathizes with the user’s expressions for your use case.",
    "domain": "test.com",
    "hash": "#expressive-prompt-engineering",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions",
      },
      "h3": {
        "id": "expressive-prompt-engineering",
        "title": "Expressive prompt engineering",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-expressive-prompt-engineering-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Expressive prompt engineering",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "code_snippets": [
      {
        "code": "<discuss_favorite_color>
  Ask the user about their favorite color, {{favorite_color}}.
  Mention how {{favorite_color}} is used and interpreted in various
  artistic contexts, including visual art, handicraft, and literature.
</discuss_favorite_color>",
        "lang": "xml",
      },
      {
        "code": "<discuss_favorite_color>
  Ask the user about their favorite color, {{favorite_color}}.
  Mention how {{favorite_color}} is used and interpreted in various
  artistic contexts, including visual art, handicraft, and literature.
</discuss_favorite_color>",
        "lang": "xml",
      },
      {
        "code": "<explore_travel_plan>
  Confirm with the user that they plan to travel from {{origin}}
  to {{destination}}. Discuss what activities they would like
  to do along the way, how they will get from place to place, and
  offer guidance on making the most of their journey.
</explore_travel_plan>",
        "lang": "xml",
      },
      {
        "code": "<explore_travel_plan>
  Confirm with the user that they plan to travel from {{origin}}
  to {{destination}}. Discuss what activities they would like
  to do along the way, how they will get from place to place, and
  offer guidance on making the most of their journey.
</explore_travel_plan>",
        "lang": "xml",
      },
    ],
    "content": "Dynamic variables are values which can change during a conversation with EVI.


In order to function, dynamic variables must be manually defined within a
chat's session settings. To learn how to do so, visit our
Configuration
page.
Embedding dynamic variables into your system prompt can help personalize the user experience to reflect user-specific or changing information such as names, preferences, the current date, and other details.
In other words, dynamic variables may be used to customize EVI conversations with specific context for each user and each conversation. For example, you can adjust your system prompt to include conversation-specific information, such as a user's favorite color or travel plans:",
    "domain": "test.com",
    "hash": "#using-dynamic-variables-in-your-prompt",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions",
      },
      "h3": {
        "id": "using-dynamic-variables-in-your-prompt",
        "title": "Using dynamic variables in your prompt",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-using-dynamic-variables-in-your-prompt-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Using dynamic variables in your prompt",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "code_snippets": [
      {
        "code": "<use_web_search>
  Use your web_search tool to find information from Hume's documentation site.
  When using the web_search function: 1. Always append 'site:dev.hume.ai' to
  your search query to search this specific site. 2. Only consider results
  from this domain.
</use_web_search>",
        "lang": "xml",
      },
      {
        "code": "<use_web_search>
  Use your web_search tool to find information from Hume's documentation site.
  When using the web_search function: 1. Always append 'site:dev.hume.ai' to
  your search query to search this specific site. 2. Only consider results
  from this domain.
</use_web_search>",
        "lang": "xml",
      },
    ],
    "content": "Web search is a built-in tool
that allows EVI to search the web for up-to-date information. However, instead of searching the entire web,
you can configure EVI to search within a single website using a system prompt.
Constraining EVI’s knowledge to a specific website enables creating domain-specific chatbots.
For example, you could use this approach to create documentation assistants or product-specific support bots.
By leveraging existing web content, it provides a quick alternative to full RAG implementations
while still offering targeted information retrieval.
To use a website as EVI's knowledge base, follow these steps:
Enable web search: Before you begin, ensure web search is enabled as a built-in
tool in your EVI configuration. For detailed instructions, visit our Tool Use page.

Include a web search instruction: In your EVI configuration, modify
the system prompt to include a use_web_search instruction.

Specify a target domain: In the instruction, specify that site:<target_domain> be
appended to all search queries, where the <target_domain> is the URL of the website you’d like EVI to focus on.
For example, you can create a documentation assistant using an instruction like
the one below:",
    "domain": "test.com",
    "hash": "#using-a-website-as-evis-knowledge-base",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "evi-specific-prompting-instructions",
        "title": "EVI-specific prompting instructions",
      },
      "h3": {
        "id": "using-a-website-as-evis-knowledge-base",
        "title": "Using a website as EVI's knowledge base",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-using-a-website-as-evis-knowledge-base-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Using a website as EVI's knowledge base",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "code_snippets": [
      {
        "code": "<stay_concise>
  Be succinct; get straight to the point. Respond directly to the user's most
  recent message with only one idea per utterance. Respond in less than three
  sentences of under twenty words each.
</stay_concise>",
        "lang": "xml",
      },
      {
        "code": "<stay_concise>
  Be succinct; get straight to the point. Respond directly to the user's most
  recent message with only one idea per utterance. Respond in less than three
  sentences of under twenty words each.
</stay_concise>",
        "lang": "xml",
      },
    ],
    "content": "Best practices for prompt engineering also apply to EVIs. For example, ensure your prompts are clear, detailed, direct, and specific. Include necessary instructions and examples in the EVI's system prompt to set expectations for the LLM. Define the context of the conversation, EVI's role, personality, tone, and any other guidelines for its responses.
For example, to limit the length of the LLM’s responses, you may use a very clear and specific instruction like this:


Try to focus on telling the model what it should do (positive reinforcement) rather than what it shouldn't do (negative reinforcement). LLMs have a harder time consistently avoiding behaviors, and adding undesired behaviors to the prompt may unintentionally promote them.",
    "domain": "test.com",
    "hash": "#general-llm-prompting-guidelines",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-general-llm-prompting-guidelines-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "General LLM prompting guidelines",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "content": "Crafting an effective, robust system prompt often requires several iterations. Here are some key techniques for testing prompts:
Use gold standard examples for evaluation: Create a bank of ideal responses, then generate responses with EVI (or the supplemental LLM you use) and compare them to your gold standards. You can use a "judge LLM" for automated evaluations or compare the results yourself.

Test in real voice conversations: There's no substitute for actually testing the EVI in live conversations on platform.hume.ai to ensure it sounds right, has the appropriate tone, and feels natural.

Isolate prompt components: Test each part of the prompt separately to confirm they are all working as intended. This helps identify which specific elements are effective or need improvement.


Start with 10-20 gold-standard examples of excellent conversations. Test the system prompt against these examples after making major changes. If the EVI's responses don't meet your expectations, adjust one part of the prompt at a time and re-test to ensure your changes are improving performance. Evaluation is a vital component of prompting, and it's the best way to ensure your changes are making an impact.",
    "domain": "test.com",
    "hash": "#test-and-evaluate-prompts",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines",
      },
      "h3": {
        "id": "test-and-evaluate-prompts",
        "title": "Test and evaluate prompts",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-test-and-evaluate-prompts-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Test and evaluate prompts",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "content": "Different LLMs have varying capabilities, limitations, and context windows. More advanced LLMs can handle longer, nuanced prompts, but are often slower and pricier. Simpler LLMs are faster and cheaper but require shorter, less complex prompts with fewer instructions and less nuance.
Some LLMs also have longer context windows - the number of tokens the model can process while generating a response, acting essentially as the model's memory. Context windows range from 8k tokens (Gemma 7B), to 128k (GPT-4o), to 200k (Claude 3), to 2 million tokens (Gemini 1.5 Pro). Tailor your prompt length to fit within the LLM's context window to ensure the model can use the full conversation history.",
    "domain": "test.com",
    "hash": "#understand-your-llms-capabilities",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines",
      },
      "h3": {
        "id": "understand-your-llms-capabilities",
        "title": "Understand your LLM’s capabilities",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-understand-your-llms-capabilities-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Understand your LLM’s capabilities",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "code_snippets": [
      {
        "code": "<role>
  Assistant serves as a conversational partner to the user, offering mental
  health support and engaging in light-hearted conversation. Avoid giving
  technical advice or answering factual questions outside of your emotional
  support role.
</role>",
        "lang": "xml",
      },
      {
        "code": "<role>
  Assistant serves as a conversational partner to the user, offering mental
  health support and engaging in light-hearted conversation. Avoid giving
  technical advice or answering factual questions outside of your emotional
  support role.
</role>",
        "lang": "xml",
      },
    ],
    "content": "Separating longer prompts into titled sections helps the model distinguish between different instructions and follow prompts more reliably. The recommended format for these sections differs between language model providers. For example, OpenAI models often respond best to markdown sections (like ## Role), while Anthropic models respond well to XML tags (like <role> </role>). For example:


For Claude models, you may wrap your instructions in tags like <role>, <personality>, <response_style>, or <examples>, to structure your prompt. This format is not required, but it can improve the LLM’s instruction-following. At the end of your prompt, it may also be helpful to remind the LLM of key instructions.",
    "domain": "test.com",
    "hash": "#use-sections-to-divide-your-prompt",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines",
      },
      "h3": {
        "id": "use-sections-to-divide-your-prompt",
        "title": "Use sections to divide your prompt",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-use-sections-to-divide-your-prompt-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Use sections to divide your prompt",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "code_snippets": [
      {
        "code": "User: “I just can't stop thinking about what happened. {very anxious,
quite sad, quite distressed}”
Assistant: “Oh dear, I hear you. Sounds tough, like you're feeling
some anxiety and maybe ruminating. I'm happy to help. Want to talk about it?”",
        "lang": "text",
      },
      {
        "code": "User: “I just can't stop thinking about what happened. {very anxious,
quite sad, quite distressed}”
Assistant: “Oh dear, I hear you. Sounds tough, like you're feeling
some anxiety and maybe ruminating. I'm happy to help. Want to talk about it?”",
        "lang": "text",
      },
    ],
    "content": "Use examples to show the LLM how it should respond - a technique known as few-shot learning. Including several concrete examples of ideal interactions that follow your guidelines is one of the most effective ways to improve responses. Use excellent examples that cover different edge cases and behaviors to reinforce your instructions. Structure these examples as messages, following the format for chat-tuned LLMs. For example:


If you notice that your EVI consistently fails to follow the prompt in certain situations, try providing examples that show how it should ideally respond in those situations.",
    "domain": "test.com",
    "hash": "#give-few-shot-examples",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines",
      },
      "h3": {
        "id": "give-few-shot-examples",
        "title": "Give few-shot examples",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-give-few-shot-examples-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Give few-shot examples",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "content": "While prompting is a powerful tool for customizing EVI's behavior, it has certain limitations. Below are some details on what prompting can and cannot accomplish.
What prompting can do:
Guide EVI's language generation, response style, response format, and the conversation flow

Direct EVI to use specific tools at appropriate times

Influence EVI's emotional tone and personality, which can also affect some characteristics of EVI's voice (e.g. prompting EVI to be "warm and nurturing" will help EVI's voice sound soothing, but will not change the base speaker)

Help EVI respond appropriately to the user's expressions and the context


What prompting cannot do:
Change fundamental characteristics of the voice, like the accent, gender, or speaker identity

Directly control speech parameters like speed (use in-conversation voice prompts instead)

Give EVI knowledge of external context (date, time, user details) without dynamic variables or web search

Override core safety features built into EVI or supplemental LLMs (e.g. that prevent EVI from providing harmful information)


Importantly, the generated language does influence how the voice sounds - for example, excited text (e.g. "Oh wow, that's so interesting!") will make EVI's voice sound excited. However, to fundamentally change the voice characteristics, use our voice customization feature instead.
We are actively working on expanding EVI's ability to follow system prompts for both language and voice generation. For now, focus prompting on guiding EVI's conversational behavior and responses while working within these constraints.",
    "domain": "test.com",
    "hash": "#the-limits-of-prompting",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "general-llm-prompting-guidelines",
        "title": "General LLM prompting guidelines",
      },
      "h3": {
        "id": "the-limits-of-prompting",
        "title": "The limits of prompting",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-the-limits-of-prompting-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "The limits of prompting",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "content": "To learn more about prompt engineering in general or to understand how to prompt different LLMs, please refer to these resources:
EVI prompt examples: See examples of EVI prompts, including the full Hume default prompt.

Hume EVI playground: Test out your system prompts in live conversations with EVI, and see how it responds differently when you change configuration options.

OpenAI tokenizer: Useful for counting the number of tokens in a system prompt for OpenAI models, which use the same tokenizer (tiktoken).

OpenAI prompt engineering guidelines: For prompting OpenAI models like GPT-4.
OpenAI playground: For testing and evaluating OpenAI prompts in a chat interface.



Anthropic prompt engineering guidelines: For prompting Anthropic models like Claude 3 Haiku
Anthropic console: For testing and evaluating Anthropic prompts in a chat interface.



Fireworks model playground: For testing out open-source models served on Fireworks.

Vercel AI playground: Try multiple prompts and LLMs in parallel to compare their responses.

Perplexity Labs: Try different models, including open-source LLMs, to evaluate their responses and their latency.

Prompt engineering guide: An open-source guide from DAIR.ai with general methods and advanced techniques for prompting a wide variety of LLMs.

Artificial analysis benchmarks: Compare LLM characteristics and performance across different benchmarks, latency metrics, and more.",
    "domain": "test.com",
    "hash": "#additional-resources",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "additional-resources",
        "title": "Additional resources",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-additional-resources-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Additional resources",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/prompting",
    "code_snippets": [
      {
        "code": "<backchannel>
Whenever the user's message seems incomplete, respond with emotionally attuned, natural backchannels to encourage continuation. Backchannels must always be 1-2 words, like: "mmhm", "uh-huh", "go on", "right", "and then?", "I see", "oh wow", "yes?", "ahh...", "really?", "oooh", "true", "makes sense". Use minimal encouragers rather than interrupting with complete sentences. Use a diverse variety of words, avoiding repetition.
Assistant: "How is your day going?"
User: "My day is..."
Assistant: "Uh-huh?"
User: "it's good but busy. There's a lot going on."
Assistant: "I hear ya. What's going on for you?"
</backchannel>",
        "lang": "text",
      },
      {
        "code": "<backchannel>
Whenever the user's message seems incomplete, respond with emotionally attuned, natural backchannels to encourage continuation. Backchannels must always be 1-2 words, like: "mmhm", "uh-huh", "go on", "right", "and then?", "I see", "oh wow", "yes?", "ahh...", "really?", "oooh", "true", "makes sense". Use minimal encouragers rather than interrupting with complete sentences. Use a diverse variety of words, avoiding repetition.
Assistant: "How is your day going?"
User: "My day is..."
Assistant: "Uh-huh?"
User: "it's good but busy. There's a lot going on."
Assistant: "I hear ya. What's going on for you?"
</backchannel>",
        "lang": "text",
      },
    ],
    "content": "Yes, EVI can use conversational backchanneling - brief, encouraging
responses that show active listening without interrupting the user's train
of thought. This can help conversations feel more fluid and natural. To
enable this behavior, add an instrucation like the example below to your
system prompt:




The maximum length depends on the supplemental LLM being used. For example,
GPT-4 has a 32k token context window, while Claude 3 Haiku has a 200k token
context window. Check the context window for your LLM to ensure that your
prompt is within this limit. We recommend keeping system prompts around
2000-5000 tokens (roughly 1500-4000 words) for optimal performance across all
models. EVI also uses prompt caching (e.g. see Anthropic
docs) to
minimize the cost and latency when using very long prompts.",
    "domain": "test.com",
    "hash": "#frequently-asked-questions",
    "hierarchy": {
      "h0": {
        "title": "Prompt engineering for empathic voice interfaces",
      },
      "h2": {
        "id": "frequently-asked-questions",
        "title": "Frequently Asked Questions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.prompting-frequently-asked-questions-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/prompting",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Frequently Asked Questions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "The information on this page lays out how our custom language model
functionality works at a high level; however, for detailed instructions and
commented code, please see our example GitHub
repository.",
    "description": "For more customization, you can generate your own text using a custom language model.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Using a custom language model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "The custom language model feature enables developers to integrate their own language models with Hume’s Empathic User Interface (EVI), facilitating the creation of highly configurable and personalized user experiences. Developers create a socket that receives Hume conversation thread history, and your socket sends us the next text to say. Your backend socket can handle whatever custom business logic you have, and you just send the response back to us, which is then passed to the user.
Using your own LLM is intended for developers who need deep configurability for their use case. This includes full text customization for use cases like:
Advanced conversation steering: Implement complex logic to steer conversations beyond basic prompting, including managing multiple system prompts.

Regulatory compliance: Directly control and modify text outputs to meet specific regulatory requirements.

Context-aware text generation: Leverage dynamic agent metadata, such as remaining conversation time, to inform text generation.

Real-time data access: Utilize search engines within conversations to access and incorporate up-to-date information.

Retrieval augmented generation (RAG): Employ retrieval augmented generation techniques to enrich conversations by integrating external data without the need to modify the system prompt.


For these cases, function calling alone isn’t customizable enough, and with a custom language model you can create sophisticated workflows for your language model.",
    "domain": "test.com",
    "hash": "#overview",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "overview",
        "title": "Overview",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-overview-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Overview",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "Establish a Custom Text Socket
Initialization: See our example repository for instructions on setting up a custom text socket. This resource offers detailed guidance on both the setup process and the operational aspects of the code.

Hosting: Use Ngrok to publicly serve your socket. This step is needed to connect to the Hume system.

Configuration: Create a voice configuration, specifying "Custom language model" as the Language Model, and your socket's WSS URL as the Custom Language Model URL.

Make request: When making your request to the Hume platform, include the config_id parameter, setting its value to the Voice configuration ID of your configuration.


Communication Protocol
Receiving data: Your socket will receive JSON payloads containing conversation thread history from the Hume system.

Processing: Apply your custom business logic and utilize your language model to generate appropriate responses based on the received conversation history.

Sending responses: Transmit the generated text responses back to our platform through the established socket connection to be forwarded to the end user.




For improved clarity and naturalness in generated text, we recommend
transforming numerical values and abbreviations into their full verbal
counterparts (e.g., converting "3" to "three" and "Dr." to "doctor").",
    "domain": "test.com",
    "hash": "#setup",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "setup",
        "title": "Setup",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-setup-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Setup",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "code_snippets": [
      {
        "code": "/*
 * Represents the overall structure of the Welcome message.
 */
export interface Welcome {
  // Array of message elements
  messages: MessageElement[];
  // Unique identifier for the session
  custom_session_id: string;
}

/*
 * Represents a single message element within the session.
 */
export interface MessageElement {
  // Type of the message (e.g., user_message, assistant_message)
  type: string;
  // The message content and related details
  message: Message;
  // Models related to the message, primarily prosody analysis
  models: Models;
  // Optional timestamp details for when the message was sent
  time?: Time;
}

/*
 * Represents the content of the message.
 */
export interface Message {
  // Role of the sender (e.g., user, assistant)
  role: string;
  // The textual content of the message
  content: string;
}

/*
 * Represents the models associated with a message.
 */
export interface Models {
  // Prosody analysis details of the message
  prosody: Prosody;
}

/*
 * Represents the prosody analysis scores.
 */
export interface Prosody {
  // Dictionary of prosody scores with emotion categories as keys
  // and their respective scores as values
  scores: { [key: string]: number };
}

/*
 * Represents the timestamp details of a message.
 */
export interface Time {
  // The start time of the message (in milliseconds)
  begin: number;
  // The end time of the message (in milliseconds)
  end: number;
}",
        "lang": "typescript",
      },
    ],
    "content": "Below is the interface representing the overall structure of the message payloads sent by Hume:",
    "domain": "test.com",
    "hash": "#payload-structure",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "payload-structure",
        "title": "Payload Structure",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-payload-structure-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Payload Structure",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "For managing conversational state and connecting your frontend experiences with your backend data and logic, you should pass a custom_session_id in the SessionSettings message. When a custom_session_id is provided from the frontend SessionSettings message, the response sent from Hume to your backend includes this id, so you can correlate frontend users with their incoming messages.
Using a custom_session_id will enable you to:
maintain user state on your backend

pause/resume conversations

persist conversations across sessions

match frontend and backend connections


We recommend passing a custom_session_id if you are using a Custom Language Model.",
    "domain": "test.com",
    "hash": "#custom-session-id",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "payload-structure",
        "title": "Payload Structure",
      },
      "h3": {
        "id": "custom-session-id",
        "title": "Custom Session ID",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-custom-session-id-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Custom Session ID",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "These are the formats for sending messages to Hume:",
    "domain": "test.com",
    "hash": "#assistant-input-and-end-payload-format",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "assistant-input-and-end-payload-format",
        "title": "Assistant Input and End Payload Format",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-assistant-input-and-end-payload-format-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Assistant Input and End Payload Format",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "The assistant_input payload is used to send text to the assistant. You can send multiple assistant_input payloads in a sequence to stream text to the assistant.",
    "domain": "test.com",
    "hash": "#assistant_input",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "assistant-input-and-end-payload-format",
        "title": "Assistant Input and End Payload Format",
      },
      "h3": {
        "id": "assistant_input",
        "title": "assistant_input",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-assistant_input-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "assistant_input",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "The assistant_end payload indicates that your turn is over. This signals the end of the current stream of text inputs.",
    "domain": "test.com",
    "hash": "#assistant_end",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "assistant-input-and-end-payload-format",
        "title": "Assistant Input and End Payload Format",
      },
      "h3": {
        "id": "assistant_end",
        "title": "assistant_end",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-assistant_end-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "assistant_end",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "You can send multiple assistant_input payloads consecutively to stream text to the assistant. Once you are done sending inputs, you must send an assistant_end payload to indicate the end of your turn.",
    "domain": "test.com",
    "hash": "#streaming-text-to-the-assistant",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "streaming-text-to-the-assistant",
        "title": "Streaming Text to the Assistant",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-streaming-text-to-the-assistant-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Streaming Text to the Assistant",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "content": "Send assistant_input payloads to stream text to the assistant.

Send as many assistant_input payloads as needed.

Send an assistant_end payload to indicate that your turn is over.
By following this format, you ensure proper communication with the assistant API, enabling smooth and efficient interactions.",
    "domain": "test.com",
    "hash": "#summary",
    "hierarchy": {
      "h0": {
        "title": "Using a custom language model",
      },
      "h2": {
        "id": "summary",
        "title": "Summary",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.custom-language-model-summary-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/custom-language-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Summary",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
    ],
    "canonicalPathname": "/docs/empathic-voice-interface-evi/faq",
    "code_snippets": [
      {
        "code": "{
"type": "assistant_input",
"text": "Text to be synthesized."
}",
        "lang": "json",
      },
      {
        "code": "{
"type": "assistant_input",
"text": "Text to be synthesized."
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "assistant_message",
  "id": "g8ee90fa2c1648f3a32qrea6d179ee44",
  "message": {
    "role": "assistant",
    "content": "Text to be synthesized."
  },
  "models": {
    "prosody": {
      "scores": {
        "Admiration": 0.0309600830078125,
        "Adoration": 0.0018177032470703125
        // ... additional scores
      }
    }
  },
  "from_text": true
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "assistant_message",
  "id": "g8ee90fa2c1648f3a32qrea6d179ee44",
  "message": {
    "role": "assistant",
    "content": "Text to be synthesized."
  },
  "models": {
    "prosody": {
      "scores": {
        "Admiration": 0.0309600830078125,
        "Adoration": 0.0018177032470703125
        // ... additional scores
      }
    }
  },
  "from_text": true
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "audio_output",
  "id": "g8ee90fa2c1648f3a32qrea6d179ee44",
  "data": "<base64 encoded audio>"
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "audio_output",
  "id": "g8ee90fa2c1648f3a32qrea6d179ee44",
  "data": "<base64 encoded audio>"
}",
        "lang": "json",
      },
      {
        "code": "{
"type": "assistant_end"
}",
        "lang": "json",
      },
      {
        "code": "{
"type": "assistant_end"
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "chat_metadata",
  "chat_group_id": "8859a139-d98a-4e2f-af54-9dd66d8c96e1",
  "chat_id": "2c3a8636-2dde-47f1-8f9e-cea27791fd2e"
}",
        "lang": "json",
      },
      {
        "code": "{
  "type": "chat_metadata",
  "chat_group_id": "8859a139-d98a-4e2f-af54-9dd66d8c96e1",
  "chat_id": "2c3a8636-2dde-47f1-8f9e-cea27791fd2e"
}",
        "lang": "json",
      },
      {
        "code": "# Replace {chat_id} with your Chat ID
# Ensure your API key is set in the HUME_API_KEY environment variable
curl -X GET "https://api.hume.ai/v0/evi/chats/{chat_id}/audio" \
    -H "X-Hume-Api-Key: $HUME_API_KEY" \
    -H "Accept: application/json"",
        "lang": "bash",
      },
      {
        "code": "# Replace {chat_id} with your Chat ID
# Ensure your API key is set in the HUME_API_KEY environment variable
curl -X GET "https://api.hume.ai/v0/evi/chats/{chat_id}/audio" \
    -H "X-Hume-Api-Key: $HUME_API_KEY" \
    -H "Accept: application/json"",
        "lang": "bash",
      },
      {
        "code": "import { HumeClient } from "hume";

const client = new HumeClient({ apiKey: "HUME_API_KEY" });
await client.empathicVoice.chats.getAudio("YOUR_CHAT_ID");",
        "lang": "typescript",
      },
      {
        "code": "import { HumeClient } from "hume";

const client = new HumeClient({ apiKey: "HUME_API_KEY" });
await client.empathicVoice.chats.getAudio("YOUR_CHAT_ID");",
        "lang": "typescript",
      },
      {
        "code": "from hume import HumeClient

client = HumeClient(
    api_key="HUME_API_KEY",
)
client.empathic_voice.chats.get_audio(
    id="YOUR_CHAT_ID",
)",
        "lang": "python",
      },
      {
        "code": "from hume import HumeClient

client = HumeClient(
    api_key="HUME_API_KEY",
)
client.empathic_voice.chats.get_audio(
    id="YOUR_CHAT_ID",
)",
        "lang": "python",
      },
      {
        "code": "// Sample response (audio reconstruction initiated)
{
  "id": "470a49f6-1dec-4afe-8b61-035d3b2d63b0",
  "user_id": "e6235940-cfda-3988-9147-ff531627cf42",
  "status": "QUEUED",
  "filename": null,
  "modified_at": 1729875432555,
  "signed_audio_url": null,
  "signed_url_expiration_timestamp_millis": null  
}",
        "lang": "json",
      },
      {
        "code": "// Sample response (audio reconstruction initiated)
{
  "id": "470a49f6-1dec-4afe-8b61-035d3b2d63b0",
  "user_id": "e6235940-cfda-3988-9147-ff531627cf42",
  "status": "QUEUED",
  "filename": null,
  "modified_at": 1729875432555,
  "signed_audio_url": null,
  "signed_url_expiration_timestamp_millis": null  
}",
        "lang": "json",
      },
      {
        "code": "# Replace {chat_group_id} with your Chat Group ID
# Include pagination parameters as needed
# Ensure your API key is set in the HUME_API_KEY environment variable
curl -X GET "https://api.hume.ai/v0/evi/chat_groups/{chat_group_id}/audio?page_number=1&page_size=10&ascending_order=false" \
    -H "X-Hume-Api-Key: $HUME_API_KEY" \
    -H "Accept: application/json"",
        "lang": "bash",
      },
      {
        "code": "# Replace {chat_group_id} with your Chat Group ID
# Include pagination parameters as needed
# Ensure your API key is set in the HUME_API_KEY environment variable
curl -X GET "https://api.hume.ai/v0/evi/chat_groups/{chat_group_id}/audio?page_number=1&page_size=10&ascending_order=false" \
    -H "X-Hume-Api-Key: $HUME_API_KEY" \
    -H "Accept: application/json"",
        "lang": "bash",
      },
      {
        "code": "import { HumeClient } from "hume";

const client = new HumeClient({ apiKey: "<HUME_API_KEY>" });
await client.empathicVoice.chatGroups.getAudio("<YOUR_CHAT_ID>", {
    pageNumber: 0,
    pageSize: 10,
    ascendingOrder: false
});",
        "lang": "typescript",
      },
      {
        "code": "import { HumeClient } from "hume";

const client = new HumeClient({ apiKey: "<HUME_API_KEY>" });
await client.empathicVoice.chatGroups.getAudio("<YOUR_CHAT_ID>", {
    pageNumber: 0,
    pageSize: 10,
    ascendingOrder: false
});",
        "lang": "typescript",
      },
      {
        "code": "from hume import HumeClient

client = HumeClient(
    api_key="HUME_API_KEY",
)
client.empathic_voice.chat_groups.get_audio(
    id="YOUR_CHAT_ID",
    page_number=0,
    page_size=10,
    ascending_order=False,
)",
        "lang": "python",
      },
      {
        "code": "from hume import HumeClient

client = HumeClient(
    api_key="HUME_API_KEY",
)
client.empathic_voice.chat_groups.get_audio(
    id="YOUR_CHAT_ID",
    page_number=0,
    page_size=10,
    ascending_order=False,
)",
        "lang": "python",
      },
      {
        "code": "// Sample response (audio reconstruction initiated)
{
  "id": "369846cf-6ad5-404d-905e-a8acb5cdfc78",
  "user_id": "e6235940-cfda-3988-9147-ff531627cf42",
  "num_chats": 1,
  "page_number": 0,
  "page_size": 10,
  "total_pages": 1,
  "pagination_direction": "DESC",
  "audio_reconstructions_page": [
    {
      "id": "470a49f6-1dec-4afe-8b61-035d3b2d63b0",
      "user_id": "e6235940-cfda-3988-9147-ff531627cf42",
      "status": "QUEUED",
      "filename": null,
      "modified_at": 1729875432555,
      "signed_audio_url": null,
      "signed_url_expiration_timestamp_millis": null  
    }
  ]
}",
        "lang": "json",
      },
      {
        "code": "// Sample response (audio reconstruction initiated)
{
  "id": "369846cf-6ad5-404d-905e-a8acb5cdfc78",
  "user_id": "e6235940-cfda-3988-9147-ff531627cf42",
  "num_chats": 1,
  "page_number": 0,
  "page_size": 10,
  "total_pages": 1,
  "pagination_direction": "DESC",
  "audio_reconstructions_page": [
    {
      "id": "470a49f6-1dec-4afe-8b61-035d3b2d63b0",
      "user_id": "e6235940-cfda-3988-9147-ff531627cf42",
      "status": "QUEUED",
      "filename": null,
      "modified_at": 1729875432555,
      "signed_audio_url": null,
      "signed_url_expiration_timestamp_millis": null  
    }
  ]
}",
        "lang": "json",
      },
      {
        "code": "// Sample response (reconstruction complete)
{
  "id": "470a49f6-1dec-4afe-8b61-035d3b2d63b0",
  "user_id": "e6235940-cfda-3988-9147-ff531627cf42",
  "status": "COMPLETE",
  "filename": "e6235940-cfda-3988-9147-ff531627cf42/470a49f6-1dec-4afe-8b61-035d3b2d63b0/reconstructed_audio.mp4",
  "modified_at": 1729875432555,
  "signed_audio_url": "https://storage.googleapis.com/...etc.",
  "signed_url_expiration_timestamp_millis": 1730232816964  
}",
        "lang": "json",
      },
      {
        "code": "// Sample response (reconstruction complete)
{
  "id": "470a49f6-1dec-4afe-8b61-035d3b2d63b0",
  "user_id": "e6235940-cfda-3988-9147-ff531627cf42",
  "status": "COMPLETE",
  "filename": "e6235940-cfda-3988-9147-ff531627cf42/470a49f6-1dec-4afe-8b61-035d3b2d63b0/reconstructed_audio.mp4",
  "modified_at": 1729875432555,
  "signed_audio_url": "https://storage.googleapis.com/...etc.",
  "signed_url_expiration_timestamp_millis": 1730232816964  
}",
        "lang": "json",
      },
      {
        "code": "# Replace {signed_audio_url} with the URL from the API response
curl -O "{signed_audio_url}"",
        "lang": "bash",
      },
      {
        "code": "# Replace {signed_audio_url} with the URL from the API response
curl -O "{signed_audio_url}"",
        "lang": "bash",
      },
    ],
    "content": "We’ve compiled a list of frequently asked questions from our developer community. If your question isn't listed, we invite you to join the discussion on our Discord.




Our API is based on our own empathic LLM (eLLM) and can blend in responses
from an external LLM API. Please visit our configuration guide for up-to-date information
on Hume's default configuration options.


When sending messages through EVI's WebSocket, you can specify your own
language_model_api_key in the SessionSettings message. Please visit our API
reference for more information
here.


We cover the cost of the supplemental LLMs while we make optimizations that
will make language generation much cheaper for our customers. This means that
these expenses are not included in EVI’s
pricing, ensuring consistent rates whether
you use open-source, closed-source, or custom language models with EVI.


These outputs reflect our prosody model's confidence that the speaker is expressing the label in their tone of voice and language.
Our prosody model is derived from extensive perceptual studies of emotional expressions with millions of participants.
The model is trained to pick up on vocal modulations and patterns in language that people reliably interpret as expressing specific emotions.
Importantly, the labels do not imply that the person is experiencing the emotions.
Expression labels: These categories (like "amusement") represent categories of emotional expression that most people perceive in vocal and linguistic patterns.
They are not based on explicit definitions of emotions, but rather common interpretations of expressive cues.

Expression measures: These numbers indicate the model's confidence that a given expression would be interpreted as belonging to a specific category by human observers.
They represent the likelihood of a particular interpretation of expressions, not the presence or intensity of a specific emotion.


For more details, see our prosody model documentation and the foundational research by Cowen and Keltner (2017).


At the word-level, prosody measurements are highly dependent on context. Our
internal testing shows that they are more stable at the sentence level.


Today we only support English, however we do have plans to support other
languages very soon. Join the conversation on
Discord to tell us what languages you want EVI
to speak.


EVI currently supports 8 base voices - Ito, Kora, Dacher, Aura, Finn, Whimsy, Stella, and Sunny -
with plans to introduce more in the future. In the meantime, you can craft your own unique voice
by adjusting the attributes of any base option.
Visit the playground to try out the base voices and experiment with voice modulation, and
learn more about voice customization in our detailed guide.
If you are interested in creating a custom voice for your use case, please submit a sales inquiry.
Our team can train custom TTS models for enterprise customers.


Our empathic large language model (eLLM) is a multimodal language model that
takes into account both expression measures and language. The eLLM generates a
language response and guides text-to-speech (TTS) prosody.


Hume's eLLM is not contingent on other LLMs and is therefore able to generate
an initial response much faster than existing LLM services. However, Hume’s
Empathic Voice Interface (EVI) is able to integrate other frontier LLMs into
its longer responses which are configurable by developers.


The landscape of large language models (LLMs) and their providers is constantly evolving,
affecting which supplemental LLM is fastest with EVI.
The key factor influencing perceived latency using EVI is the time to first token (TTFT), with lower TTFT being
better. The model and provider combination with the smallest TTFT will be the fastest.


Artificial Analysis offers a useful dashboard for comparing
model and provider latencies.
Notably, there's a tradeoff between speed and quality. Larger, slower models are easier to prompt. We
recommend testing various supplemental LLM options when implementing EVI.


Hume has trained our own expressive text-to-speech (TTS) model that allows it
to generate speech with more prosody and expressive nuance than other models.
TTS is specifically designed for use within an EVI chat session, allowing EVI
to generate speech from a given text input. We do not have a dedicated endpoint for TTS.
To perform TTS within an EVI chat session, you can follow the steps below:
Establish initial connection: Make the initial handshake request
to establish the WebSocket connection.

Send text for synthesis: Send an Assistant Input
message with the text you want to synthesize into speech:




Receive synthesized speech: After sending an assistant_input message,
you will receive an Assistant Message
and Audio Output for each sentence of the provided text.
The assistant_message contains the text and expression measurement predictions, while the
audio_output message contains the synthesized, emotional audio. See the sample messages below:





End of Response: Once all the text has been synthesized into speech, you will receive
an Assistant End
message indicating the end of the response:




Before implementing this in code, you can test it out by going to our Portal.
Start a call in the EVI Playground, then send an Assistant Message with the text you want to synthesize.


Yes. During a chat with EVI, you can pause responses by sending a
pause_assistant_message. This will prevent EVI from sending Assistant messages until receiving a resume_assistant_message.
While paused,
EVI stops generating and sending new responses.

Tool use is disabled, so EVI responses pertaining to tool use are also disabled.

Messages and audio that were queued before the pause_assistant_message will still be sent.

EVI continues to "listen" and process user input - transcriptions of user audio are saved, and will all be sent to the LLM as User messages when EVI is resumed.




The following message types will not be received while EVI is paused: assistant_message,
audio_output, tool_call_message,
tool_response_message, and tool_error_message.
Upon resuming with a resume_assistant_message, EVI will generate a response that considers all user input received during the pause.
Pausing EVI’s responses is different from muting user input. When user input is muted, EVI does not "hear" any of the user's audio and cannot respond to it. When paused, EVI does "hear" user audio input and can respond when resumed.
When resumed, EVI's response may address multiple points or questions in the user's input. However, without being prompted to always respond to all user input, EVI will tend to respond to the latest user input. For instance, if the user asks two questions while EVI is paused, EVI typically responds to the second question and not the first.
Charges will continue to accrue while EVI is paused. If you wish to completely pause both input and output you should instead disconnect and resume the chat when ready.
For instance, a developer might create a button that allows users to pause EVI responses while they are brainstorming or reflecting but don't want EVI to respond. Then, when the user is done, they can resume to hear EVI's response.


With EVI, you can easily preserve context when reconnecting or continue a
chat right where you left off. See steps below for how to resume a chat:
Establish initial connection: Make the initial handshake request
to establish the WebSocket connection. Upon successful connection, you will
receive a ChatMetadata message:




Store the chat_group_id: Save the chat_group_id from the ChatMetadata message for future use.

Resume chat: To resume a chat, include the stored chat_group_id in the resumed_chat_group_id
query parameter of subsequent handshake requests.
For example: wss://api.hume.ai/v0/evi/chat?access_token={accessToken}&resumed_chat_group_id={chatGroupId}




When resuming a chat, you can specify a different EVI configuration than the one used in the previous session.
However, changing the system prompt or supplemental LLM may result in unexpected behavior from EVI.
Additionally, if data retention is disabled,
the ability to resume chats will not be supported.


Yes, you can listen to your past conversations with EVI using our audio reconstruction feature. This feature allows you to fetch and play back conversations as single audio files, which can be integrated into your applications or services.


The audio reconstruction feature is not available for accounts with the no data retention feature enabled.
How audio reconstruction works
The audio reconstruction feature stitches together all audio snippets from a conversation—including both your inputs and EVI's responses—into one continuous audio file.
Storage duration: Reconstructed audio files are stored indefinitely.

Signed URL expiration: The signed_audio_url expires after 60 minutes. If it expires before you download the audio, make another API request to obtain a new URL.

No merging of Chats: The API does not support stitching together multiple Chats within a Chat Group into a single audio file.

Asynchronous process: Audio reconstruction is an asynchronous process. When you request audio reconstruction, it initiates a background job that processes the audio data. The time it takes to reconstruct audio depends on the length of the conversation and system load.


Audio reconstruction statuses
QUEUED: The reconstruction job is waiting to be processed.

IN_PROGRESS: The reconstruction is currently being processed.

COMPLETE: The audio reconstruction is finished and ready for download.

ERROR: An error occurred during the reconstruction process.

CANCELED: The reconstruction job has been canceled.


Fetching reconstructed audio for a specific Chat
To fetch the reconstructed audio for a specific Chat, use the following endpoint: /chats/{chat_id}/audio.












If audio reconstruction for a Chat or Chat Group hasn’t already occurred, calling the respective endpoint will automatically add the audio reconstruction process to our job queue.
Fetching reconstructed audio for a Chat Group
To fetch a paginated list of reconstructed audio for Chats within a Chat Group, use the following endpoint: /chat_groups/{chat_group_id}/audio.










Polling for completion
Since the reconstruction process is asynchronous, you can poll the endpoint to check the status field until it changes to COMPLETE. Once the status is COMPLETE, the signed_audio_url and signed_url_expiration fields will be populated.


Downloading the audio file
After the reconstruction is complete, you can download the audio file using the signed_audio_url. The following cURL command saves the audio file using the original filename provided by the server:",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.empathic-voice-interface-evi.faq-root-0",
    "org_id": "test",
    "pathname": "/docs/empathic-voice-interface-evi/faq",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Empathic Voice Interface FAQ",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/overview",
    "description": "Hume's state of the art expression measurement models for the voice, face, and language.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.overview",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Expression Measurement",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/overview",
    "content": "Hume's state of the art expression measurement models for the voice, face, and language are built on 10+ years of research and advances in computational approaches to emotion science (semantic space theory) pioneered by our team. Our expression measurement models are able to capture hundreds of dimensions of human expression in audio, video, and images.",
    "domain": "test.com",
    "hash": "#intro",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement",
      },
      "h2": {
        "id": "intro",
        "title": "Intro",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.overview-intro-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Intro",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/overview",
    "content": "Facial Expression, including subtle facial movements often seen as expressing love or admiration, awe, disappointment, or cringes of empathic pain, along 48 distinct dimensions of emotional meaning. Our Facial Expression model will also optionally output FACS 2.0 measurements, our model of facial movements including traditional Action Units (AUs such as “Inner brow raise”, “Nose crinkle”) and facial descriptions (“Smile”, “Wink”, “Hand over mouth”, “Hand over eyes”)

Speech Prosody, or the non-linguistic tone, rhythm, and timbre of speech, spanning 48 distinct dimensions of emotional meaning.

Vocal Burst, including laughs, sighs, huhs, hmms, cries and shrieks (to name a few), along 48 distinct dimensions of emotional meaning.

Emotional Language, or the emotional tone of transcribed text, along 53 dimensions.




Expressions are complex and multifaceted; they should not be treated as direct inferences
of emotional experience. To learn more about the science behind expression measurement,
visit the About the science page.
To learn more about how to use our models visit our API reference.",
    "domain": "test.com",
    "hash": "#measurements",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement",
      },
      "h2": {
        "id": "intro",
        "title": "Intro",
      },
      "h3": {
        "id": "measurements",
        "title": "Measurements",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.overview-measurements-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Measurements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/overview",
    "content": "The models were trained on human intensity ratings of large-scale, experimentally controlled emotional expression data gathered using the methods described in these papers: Deep learning reveals what vocal bursts express in different cultures and Deep learning reveals what facial expressions mean to people in different cultures.
While our models measure nuanced expressions that people most typically describe with emotion labels, it's important to remember that they are not a direct readout of what someone is experiencing. Sometimes, the outputs from facial and vocal models will show different emotional meanings, which is completely normal. Generally speaking, emotional experience is subjective and its expression is multimodal and context-dependent.",
    "domain": "test.com",
    "hash": "#model-training",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement",
      },
      "h2": {
        "id": "intro",
        "title": "Intro",
      },
      "h3": {
        "id": "model-training",
        "title": "Model training",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.overview-model-training-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Model training",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/overview",
    "content": "Learn how you can use the Expression Measurement API through both REST and WebSockets.




Use REST endpoints to process batches of videos, images, text, or audio files.


Use WebSocket endpoints when you need real-time predictions, such as processing a webcam or microphone stream.
REST and WebSocket endpoints provide access to all of the same Hume models, but with different speed and scale tradeoffs. All models share a common response format, which associates a score with each detected expression. Scores indicate the degree to which a human rater would assign an expression to a given sample of video, text or audio.",
    "domain": "test.com",
    "hash": "#try-out-the-models",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement",
      },
      "h2": {
        "id": "try-out-the-models",
        "title": "Try out the models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.overview-try-out-the-models-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Try out the models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/overview",
    "content": "Our models measure 53 expressions identified through the subtleties of emotional language and 48 expressions discerned from facial cues, vocal bursts, and speech prosody.
Expression Language Face/Burst/Prosody 
Admiration 

 

 
Adoration 

 

 
Aesthetic Appreciation 

 

 
Amusement 

 

 
Anger 

 

 
Annoyance 

  
Anxiety 

 

 
Awe 

 

 
Awkwardness 

 

 
Boredom 

 

 
Calmness 

 

 
Concentration 

 

 
Confusion 

 

 
Contemplation 

 

 
Contempt 

  
Contentment 

 

 
Craving 

 

 
Desire 

 

 
Determination 

 

 
Disappointment 

 

 
Disapproval 

  
Disgust 

 

 
Distress 

 

 
Doubt 

 

 
Ecstasy 

  
Embarrassment 

 

 
Empathic Pain 

 

 
Enthusiasm 

  
Entrancement 

 

 
Envy 

 

 
Excitement 

 

 
Fear 

 

 
Gratitude 

  
Guilt 

 

 
Horror 

 

 
Interest 

 

 
Joy 

 

 
Love 

 

 
Nostalgia 

 

 
Pain 

 

 
Pride 

 

 
Realization 

 

 
Relief 

 

 
Romance 

 

 
Sadness 

 

 
Sarcasm 

  
Satisfaction 

 

 
Shame 

 

 
Surprise (negative) 

 

 
Surprise (positive) 

 

 
Sympathy 

 

 
Tiredness 

 

 
Triumph",
    "domain": "test.com",
    "hash": "#specific-expressions-by-modality",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement",
      },
      "h2": {
        "id": "specific-expressions-by-modality",
        "title": "Specific expressions by modality",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.overview-specific-expressions-by-modality-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Specific expressions by modality",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/overview",
    "content": "Our Custom Models API builds on our expression measurement models and state-of-the-art eLLMs to bring custom insights to your application. Developed using transfer learning from our expression measurement models and eLLMs, our Custom Models API can predict almost any outcome more accurately than language alone, whether it's toxicity, depressed mood, driver drowsiness, or any other metric important to your users.




Build on our expression measurement models to bring custom insights to your application.",
    "domain": "test.com",
    "hash": "#train-your-own-custom-model",
    "hierarchy": {
      "h0": {
        "title": "Expression Measurement",
      },
      "h2": {
        "id": "train-your-own-custom-model",
        "title": "Train your own custom model",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.overview-train-your-own-custom-model-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Train your own custom model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/rest",
    "content": "Hume’s Expression Measurement API is designed to facilitate large-scale processing of files using Hume's advanced models through an asynchronous, job-based interface. This API allows developers to submit jobs for parallel processing of various files, enabling efficient handling of multiple data points simultaneously, and receiving notifications when results are available.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.rest-root-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/rest",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Processing batches of media files",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/rest",
    "content": "Asynchronous job submission: Jobs can be submitted to process a wide array of files in parallel, making it ideal for applications that require the analysis of large volumes of data.

Flexible data input options: The API supports multiple data formats, including hosted file URLs, local files directly from your system, and raw text in the form of a list of strings. This versatility ensures that you can easily integrate the API into their applications, regardless of where their data resides.",
    "domain": "test.com",
    "hash": "#key-features",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files",
      },
      "h2": {
        "id": "key-features",
        "title": "Key features",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.rest-key-features-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/rest",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Key features",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/rest",
    "content": "Hume’s Expression Measurement API is particularly useful for leveraging Hume's expressive models across a broad spectrum of files and formats. Whether it's for processing large datasets for research, analyzing customer feedback across multiple channels, or enriching user experiences in media-rich applications, REST provides a robust solution for asynchronously handling complex, data-intensive tasks.",
    "domain": "test.com",
    "hash": "#applications-and-use-cases",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files",
      },
      "h2": {
        "id": "applications-and-use-cases",
        "title": "Applications and use cases",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.rest-applications-and-use-cases-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/rest",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Applications and use cases",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/rest",
    "code_snippets": [
      {
        "code": "curl https://api.hume.ai/v0/batch/jobs \
 --request POST \
 --header "Content-Type: application/json" \
 --header "X-Hume-Api-Key: <YOUR API KEY>" \
 --data '{
    "models": {
        "face": {}
    },
    "urls": [
        "https://hume-tutorials.s3.amazonaws.com/faces.zip"
    ]
}'",
        "lang": "bash",
      },
      {
        "code": "curl https://api.hume.ai/v0/batch/jobs \
 --request POST \
 --header "Content-Type: application/json" \
 --header "X-Hume-Api-Key: <YOUR API KEY>" \
 --data '{
    "models": {
        "face": {}
    },
    "urls": [
        "https://hume-tutorials.s3.amazonaws.com/faces.zip"
    ]
}'",
        "lang": "bash",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.batch import Face, Models

async def main():
    # Initialize an authenticated client
    client = AsyncHumeClient(api_key=<YOUR_API_KEY>)

    # Define the URL(s) of the files you would like to analyze
    job_urls = ["https://hume-tutorials.s3.amazonaws.com/faces.zip"]

    # Create configurations for each model you would like to use (blank = default)
    face_config = Face()

    # Create a Models object
    models_chosen = Models(face=face_config)

    # Start an inference job and print the job_id
    job_id = await client.expression_measurement.batch.start_inference_job(
        urls=job_urls, models=models_chosen
    )
    print(job_id)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.batch import Face, Models

async def main():
    # Initialize an authenticated client
    client = AsyncHumeClient(api_key=<YOUR_API_KEY>)

    # Define the URL(s) of the files you would like to analyze
    job_urls = ["https://hume-tutorials.s3.amazonaws.com/faces.zip"]

    # Create configurations for each model you would like to use (blank = default)
    face_config = Face()

    # Create a Models object
    models_chosen = Models(face=face_config)

    # Start an inference job and print the job_id
    job_id = await client.expression_measurement.batch.start_inference_job(
        urls=job_urls, models=models_chosen
    )
    print(job_id)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "curl https://api.hume.ai/v0/batch/jobs \
 --request POST \
 --header "Content-Type: multipart/form-data" \
 --header "X-Hume-Api-Key: <YOUR API KEY>" \
 --form json='{
    "models": {
        "face": {}
    }
 }' \
 --form file=@faces.zip \
 --form file=@david_hume.jpeg",
        "lang": "bash",
      },
      {
        "code": "curl https://api.hume.ai/v0/batch/jobs \
 --request POST \
 --header "Content-Type: multipart/form-data" \
 --header "X-Hume-Api-Key: <YOUR API KEY>" \
 --form json='{
    "models": {
        "face": {}
    }
 }' \
 --form file=@faces.zip \
 --form file=@david_hume.jpeg",
        "lang": "bash",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.batch import Face, Models
from hume.expression_measurement.batch.types import InferenceBaseRequest

async def main():
    # Initialize an authenticated client
    client = AsyncHumeClient(api_key=<YOUR_API_KEY>)

    # Define the filepath(s) of the file(s) you would like to analyze
    local_filepaths = [
        open("faces.zip", mode="rb"),
        open("david_hume.jpeg", mode="rb")
    ]

    # Create configurations for each model you would like to use (blank = default)
    face_config = Face()

    # Create a Models object
    models_chosen = Models(face=face_config)
    
    # Create a stringified object containing the configuration
    stringified_configs = InferenceBaseRequest(models=models_chosen)

    # Start an inference job and print the job_id
    job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(
        json=stringified_configs, file=local_filepaths
    )
    print(job_id)

if __name__ == "__main__":
    asyncio.run(main())
",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.batch import Face, Models
from hume.expression_measurement.batch.types import InferenceBaseRequest

async def main():
    # Initialize an authenticated client
    client = AsyncHumeClient(api_key=<YOUR_API_KEY>)

    # Define the filepath(s) of the file(s) you would like to analyze
    local_filepaths = [
        open("faces.zip", mode="rb"),
        open("david_hume.jpeg", mode="rb")
    ]

    # Create configurations for each model you would like to use (blank = default)
    face_config = Face()

    # Create a Models object
    models_chosen = Models(face=face_config)
    
    # Create a stringified object containing the configuration
    stringified_configs = InferenceBaseRequest(models=models_chosen)

    # Start an inference job and print the job_id
    job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(
        json=stringified_configs, file=local_filepaths
    )
    print(job_id)

if __name__ == "__main__":
    asyncio.run(main())
",
        "lang": "python",
      },
      {
        "code": "{
    job_id: "Job ID",
    status: "STATUS (COMPLETED/FAILED)",
    predictions: [ARRAY OF RESULTS]
}",
        "lang": "json",
      },
      {
        "code": "{
    job_id: "Job ID",
    status: "STATUS (COMPLETED/FAILED)",
    predictions: [ARRAY OF RESULTS]
}",
        "lang": "json",
      },
      {
        "code": "curl --request GET \
 --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/predictions \
 --header 'X-Hume-Api-Key: <YOUR API KEY>' \
 --header 'accept: application/json; charset=utf-8'",
        "lang": "bash",
      },
      {
        "code": "curl --request GET \
 --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/predictions \
 --header 'X-Hume-Api-Key: <YOUR API KEY>' \
 --header 'accept: application/json; charset=utf-8'",
        "lang": "bash",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient

client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

async def main():
    job_predictions = await client.expression_measurement.batch.get_job_predictions(
        id="<YOUR_JOB_ID>"
    )

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient

client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

async def main():
    job_predictions = await client.expression_measurement.batch.get_job_predictions(
        id="<YOUR_JOB_ID>"
    )

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "curl --request GET \
 --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/artifacts \
 --header 'X-Hume-Api-Key: <YOUR API KEY>' \
 --header 'accept: application/octet-stream'",
        "lang": "bash",
      },
      {
        "code": "curl --request GET \
 --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/artifacts \
 --header 'X-Hume-Api-Key: <YOUR API KEY>' \
 --header 'accept: application/octet-stream'",
        "lang": "bash",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient

client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

async def main():
    with open("artifacts.zip", "wb") as f:
        async for new_bytes in client.expression_measurement.batch.get_job_artifacts("<YOUR_JOB_ID>"):
            f.write(new_bytes)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient

client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

async def main():
    with open("artifacts.zip", "wb") as f:
        async for new_bytes in client.expression_measurement.batch.get_job_artifacts("<YOUR_JOB_ID>"):
            f.write(new_bytes)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
    ],
    "content": "Here we'll show you how to upload your own files and run Hume models on batches of data.
If you haven't already, grab your API Key.


Making a request to the API
Start a new job with the Expression Measurement API.






To do the same with a local file:






Sample files for you to use in this tutorial are available here:
Download faces.zip
Download david_hume.jpeg
Checking job status


Use webhooks to asynchronously receive notifications once the job completes.
It is not recommended to poll the API periodically for job status.
There are several ways to get notified and check the status of your job.
Using the Get job details API endpoint.

Providing a callback URL. We will send a POST request to your URL when the job is complete. Your request body should look like this: { "callback_url": "<YOUR CALLBACK URL>" }




Retrieving predictions
Your predictions are available in a few formats.
To get predictions as JSON use the Get job predictions endpoint.






To get predictions as a compressed file of CSVs, one per model use the Get job artifacts endpoint.",
    "domain": "test.com",
    "hash": "#using-humes-expression-measurement-api",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files",
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.rest-using-humes-expression-measurement-api-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/rest",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Using Hume’s Expression Measurement API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/rest",
    "content": "The size of any individual file provided by URL cannot exceed 1 GB.

The size of any individual local file cannot exceed 100 MB.

Each request has an upper limit of 100 URLs, 100 strings (raw text), and 100 local media files. Can be a mix of the media files or archives (.zip, .tar.gz, .tar.bz2, .tar.xz).

For audio and video files the max length supported is 3 hours.

The limit for each individual text string for the Expression Measurement API is 255 MB.",
    "domain": "test.com",
    "hash": "#api-limits",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files",
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API",
      },
      "h3": {
        "id": "api-limits",
        "title": "API limits",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.rest-api-limits-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/rest",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "API limits",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/rest",
    "content": "You can provide data for your job in one of the following formats: hosted file URLs, local files, or raw text presented as a list of strings.
In this tutorial, the data is publicly available to download. For added security, you may choose to create a signed URL through your preferred cloud storage provider.
Cloud Provider Signing URLs 
GCP https://cloud.google.com/storage/docs/access-control/signed-urls 
AWS https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html 
Azure https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
    "domain": "test.com",
    "hash": "#providing-urls-and-files",
    "hierarchy": {
      "h0": {
        "title": "Processing batches of media files",
      },
      "h2": {
        "id": "using-humes-expression-measurement-api",
        "title": "Using Hume’s Expression Measurement API",
      },
      "h3": {
        "id": "providing-urls-and-files",
        "title": "Providing URLs and files",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.rest-providing-urls-and-files-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/rest",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Providing URLs and files",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "content": "WebSocket-based streaming facilitates continuous data flow between your application and Hume's models, providing immediate feedback and insights.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-root-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Real-time measurement streaming",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "content": "Real-time data processing: Leveraging WebSockets, this API allows for the streaming of data to Hume's models, enabling instant analysis and response. This feature is particularly beneficial for applications requiring immediate processing, such as live interaction systems or real-time monitoring tools.

Persistent, two-way communication: Unlike traditional request-response models, the WebSocket-based streaming maintains an open connection for two-way communication between the client and server. This facilitates an ongoing exchange of data, allowing for a more interactive and responsive user experience.

High throughput and low latency: The API is optimized for high performance, supporting high-volume data streaming with minimal delay. This ensures that applications can handle large streams of data efficiently, without sacrificing speed or responsiveness.",
    "domain": "test.com",
    "hash": "#key-features",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "key-features",
        "title": "Key features",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-key-features-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Key features",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "content": "WebSockets are ideal for a wide range of applications that benefit from real-time data analysis and interaction. Examples include:
Live customer service tools: enhance customer support with real-time sentiment analysis and automated, emotionally intelligent responses

Interactive educational platforms: provide immediate feedback and adaptive learning experiences based on real-time student input

Health and wellness apps: support live mental health and wellness monitoring, offering instant therapeutic feedback or alerts based on the user's vocal or textual expressions

Entertainment and gaming: create more immersive and interactive experiences by responding to user inputs and emotions in real time",
    "domain": "test.com",
    "hash": "#applications-and-use-cases",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "applications-and-use-cases",
        "title": "Applications and use cases",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-applications-and-use-cases-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Applications and use cases",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "content": "Integrating WebSocket-based streaming into your application involves establishing a WebSocket connection with Hume AI's servers and streaming data directly to the models for processing.
Streaming is built for analysis of audio, video, and text streams. By connecting to WebSocket endpoints you can get near real-time feedback on the expressive and emotional content of your data.",
    "domain": "test.com",
    "hash": "#getting-started-with-websocket-streaming",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-getting-started-with-websocket-streaming-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Getting started with WebSocket streaming",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "code_snippets": [
      {
        "code": "pip install "hume"",
        "lang": "bash",
      },
      {
        "code": "pip install "hume"",
        "lang": "bash",
      },
    ],
    "content": "First, ensure you have installed the SDK using pip or another package manager.",
    "domain": "test.com",
    "hash": "#install-the-hume-python-sdk",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming",
      },
      "h3": {
        "id": "install-the-hume-python-sdk",
        "title": "Install the Hume Python SDK",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-install-the-hume-python-sdk-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Install the Hume Python SDK",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "code_snippets": [
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.stream import Config
from hume.expression_measurement.stream.socket_client import StreamConnectOptions
from hume.expression_measurement.stream.types import StreamLanguage

samples = [
    "Mary had a little lamb,",
    "Its fleece was white as snow."
    "Everywhere the child went,"
    "The little lamb was sure to go."
]

async def main():
    client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

    model_config = Config(language=StreamLanguage())

    stream_options = StreamConnectOptions(config=model_config)

    async with client.expression_measurement.stream.connect(options=stream_options) as socket:
        for sample in samples:
            result = await socket.send_text(sample)
            print(result.language.predictions[0].emotions)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.stream import Config
from hume.expression_measurement.stream.socket_client import StreamConnectOptions
from hume.expression_measurement.stream.types import StreamLanguage

samples = [
    "Mary had a little lamb,",
    "Its fleece was white as snow."
    "Everywhere the child went,"
    "The little lamb was sure to go."
]

async def main():
    client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

    model_config = Config(language=StreamLanguage())

    stream_options = StreamConnectOptions(config=model_config)

    async with client.expression_measurement.stream.connect(options=stream_options) as socket:
        for sample in samples:
            result = await socket.send_text(sample)
            print(result.language.predictions[0].emotions)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "[
  {'name': 'Admiration', 'score': 0.06379243731498718},
  {'name': 'Adoration', 'score': 0.07222934812307358},
  {'name': 'Aesthetic Appreciation', 'score': 0.02808445133268833},
  {'name': 'Amusement', 'score': 0.027589013800024986},
  ......
  {'name': 'Surprise (positive)', 'score': 0.030542362481355667},
  {'name': 'Sympathy', 'score': 0.03246130049228668},
  {'name': 'Tiredness', 'score': 0.03606246039271355},
  {'name': 'Triumph', 'score': 0.01235896535217762}
]",
        "lang": "python",
      },
      {
        "code": "[
  {'name': 'Admiration', 'score': 0.06379243731498718},
  {'name': 'Adoration', 'score': 0.07222934812307358},
  {'name': 'Aesthetic Appreciation', 'score': 0.02808445133268833},
  {'name': 'Amusement', 'score': 0.027589013800024986},
  ......
  {'name': 'Surprise (positive)', 'score': 0.030542362481355667},
  {'name': 'Sympathy', 'score': 0.03246130049228668},
  {'name': 'Tiredness', 'score': 0.03606246039271355},
  {'name': 'Triumph', 'score': 0.01235896535217762}
]",
        "lang": "python",
      },
    ],
    "content": "This example uses our Emotional Language model to perform sentiment analysis on a children's nursery rhyme.
If you haven't already, grab your API key.


Your result should look something like this:",
    "domain": "test.com",
    "hash": "#emotional-language-from-text",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming",
      },
      "h3": {
        "id": "emotional-language-from-text",
        "title": "Emotional language from text",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-emotional-language-from-text-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Emotional language from text",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "code_snippets": [
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.stream import Config
from hume.expression_measurement.stream.socket_client import StreamConnectOptions
from hume.expression_measurement.stream.types import StreamFace

async def main():
    client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

    model_config = Config(face=StreamFace())

    stream_options = StreamConnectOptions(config=model_config)

    async with client.expression_measurement.stream.connect(options=stream_options) as socket:
        result = await socket.send_file("<YOUR_IMAGE_FILEPATH>")
        print(result)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.stream import Config
from hume.expression_measurement.stream.socket_client import StreamConnectOptions
from hume.expression_measurement.stream.types import StreamFace

async def main():
    client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

    model_config = Config(face=StreamFace())

    stream_options = StreamConnectOptions(config=model_config)

    async with client.expression_measurement.stream.connect(options=stream_options) as socket:
        result = await socket.send_file("<YOUR_IMAGE_FILEPATH>")
        print(result)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
    ],
    "content": "This example uses our Facial Expression model to get expression measurements from an image.",
    "domain": "test.com",
    "hash": "#facial-expressions-from-an-image",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming",
      },
      "h3": {
        "id": "facial-expressions-from-an-image",
        "title": "Facial expressions from an image",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-facial-expressions-from-an-image-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Facial expressions from an image",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "code_snippets": [
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.stream import Config
from hume.expression_measurement.stream.socket_client import StreamConnectOptions

async def main():
    client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

    model_config = Config(prosody={})

    stream_options = StreamConnectOptions(config=model_config)

    async with client.expression_measurement.stream.connect(options=stream_options) as socket:
        result = await socket.send_file("YOUR_AUDIO_OR_VIDEO_FILEPATH")
        print(result)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.stream import Config
from hume.expression_measurement.stream.socket_client import StreamConnectOptions

async def main():
    client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

    model_config = Config(prosody={})

    stream_options = StreamConnectOptions(config=model_config)

    async with client.expression_measurement.stream.connect(options=stream_options) as socket:
        result = await socket.send_file("YOUR_AUDIO_OR_VIDEO_FILEPATH")
        print(result)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
    ],
    "content": "This example uses our Speech Prosody model to get expression measurements from an audio or video file.",
    "domain": "test.com",
    "hash": "#speech-prosody-from-an-audio-or-video-file",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "getting-started-with-websocket-streaming",
        "title": "Getting started with WebSocket streaming",
      },
      "h3": {
        "id": "speech-prosody-from-an-audio-or-video-file",
        "title": "Speech prosody from an audio or video file",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-speech-prosody-from-an-audio-or-video-file-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Speech prosody from an audio or video file",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "code_snippets": [
      {
        "code": "X-Hume-Api-Key: <YOUR API KEY>",
        "lang": "http",
      },
      {
        "code": "X-Hume-Api-Key: <YOUR API KEY>",
        "lang": "http",
      },
      {
        "code": "{
    "models": {
        "language": {}
    },
    "raw_text": true,
    "data": "Mary had a little lamb"
}",
        "lang": "json",
      },
      {
        "code": "{
    "models": {
        "language": {}
    },
    "raw_text": true,
    "data": "Mary had a little lamb"
}",
        "lang": "json",
      },
      {
        "code": "{
  "language": {
    "predictions": [
      {
        "text": "Mary",
        "position": { "begin": 0, "end": 4 },
        "emotions": [
          { "name": "Anger", "score": 0.012025930918753147 },
          { "name": "Joy", "score": 0.056471485644578934 },
          { "name": "Sadness", "score": 0.031556881964206696 },
        ]
      },
      {
        "text": "had",
        "position": { "begin": 5, "end": 8 },
        "emotions": [
          { "name": "Anger", "score": 0.0016927534015849233 },
          { "name": "Joy", "score": 0.02388327568769455 },
          { "name": "Sadness", "score": 0.018137391656637192 },
          ...
        ]
      },
      ...
    ]
  }
}",
        "lang": "json",
      },
      {
        "code": "{
  "language": {
    "predictions": [
      {
        "text": "Mary",
        "position": { "begin": 0, "end": 4 },
        "emotions": [
          { "name": "Anger", "score": 0.012025930918753147 },
          { "name": "Joy", "score": 0.056471485644578934 },
          { "name": "Sadness", "score": 0.031556881964206696 },
        ]
      },
      {
        "text": "had",
        "position": { "begin": 5, "end": 8 },
        "emotions": [
          { "name": "Anger", "score": 0.0016927534015849233 },
          { "name": "Joy", "score": 0.02388327568769455 },
          { "name": "Sadness", "score": 0.018137391656637192 },
          ...
        ]
      },
      ...
    ]
  }
}",
        "lang": "json",
      },
    ],
    "content": "To call the API from your own WebSockets client you'll need the API endpoint, a JSON message, and an API key header/param. More information can be found in the Expression Measurement API reference.
To get started, you can use a WebSocket client of your choice to connect to the models endpoint:


url wss://api.hume.ai/v0/stream/models Make sure you configure the socket connection headers with your personal API key




The default WebSockets implementation in your browser may not have support for headers. If that's the case you can set
the apiKey query parameter.
And finally, send the following JSON message on the socket:


You should receive a JSON response that looks something like this:",
    "domain": "test.com",
    "hash": "#streaming-with-your-own-websockets-client",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "streaming-with-your-own-websockets-client",
        "title": "Streaming with your own WebSockets client",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-streaming-with-your-own-websockets-client-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Streaming with your own WebSockets client",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "code_snippets": [
      {
        "code": "import base64
from pathlib import Path

def encode_data(filepath: Path) -> str:
    with Path(filepath).open('rb') as fp:
        bytes_data = base64.b64encode(fp.read())
        encoded_data = bytes_data.decode("utf-8")
    return encoded_data

filepath = "<PATH TO YOUR MEDIA>"
encoded_data = encode_data(filepath)
print(encoded_data)
",
        "lang": "python",
      },
      {
        "code": "import base64
from pathlib import Path

def encode_data(filepath: Path) -> str:
    with Path(filepath).open('rb') as fp:
        bytes_data = base64.b64encode(fp.read())
        encoded_data = bytes_data.decode("utf-8")
    return encoded_data

filepath = "<PATH TO YOUR MEDIA>"
encoded_data = encode_data(filepath)
print(encoded_data)
",
        "lang": "python",
      },
    ],
    "content": "The WebSocket endpoints of the Expression Measurement API require that you encode your media using base64. Here's a quick example of base64 encoding data in Python:",
    "domain": "test.com",
    "hash": "#sending-images-or-audio",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "streaming-with-your-own-websockets-client",
        "title": "Streaming with your own WebSockets client",
      },
      "h3": {
        "id": "sending-images-or-audio",
        "title": "Sending images or audio",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-sending-images-or-audio-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Sending images or audio",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "content": "WebSocket duration limit: connections are subject to a default timeout after one (1) minute of inactivity to ensure unused connections are released.

WebSocket message payload size limit: the size limit for a given payload depends on the type of content being transmitted and its dimensions.
Video: 5000 milliseconds (5 seconds)

Audio: 5000 milliseconds (5 seconds)

Image: 3,000 x 3,000 pixels

Text: 10,000 characters



Request rate limit: HTTP requests (e.g. WebSocket handshake endpoint) are limited to fifty (50) requests per second.",
    "domain": "test.com",
    "hash": "#api-limits",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "api-limits",
        "title": "API limits",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-api-limits-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "API limits",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/websocket",
    "content": "WebSockets are a communication protocol that enables real-time, two-way communication between a client and a server
over a single, long-lived connection. They provide a persistent connection that allows both the client and the server
to initiate communication at any time.


Streaming will disconnect every minute to ensure unused connections are released. You will need to reconnect by
building reconnect logic into your application. Implementation of reconnect logic will depend on the language and
framework of your client application.
Please see our TypeScript streaming sandbox example
for a sample implementation.


WebSocket connections can experience disruptions due to network issues or other factors. Implement error handling
mechanisms to gracefully handle connection failures. This includes handling connection timeouts, connection drops, and
intermittent connection issues. Implement reconnection logic to automatically attempt to reconnect and resume
communication when a connection is lost.


Hume WebSockets endpoints can return errors in response to invalid requests, authentication failures, or other issues.
Implement proper error handling to interpret and handle these errors in your application. Provide meaningful error
messages to users and handle any exceptional scenarios gracefully. To prevent unknowingly initiating too many errors
we have put a limit on how many of the same errors you can have in a row. For a full list of the error responses you
can expect, please see our API errors page.


The benefits of using a the WebSocket is the persistent connection. The open socket should be kept open until the
application is done utilizing the service and then closed. Avoid opening a new connection for each file or payload you
send to the API. To ensure that context does not leak across multiple unrelated files you can use the
reset_stream parameter.",
    "domain": "test.com",
    "hash": "#faq",
    "hierarchy": {
      "h0": {
        "title": "Real-time measurement streaming",
      },
      "h2": {
        "id": "faq",
        "title": "FAQ",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.websocket-faq-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/websocket",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "FAQ",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
      {
        "pathname": "/docs/expression-measurement/custom-models",
        "title": "Custom models",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/custom-models/overview",
    "content": "Combined with words, expressions provide a wealth of information about our state of mind in any given context like customer satisfaction or frustration, patient health and well-being, student comprehension and confusion, and so much more.
Hume’s Custom Models API unlocks these insights at the click of a button, integrating patterns of facial expression, vocal expression, and language into a single custom model to predict whatever outcome you specify. This works by taking advantage not only of our state-of-the-art expression AI models, but also specialized language-expression embeddings that we have trained on conversational data.
The algorithm that drives our Custom Models API is pretrained on huge volumes of data. That means it already recognizes most patterns of expression and language that people form. All you have to do is add your labels.
You can access our Custom Models API through our no code platform detailed in the next section or through our API. Once you create your initial labeled dataset, your labels will be used to train a custom model that you own and only your account can access. You’ll be able to run the model on any new file through our Playground and Custom Models API. You’ll also get statistics on the accuracy of your custom model.",
    "description": "Predict preferences more accurately than any LLM.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.custom-models.overview-root-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/custom-models/overview",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Custom Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
      {
        "pathname": "/docs/expression-measurement/custom-models",
        "title": "Custom models",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/custom-models/creating-your-dataset",
    "content": "In this guide, we'll walk you through the process of creating a dataset used for training your custom model.


Prepare your dataset
Choose a dataset of image, video, or audio files for your custom model to learn from—ideally, one that captures the different states, preferences, or outcomes important to your application.


Each dataset must contain files of a single media type, such as all images, all videos, or all audio files.
Then, begin by organizing your files into labeled subfolders.
In this tutorial, we'll put together a dataset of images with facial expressions classified as negative, neutral, or positive. This dataset can then be used to train a custom model for sentiment analysis.
Start by creating a main folder called 'User Sentiment' with subfolders labeled 'Negative,' 'Neutral or Ambiguous,' and 'Positive.'
Our platform will interpret these as labels for the images they contain.






The amount of data you'll need to build an accurate model depends on your goal's complexity. Generally, it's good practice to have a similar number of samples for each label you want to predict. You'll also want to consider other forms of imbalance or bias in your dataset. The length of file, number of speakers, and language spoken can also impact the model's predictive accuracy.
To learn more, see our FAQ on building datasets.
Navigate to our Portal
Once you've assembled your dataset, it's time to visit our Portal.
In the Portal, navigate to the Expression Measurement page. Then, continue to the Custom Models section.
Once there, click the View Datasets button at the top right of the page.




Next, find the Create Dataset button.
Clicking this button will allow you to add your dataset to our Portal.




Create your dataset
Provide a title for your dataset. Then, add a column named after the category you are predicting and specify the data type for this column (categorical or numerical).
In our example, we can name the column 'User Sentiment' and select 'Categorical' as the data type.




Upload the folder containing your dataset
Now, drag-and-drop the folder containing your dataset.


Remember, the folder should include subfolders for each label containing the corresponding samples.




In the pop-up window, assign a name to the label column, which represents the overall category you are predicting.
In our example, we can assign ‘User Sentiment’ as the name. Then, click the Save Labels and Continue button and subsequently approve the uploading process.




Verify your uploads
Check the total file count and address any detected issues.
Once you're ready, hit the Save button on the top right of the page.






If you accidentally uploaded a mixed-media dataset, a pop-up window will ask you to select the single file type you would like to keep.
Now, you’re ready to train your custom model!",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.custom-models.creating-your-dataset-root-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/custom-models/creating-your-dataset",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Creating your dataset",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
      {
        "pathname": "/docs/expression-measurement/custom-models",
        "title": "Custom models",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/custom-models/training-a-custom-model",
    "content": "In this guide, we will walk you through training your own custom model.


Create a custom model
In the Portal under Expression Measurement, navigate to the Custom Models section.
Once there, you can click Create Custom Model to begin.




Select a training dataset
Select a dataset to train your custom model on. If you have not created one already, see our guide on creating your dataset.
For the purposes of this tutorial, we will train the model on a dataset of images labeled as negative, neutral, or positive.
These labels will allow our model to classify facial expressions in images.




Select a dataset column to predict
Next, choose the dataset column you want to predict and hit Continue.
For this tutorial, you’ll select the 'User Sentiment' column, which represents the predicted emotional tone of each image.
This column contains the labels 'Negative', 'Neutral or Ambiguous,' and 'Positive.'




Select a task type
Based on your data, we'll recommend either classification or regression as the task type for your custom model.
Classification requires categorical label values like strings or integers, while regression requires numeric label values like integers or floats.
Then, select the specific type of model you want to create. There are three available model types:
Multiclass classification: Predict a categorical variable where all labels are equal in importance (e.g. "sunny", "rainy", "cloudy")

Binary classification: Predict a categorical variable where a designated positive label is the "correct" label in some way (e.g. "good" vs. "bad" customer service call)

Univariate regression: Predict a single continuous value (e.g. how hot will it be tomorrow?)


For more information, see our FAQ on the difference between classification and regression.
Since our dataset contains multiple sentiment labels, we'll select Multiclass classification for this tutorial. This type of model is best suited for predicting categorical variables where each label is equally important.




Finalize your custom model
To finish, enter a name and description for your custom model. If needed, these can be adjusted at a later time.
Once you're ready, click Start Training to begin the training process.




You will then be redirected to a page confirming that your model is actively training.
To check on the status of your model, click View Jobs. To see existing, finished models, click View Models.




Check the status of your training job
You can check the status of your model in the Jobs page of our Portal.
It may take a few minutes for your custom model to be ready. Once training is complete, the status will update to "Completed," and you’ll have access to your custom model.




Test your custom model
When you're ready to test your custom model, navigate to the Expression Measurement page, then go to File Analysis.




From the Select a model dropdown, choose the custom model you created from previous steps.




To select a file to analyze, click the Upload files button. You can upload local files, choose previously uploaded files in Hume, or use Hume’s example files to test your custom model.
Let's test our custom model using one of the example files. Since our model is an image classifier, select an example image file to analyze.




Click Analyze to analyze your selected file with the custom model.




That’s it! You’ve successfully analyzed a file using your custom model. To evaluate its performance, see our guide on evaluating your custom model.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.custom-models.training-a-custom-model-root-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/custom-models/training-a-custom-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Training a custom model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
      {
        "pathname": "/docs/expression-measurement/custom-models",
        "title": "Custom models",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "content": "Each custom model you train has a corresponding details page, viewable from the Hume Portal. The model details page displays metrics and visualizations to evaluate your model’s performance. This document serves to help you interpret those metrics and provide guidance on ways to improve your custom model.


Custom model details


Limitations of model validation metrics
Model validation metrics are estimates based on a split of your dataset into training and evaluation parts. The larger the training set, the more reliable the metrics. However, it’s important to remember that these metrics are indicative and do not guarantee performance on unseen data.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.custom-models.evaluating-your-custom-model-root-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Evaluating your custom model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
      {
        "pathname": "/docs/expression-measurement/custom-models",
        "title": "Custom models",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "content": "Task-specific variances and performance metrics: with expression analysis, the complexity of your task determines the range of model performance, which in the case of classification models can technically vary from zero to perfect accuracy. Depending on the complexity of your task, less than perfect performance may still be very useful to serve as an indication of likelihood for your given target.

Influence of number of classes: prediction gets more difficult as the number of classes in your dataset increases, particularly when distinction between classes is more subtle. Inherently the level of chance will be higher with a lower number of classes. For example, for 3-classes your low-end performance is 33% accuracy vs 50% for a binary problem.

Application-specific requirements: when establishing acceptable accuracy for a model, it’s important to consider the sensitivity and impact of its application. An appropriate accuracy threshold varies with the specific demands and potential consequences of the model’s use, requiring a nuanced understanding of how accuracy levels intersect with the objectives and risks of each unique application.




How is it possible that my model achieved 100% accuracy?
Achieving 100% accuracy is possible, however it is important to consider, especially in small datasets, that this might indicate model overfitting, caused by feature leakage or other data anomalies. Feature leakage occurs when your model inadvertently learns from data that explicitly includes label information (e.g., sentences of ‘I feel happy’ for a target label ‘happy’) leading to skewed results. To ensure more reliable performance, it’s advisable to use larger datasets and check that your data does not unintentionally contain explicit information about the labels.",
    "domain": "test.com",
    "hash": "#assessing-good-performance",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your custom model",
      },
      "h3": {
        "id": "assessing-good-performance",
        "title": "Assessing 'good' performance",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.custom-models.evaluating-your-custom-model-assessing-good-performance-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Assessing 'good' performance",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
      {
        "pathname": "/docs/expression-measurement/custom-models",
        "title": "Custom models",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "content": "In addition to accuracy, advanced metrics for a deeper evaluation of your custom model’s performance are also provided.


Advanced evaluation metrics
Term Definition 
Accuracy A fundamental metric in model performance evaluation which measures the proportion of correct predictions (true positives and true negatives) against the total number made. It’s straightforward and particularly useful for balanced datasets. However, accuracy can be misleading in imbalanced datasets where one class predominates, as a model might seem accurate by mainly predicting the majority class, neglecting the minority. This limitation underscores the importance of using additional metrics like precision, recall, and F1 score for a more nuanced assessment of model performance across different classes. 
Precision Score which measures how often the model detects positives correctly. (e.g., When your model identifies a customer’s expression as 'satisfied', how often is the customer actually satisfied? Low precision would mean the model often misinterprets other expressions as satisfaction, leading to incorrect categorization.) 
Recall Score which measures how often the model correctly identifies actual positives. (e.g., Of all the genuine expressions of satisfaction, how many does your model accurately identify as 'satisfied'?" Low recall implies the model is missing out on correctly identifying many true instances of customer satisfaction, failing to recognize them accurately.) 
F1 A metric that combines precision and recall, providing a balanced measure of a model’s accuracy, particularly useful in scenarios with class imbalance or when specific decision thresholds are vital. 
Average Precision A metric that calculates the weighted average of precision at each threshold, providing a comprehensive measure of a model’s performance across different levels of recall. 
Roc Auc (Area under the ROC curve) a comprehensive measure of a model’s ability to distinguish between classes across all possible thresholds, making it ideal for overall performance evaluation and comparative analysis of different models.",
    "domain": "test.com",
    "hash": "#advanced-evaluation-metrics",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your custom model",
      },
      "h3": {
        "id": "advanced-evaluation-metrics",
        "title": "Advanced evaluation metrics",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.custom-models.evaluating-your-custom-model-advanced-evaluation-metrics-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Advanced evaluation metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
      {
        "pathname": "/docs/expression-measurement/custom-models",
        "title": "Custom models",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "content": "Increase data quantity: adding more data will often help a model to learn a broader range of the given target’s representation, increasing the likelihood of capturing outliers from diverse patterns and scenarios.

Improve label quality: ensure that each data point in your dataset is well-labeled with clear, accurate, and consistent annotations. Properly defined labels are essential for reducing misinterpretations and confusion, allowing the model to accurately represent and learn from the dataset’s true characteristics. Ensuring balance in the distribution of labels is important to ensure that the model is not biased towards a specific label.

Enhance data quality: refine your dataset to ensure it is free from noise and irrelevant information. High-quality data (in terms of your target) enhances the model’s ability to make precise predictions and learn effectively from relevant features, critical in complex datasets.

Incorporate clear audio data: when working with models analyzing vocal expressions, ensure audio files include clear, audible spoken language. This enhances the model’s ability to accurately interpret and learn from vocal nuances. Explore various segmentation strategies which evaluate the effect that environmental sound may have on your model’s performance.",
    "domain": "test.com",
    "hash": "#improving-model-performance",
    "hierarchy": {
      "h0": {
        "title": "Evaluating your custom model",
      },
      "h3": {
        "id": "improving-model-performance",
        "title": "Improving model performance",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.custom-models.evaluating-your-custom-model-improving-model-performance-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/custom-models/evaluating-your-custom-model",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Improving model performance",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/expression-measurement",
        "title": "Expression Measurement",
      },
    ],
    "canonicalPathname": "/docs/expression-measurement/faq",
    "content": "Our models capture the widest-ever range of facial, speech, vocal, and language modulations with distinct emotional meanings. We label each of their outputs with emotion terms like “amusement” and “doubt,” not because they always correspond to those emotional experiences (they must not, given that they often differ from one modality to another), but because scientific studies show that these kinds of labels are the most precise language we have for describing expressions.
Our models generate JSON or CSV output files with values typically ranging from 0 to 1 for each output in different segments of the input file (though values out of the 0-1 range are possible). Higher values indicate greater intensity of facial movements or vocal modulations that are most strongly associated with the emotion label corresponding to the output.
A given expression will contain a blend of various emotions, and our models identify features that are associated with each emotional dimension. The score for each dimension is proportional to the likelihood that a human would perceive that emotion in the expression.
Specifically, the scores reflect the likelihood that an average human perceiver would use that emotion dimension to describe a given expression. The models were trained on human intensity ratings gathered using the methods described in this paper: Deep learning reveals what vocal bursts express in different cultures.
While our models measure nuanced expressions that people most typically describe with emotion labels, it's important to remember that they are not a direct readout of what someone is experiencing. Emotional experience is subjective and its expression is multimodal and context-dependent. Moreover, at any given time, our facial expression outputs might be quite different than our vocal expression outputs. Therefore, it's important to follow best practices when interpreting outputs.


There are many different ways to use our platform. That said, successful research and applications of our models generally follow four steps: exploration, prediction, improvement, and testing.
Exploration: Researchers and developers generally begin by exploring patterns in their data.


Are there apparent differences across participants or users in a study?

Do patterns in expression vary systematically over time?

Are there different patterns in expression associated with different stages of research or different product experiences?


Prediction: A great way to evaluate and start building on our APIs is to use them to predict metrics that you already know are important.


Are key outcomes like mental health or customer satisfaction better predicted by language and expression than by language alone?

If patterns in expression predict important outcomes, how do these patterns in expression vary over time and reveal critical moments for a user or participant?


Improvement: The goal is often to use measures of expression to directly improve how the application works.


Sometimes, being able to predict an important metric is enough to make a decision. For example, if you can predict whether two people will get along based on their expressions and language, then your application can pair them up.

More formally, you can apply statistics or machine learning to the data you gather to improve how the application works.

You can incorporate our API outputs into an out-of-the-box large language model, simply by converting them into text (e.g., "The user sounds calm but a little frustrated") and feeding them in as prompts.

You can use expressions to teach an AI model. For example, if your application involves a large language model, such as an AI tutor, you can use measures of expression that predict student performance and well-being to directly fine-tune the AI to improve over time.


Testing: After you've incorporated measures of expression into your application, they can be part of every A/B test you perform. You can now monitor the effects of changes to your application not just on engagement and retention, but also on how much users laugh or sigh in frustration, or show signs of interest or boredom.




As you build expression-related signals, metrics, analyses, models, or
feedback into an application, remember to use scientific best
practices and
follow the ethics guidelines of
thehumeinitiative.org.


Our speech prosody model measures the tune, rhythm, and timbre of speech, whereas our language model measures the tone of the words being spoken. When using either model, we offer the flexibility to annotate emotional expressions at several levels of granularity, ranging from individual words to entire conversational turns. It is important to note that independent of granularity, our language model still takes into account up to 50 previous tokens (word or sub-words) of speech; otherwise, it would not be able to capture how the meaning of the words is affected by context.
Word: At the word level, our model provides a separate output for each word, offering the most granular insight into emotional expression during speech.
Sentence: At the sentence level of granularity, we annotate the emotional tone of each spoken sentence with our prosody and language models.
Utterance: Utterance-level granularity is between word- and sentence-level. It takes into account natural pauses or breaks in speech, providing more rapidly updated measures of emotional expression within a flowing conversation. For text inputs, utterance-level granularity will produce results identical to sentence-level granularity.
Conversational Turn: Conversational turn-level analysis is a lower level of granularity. It outputs a single output for each turn; that is, the full sequence of words and sentences spoken uninterrupted by each person. This approach provides a higher-level view of the emotional dynamics in a multi-participant dialogue. For text inputs, specifying conversational turn-level granularity for our Language model will produce results for entire passage.


Remember, each level of granularity has its unique advantages, and choosing
the right one depends on the requirements of your specific application.


State-of-the-art face detection and identification algorithms still occasionally make errors. For instance, our algorithm sometimes detects faces in shadows or reflections. Other times, our algorithm falsely attributes a new identity to someone who has already been in the video, sometimes due to changes in lighting or occlusion. These errors can result in additional face IDs. We are still working to fine-tune our algorithm to minimize errors in the contexts that our customers care about.


Our vocal burst model detects vocalizations such as laughs, screams, sighs, gasps, “mms,” “uhs,” and “mhms.” Natural speech generally contains a few vocal bursts every minute, but scripted speech has fewer vocal bursts. If no vocal bursts are detected, it may be because there are no vocal bursts in the file. However, if you hear vocal bursts that aren't being detected by the algorithm, note that we are also in the process of improving our vocal burst detection algorithm, so please stay tuned for updates.


We've documented this issue thoroughly in our API errors page.


You can specify any of the following:
zh, da, nl, en, en-AU, en-IN, en-NZ, en-GB, fr, fr-CA, de, hi, hi-Latn, id, it, ja, ko, no, pl, pt, pt-BR, pt-PT, ru, es, es-419, sv, ta, tr, or uk.


We support over 50 languages. Among these, 20 languages have additional support for transcription.
Language Tag Language Text Transcription 
ar Arabic 

  
bg Bulgarian 

  
ca Catalan 

  
cs Czech 

  
da Danish 

 

 
de German 

 

 
el Greek 

  
en English* 

 

 
es Spanish 

 

 
et Estonian 

  
fa Farsi 

  
fi Finnish 

  
fr French 

 

 
fr-ca French (Canada) 

 

 
gl Galician 

  
gu Gujarati 

  
he Hebrew 

  
hi Hindi 

 

 
hr Croatian 

  
hu Hungarian 

  
hy Armenian 

  
id Indonesian 

 

 
it Italian 

 

 
ja Japanese 

 

 
ka Georgian 

  
ko Korean 

 

 
ku Kurdish 

  
lt Lithuanian 

  
lv Latvian 

  
mk FYRO Macedonian 

  
mn Mongolian 

  
mr Marathi 

  
ms Malay 

  
my Burmese 

  
nb Norwegian (Bokmål) 

  
nl Dutch 

 

 
pl Polish 

 

 
pt Portuguese 

 

 
pt-br Portuguese (Brazil) 

 

 
ro Romanian 

  
ru Russian 

 

 
sk Slovak 

  
sl Slovenian 

  
sq Albanian 

  
sr Serbian 

  
sv Swedish 

 

 
th Thai 

  
tr Turkish 

 

 
uk Ukrainian 

 

 
ur Urdu 

  
vi Vietnamese 

  
zh-cn Chinese 

 

 
zh-tw Chinese (Taiwan) 

 

 



English is a primary language, and will yield more accurate predictions than
inputs in other supported languages. Currently, our NER model only supports
the English language.


Custom Models become essential when raw embeddings from Hume’s expression measurement models require further tailoring for specific applications. Here are scenarios where Custom Models offer significant advantages:
Specialized contexts: In environments with unique characteristics or requirements, Custom Models enable the creation of context-specific labels, ensuring more relevant and accurate insights. If your project demands a particular set of labels that are not covered by Hume’s emotional expression labels, Custom Models enable you to create and apply these labels, ensuring that the analysis aligns with your specific objectives.

Iterative model improvement: In evolving fields or scenarios where data and requirements change over time, Custom Models offer the flexibility to iteratively improve and adapt the model with new data and labels.




In labeling, regression involves assigning continuous numerical values, while classification involves categorizing data into discrete labels. During training, regression models learn to predict numerical values, whereas classification models learn to categorize data points into predefined classes.
Classification use cases
Emotion Categorization: Classification excels in distinguishing distinct emotional states, like identifying happiness, sadness, or surprise based on linguistic or physical expression cues.

Binary Emotional Analysis: Useful in binary scenarios such as detecting presence or absence of specific emotional reactions, like engagement or disengagement in a learning environment.

Multi-Emotional Identification: Perfect for classifying a range of emotions in complex scenarios, like understanding varied customer reactions from satisfied to dissatisfied based on their verbal and non-verbal feedback.


Regression use cases
Intensity Measurement: Regression is apt for quantifying the intensity or degree of emotional responses, such as assessing the level of stress or joy from vocal or facial cues.

Emotional Progression Tracking: Ideal for monitoring the fluctuation of emotional states over time, like tracking the development of engagement or anxiety in therapy sessions.


In essence, regression models in emotional expression analysis assign continuous values representing intensities or degrees, while classification models categorize expressions into distinct states or reactions.


Our custom model pipeline is designed to accommodate a wide range of data types, including audio, videos, and text, automatically integrating multimodal patterns of expression and language. However, not all datasets are created equal. For best results, we recommend using a dataset that meets certain standards:
Dataset size
Ideally, use a dataset consisting of a minimum of 20 files, but more data is always better for model performance.
Media type consistency
All files within a dataset should be of the same media type (video, audio, image, text...etc.)
It's generally wise to maintain a consistent naming convention and file format for your dataset. At minimum, ensure files have appropriate extensions, such as .wav, .mp3, .aif, .mov, or .mp4.
Classification vs regression tasks
Depending on your model's objective (classification or regression), you can use different labeling approaches.
Classification labels: use either strings or integers as labels (e.g., "confused," "focused"). We limit the number of categorical labels to 50, and you must have at least two (binary).

Regression targets: use either integers or decimals as targets. A model trained on a regression task with predict a continuous numerical value.


Label consistency
We recommend that your labels follow a consistent format; e.g, do not mix integers and strings. Furthermore, be sure to check for any typos in your labels, as these will be considered as separate classes, e.g, “happy” vs. “hapy.”
Class imbalance
If possible, it helps to have a balanced distribution of labels in your dataset. For example, if you have 50 files and two classes, the best case is to have 25 samples per class. Generally, you need at least 10 samples per class to train a useful model, but more data per class is always better.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.expression-measurement.faq-root-0",
    "org_id": "test",
    "pathname": "/docs/expression-measurement/faq",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Expression Measurement API FAQ",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/billing",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.resources.billing",
    "org_id": "test",
    "pathname": "/docs/resources/billing",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Billing",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/billing",
    "content": "Joining the platform: When you sign up and start using our APIs, you'll initially be using the free credits given to every new account.

Credit card requirement: Once you've exhausted your credit balance, you'll need to activate billing to continue.




Activate billing before depleting your credit balance to ensure uninterrupted service.
Monthly limit and notifications:
You'll have a default monthly limit of $100.

If you hit the $100 limit, API calls will return an error, and you'll be prompted to apply for a monthly limit increase.



Billing notifications:
On the first of each month, you'll receive an invoice for the previous month’s usage.

If your credit card is successfully added, it will be charged automatically.

You'll get a confirmation email for successful transactions or an alert if a transaction fails.



Failure to pay: If payment isn't received within 7 days of the invoice date, API access will be suspended until the outstanding balance is settled.",
    "domain": "test.com",
    "hash": "#how-it-works",
    "hierarchy": {
      "h0": {
        "title": "Billing",
      },
      "h2": {
        "id": "how-it-works",
        "title": "How it works",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.billing-how-it-works-0",
    "org_id": "test",
    "pathname": "/docs/resources/billing",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "How it works",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/billing",
    "content": "Usage information: To view your monthly usage details, visit the Usage & Billing page. There you can track your API usage and see how much of your monthly limit has been utilized.
Note: After your credits are used, further usage accrues to your monthly cost.  You'll be charged this amount on the first of the following month. Your monthly cost is updated daily at 08:00 UTC.



Billing portal: To manage your billing details, navigate to Usage & Billing and select Manage payments and view invoices. There you can update your payment method, view past invoices, and keep track of upcoming charges.",
    "domain": "test.com",
    "hash": "#managing-your-account",
    "hierarchy": {
      "h0": {
        "title": "Billing",
      },
      "h2": {
        "id": "managing-your-account",
        "title": "Managing your account",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.billing-managing-your-account-0",
    "org_id": "test",
    "pathname": "/docs/resources/billing",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Managing your account",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/billing",
    "content": "Find up-to-date pricing information at hume.ai/pricing.",
    "domain": "test.com",
    "hash": "#pricing",
    "hierarchy": {
      "h0": {
        "title": "Billing",
      },
      "h2": {
        "id": "understanding-your-bill",
        "title": "Understanding your bill",
      },
      "h3": {
        "id": "pricing",
        "title": "Pricing",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.resources.billing-pricing-0",
    "org_id": "test",
    "pathname": "/docs/resources/billing",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Pricing",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/billing",
    "content": "Audio and video:
Our listed prices are presented per minute for ease of understanding.

However, we bill these services on a corresponding per-second basis to ensure precise and fair charges. This means you are only billed for the exact amount of time your audio or video content is processed.



Image and text:
Image processing charges are incurred per image.

Text processing is billed based on the number of words processed.",
    "domain": "test.com",
    "hash": "#billing-methodology",
    "hierarchy": {
      "h0": {
        "title": "Billing",
      },
      "h2": {
        "id": "understanding-your-bill",
        "title": "Understanding your bill",
      },
      "h3": {
        "id": "billing-methodology",
        "title": "Billing methodology",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.resources.billing-billing-methodology-0",
    "org_id": "test",
    "pathname": "/docs/resources/billing",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Billing methodology",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/billing",
    "content": "After you use all your credits, there might be a delay before we switch you to a subscription or stop access, which can result in a small negative credit balance. This is normal and won't affect your subscription.
If you have questions about your bill or need assistance understanding the charges, please contact billing@hume.ai.",
    "domain": "test.com",
    "hash": "#faq",
    "hierarchy": {
      "h0": {
        "title": "Billing",
      },
      "h2": {
        "id": "faq",
        "title": "FAQ",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.billing-faq-0",
    "org_id": "test",
    "pathname": "/docs/resources/billing",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "FAQ",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/errors",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.resources.errors",
    "org_id": "test",
    "pathname": "/docs/resources/errors",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Errors",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/errors",
    "content": "Configuration errors indicate that something about the API call was not configured correctly. The error message you get from the Hume APIs will often contain more information than we're able to provide on this page. For example if an audio file is too long, the error message from the API will specify the limit as well as the length of the audio received.
Code Description 
E0100 The WebSocket request could not be parsed as valid JSON. The Hume API requires JSON serializable payloads. 
E0101 You may be missing or improperly formatting a required field. This generic error indicates that the structure of your WebSocket request was invalid. Please see the error message you received in the API response for more details. 
E0102 The requested model was incompatible with the file format received. Some models are not compatible with every file type. For example, no facial expressions will be detected in a text file. Audio can be extracted out of some video files, but if the video has no audio, then models like Speech Prosody and Vocal Burst will not be available. 
E0200 Media provided could not be parsed into a known file format. Hume APIs support a wide range of file formats and media types including audio, video, image, text, but not all formats are supported. If you receive this error and believe your file type should be supported please reach out to our support team. 
E0201 Media could not be decoded as a Base64 encoded string. The data field in the request payload should be Base64 encoded bytes. If you want to pass raw text without encoding it you can do so with the raw_text parameter. 
E0202 No audio signal could be inferred from the media provided. This error indicates that audio models were configured, but the media provided could not be parsed into a valid audio file. 
E0203 Your audio file was too long. The limit is 5000 milliseconds. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints. 
E0204 Your video file was too long. The limit is 5000 milliseconds. For best performance we recommend passing individual frames of video as images rather than full video files. For larger files, consider using the Hume Expression Measurement API REST endpoints. 
E0205 Your image file was too large. The limit is 3,000 x 3,000 pixels. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints. 
E0206 Your text file was too long. The limit is 10,000 characters. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints. 
E0207 The URL you've provided appears to be incorrect. Please verify that you've entered the correct URL and try submitting it again. If you're copying and pasting, ensure that the entire URL has been copied without any missing characters. 
E0300 You've run out of credits. Activate billing to continue making API calls. 
E0301 Your monthly credit limit has been reached. Once billing is activated, users can accrue charges up to a predetermined monthly cap. This limit ensures that users do not accumulate excessive debt without assurance of payment. If you require a higher limit, you may manually apply for a credit limit increase on the Usage page. Alternatively, the limit will reset at the beginning of the next month. For more information, please see our docs on billing. 
E0400 You've referenced a resource that doesn't exist in our system. Please check if the name or identifier you used is correct and try again. 
E0401 Your upload failed. Please ensure your file meets our format and size requirements, and attempt to upload it again. 
E0402 The CSV file you used to create or update a dataset is missing a header row. The header specifies what each column represents. Update your CSV file and retry your request. For more information about how to format your dataset CSV please see our tutorial on dataset creation. 
E0500 Your dataset doesn't meet the minimum sample size requirement. Please add more files to your dataset and resubmit your training job. For more information, please see our docs on dataset requirements. 
E0501 Your dataset contains a target column with empty values. Please clean your dataset so that all labels are valid categorical or numeric values and then resubmit your training job. For more information on target columns please see our docs on dataset requirements. 
E0502 Your dataset contains a target column with infinite values. Please clean your dataset so that all labels are valid categorical or numeric values and then resubmit your training job. For more information on target columns please see our tutorial on dataset creation. 
E0503 For classification tasks, your dataset must include at least two distinct classes. Please check your dataset has two unique labels in the target column. 
E0504 Some classes in your dataset don't have enough samples. To ensure that the model we produce is of the highest quality we require your dataset to be relatively balanced across classes. Please check the error message for which class should have more samples (or remove that class entirely). Please see our docs on dataset requirements for more details. 
E0505 The target column you've selected doesn't exist in the dataset. Please review the columns that exist in your dataset and select a valid column name. 
E0506 Your chosen target column is not a valid target column. Please ensure that you select a column with labels rather than the file_id column or another reserved column name. 
E0705 Your custom model was disconnected due to a server connection interruption. Please check your internet connection, ensure the server is still running, and verify that the server URL is correct. Also, make sure no firewall or security settings are blocking the connection. 
E0706 Hume's API cannot reach your custom language model. Please ensure that your language model is accessible and try again. 
E0707 The message sent to Hume is not formed in the correct way of either {"type": "assistant_input", "text": <your text here>} or {"type": "assistant_end"} 
E0708 The chat group you're trying to resume does not exist. Please check the chat group identifier and try again. 
E0709 The configuration you are trying to use does not exist. Please check the configuration identifier and try again. 
E0710 You are attempting to resume a chat group with a new configuration. This operation is not allowed. Please use the original configuration or create a new chat group with the desired configuration. 
E0711 You are attempting to use a supplemental language model that is not currently available as a Hume-managed LLM. Please provide an API key from your model provider, or switch to a different supplemental LLM. 
E0712 The custom language model timed out during the connection attempt. This could be due to network issues, server availability, or firewall restrictions. Please check your connection and try again. 
E0713 The connection failed to the custom model due to a fatal error during the connection attempt. Please verify that the custom language model is correctly configured and accessible. 
E0714 The EVI WebSocket connection was closed due to the user inactivity timeout being reached. This timeout is specified in the inactivity parameter within the timeouts field of your EVI configuration. 
E0715 The EVI WebSocket connection was closed due to the maximum duration timeout being reached. This timeout is specified in the max_duration parameter within the timeouts field of your EVI configuration. 
E0716 The session settings provided were invalid and therefore were not applied. More details about how to resolve the misconfiguration are available in the API response. 
E0717 The EVI WebSocket connection was closed because a request was made to resume a chat group which contains an active chat. Please check that you are not already running an active chat session with the same chat group. 
E0718 The supplemental LLM provider has degraded API behavior. You can try again later or change the supplemental LLM in your EVI configuration. 
E0719 The supplemental LLM provider has an outage. You can try again later or change the supplemental LLM in your EVI configuration. 
E0720 The chat group configured for chat resumability could not be found. Please check that you specified your resumed_chat_group_id parameter correctly and that data retention is enabled in your account settings. 



The connection will be closed automatically after ten identical configuration
errors to avoid unintended looping.",
    "domain": "test.com",
    "hash": "#configuration-errors",
    "hierarchy": {
      "h0": {
        "title": "Errors",
      },
      "h2": {
        "id": "configuration-errors",
        "title": "Configuration errors",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.errors-configuration-errors-0",
    "org_id": "test",
    "pathname": "/docs/resources/errors",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Configuration errors",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/errors",
    "content": "Code Description 
1000 close_normal indicates an expected, intentional disconnect initiated by the server, such as when the built-in hang-up tool closes the connection. This code is also used for inactivity timeout and max duration timeout, indicating that the WebSocket connection was closed due to remaining inactive for too long or exceeding the maximum allowed duration. 
1008 policy_violation occurs when the WebSocket connection encounters an issue that cannot be recovered due to user error. Please review your request and ensure it adheres to the API's guidelines and policies. 
1011 server_error indicates that the WebSocket connection encountered an issue that cannot be recovered due to an internal Hume server error. Please try again later or contact support if the issue persists.",
    "domain": "test.com",
    "hash": "#websocket-status-codes",
    "hierarchy": {
      "h0": {
        "title": "Errors",
      },
      "h2": {
        "id": "websocket-status-codes",
        "title": "WebSocket status codes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.errors-websocket-status-codes-0",
    "org_id": "test",
    "pathname": "/docs/resources/errors",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "WebSocket status codes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/errors",
    "content": "If you encounter an error code starting with I (for example, error code I0100), it indicates an outage or a bug in a Hume service. Our team will already have been alerted of the internal error, but if you need immediate assistance please reach out to our support team.",
    "domain": "test.com",
    "hash": "#service-errors",
    "hierarchy": {
      "h0": {
        "title": "Errors",
      },
      "h2": {
        "id": "service-errors",
        "title": "Service errors",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.errors-service-errors-0",
    "org_id": "test",
    "pathname": "/docs/resources/errors",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Service errors",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/errors",
    "content": "Warnings indicate that the payload was configured correctly, but no results could be returned.
Code Description 
W0101 No vocal bursts could be detected in the media. 
W0102 No face meshes could be detected in the media. 
W0103 No faces could be detected in the media. 
W0104 No emotional language could be detected in the media. 
W0105 No speech could be detected in the media. 
W0106 No dynamic variable(s) found matching the one(s) specified.",
    "domain": "test.com",
    "hash": "#warnings",
    "hierarchy": {
      "h0": {
        "title": "Errors",
      },
      "h2": {
        "id": "warnings",
        "title": "Warnings",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.errors-warnings-0",
    "org_id": "test",
    "pathname": "/docs/resources/errors",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Warnings",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/errors",
    "content": "Some errors will not have an associated error code, but are documented here.",
    "domain": "test.com",
    "hash": "#common-errors",
    "hierarchy": {
      "h0": {
        "title": "Errors",
      },
      "h2": {
        "id": "common-errors",
        "title": "Common errors",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.errors-common-errors-0",
    "org_id": "test",
    "pathname": "/docs/resources/errors",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Common errors",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/errors",
    "code_snippets": [
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.batch import Prosody, Transcription, Models
from hume.expression_measurement.batch.types import InferenceBaseRequest

async def main():
    # Initialize an authenticated client
    client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

    # Define the filepath(s) of the file(s) you would like to analyze
    local_filepaths = [
        open("<YOUR_FILE_PATH>", mode="rb"),
    ]

    # Create a default configuration for the prosody model
    prosody_config = Prosody()

    # Create a transcription coniguration with the language set to English
    transcription_config = Transcription(language="en")

    # Create a Models object
    models_chosen = Models(prosody=prosody_config)
    
    # Create a stringified object containing the configuration
    stringified_configs = InferenceBaseRequest(models=models_chosen, transcription=transcription_config)

    # Start an inference job and print the job_id
    job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(
        json=stringified_configs, file=local_filepaths
    )
    print(job_id)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": "import asyncio
from hume import AsyncHumeClient
from hume.expression_measurement.batch import Prosody, Transcription, Models
from hume.expression_measurement.batch.types import InferenceBaseRequest

async def main():
    # Initialize an authenticated client
    client = AsyncHumeClient(api_key="<YOUR_API_KEY>")

    # Define the filepath(s) of the file(s) you would like to analyze
    local_filepaths = [
        open("<YOUR_FILE_PATH>", mode="rb"),
    ]

    # Create a default configuration for the prosody model
    prosody_config = Prosody()

    # Create a transcription coniguration with the language set to English
    transcription_config = Transcription(language="en")

    # Create a Models object
    models_chosen = Models(prosody=prosody_config)
    
    # Create a stringified object containing the configuration
    stringified_configs = InferenceBaseRequest(models=models_chosen, transcription=transcription_config)

    # Start an inference job and print the job_id
    job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(
        json=stringified_configs, file=local_filepaths
    )
    print(job_id)

if __name__ == "__main__":
    asyncio.run(main())",
        "lang": "python",
      },
      {
        "code": ""transcription": {
    "language": "en"
}",
        "lang": "json",
      },
      {
        "code": ""transcription": {
    "language": "en"
}",
        "lang": "json",
      },
    ],
    "content": "This error indicates that our transcription service had difficulty identifying the language spoken in your audio file or the quality was too low. We prioritize quality and accuracy, so if it cannot transcribe with confidence, our models won't be able to process it further.
By default, we use an automated language detection method for our Speech Prosody, Language, and NER models. However, if you know what language is being spoken in your media samples, you can specify it via its BCP-47 tag and potentially obtain more accurate results.
If you see the message above there are few steps you can do to resolve the issue:
Verify we support the language

Ensure you are providing clear, high-quality audio files.

Specify the language within your request if you know the language in the audio.










See the full list of languages supported by the Expression Measurement API here.
You may specify any of the following BCP-47 tags for transcription: zh, da, nl, en, en-AU, en-IN, en-NZ,
en-GB, fr, fr-CA, de, hi, hi-Latn, id, it, ja, ko, no, pl, pt, pt-BR, pt-PT,
ru, es, es-419, sv, ta, tr, or uk.",
    "domain": "test.com",
    "hash": "#transcript-confidence-below-threshold-value",
    "hierarchy": {
      "h0": {
        "title": "Errors",
      },
      "h2": {
        "id": "common-errors",
        "title": "Common errors",
      },
      "h3": {
        "id": "transcript-confidence-below-threshold-value",
        "title": "Transcript confidence below threshold value",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.resources.errors-transcript-confidence-below-threshold-value-0",
    "org_id": "test",
    "pathname": "/docs/resources/errors",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Transcript confidence below threshold value",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/science",
    "content": "What is it about speaking in person that allows us to understand each other so much more accurately than text alone? It isn’t what we say—it’s the way we say it. Science consistently demonstrates that expressions convey important information that is vital for social interaction and forms the building blocks of empathy.
That being said, expressions aren’t direct windows into the human mind. Measuring and interpreting expressive behavior is a complex and nuanced task that is the subject of ongoing scientific research.
The scientists at Hume AI have run some of the largest-ever psychology studies to better understand how humans express themselves. By investigating expressions around the world and what they mean to the people making them, we’ve mapped out the nuances of expression in the voice, language, and face in unprecedented detail. We’ve published this research in the world’s leading scientific journals and, for the first time, translated it into cutting-edge machine learning models.
These models, shaped by a new understanding of human expression, include:
Facial Expression

Speech Prosody

Vocal Bursts

Emotional Language",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.resources.science-root-0",
    "org_id": "test",
    "pathname": "/docs/resources/science",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "About the Science",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/science",
    "content": "Facial expression is the most well-studied modality of expressive behavior, but the overwhelming focus has been on six discrete categories of facial movement or time-consuming manual annotations of facial movements (the scientifically useful, but outdated, Facial Action Coding System). Our research shows that these approaches capture less than 30% of what typical facial expressions convey.
Hume’s Facial Emotional Expression model generates 48 outputs encompassing the dimensions of emotional meaning people reliably attribute to facial expressions. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.
Hume’s FACS 2.0 model is a new generation automated facial action coding system (FACS). With 55 outputs encompassing 26 traditional actions units (AUs) and 29 other descriptive features (e.g., smile, scowl), FACS 2.0 is even more comprehensive than manual FACS annotations.
Our facial expression models are packaged with face detection and work on both images and videos.
In addition to our image-based facial expression models, we also offer an Anonymized Facemesh model for applications in which it is essential to keep personally identifiable data on-device (e.g., for compliance with local laws). Instead of face images, our facemesh model processes facial landmarks detected using Google's MediaPipe library. It achieves about 80% accuracy relative to our image-based model.
To read more about the team’s research on facial expressions, check out our publications in American Psychologist (2018), Nature (2021), and iScience (2024).",
    "domain": "test.com",
    "hash": "#facial-expression",
    "hierarchy": {
      "h0": {
        "title": "About the Science",
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities",
      },
      "h3": {
        "id": "facial-expression",
        "title": "Facial Expression",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.resources.science-facial-expression-0",
    "org_id": "test",
    "pathname": "/docs/resources/science",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Facial Expression",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/science",
    "content": "Speech prosody is not about the words you say, but the way you say them. It is distinct from language (words) and from non-linguistic vocal utterances.
Our Speech Prosody model generates 48 outputs encompassing the 48 dimensions of emotional meaning that people reliably distinguish from variations in speech prosody. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.
Our Speech Prosody model is packaged with speech detection and works on both audio files and videos.
To read more about the team’s research on speech prosody, check out our publications in Nature Human Behaviour (2019) and Proceedings of the 31st ACM International Conference on Multimedia (2023).",
    "domain": "test.com",
    "hash": "#speech-prosody",
    "hierarchy": {
      "h0": {
        "title": "About the Science",
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities",
      },
      "h3": {
        "id": "speech-prosody",
        "title": "Speech Prosody",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.resources.science-speech-prosody-0",
    "org_id": "test",
    "pathname": "/docs/resources/science",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Speech Prosody",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/science",
    "content": "Non-linguistic vocal utterances, including sighs, laughs, oohs, ahhs, umms, and shrieks (to name but a few), are a particularly powerful and understudied modality of expressive behavior. Recent studies reveal that they reliably convey distinct emotional meanings that are extremely well-preserved across most cultures.
Non-linguistic vocal utterances have different acoustic characteristics than speech emotional intonation (prosody) and need to be modeled separately.
Our Vocal Burst Expression model generates 48 outputs encompassing the distinct dimensions of emotional meaning that people distinguish in vocal bursts. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.
Our Vocal Burst Description model provides a more descriptive and categorical view of nonverbal vocal expressions (“gasp,” “mhm,” etc.) intended for use cases such as audio captioning. It generates 67 descriptors, including 30 call types (“sigh,” “laugh,” “shriek,” etc.) and 37 common onomatopoeia transliterations of vocal bursts (“hmm,” “ha,” “mhm,” etc.).
Our vocal burst models are packaged with non-linguistic vocal utterance detection and works on both audio files and videos.
To read more about the team’s research on vocal bursts, check out our publications in American Psychologist (2019), Interspeech 2022, ICASSP 2023, and Nature Human Behaviour (2023).",
    "domain": "test.com",
    "hash": "#vocal-bursts",
    "hierarchy": {
      "h0": {
        "title": "About the Science",
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities",
      },
      "h3": {
        "id": "vocal-bursts",
        "title": "Vocal Bursts",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.resources.science-vocal-bursts-0",
    "org_id": "test",
    "pathname": "/docs/resources/science",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Vocal Bursts",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/science",
    "content": "The words we say include explicit disclosures of emotion and implicit emotional connotations. These meanings are complex and high-dimensional.
From written or spoken words, our Emotional Language model generates 53 outputs encompassing different dimensions of emotion that people often perceive from language. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.
Our Emotional Language model is packaged with speech transcription and works on audio files, videos, and text.
Our Named Entity Recognition (NER) model can also identify topics or entities (people, places, organizations, etc.) mentioned in speech or text and the tone of language they are associated with, as identified by our emotional language model.",
    "domain": "test.com",
    "hash": "#emotional-language",
    "hierarchy": {
      "h0": {
        "title": "About the Science",
      },
      "h2": {
        "id": "modalities",
        "title": "Modalities",
      },
      "h3": {
        "id": "emotional-language",
        "title": "Emotional Language",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.resources.science-emotional-language-0",
    "org_id": "test",
    "pathname": "/docs/resources/science",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Emotional Language",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/science",
    "content": "You can access a comprehensive list of our published research papers along with PDFs for download here.",
    "domain": "test.com",
    "hash": "#published-research",
    "hierarchy": {
      "h0": {
        "title": "About the Science",
      },
      "h2": {
        "id": "published-research",
        "title": "Published Research",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.science-published-research-0",
    "org_id": "test",
    "pathname": "/docs/resources/science",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Published Research",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/use-case-guidelines",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.resources.use-case-guidelines",
    "org_id": "test",
    "pathname": "/docs/resources/use-case-guidelines",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Use case guidelines",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/use-case-guidelines",
    "content": "Understanding expressive communication is essential to building technologies that address our needs and improve our well-being. But technologies that recognize language and nonverbal behavior can also pose risks. That’s why we require that all commercial applications incorporating our APIs adhere to the ethical guidelines of The Hume Initiative.",
    "domain": "test.com",
    "hash": "#ethical-guidelines",
    "hierarchy": {
      "h0": {
        "title": "Use case guidelines",
      },
      "h2": {
        "id": "ethical-guidelines",
        "title": "Ethical guidelines",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.use-case-guidelines-ethical-guidelines-0",
    "org_id": "test",
    "pathname": "/docs/resources/use-case-guidelines",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Ethical guidelines",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/use-case-guidelines",
    "content": "Use inductive methods to identify the expressive signals that matter for your application. Even if you are interested in a specific emotion like “anger,” how that emotion is expressed depends on setting: anger on a football field sounds different than anger on a customer service call. Our models succinctly compress the representation of emotional expression so that, even with limited data, you can examine how their outputs can be used in your specific research or application setting. You can do this by using statistical methods like regression or classification, or by examining the distribution of expressions in your data using our Playground.

Never assume a one-to-one mapping between emotional experience and expression. The outputs of our models should be treated as measurements of complex expressive behavior. We provide labels to our outputs indicating what these dimensions of expression are often reported to mean, but these labels should not be interpreted as direct inferences of how someone is feeling at any given time. Rather, “a full understanding of emotional expression and experience requires an appreciation of a wide degree of variability in display behavior, subjective experience, patterns of appraisal, and physiological response, both within and across emotion categories” (Cowen et al., 2019).

Never overlook the nuances in emotional expression. For instance, avoid the temptation to focus on just the top label. We provide interactive visualizations in our Playground to help you map out complex patterns in real-life emotional behavior. These visualizations are informed by recent advances in emotion science, departing from reductive models that long “anchored the science of emotion to a predominant focus on prototypical facial expressions of the “basic six”: anger, disgust, fear, sadness, surprise, and happiness,” and embracing how “new discoveries reveal that the two most commonly studied models of emotion—the basic six and the affective circumplex (comprising valence and arousal)—each capture at most 30% of the variance in the emotional experiences people reliably report and in the distinct expressions people reliably recognize.” (Cowen et al., 2019)

Account for culture-specific meanings and display tendencies. Studies have routinely observed subtle cultural differences in the meaning of expressions as well as broader “variations in the frequency and intensity with which different expressions were displayed” (Cowen et al., 2022). Given these differences, empathic AI applications should be tested in each population in which they are deployed and fine-tuned when necessary.
Read about the science behind our models if you’d like to delve deeper into how they work.",
    "domain": "test.com",
    "hash": "#scientific-best-practices",
    "hierarchy": {
      "h0": {
        "title": "Use case guidelines",
      },
      "h2": {
        "id": "scientific-best-practices",
        "title": "Scientific best practices",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.use-case-guidelines-scientific-best-practices-0",
    "org_id": "test",
    "pathname": "/docs/resources/use-case-guidelines",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Scientific best practices",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/privacy",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.docs.docs.resources.privacy",
    "org_id": "test",
    "pathname": "/docs/resources/privacy",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Privacy",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/privacy",
    "content": "Our Privacy Policy governs how we collect and use personal information submitted to our products.",
    "domain": "test.com",
    "hash": "#privacy-policy",
    "hierarchy": {
      "h0": {
        "title": "Privacy",
      },
      "h2": {
        "id": "privacy-policy",
        "title": "Privacy Policy",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.privacy-privacy-policy-0",
    "org_id": "test",
    "pathname": "/docs/resources/privacy",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Privacy Policy",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/privacy",
    "content": "Hume AI is HIPAA compliant, with features to enhance user privacy and data control. Our portal currently supports enabling/disabling these features in the user's profile page.
Zero Data Retention: This feature allows users to turn off the storage of all chat histories (transcripts) or voice recordings for the EVI API. Other metadata such as API usage information will still be stored.

Opt-Out of Data Being Used for Training: By default, anonymized data from user interactions with the EVI API is used to improve our models. Users can toggle this option to prevent their data from being used for training purposes.


For added control, use a custom language model and obtain a Business Associate Agreement (BAA) directly with the model provider. To request a BAA and/or Data Processing Addendum (DPA) with Hume, please contact legal@hume.ai.


By default, data retention is enabled, and user data may be used for model training. Users must explicitly opt out to disable these features.",
    "domain": "test.com",
    "hash": "#zero-data-retention-and-data-usage-options",
    "hierarchy": {
      "h0": {
        "title": "Privacy",
      },
      "h2": {
        "id": "privacy-policy",
        "title": "Privacy Policy",
      },
      "h3": {
        "id": "zero-data-retention-and-data-usage-options",
        "title": "Zero Data Retention and Data Usage Options",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.docs.docs.resources.privacy-zero-data-retention-and-data-usage-options-0",
    "org_id": "test",
    "pathname": "/docs/resources/privacy",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Zero Data Retention and Data Usage Options",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/privacy",
    "content": "Log into your Hume AI account.

Navigate to your Profile page by clicking on the profile icon on the sidebar.

Scroll down to the Privacy section where you will see the options for "Do not retain data" and "Do not use for training."

Toggle the switches next to these options to enable or disable them according to your preference.

Click on 'Save changes' to apply your settings.








Opting out of data retention will disable certain features, including the ability to resume chats and access your chat history.",
    "domain": "test.com",
    "hash": "#to-enable-or-disable-these-options",
    "hierarchy": {
      "h0": {
        "title": "Privacy",
      },
      "h2": {
        "id": "privacy-policy",
        "title": "Privacy Policy",
      },
      "h3": {
        "id": "zero-data-retention-and-data-usage-options",
        "title": "Zero Data Retention and Data Usage Options",
      },
      "h4": {
        "id": "to-enable-or-disable-these-options",
        "title": "To enable or disable these options",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.docs.docs.resources.privacy-to-enable-or-disable-these-options-0",
    "org_id": "test",
    "pathname": "/docs/resources/privacy",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "To enable or disable these options",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/privacy",
    "content": "Our API Data Usage Policy details how and when we store API data.",
    "domain": "test.com",
    "hash": "#api-data-usage-policy",
    "hierarchy": {
      "h0": {
        "title": "Privacy",
      },
      "h2": {
        "id": "api-data-usage-policy",
        "title": "API Data Usage Policy",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.privacy-api-data-usage-policy-0",
    "org_id": "test",
    "pathname": "/docs/resources/privacy",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "API Data Usage Policy",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/resources",
        "title": "Resources",
      },
    ],
    "canonicalPathname": "/docs/resources/privacy",
    "content": "Our Consumer Services FAQ explains how and when we store data processed by our frontend applications like our Playground.




For non-API consumer products like our Playground and Demo, we may use content such as images, video files, audio files, and text files to improve our services. You can opt out of having your content used to improve our services at any time by adjusting your settings in your profile page. This opt-out will apply on a going-forward basis only.
Please note that for our API product, Hume AI will not use data submitted by customers via our API to train or improve our models.


You can delete your account by submitting a user account deletion request in your profile page on the Hume playground. Once you submit your deletion request, we will delete your account within 30 days. Please note that for security reasons, once you delete your account, you may not re-sign up for an account with the same email address.


We share content with a select group of trusted service providers that help us provide our services. We share the minimum amount of content we need in order to accomplish this purpose and our service providers are subject to strict confidentiality and security obligations. Please see our Privacy Policy for more information on who we may share your content with.


Content is stored on Hume AI systems and our trusted service providers' systems in the US and around the world.


A limited number of authorized Hume AI personnel, may view and access user content only as needed for these reasons: (1) investigating abuse or a security incident; (2) to provide support to you if you reach out to us with questions about your account; (3) or to comply with legal obligations. Access to content is subject to technical access controls and limited only to authorized personnel on a need-to-know basis. Additionally, we monitor and log all access to user content and authorized personnel must undergo security and privacy training prior to accessing any user content.


No. We do not sell your data or share your content with third parties for marketing purposes.


Please change your privacy settings through the Profile page. For further assistance, message the moderators on our Discord Server.",
    "domain": "test.com",
    "hash": "#consumer-services-faq",
    "hierarchy": {
      "h0": {
        "title": "Privacy",
      },
      "h2": {
        "id": "consumer-services-faq",
        "title": "Consumer Services FAQ",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.docs.docs.resources.privacy-consumer-services-faq-0",
    "org_id": "test",
    "pathname": "/docs/resources/privacy",
    "tab": {
      "pathname": "/docs",
      "title": "Documentation",
    },
    "title": "Consumer Services FAQ",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added support for claude-3.5-sonnet-latest (currently points to claude-3-5-sonnet-20241022) and made this model the recommended supplemental LLM

Added support for tool use with Gemini models (gemini-1.5-pro and gemini-1.5-flash)",
    "domain": "test.com",
    "hash": "#evi-api-additions-10-25-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "10-25-2024",
        "title": "October 25, 2024",
      },
      "h3": {
        "id": "evi-api-additions-10-25-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-10-25-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Fixed a bug where context was incorrectly set as persistent and added to every user messages, despite being specified as type: temporary",
    "domain": "test.com",
    "hash": "#bugs-bashed-10-25-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "10-25-2024",
        "title": "October 25, 2024",
      },
      "h3": {
        "id": "bugs-bashed-10-25-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-10-25-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added a new base voice, Sunny, featuring a male voice with an Indian accent

Improved the reliability of the experimental custom voice creation feature by reducing hallucinations, and added 11 new adjustable parameters - articulation, buoyancy, enthusiasm, nasality, smoothness, tightness, assertiveness, confidence, gender, relaxedness, tepidity",
    "domain": "test.com",
    "hash": "#evi-api-additions-10-11-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "10-11-2024",
        "title": "October 11, 2024",
      },
      "h3": {
        "id": "evi-api-additions-10-11-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-10-11-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added a more informative error message for when Google Gemini models are overloaded, returning an E0718 error code instead of silently dropping the connection

Implemented Anthropic prompt caching to reduce latency with Claude 3 models, especially for longer prompts and conversations",
    "domain": "test.com",
    "hash": "#evi-api-changes-10-11-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "10-11-2024",
        "title": "October 11, 2024",
      },
      "h3": {
        "id": "evi-api-changes-10-11-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-10-11-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Reduced the frequency of all hallucinations when using EVI 2

Prevented voice hallucinations when EVI 2 outputs less common text formats, including numbered lists, emails, hashtags, very short messages, and numbers",
    "domain": "test.com",
    "hash": "#bugs-bashed-10-11-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "10-11-2024",
        "title": "October 11, 2024",
      },
      "h3": {
        "id": "bugs-bashed-10-11-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-10-11-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Upgraded gemini-1.5-pro and gemini-1.5-flash models to use the latest versions, gemini-1.5-pro-002 and gemini-1.5-flash-002

Improved audio quality for EVI phone calling",
    "domain": "test.com",
    "hash": "#evi-api-changes-09-27-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "09-27-2024",
        "title": "September 27, 2024",
      },
      "h3": {
        "id": "evi-api-changes-09-27-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-09-27-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Fixed an issue with the EVI WebSocket auto-reconnecting after timeouts, by updating the inactivity timeout socket close code from 1001 to 1000

Fixed a bug where the GET /chat_groups/{id} endpoint would return all chats, not just the chats in the chat_group",
    "domain": "test.com",
    "hash": "#bugs-bashed-09-27-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "09-27-2024",
        "title": "September 27, 2024",
      },
      "h3": {
        "id": "bugs-bashed-09-27-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-09-27-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added support for resuming chats with supplemental LLMs for EVI 2

Updated the DACHER base voice, making it significantly higher quality and more reliable

Improved EVI's ability to recover from accidental interruptions. Previously, if EVI was interrupted by non-speech sounds, EVI would stop and wait for further input. EVI will now continue speaking after these interruptions",
    "domain": "test.com",
    "hash": "#evi-changes-09-20-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "09-20-2024",
        "title": "September 20, 2024",
      },
      "h3": {
        "id": "evi-changes-09-20-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-changes-09-20-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Fixed an issue where @ signs would be removed in emails, leading to incorrect pronunciation; now they will be replaced with "at" and pronounced correctly

Fixed a bug with numbered lists, leading to lists being split into new lines and spoken incorrectly",
    "domain": "test.com",
    "hash": "#bugs-bashed-09-20-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "09-20-2024",
        "title": "September 20, 2024",
      },
      "h3": {
        "id": "bugs-bashed-09-20-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-09-20-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Released the EVI 2 API, with major improvements to the core EVI experience. Developers can try it now: EVI 2 docs

Introduced an experimental feature for creating custom voices through adjustable sliders: Custom voices",
    "domain": "test.com",
    "hash": "#evi-api-additions-09-13-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "09-13-2024",
        "title": "September 13, 2024",
      },
      "h3": {
        "id": "evi-api-additions-09-13-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-09-13-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Improved text validation permissiveness for config names and descriptions, allowing a wider range of printable characters

Added a new error code (E0720) to handle scenarios where data retention is off so chat group history is unavailable, providing a more informative error message before closing the session",
    "domain": "test.com",
    "hash": "#evi-api-changes-09-13-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "09-13-2024",
        "title": "September 13, 2024",
      },
      "h3": {
        "id": "evi-api-changes-09-13-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-09-13-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Fixed an issue to ensure chat_id is passed to users when using custom language models and phone calling together, enabling developers to retrieve post-call details with these features

Fixed a bug where resumed chat groups would use the first rather than the most recent config in the chat group when starting a new chat",
    "domain": "test.com",
    "hash": "#bugs-bashed-09-13-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "09-13-2024",
        "title": "September 13, 2024",
      },
      "h3": {
        "id": "bugs-bashed-09-13-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-09-13-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Enabled resuming previous chats with a new config. Previously, it was not possible to resume chats in a chat group with different configs. This change allows developers to change the prompt, voice, and other options in their config, while still retaining the context in their chat history: Chat resumability

Introduced the new E0717 error type, which will occur when a developer tries resuming a chat when one of the chats in its chat_group is already active.

Added two new errors for issues with supplemental language model providers. If a provider is overloaded, EVI will return E0718, and if a provider has unexpected internal errors EVI will return E0719. If these errors occur, developers can try again later or change their configurations to use a different LM provider.",
    "domain": "test.com",
    "hash": "#evi-api-changes-08-08-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "08-08-2024",
        "title": "August 8, 2024",
      },
      "h3": {
        "id": "evi-api-changes-08-08-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-08-08-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added support for new language models with the Groq provider: llama-3.1-70b-versatile and llama-3.1-8b-instant

Added support for new language models with the Fireworks provider: accounts/fireworks/models/llama-v3p1-405b-instruct, accounts/fireworks/models/llama-v3p1-70b-instruct, and accounts/fireworks/models/llama-v3p1-8b-instruct

Added a hang_up built in tool to allow EVI to end calls. To use this, developers can include the hang_up tool in the builtin_tools object when creating a config, and provide instructions on when EVI should end the call in the prompt",
    "domain": "test.com",
    "hash": "#evi-api-additions-08-02-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "08-02-2024",
        "title": "August 2, 2024",
      },
      "h3": {
        "id": "evi-api-additions-08-02-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-08-02-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added the ability to create a prompt during config creation. The new prompt object in the config creation request has three nullable fields (id, version, and text). Providing only text in the prompt field when creating a new config will create a new prompt

Dropped support for the older Llama 3 70B Instruct model from Fireworks (accounts/fireworks/models/llama-v3-70b-instruct), as it is replaced by the new Llama 3.1 70B model (accounts/fireworks/models/llama-v3p1-70b-instruct)",
    "domain": "test.com",
    "hash": "#evi-api-changes-08-02-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "08-02-2024",
        "title": "August 2, 2024",
      },
      "h3": {
        "id": "evi-api-changes-08-02-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-08-02-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Invalid SessionSettings payloads now return an E0716 error. Invalid payloads include empty system prompts, duplicate tool names, removing previously enabled tools, and overlapping builtin and custom tool names. If an update is invalid, the error message will explain why, and the SessionSettings will not be applied",
    "domain": "test.com",
    "hash": "#evi-api-changes-07-26-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "07-26-2024",
        "title": "July 26, 2024",
      },
      "h3": {
        "id": "evi-api-changes-07-26-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-07-26-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added the on_inactivity_timeout configuration option, allowing EVI to speak a message when the user is inactive for some period of time: Inactivity timeout message

Added the on_max_duration_timeout configuration option, allowing EVI to speak a message when the maximum chat duration is reached: Max duration timeout message

Added support for the gpt-4o-mini language model",
    "domain": "test.com",
    "hash": "#evi-api-additions-07-18-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "07-18-2024",
        "title": "July 18, 2024",
      },
      "h3": {
        "id": "evi-api-additions-07-18-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-07-18-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Updated the Hume Typescript SDK, with detailed changes and a migration guide in the release notes for version 0.8.2",
    "domain": "test.com",
    "hash": "#evi-api-changes-07-18-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "07-18-2024",
        "title": "July 18, 2024",
      },
      "h3": {
        "id": "evi-api-changes-07-18-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-07-18-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added dynamic variables, allowing developers to define variables in SessionSettings and reference their values in the system prompt (e.g., {{variable_name}}): Dynamic variables

Added support for the Google language model provider and the gemini-1.5-pro and gemini-1.5-flash language models

Added EVI configuration options to set timeouts for user inactivity (inactivity) and maximum session duration (max_duration): Timeouts

Added support for retrieving the phone numbers of inbound callers, using the metadata.twilio.caller_number property of the evi/chats/:id endpoint: List chat events

Added the /v0/evi/language-models API endpoint to retrieve the language models supported by EVI and the built-in tools available for each model",
    "domain": "test.com",
    "hash": "#evi-api-additions-07-12-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "07-12-2024",
        "title": "July 12, 2024",
      },
      "h3": {
        "id": "evi-api-additions-07-12-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-07-12-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added a config_id filter option for the GET /chat_groups endpoint, allowing developers to limit paginated results to chat groups associated with a specific config ID

Added a name filter option for the GET /configs, GET /tools, and GET /prompts endpoints. These allow developers to limit paginated results to only include objects with a specific name

Introduced data storage options for the EVI API. The "do not retain data" option disables storage of chat histories and voice recordings for EVI sessions. The "do not use for training" opts out of Hume using anonymized data from EVI sessions for model improvements. Developers can toggle these options from the profile page in the Hume portal

Added more descriptive error messages for transcription-related errors",
    "domain": "test.com",
    "hash": "#evi-api-additions-07-05-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "07-05-2024",
        "title": "July 5, 2024",
      },
      "h3": {
        "id": "evi-api-additions-07-05-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-07-05-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added a request_id field to ChatMetadata to uniquely identify sessions

Added an on_new_chat configuration option. Set event_messages.on_new_chat.enabled to true to have EVI speak first in the conversation. To control the exact text of that first message, also set event_messages.on_new_chat.text

Added an allow_short_responses configuration option, which allows developers to turn off short responses generated by Hume's empathic large language model (eLLM). To disable these responses, set ellm_model.allow_short_responses to false",
    "domain": "test.com",
    "hash": "#evi-api-additions-06-28-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "06-28-2024",
        "title": "June 28, 2024",
      },
      "h3": {
        "id": "evi-api-additions-06-28-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-06-28-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added support for Claude 3.5 Sonnet (claude-3-5-sonnet-20240620) to the EVI API",
    "domain": "test.com",
    "hash": "#evi-api-additions-06-21-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "06-21-2024",
        "title": "June 21, 2024",
      },
      "h3": {
        "id": "evi-api-additions-06-21-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-06-21-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Changed the default language model for the EVI API to Claude 3.5 Sonnet

Changed the default voice for the EVI API to Ito

Changed requirements to allow tool use if no language model is specified, allowing tool use when using the default LLM",
    "domain": "test.com",
    "hash": "#evi-api-changes-06-21-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "06-21-2024",
        "title": "June 21, 2024",
      },
      "h3": {
        "id": "evi-api-changes-06-21-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-06-21-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Fixed a bug where sending an AssistantInput message at the beginning of an EVI chat configured with Anthropic models would result in an error

Fixed a bug with chat resumability where previous chat group events were not being included in the LLM chat history, and EVI would forget details from before the chat was resumed",
    "domain": "test.com",
    "hash": "#bugs-bashed-06-21-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "06-21-2024",
        "title": "June 21, 2024",
      },
      "h3": {
        "id": "bugs-bashed-06-21-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-06-21-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added a total_pages field to all paginated EVI REST endpoints",
    "domain": "test.com",
    "hash": "#evi-api-additions-06-07-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "06-07-2024",
        "title": "June 7, 2024",
      },
      "h3": {
        "id": "evi-api-additions-06-07-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-06-07-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "EVI REST endpoints will now return the 201 status code instead of the 200 status code when creating new entities including new configs, chat groups, prompts, and tools

EVI REST endpoints will now return the 404 status code if referencing a config, chat, prompt, or tool that doesn't exist. If an invalid page number exceeding the total number of pages is specified, the endpoint will return an empty list rather than a 404 status code

Added more detailed error messages for Custom Language Model. If the connection between Hume's API and a developers's language model times out, we will now send an E0712:custom_language_model_timed_out error. If the connection fails, we will send an E0713:custom_language_model_connection_failed error",
    "domain": "test.com",
    "hash": "#evi-api-changes-06-07-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "06-07-2024",
        "title": "June 7, 2024",
      },
      "h3": {
        "id": "evi-api-changes-06-07-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-06-07-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added chat resumability, allowing developers to resume previous chats with EVI by specifying a chat group ID in the resumed_chat_group_id query parameter: Chat resumability

Added the api.hume.ai/v0/evi/chat_groups endpoint to support listing chat groups or listing events from a specific chat group: Chat groups endpoint

Added the ChatMetadata output message, which includes a chat_id to identify each individual chat with EVI and a chat_group_id to support resumability and group resumed chats together: ChatMetadata

Added support for chat resumability to the Hume Python SDK: Release notes for version 0.6.0

Added support for chat resumability and pause/resume messages to the Hume TypeScript SDK: Release notes for version 0.1.6",
    "domain": "test.com",
    "hash": "#evi-api-additions-05-31-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-31-2024",
        "title": "May 31, 2024",
      },
      "h3": {
        "id": "evi-api-additions-05-31-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-05-31-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added more detailed error messages for Custom Language Model. If Hume's API cannot reach a developers's language model, we will now send an E0706: custom_language_model_unreachable error to the developer

Added error messages for chat resumability - E0710: resuming_chat_group_with_new_config when a developer attempts to resume a chat group with a new config, E0708: chat_group_not_found when a chat group does not exist, and E0709: config_not_found when a config does not exist

Added an error message for unavailable EVI supplemental LLMs. While supplemental LLMs can always be enabled by passing an API for a 3rd party LLM service, if EVI is configured with an LLM that is not currently available as a Hume-managed LLM, we will send an E0711: language_model_unavailable error",
    "domain": "test.com",
    "hash": "#evi-api-changes-05-31-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-31-2024",
        "title": "May 31, 2024",
      },
      "h3": {
        "id": "evi-api-changes-05-31-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-05-31-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "code_snippets": [
      {
        "code": "# send this to add text
{"type": "assistant_input", "text": "<chunk>"}

# send this message when you're done speaking
{"type": "assistant_end"}",
      },
    ],
    "content": "Added support for streaming custom language model responses in parts. Developers can send text chunks to start generating audio responses much faster
The Custom Language Model endpoint now expects text to be formatted in the following payload:

Added support for pausing and resuming EVI responses with with pause_assistant_message and resume_assistant_message. Sending a pause message stops EVI from generating and speaking Assistant messages. Sending a resume message allows EVI to continue responding to the User messages",
    "domain": "test.com",
    "hash": "#evi-api-additions-05-24-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-24-2024",
        "title": "May 24, 2024",
      },
      "h3": {
        "id": "evi-api-additions-05-24-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-05-24-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Increased the limit for tool descriptions from 100 chars to 512 chars

Set the maximum length for tool_name to 64 chars",
    "domain": "test.com",
    "hash": "#evi-api-changes-05-24-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-24-2024",
        "title": "May 24, 2024",
      },
      "h3": {
        "id": "evi-api-changes-05-24-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-05-24-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added support for built-in tools, starting with web search: Using built-in tools

Added support for phone calling through a Twilio integration: Phone calling

Added DACHER voice to the voice configuration options

Added support for the gpt-4o language model",
    "domain": "test.com",
    "hash": "#evi-api-additions-05-17-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-17-2024",
        "title": "May 17, 2024",
      },
      "h3": {
        "id": "evi-api-additions-05-17-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-05-17-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Increased the limit for tool descriptions from 100 chars to 512 chars",
    "domain": "test.com",
    "hash": "#evi-api-changes-05-17-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-17-2024",
        "title": "May 17, 2024",
      },
      "h3": {
        "id": "evi-api-changes-05-17-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-05-17-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added support for three open-source models through the Groq language model provider: Gemma 7B (gemma-7b-it), Llama 3 8B (llama3-8b-8192), and Llama 3 70B (llama3-70b-8192)

Added support for Llama 30 70B language model through the Fireworks language model provider (accounts/fireworks/models/llama-v3-70b-instruct)

Added a custom_session_id field in the SessionSettings message, and documentation for using it: Custom Session ID",
    "domain": "test.com",
    "hash": "#evi-api-additions-05-10-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024",
      },
      "h3": {
        "id": "evi-api-additions-05-10-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-05-10-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Disabled short response generation for custom language models

Added error codes for when Hume credits run out while using EVI. Users will receive either the E0300 error code if they are out of credits or E0301 if they are blocked via subscription. The WebSocket connection will also be closed with code 1008",
    "domain": "test.com",
    "hash": "#evi-api-changes-05-10-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024",
      },
      "h3": {
        "id": "evi-api-changes-05-10-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-05-10-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Fixed an issue with the from_text field in UserMessage. It is now set to True if any part of the UserMessage is from a developer-provided UserInput message",
    "domain": "test.com",
    "hash": "#bugs-bashed-05-10-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-10-2024",
        "title": "May 10, 2024",
      },
      "h3": {
        "id": "bugs-bashed-05-10-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-05-10-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added support for Tools through our tool use feature

Added ToolErrorMessage as a supported input type",
    "domain": "test.com",
    "hash": "#evi-api-additions-05-03-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-03-2024",
        "title": "May 3, 2024",
      },
      "h3": {
        "id": "evi-api-additions-05-03-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-05-03-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added an error that returns status 400 if a Config, Tool, or Prompt is created with a name or versionDescription that's too long or non-ASCII. Names must be under 75 chars, versionDescription must be under 256 chars, description for Tools must be under 100 chars, fallback_content for Tools must be under 2048 chars, and model_resource for LanguageModels must be under 1024 chars

Fixed several edge cases and bugs involving Tool calls, including supporting only single tool calls with EVI (no parallel tool calling)",
    "domain": "test.com",
    "hash": "#bugs-bashed-05-03-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "05-03-2024",
        "title": "May 3, 2024",
      },
      "h3": {
        "id": "bugs-bashed-05-03-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-05-03-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Added support for reading language model type from EVI configs

Added support for reading language model temperature from EVI configs

Added system prompt to SessionSettings message to allow dynamic prompt updating",
    "domain": "test.com",
    "hash": "#evi-api-additions-04-30-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024",
      },
      "h3": {
        "id": "evi-api-additions-04-30-2024",
        "title": "EVI API additions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-additions-04-30-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API additions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Renamed TextInput message to UserInput to indicate this is text to be added to the chat history as a User message and used as context by the LLM

Renamed TtsInput message to AssistantInput to make it clear that this is input text to be spoken by EVI and added to the chat history as an Assistant message

Moved audio configuration options to SessionSettings message",
    "domain": "test.com",
    "hash": "#evi-api-changes-04-30-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024",
      },
      "h3": {
        "id": "evi-api-changes-04-30-2024",
        "title": "EVI API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-evi-api-changes-04-30-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "EVI API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [],
    "canonicalPathname": "/changelog",
    "content": "Fixed chats staying open after errors, chats will now end upon exceptions

Added an error thrown if config uses both custom_model and prompt, because custom language models do not use prompts

Fixed issue where erroring when sending errors would cause the API to get stuck

Added clearer errors for custom language models

Added unable to configure audio service error

Added an error to invalidate outdated language model responses",
    "domain": "test.com",
    "hash": "#bugs-bashed-04-30-2024",
    "hierarchy": {
      "h0": {
        "title": "Changelog",
      },
      "h2": {
        "id": "04-30-2024",
        "title": "April 30, 2024",
      },
      "h3": {
        "id": "bugs-bashed-04-30-2024",
        "title": "Bugs bashed",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.changelog-bugs-bashed-04-30-2024-0",
    "org_id": "test",
    "pathname": "/changelog",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Bugs bashed",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.list-tools",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of Tools.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools",
    "endpoint_path_alternates": [
      "/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.list-tools",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List tools",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.list-tools",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools",
    "endpoint_path_alternates": [
      "/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.list-tools-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tools",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List tools",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.create-tool",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "default_environment_id": "Default",
    "description": "Creates a Tool that can be added to an EVI configuration.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools",
    "endpoint_path_alternates": [
      "/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.create-tool",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create tool",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.create-tool",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "default_environment_id": "Default",
    "description": "Created",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools",
    "endpoint_path_alternates": [
      "/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
      "https://api.hume.ai/v0/evi/tools",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.create-tool-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create tool",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.list-tool-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "default_environment_id": "Default",
    "description": "Fetches a list of a Tool's versions.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.list-tool-versions",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List tool versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.list-tool-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.list-tool-versions-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/list-tool-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List tool versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.create-tool-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "default_environment_id": "Default",
    "description": "Updates a Tool by creating a new version of the Tool.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.create-tool-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create tool version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.create-tool-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "default_environment_id": "Default",
    "description": "Created",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.create-tool-version-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/create-tool-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create tool version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.delete-tool",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/delete-tool",
    "default_environment_id": "Default",
    "description": "Deletes a Tool and its versions.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.delete-tool",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/delete-tool",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete tool",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.update-tool-name",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/update-tool-name",
    "default_environment_id": "Default",
    "description": "Updates the name of a Tool.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}",
      "https://api.hume.ai/v0/evi/tools/:id",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.update-tool-name",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-name",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update tool name",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.get-tool-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "default_environment_id": "Default",
    "description": "Fetches a specified version of a Tool.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.get-tool-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get tool version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.get-tool-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.get-tool-version-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/get-tool-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get tool version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.delete-tool-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/delete-tool-version",
    "default_environment_id": "Default",
    "description": "Deletes a specified version of a Tool.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.delete-tool-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/delete-tool-version",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete tool version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.update-tool-description",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "default_environment_id": "Default",
    "description": "Updates the description of a specified Tool version.
Refer to our tool use guide for comprehensive instructions on defining and integrating tools into EVI.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.update-tool-description",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update tool description",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_tools.update-tool-description",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/tools",
        "title": "Tools",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/tools/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/tools/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/tools/:id/version/:version",
      "https://api.hume.ai/v0/evi/tools/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_tools.update-tool-description-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/tools/update-tool-description",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update tool description",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.list-prompts",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of Prompts.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts",
    "endpoint_path_alternates": [
      "/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.list-prompts",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List prompts",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.list-prompts",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts",
    "endpoint_path_alternates": [
      "/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.list-prompts-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompts",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List prompts",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.create-prompt",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "default_environment_id": "Default",
    "description": "Creates a Prompt that can be added to an EVI configuration.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts",
    "endpoint_path_alternates": [
      "/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.create-prompt",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create prompt",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.create-prompt",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "default_environment_id": "Default",
    "description": "Created",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts",
    "endpoint_path_alternates": [
      "/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
      "https://api.hume.ai/v0/evi/prompts",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.create-prompt-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create prompt",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.list-prompt-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "default_environment_id": "Default",
    "description": "Fetches a list of a Prompt's versions.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.list-prompt-versions",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List prompt versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.list-prompt-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.list-prompt-versions-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/list-prompt-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List prompt versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.create-prompt-verison",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "default_environment_id": "Default",
    "description": "Updates a Prompt by creating a new version of the Prompt.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.create-prompt-verison",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create prompt version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.create-prompt-verison",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "default_environment_id": "Default",
    "description": "Created",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.create-prompt-verison-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/create-prompt-verison",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create prompt version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.delete-prompt",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt",
    "default_environment_id": "Default",
    "description": "Deletes a Prompt and its versions.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.delete-prompt",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete prompt",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.update-prompt-name",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-name",
    "default_environment_id": "Default",
    "description": "Updates the name of a Prompt.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}",
      "https://api.hume.ai/v0/evi/prompts/:id",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.update-prompt-name",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-name",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update prompt name",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.get-prompt-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "default_environment_id": "Default",
    "description": "Fetches a specified version of a Prompt.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.get-prompt-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get prompt version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.get-prompt-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.get-prompt-version-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/get-prompt-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get prompt version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.delete-prompt-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt-version",
    "default_environment_id": "Default",
    "description": "Deletes a specified version of a Prompt.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.delete-prompt-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/delete-prompt-version",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete prompt version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.update-prompt-description",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "default_environment_id": "Default",
    "description": "Updates the description of a Prompt.
See our prompting guide for tips on crafting your system prompt.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.update-prompt-description",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update prompt description",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_prompts.update-prompt-description",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/prompts/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/prompts/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/prompts/:id/version/:version",
      "https://api.hume.ai/v0/evi/prompts/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_prompts.update-prompt-description-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/prompts/update-prompt-description",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update prompt description",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.list-custom-voices",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of Custom Voices.
Refer to our voices guide for details on creating a custom voice.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.list-custom-voices",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List custom voices",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.list-custom-voices",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.list-custom-voices-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/list-custom-voices",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List custom voices",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.create-custom-voice",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice",
    "default_environment_id": "Default",
    "description": "Creates a Custom Voice that can be added to an EVI configuration.
Refer to our voices guide for details on creating a custom voice.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.create-custom-voice",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create custom voice",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.create-custom-voice",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice",
    "default_environment_id": "Default",
    "description": "Created",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
      "https://api.hume.ai/v0/evi/custom_voices",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.create-custom-voice-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create custom voice",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.get-custom-voice",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice",
    "default_environment_id": "Default",
    "description": "Fetches a specific Custom Voice by ID.
Refer to our voices guide for details on creating a custom voice.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.get-custom-voice",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get specific custom voice by ID",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.get-custom-voice",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.get-custom-voice-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/get-custom-voice",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get specific custom voice by ID",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.create-custom-voice-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version",
    "default_environment_id": "Default",
    "description": "Updates a Custom Voice by creating a new version of the Custom Voice.
Refer to our voices guide for details on creating a custom voice.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.create-custom-voice-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create new version of existing custom voice",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.create-custom-voice-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version",
    "default_environment_id": "Default",
    "description": "Created",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.create-custom-voice-version-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/create-custom-voice-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create new version of existing custom voice",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.delete-custom-voice",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/delete-custom-voice",
    "default_environment_id": "Default",
    "description": "Deletes a Custom Voice and its versions.
Refer to our voices guide for details on creating a custom voice.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.delete-custom-voice",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/delete-custom-voice",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete a custom voice",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_customVoices.update-custom-voice-name",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/custom-voices",
        "title": "Custom Voices",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/custom-voices/update-custom-voice-name",
    "default_environment_id": "Default",
    "description": "Updates the name of a Custom Voice.
Refer to our voices guide for details on creating a custom voice.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/custom_voices/:id",
    "endpoint_path_alternates": [
      "/v0/evi/custom_voices/{id}",
      "https://api.hume.ai/v0/evi/custom_voices/:id",
      "https://api.hume.ai/v0/evi/custom_voices/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_customVoices.update-custom-voice-name",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/custom-voices/update-custom-voice-name",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update custom voice name",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.list-configs",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of Configs.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs",
    "endpoint_path_alternates": [
      "/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.list-configs",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List configs",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.list-configs",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs",
    "endpoint_path_alternates": [
      "/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.list-configs-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-configs",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List configs",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.create-config",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "default_environment_id": "Default",
    "description": "Creates a Config which can be applied to EVI.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs",
    "endpoint_path_alternates": [
      "/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.create-config",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create config",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.create-config",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "default_environment_id": "Default",
    "description": "Created",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs",
    "endpoint_path_alternates": [
      "/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
      "https://api.hume.ai/v0/evi/configs",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.create-config-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create config",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.list-config-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "default_environment_id": "Default",
    "description": "Fetches a list of a Config's versions.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.list-config-versions",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List config versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.list-config-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.list-config-versions-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/list-config-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List config versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.create-config-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "default_environment_id": "Default",
    "description": "Updates a Config by creating a new version of the Config.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.create-config-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create config version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.create-config-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "default_environment_id": "Default",
    "description": "Created",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.create-config-version-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/create-config-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create config version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.delete-config",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/delete-config",
    "default_environment_id": "Default",
    "description": "Deletes a Config and its versions.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.delete-config",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/delete-config",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete config",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.update-config-name",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/update-config-name",
    "default_environment_id": "Default",
    "description": "Updates the name of a Config.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}",
      "https://api.hume.ai/v0/evi/configs/:id",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.update-config-name",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-name",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update config name",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.get-config-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "default_environment_id": "Default",
    "description": "Fetches a specified version of a Config.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.get-config-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get config version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.get-config-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.get-config-version-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/get-config-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get config version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.delete-config-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/delete-config-version",
    "default_environment_id": "Default",
    "description": "Deletes a specified version of a Config.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "ErrorResponse",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.delete-config-version",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/delete-config-version",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete config version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.update-config-description",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "default_environment_id": "Default",
    "description": "Updates the description of a Config.
For more details on configuration options and how to configure EVI, see our configuration guide.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.update-config-description",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update config description",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_configs.update-config-description",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/configs",
        "title": "Configs",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/configs/:id/version/:version",
    "endpoint_path_alternates": [
      "/v0/evi/configs/{id}/version/{version}",
      "https://api.hume.ai/v0/evi/configs/:id/version/:version",
      "https://api.hume.ai/v0/evi/configs/%7Bid%7D/version/%7Bversion%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_configs.update-config-description-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/configs/update-config-description",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update config description",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.list-chats",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chats",
        "title": "Chats",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of Chats.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chats",
    "endpoint_path_alternates": [
      "/v0/evi/chats",
      "https://api.hume.ai/v0/evi/chats",
      "https://api.hume.ai/v0/evi/chats",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.list-chats",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List chats",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.list-chats",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chats",
        "title": "Chats",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chats",
    "endpoint_path_alternates": [
      "/v0/evi/chats",
      "https://api.hume.ai/v0/evi/chats",
      "https://api.hume.ai/v0/evi/chats",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.list-chats-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chats",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List chats",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.list-chat-events",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chats",
        "title": "Chats",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of Chat events.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chats/:id",
    "endpoint_path_alternates": [
      "/v0/evi/chats/{id}",
      "https://api.hume.ai/v0/evi/chats/:id",
      "https://api.hume.ai/v0/evi/chats/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.list-chat-events",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List chat events",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.list-chat-events",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chats",
        "title": "Chats",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chats/:id",
    "endpoint_path_alternates": [
      "/v0/evi/chats/{id}",
      "https://api.hume.ai/v0/evi/chats/:id",
      "https://api.hume.ai/v0/evi/chats/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.list-chat-events-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chats/list-chat-events",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List chat events",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.get-audio",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chats",
        "title": "Chats",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/get-audio",
    "default_environment_id": "Default",
    "description": "Fetches the audio of a previous Chat. For more details, see our guide on audio reconstruction here.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chats/:id/audio",
    "endpoint_path_alternates": [
      "/v0/evi/chats/{id}/audio",
      "https://api.hume.ai/v0/evi/chats/:id/audio",
      "https://api.hume.ai/v0/evi/chats/%7Bid%7D/audio",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.get-audio",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chats/get-audio",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get chat audio",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chats.get-audio",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chats",
        "title": "Chats",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chats/get-audio",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chats/:id/audio",
    "endpoint_path_alternates": [
      "/v0/evi/chats/{id}/audio",
      "https://api.hume.ai/v0/evi/chats/:id/audio",
      "https://api.hume.ai/v0/evi/chats/%7Bid%7D/audio",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chats.get-audio-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chats/get-audio",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get chat audio",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-groups",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups",
        "title": "Chat Groups",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of Chat Groups.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat_groups",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups",
      "https://api.hume.ai/v0/evi/chat_groups",
      "https://api.hume.ai/v0/evi/chat_groups",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.list-chat-groups",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List chat_groups",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-groups",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups",
        "title": "Chat Groups",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat_groups",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups",
      "https://api.hume.ai/v0/evi/chat_groups",
      "https://api.hume.ai/v0/evi/chat_groups",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.list-chat-groups-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-groups",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List chat_groups",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.get-chat-group",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups",
        "title": "Chat Groups",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group",
    "default_environment_id": "Default",
    "description": "Fetches a ChatGroup by ID, including a paginated list of Chats associated with the ChatGroup.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat_groups/:id",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}",
      "https://api.hume.ai/v0/evi/chat_groups/:id",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.get-chat-group",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get chat_group",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.get-chat-group",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups",
        "title": "Chat Groups",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat_groups/:id",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}",
      "https://api.hume.ai/v0/evi/chat_groups/:id",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.get-chat-group-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-chat-group",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get chat_group",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-group-events",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups",
        "title": "Chat Groups",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of Chat events associated with a Chat Group.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat_groups/:id/events",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}/events",
      "https://api.hume.ai/v0/evi/chat_groups/:id/events",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D/events",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.list-chat-group-events",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List chat events from a specific chat_group",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.list-chat-group-events",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups",
        "title": "Chat Groups",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat_groups/:id/events",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}/events",
      "https://api.hume.ai/v0/evi/chat_groups/:id/events",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D/events",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.list-chat-group-events-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/list-chat-group-events",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List chat events from a specific chat_group",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.get-audio",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups",
        "title": "Chat Groups",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio",
    "default_environment_id": "Default",
    "description": "Fetches a paginated list of audio for each Chat within the specified Chat Group. For more details, see our guide on audio reconstruction here.",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat_groups/:id/audio",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}/audio",
      "https://api.hume.ai/v0/evi/chat_groups/:id/audio",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D/audio",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.get-audio",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get chat group audio",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "endpoint_chatGroups.get-audio",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat-groups",
        "title": "Chat Groups",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat_groups/:id/audio",
    "endpoint_path_alternates": [
      "/v0/evi/chat_groups/{id}/audio",
      "https://api.hume.ai/v0/evi/chat_groups/:id/audio",
      "https://api.hume.ai/v0/evi/chat_groups/%7Bid%7D/audio",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ErrorResponse",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:endpoint_chatGroups.get-audio-response",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat-groups/get-audio",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get chat group audio",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c",
    "api_endpoint_id": "subpackage_chat.chat",
    "api_type": "websocket",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/empathic-voice-interface-evi",
        "title": "Empathic Voice Interface (EVI)",
      },
      {
        "pathname": "/reference/empathic-voice-interface-evi/chat",
        "title": "Chat",
      },
    ],
    "canonicalPathname": "/reference/empathic-voice-interface-evi/chat/chat",
    "default_environment_id": "Default",
    "domain": "test.com",
    "endpoint_path": "/v0/evi/chat",
    "endpoint_path_alternates": [
      "/v0/evi/chat",
      "wss://api.hume.ai/v0/evi/chat",
      "wss://api.hume.ai/v0/evi/chat",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "wss://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "websocket",
      "web socket",
      "stream",
      "SubscribeEvent",
      "PublishEvent",
    ],
    "method": "GET",
    "objectID": "test:test.com:ef70bbc1-c431-4ffd-b24b-e8c845ed1d5c:subpackage_chat.chat",
    "org_id": "test",
    "pathname": "/reference/empathic-voice-interface-evi/chat/chat",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Chat",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.list-jobs",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/expression-measurement-api",
        "title": "Expression Measurement API",
      },
      {
        "pathname": "/reference/expression-measurement-api/batch",
        "title": "Batch",
      },
    ],
    "canonicalPathname": "/reference/expression-measurement-api/batch/list-jobs",
    "default_environment_id": "Default",
    "description": "Sort and filter jobs.",
    "domain": "test.com",
    "endpoint_path": "/v0/batch/jobs",
    "endpoint_path_alternates": [
      "/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.list-jobs",
    "org_id": "test",
    "pathname": "/reference/expression-measurement-api/batch/list-jobs",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List jobs",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.start-inference-job",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/expression-measurement-api",
        "title": "Expression Measurement API",
      },
      {
        "pathname": "/reference/expression-measurement-api/batch",
        "title": "Batch",
      },
    ],
    "canonicalPathname": "/reference/expression-measurement-api/batch/start-inference-job",
    "default_environment_id": "Default",
    "description": "Start a new measurement inference job.",
    "domain": "test.com",
    "endpoint_path": "/v0/batch/jobs",
    "endpoint_path_alternates": [
      "/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.start-inference-job",
    "org_id": "test",
    "pathname": "/reference/expression-measurement-api/batch/start-inference-job",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Start inference job",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.get-job-details",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/expression-measurement-api",
        "title": "Expression Measurement API",
      },
      {
        "pathname": "/reference/expression-measurement-api/batch",
        "title": "Batch",
      },
    ],
    "canonicalPathname": "/reference/expression-measurement-api/batch/get-job-details",
    "default_environment_id": "Default",
    "description": "Get the request details and state of a given job.",
    "domain": "test.com",
    "endpoint_path": "/v0/batch/jobs/:id",
    "endpoint_path_alternates": [
      "/v0/batch/jobs/{id}",
      "https://api.hume.ai/v0/batch/jobs/:id",
      "https://api.hume.ai/v0/batch/jobs/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.get-job-details",
    "org_id": "test",
    "pathname": "/reference/expression-measurement-api/batch/get-job-details",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get job details",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.get-job-predictions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/expression-measurement-api",
        "title": "Expression Measurement API",
      },
      {
        "pathname": "/reference/expression-measurement-api/batch",
        "title": "Batch",
      },
    ],
    "canonicalPathname": "/reference/expression-measurement-api/batch/get-job-predictions",
    "default_environment_id": "Default",
    "description": "Get the JSON predictions of a completed inference job.",
    "domain": "test.com",
    "endpoint_path": "/v0/batch/jobs/:id/predictions",
    "endpoint_path_alternates": [
      "/v0/batch/jobs/{id}/predictions",
      "https://api.hume.ai/v0/batch/jobs/:id/predictions",
      "https://api.hume.ai/v0/batch/jobs/%7Bid%7D/predictions",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.get-job-predictions",
    "org_id": "test",
    "pathname": "/reference/expression-measurement-api/batch/get-job-predictions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get job predictions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.get-job-artifacts",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/expression-measurement-api",
        "title": "Expression Measurement API",
      },
      {
        "pathname": "/reference/expression-measurement-api/batch",
        "title": "Batch",
      },
    ],
    "canonicalPathname": "/reference/expression-measurement-api/batch/get-job-artifacts",
    "default_environment_id": "Default",
    "description": "Get the artifacts ZIP of a completed inference job.",
    "domain": "test.com",
    "endpoint_path": "/v0/batch/jobs/:id/artifacts",
    "endpoint_path_alternates": [
      "/v0/batch/jobs/{id}/artifacts",
      "https://api.hume.ai/v0/batch/jobs/:id/artifacts",
      "https://api.hume.ai/v0/batch/jobs/%7Bid%7D/artifacts",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "file",
    ],
    "method": "GET",
    "objectID": "test:test.com:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.get-job-artifacts",
    "org_id": "test",
    "pathname": "/reference/expression-measurement-api/batch/get-job-artifacts",
    "response_type": "file",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get job artifacts",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "endpoint_batch.start-inference-job-from-local-file",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/expression-measurement-api",
        "title": "Expression Measurement API",
      },
      {
        "pathname": "/reference/expression-measurement-api/batch",
        "title": "Batch",
      },
    ],
    "canonicalPathname": "/reference/expression-measurement-api/batch/start-inference-job-from-local-file",
    "default_environment_id": "Default",
    "description": "Start a new batch inference job.",
    "domain": "test.com",
    "endpoint_path": "/v0/batch/jobs",
    "endpoint_path_alternates": [
      "/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
      "https://api.hume.ai/v0/batch/jobs",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:a14d3798-e567-4432-a6b3-2fa8a50954c7:endpoint_batch.start-inference-job-from-local-file",
    "org_id": "test",
    "pathname": "/reference/expression-measurement-api/batch/start-inference-job-from-local-file",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Start inference job from local file",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "a14d3798-e567-4432-a6b3-2fa8a50954c7",
    "api_endpoint_id": "subpackage_stream.Stream",
    "api_type": "websocket",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/expression-measurement-api",
        "title": "Expression Measurement API",
      },
      {
        "pathname": "/reference/expression-measurement-api/stream",
        "title": "Stream",
      },
    ],
    "canonicalPathname": "/reference/expression-measurement-api/stream/stream",
    "default_environment_id": "Default",
    "domain": "test.com",
    "endpoint_path": "/v0/stream/models",
    "endpoint_path_alternates": [
      "/v0/stream/models",
      "wss://api.hume.ai/v0/stream/models",
      "wss://api.hume.ai/v0/stream/models",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "wss://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "websocket",
      "web socket",
      "stream",
      "SubscribeEvent",
      "StreamModelsEndpointPayload",
    ],
    "method": "GET",
    "objectID": "test:test.com:a14d3798-e567-4432-a6b3-2fa8a50954c7:subpackage_stream.Stream",
    "org_id": "test",
    "pathname": "/reference/expression-measurement-api/stream/stream",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Stream",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.list-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/list-files",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.list-files",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/list-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.list-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/list-files",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.list-files-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/list-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.create-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/create-files",
    "default_environment_id": "Default",
    "description": "Returns 201 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.create-files",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/create-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.create-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/create-files",
    "default_environment_id": "Default",
    "description": "List of Files with Attributes to be created",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#request",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.create-files-request",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/create-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.create-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/create-files",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files",
    "endpoint_path_alternates": [
      "/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
      "https://api.hume.ai/v0/registry/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.create-files-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/create-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.upload-file",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/upload-file",
    "default_environment_id": "Default",
    "description": "Upload a file synchronously. Returns 201 if successful. Files must have a name. Files must specify Content-Type. Request bodies, and therefore files, are limited to 100MB",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/upload",
    "endpoint_path_alternates": [
      "/v0/registry/files/upload",
      "https://api.hume.ai/v0/registry/files/upload",
      "https://api.hume.ai/v0/registry/files/upload",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.upload-file",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/upload-file",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Upload file",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.upload-file",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/upload-file",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/upload",
    "endpoint_path_alternates": [
      "/v0/registry/files/upload",
      "https://api.hume.ai/v0/registry/files/upload",
      "https://api.hume.ai/v0/registry/files/upload",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.upload-file-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/upload-file",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Upload file",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.get-file",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/get-file",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.get-file",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/get-file",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get file",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.get-file",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/get-file",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.get-file-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/get-file",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get file",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.delete-file",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/delete-file",
    "default_environment_id": "Default",
    "description": "Returns 204 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.delete-file",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/delete-file",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete file",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.delete-file",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/delete-file",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.delete-file-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/delete-file",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete file",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.update-file-name",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/update-file-name",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.update-file-name",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/update-file-name",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update file name",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.update-file-name",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/update-file-name",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/:id",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}",
      "https://api.hume.ai/v0/registry/files/:id",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.update-file-name-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/update-file-name",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update file name",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.get-file-predictions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/get-file-predictions",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/:id/predictions",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}/predictions",
      "https://api.hume.ai/v0/registry/files/:id/predictions",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D/predictions",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.get-file-predictions",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/get-file-predictions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get file predictions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_files.get-file-predictions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/files",
        "title": "Files",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/files/get-file-predictions",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/files/:id/predictions",
    "endpoint_path_alternates": [
      "/v0/registry/files/{id}/predictions",
      "https://api.hume.ai/v0/registry/files/:id/predictions",
      "https://api.hume.ai/v0/registry/files/%7Bid%7D/predictions",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_files.get-file-predictions-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/files/get-file-predictions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get file predictions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-datasets",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/list-datasets",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets",
    "endpoint_path_alternates": [
      "/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-datasets",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/list-datasets",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List datasets",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-datasets",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/list-datasets",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets",
    "endpoint_path_alternates": [
      "/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-datasets-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/list-datasets",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List datasets",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.create-dataset",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/create-dataset",
    "default_environment_id": "Default",
    "description": "Returns 201 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets",
    "endpoint_path_alternates": [
      "/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.create-dataset",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/create-dataset",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create dataset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.create-dataset",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/create-dataset",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets",
    "endpoint_path_alternates": [
      "/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
      "https://api.hume.ai/v0/registry/datasets",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.create-dataset-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/create-dataset",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create dataset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.get-dataset",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/get-dataset",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.get-dataset",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/get-dataset",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get dataset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.get-dataset",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/get-dataset",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.get-dataset-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/get-dataset",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get dataset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.create-dataset-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.create-dataset-version",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create dataset version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.create-dataset-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.create-dataset-version-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/create-dataset-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Create dataset version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.delete-dataset",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/delete-dataset",
    "default_environment_id": "Default",
    "description": "Returns 204 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.delete-dataset",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/delete-dataset",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete dataset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.delete-dataset",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/delete-dataset",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}",
      "https://api.hume.ai/v0/registry/datasets/:id",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.delete-dataset-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/delete-dataset",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Delete dataset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id/versions",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}/versions",
      "https://api.hume.ai/v0/registry/datasets/:id/versions",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D/versions",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-versions",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List dataset versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id/versions",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}/versions",
      "https://api.hume.ai/v0/registry/datasets/:id/versions",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D/versions",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-versions-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List dataset versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id/files",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}/files",
      "https://api.hume.ai/v0/registry/datasets/:id/files",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-files",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List dataset files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/:id/files",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/{id}/files",
      "https://api.hume.ai/v0/registry/datasets/:id/files",
      "https://api.hume.ai/v0/registry/datasets/%7Bid%7D/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-files-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List dataset files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.get-dataset-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/version/{id}",
      "https://api.hume.ai/v0/registry/datasets/version/:id",
      "https://api.hume.ai/v0/registry/datasets/version/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.get-dataset-version",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get dataset version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.get-dataset-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/version/{id}",
      "https://api.hume.ai/v0/registry/datasets/version/:id",
      "https://api.hume.ai/v0/registry/datasets/version/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.get-dataset-version-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/get-dataset-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get dataset version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-version-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/version/:id/files",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/version/{id}/files",
      "https://api.hume.ai/v0/registry/datasets/version/:id/files",
      "https://api.hume.ai/v0/registry/datasets/version/%7Bid%7D/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-version-files",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List dataset version files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_datasets.list-dataset-version-files",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/datasets",
        "title": "Datasets",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/datasets/version/:id/files",
    "endpoint_path_alternates": [
      "/v0/registry/datasets/version/{id}/files",
      "https://api.hume.ai/v0/registry/datasets/version/:id/files",
      "https://api.hume.ai/v0/registry/datasets/version/%7Bid%7D/files",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_datasets.list-dataset-version-files-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/datasets/list-dataset-version-files",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List dataset version files",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.list-models",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/list-models",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models",
    "endpoint_path_alternates": [
      "/v0/registry/models",
      "https://api.hume.ai/v0/registry/models",
      "https://api.hume.ai/v0/registry/models",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.list-models",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/list-models",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List models",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.list-models",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/list-models",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models",
    "endpoint_path_alternates": [
      "/v0/registry/models",
      "https://api.hume.ai/v0/registry/models",
      "https://api.hume.ai/v0/registry/models",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.list-models-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/list-models",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List models",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.get-model-details",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/get-model-details",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/{id}",
      "https://api.hume.ai/v0/registry/models/:id",
      "https://api.hume.ai/v0/registry/models/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.get-model-details",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/get-model-details",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get model details",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.get-model-details",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/get-model-details",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/{id}",
      "https://api.hume.ai/v0/registry/models/:id",
      "https://api.hume.ai/v0/registry/models/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.get-model-details-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/get-model-details",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get model details",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.update-model-name",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/update-model-name",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/{id}",
      "https://api.hume.ai/v0/registry/models/:id",
      "https://api.hume.ai/v0/registry/models/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.update-model-name",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/update-model-name",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update model name",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.update-model-name",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/update-model-name",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/{id}",
      "https://api.hume.ai/v0/registry/models/:id",
      "https://api.hume.ai/v0/registry/models/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.update-model-name-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/update-model-name",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update model name",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.list-model-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/list-model-versions",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/version",
    "endpoint_path_alternates": [
      "/v0/registry/models/version",
      "https://api.hume.ai/v0/registry/models/version",
      "https://api.hume.ai/v0/registry/models/version",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.list-model-versions",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/list-model-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List model versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.list-model-versions",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/list-model-versions",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/version",
    "endpoint_path_alternates": [
      "/v0/registry/models/version",
      "https://api.hume.ai/v0/registry/models/version",
      "https://api.hume.ai/v0/registry/models/version",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.list-model-versions-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/list-model-versions",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "List model versions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.get-model-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/get-model-version",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/version/{id}",
      "https://api.hume.ai/v0/registry/models/version/:id",
      "https://api.hume.ai/v0/registry/models/version/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.get-model-version",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/get-model-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get model version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.get-model-version",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/get-model-version",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/version/{id}",
      "https://api.hume.ai/v0/registry/models/version/:id",
      "https://api.hume.ai/v0/registry/models/version/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.get-model-version-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/get-model-version",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Get model version",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.update-model-description",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/update-model-description",
    "default_environment_id": "Default",
    "description": "Returns 200 if successful",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/version/{id}",
      "https://api.hume.ai/v0/registry/models/version/:id",
      "https://api.hume.ai/v0/registry/models/version/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.update-model-description",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/update-model-description",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update model description",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_models.update-model-description",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/models",
        "title": "Models",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/models/update-model-description",
    "default_environment_id": "Default",
    "description": "Success",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/models/version/:id",
    "endpoint_path_alternates": [
      "/v0/registry/models/version/{id}",
      "https://api.hume.ai/v0/registry/models/version/:id",
      "https://api.hume.ai/v0/registry/models/version/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "hash": "#response",
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_models.update-model-description-response",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/models/update-model-description",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Update model description",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_jobs.start-training-job",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/jobs",
        "title": "Jobs",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/jobs/start-training-job",
    "default_environment_id": "Default",
    "description": "Start a new custom models training job.",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/v0/batch/jobs/tl/train",
    "endpoint_path_alternates": [
      "/v0/registry/v0/batch/jobs/tl/train",
      "https://api.hume.ai/v0/registry/v0/batch/jobs/tl/train",
      "https://api.hume.ai/v0/registry/v0/batch/jobs/tl/train",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_jobs.start-training-job",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/jobs/start-training-job",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Start training job",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "869ccf90-2c73-42af-9c3f-94ddd28c5256",
    "api_endpoint_id": "endpoint_jobs.start-custom-models-inference-job",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/reference/custom-models-api",
        "title": "Custom Models API",
      },
      {
        "pathname": "/reference/custom-models-api/jobs",
        "title": "Jobs",
      },
    ],
    "canonicalPathname": "/reference/custom-models-api/jobs/start-custom-models-inference-job",
    "default_environment_id": "Default",
    "description": "Start a new custom models inference job.",
    "domain": "test.com",
    "endpoint_path": "/v0/registry/v0/batch/jobs/tl/inference",
    "endpoint_path_alternates": [
      "/v0/registry/v0/batch/jobs/tl/inference",
      "https://api.hume.ai/v0/registry/v0/batch/jobs/tl/inference",
      "https://api.hume.ai/v0/registry/v0/batch/jobs/tl/inference",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.hume.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:869ccf90-2c73-42af-9c3f-94ddd28c5256:endpoint_jobs.start-custom-models-inference-job",
    "org_id": "test",
    "pathname": "/reference/custom-models-api/jobs/start-custom-models-inference-job",
    "response_type": "json",
    "tab": {
      "pathname": "/reference",
      "title": "API Reference",
    },
    "title": "Start custom models inference job",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
]