[
  {
    "breadcrumbs": [],
    "description": "The easiest way to use LLMs

**BAML is a domain-specific language to generate structured outputs from LLMs -- with the best developer experience.**

With BAML you can build reliable Agents, Chatbots with RAG, extract data from PDFs, and more.",
    "indexSegmentId": "0",
    "slug": "home",
    "title": "🏠 Welcome",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "BAML is a domain-specific language to generate structured outputs from LLMs -- with the best developer experience.
With BAML you can build reliable Agents, Chatbots with RAG, extract data from PDFs, and more.
A small sample of features:

An amazingly fast developer experience for prompting in the BAML VSCode playground
Fully type-safe outputs, even when streaming structured data (that means autocomplete!)
Flexibility -- it works with any LLM, any language, and any schema.
State-of-the-art structured outputs that even outperform OpenAI with their own models -- plus it works with OpenSource models.

Products
Everything you need to know about how to get started with BAML. From installation to prompt engineering techniques.An online interactive playground to playaround with BAML without any installations.Examples of prompts, projects, and more.Language docs on all BAML syntax. Quickly learn syntax with simple examples and code snippets.
Motivation
Prompts are more than just f-strings; they're actual functions with logic that can quickly become complex to organize, maintain, and test.
Currently, developers craft LLM prompts as if they're writing raw HTML and CSS in text files, lacking:

Type safety
Hot-reloading or previews
Linting

The situation worsens when dealing with structured outputs. Since most prompts rely on Python and Pydantic, developers must execute their code and set up an entire Python environment just to test a minor prompt adjustment, or they have to setup a whole Python microservice just to call an LLM.
BAML allows you to view and run prompts directly within your editor, similar to how Markdown Preview function -- no additional setup necessary, that interoperates with all your favorite languages and frameworks.
Just as TSX/JSX provided the ideal abstraction for web development, BAML offers the perfect abstraction for prompt engineering. Watch our demo video to see it in action.
Comparisons
Here's our in-depth comparison with a couple of popular frameworks:

BAML vs Pydantic
BAML vs Marvin

",
    "indexSegmentId": "0",
    "slug": "home",
    "title": "🏠 Welcome",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "
An amazingly fast developer experience for prompting in the BAML VSCode playground
Fully type-safe outputs, even when streaming structured data (that means autocomplete!)
Flexibility -- it works with any LLM, any language, and any schema.
State-of-the-art structured outputs that even outperform OpenAI with their own models -- plus it works with OpenSource models.
",
    "indexSegmentId": "0",
    "slug": "home#a-small-sample-of-features",
    "title": "A small sample of features:",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "
An amazingly fast developer experience for prompting in the BAML VSCode playground
Fully type-safe outputs, even when streaming structured data (that means autocomplete!)
Flexibility -- it works with any LLM, any language, and any schema.
State-of-the-art structured outputs that even outperform OpenAI with their own models -- plus it works with OpenSource models.
",
    "indexSegmentId": "0",
    "slug": "home#a-small-sample-of-features",
    "title": "A small sample of features:",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "Everything you need to know about how to get started with BAML. From installation to prompt engineering techniques.An online interactive playground to playaround with BAML without any installations.Examples of prompts, projects, and more.Language docs on all BAML syntax. Quickly learn syntax with simple examples and code snippets.",
    "indexSegmentId": "0",
    "slug": "home#products",
    "title": "Products",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "Everything you need to know about how to get started with BAML. From installation to prompt engineering techniques.An online interactive playground to playaround with BAML without any installations.Examples of prompts, projects, and more.Language docs on all BAML syntax. Quickly learn syntax with simple examples and code snippets.",
    "indexSegmentId": "0",
    "slug": "home#products",
    "title": "Products",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "Prompts are more than just f-strings; they're actual functions with logic that can quickly become complex to organize, maintain, and test.
Currently, developers craft LLM prompts as if they're writing raw HTML and CSS in text files, lacking:

Type safety
Hot-reloading or previews
Linting

The situation worsens when dealing with structured outputs. Since most prompts rely on Python and Pydantic, developers must execute their code and set up an entire Python environment just to test a minor prompt adjustment, or they have to setup a whole Python microservice just to call an LLM.
BAML allows you to view and run prompts directly within your editor, similar to how Markdown Preview function -- no additional setup necessary, that interoperates with all your favorite languages and frameworks.
Just as TSX/JSX provided the ideal abstraction for web development, BAML offers the perfect abstraction for prompt engineering. Watch our demo video to see it in action.",
    "indexSegmentId": "0",
    "slug": "home#motivation",
    "title": "Motivation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "Prompts are more than just f-strings; they're actual functions with logic that can quickly become complex to organize, maintain, and test.
Currently, developers craft LLM prompts as if they're writing raw HTML and CSS in text files, lacking:

Type safety
Hot-reloading or previews
Linting

The situation worsens when dealing with structured outputs. Since most prompts rely on Python and Pydantic, developers must execute their code and set up an entire Python environment just to test a minor prompt adjustment, or they have to setup a whole Python microservice just to call an LLM.
BAML allows you to view and run prompts directly within your editor, similar to how Markdown Preview function -- no additional setup necessary, that interoperates with all your favorite languages and frameworks.
Just as TSX/JSX provided the ideal abstraction for web development, BAML offers the perfect abstraction for prompt engineering. Watch our demo video to see it in action.",
    "indexSegmentId": "0",
    "slug": "home#motivation",
    "title": "Motivation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "Here's our in-depth comparison with a couple of popular frameworks:

BAML vs Pydantic
BAML vs Marvin

",
    "indexSegmentId": "0",
    "slug": "home#comparisons",
    "title": "Comparisons",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "Here's our in-depth comparison with a couple of popular frameworks:

BAML vs Pydantic
BAML vs Marvin

",
    "indexSegmentId": "0",
    "slug": "home#comparisons",
    "title": "Comparisons",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/introduction/what-is-baml",
        "title": "Introduction",
      },
    ],
    "description": "The best way to understand BAML and its developer experience is to see it live in a demo (see below).",
    "indexSegmentId": "0",
    "slug": "guide/introduction/what-is-baml",
    "title": "What is BAML?",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Introduction",
    ],
    "content": "The best way to understand BAML and its developer experience is to see it live in a demo (see below).
Demo video
Here we write a BAML function definition, and then call it from a Python script.


Examples

Interactive NextJS app with streaming
Starter boilerplates for Python, Typescript, Ruby, etc.

High-level Developer Flow
Write a BAML function definitionclass WeatherAPI {
  city string @description("the user's city")
  timeOfDay string @description("As an ISO8601 timestamp")
}

function UseTool(user_message: string) -> WeatherAPI {
  client "openai/gpt-4o"
  prompt #"
    Extract.... {# we will explain the rest in the guides #}
  "#
}
Here you can run tests in the VSCode Playground.Generate baml_client from those .baml files.This is auto-generated code with all boilerplate to call the LLM endpoint, parse the output, fix broken JSON, and handle errors.Call your function in any languagewith type-safety, autocomplete, retry-logic, robust JSON parsing, etc..import asyncio
from baml_client import b
from baml_client.types import WeatherAPI

def main():
    weather_info = b.UseTool("What's the weather like in San Francisco?")
    print(weather_info)
    assert isinstance(weather_info, WeatherAPI)
    print(f"City: {weather_info.city}")
    print(f"Time of Day: {weather_info.timeOfDay}")

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI } from './baml_client/types'
import assert from 'assert'

const main = async () => {
  const weatherInfo = await b.UseTool("What's the weather like in San Francisco?")
  console.log(weatherInfo)
  assert(weatherInfo instanceof WeatherAPI)
  console.log(`City: ${weatherInfo.city}`)
  console.log(`Time of Day: ${weatherInfo.timeOfDay}`)
}
require_relative "baml_client/client"

$b = Baml.Client

def main
  weather_info = $b.UseTool(user_message: "What's the weather like in San Francisco?")
  puts weather_info
  raise unless weather_info.is_a?(Baml::Types::WeatherAPI)
  puts "City: #{weather_info.city}"
  puts "Time of Day: #{weather_info.timeOfDay}"
end
# read the installation guide for other languages!

Continue on to the Installation Guides for your language to setup BAML in a few minutes!
You don't need to migrate 100% of your LLM code to BAML in one go! It works along-side any existing LLM framework.",
    "indexSegmentId": "0",
    "slug": "guide/introduction/what-is-baml",
    "title": "What is BAML?",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/introduction/what-is-baml",
        "title": "Introduction",
      },
    ],
    "description": "Here we write a BAML function definition, and then call it from a Python script.

",
    "indexSegmentId": "0",
    "slug": "guide/introduction/what-is-baml#demo-video",
    "title": "Demo video",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Introduction",
    ],
    "content": "Here we write a BAML function definition, and then call it from a Python script.

",
    "indexSegmentId": "0",
    "slug": "guide/introduction/what-is-baml#demo-video",
    "title": "Demo video",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/introduction/what-is-baml",
        "title": "Introduction",
      },
    ],
    "description": "
Interactive NextJS app with streaming
Starter boilerplates for Python, Typescript, Ruby, etc.
",
    "indexSegmentId": "0",
    "slug": "guide/introduction/what-is-baml#examples",
    "title": "Examples",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Introduction",
    ],
    "content": "
Interactive NextJS app with streaming
Starter boilerplates for Python, Typescript, Ruby, etc.
",
    "indexSegmentId": "0",
    "slug": "guide/introduction/what-is-baml#examples",
    "title": "Examples",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/introduction/what-is-baml",
        "title": "Introduction",
      },
    ],
    "description": "Write a BAML function definitionclass WeatherAPI {
  city string @description("the user's city")
  timeOfDay string @description("As an ISO8601 timestamp")
}

function UseTool(user_message: string) -> WeatherAPI {
  client "openai/gpt-4o"
  prompt #"
    Extract.... {# we will explain the rest in the guides #}
  "#
}
Here you can run tests in the VSCode Playground.Generate baml_client from those .baml files.This is auto-generated code with all boilerplate to call the LLM endpoint, parse the output, fix broken JSON, and handle errors.Call your function in any languagewith type-safety, autocomplete, retry-logic, robust JSON parsing, etc..import asyncio
from baml_client import b
from baml_client.types import WeatherAPI

def main():
    weather_info = b.UseTool("What's the weather like in San Francisco?")
    print(weather_info)
    assert isinstance(weather_info, WeatherAPI)
    print(f"City: {weather_info.city}")
    print(f"Time of Day: {weather_info.timeOfDay}")

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI } from './baml_client/types'
import assert from 'assert'

const main = async () => {
  const weatherInfo = await b.UseTool("What's the weather like in San Francisco?")
  console.log(weatherInfo)
  assert(weatherInfo instanceof WeatherAPI)
  console.log(`City: ${weatherInfo.city}`)
  console.log(`Time of Day: ${weatherInfo.timeOfDay}`)
}
require_relative "baml_client/client"

$b = Baml.Client

def main
  weather_info = $b.UseTool(user_message: "What's the weather like in San Francisco?")
  puts weather_info
  raise unless weather_info.is_a?(Baml::Types::WeatherAPI)
  puts "City: #{weather_info.city}"
  puts "Time of Day: #{weather_info.timeOfDay}"
end
# read the installation guide for other languages!

Continue on to the Installation Guides for your language to setup BAML in a few minutes!
You don't need to migrate 100% of your LLM code to BAML in one go! It works along-side any existing LLM framework.",
    "indexSegmentId": "0",
    "slug": "guide/introduction/what-is-baml#high-level-developer-flow",
    "title": "High-level Developer Flow",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Introduction",
    ],
    "content": "Write a BAML function definitionclass WeatherAPI {
  city string @description("the user's city")
  timeOfDay string @description("As an ISO8601 timestamp")
}

function UseTool(user_message: string) -> WeatherAPI {
  client "openai/gpt-4o"
  prompt #"
    Extract.... {# we will explain the rest in the guides #}
  "#
}
Here you can run tests in the VSCode Playground.Generate baml_client from those .baml files.This is auto-generated code with all boilerplate to call the LLM endpoint, parse the output, fix broken JSON, and handle errors.Call your function in any languagewith type-safety, autocomplete, retry-logic, robust JSON parsing, etc..import asyncio
from baml_client import b
from baml_client.types import WeatherAPI

def main():
    weather_info = b.UseTool("What's the weather like in San Francisco?")
    print(weather_info)
    assert isinstance(weather_info, WeatherAPI)
    print(f"City: {weather_info.city}")
    print(f"Time of Day: {weather_info.timeOfDay}")

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI } from './baml_client/types'
import assert from 'assert'

const main = async () => {
  const weatherInfo = await b.UseTool("What's the weather like in San Francisco?")
  console.log(weatherInfo)
  assert(weatherInfo instanceof WeatherAPI)
  console.log(`City: ${weatherInfo.city}`)
  console.log(`Time of Day: ${weatherInfo.timeOfDay}`)
}
require_relative "baml_client/client"

$b = Baml.Client

def main
  weather_info = $b.UseTool(user_message: "What's the weather like in San Francisco?")
  puts weather_info
  raise unless weather_info.is_a?(Baml::Types::WeatherAPI)
  puts "City: #{weather_info.city}"
  puts "Time of Day: #{weather_info.timeOfDay}"
end
# read the installation guide for other languages!

Continue on to the Installation Guides for your language to setup BAML in a few minutes!
You don't need to migrate 100% of your LLM code to BAML in one go! It works along-side any existing LLM framework.",
    "indexSegmentId": "0",
    "slug": "guide/introduction/what-is-baml#high-level-developer-flow",
    "title": "High-level Developer Flow",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/introduction/what-is-baml",
        "title": "Introduction",
      },
    ],
    "description": "**baml_src** is where you keep all your BAML files, and where all the prompt-related code lives. It must be named `baml_src` for our tooling to pick it up, but it can live wherever you want.

It helps keep your project organized, and makes it easy to separate prompt engineering from the rest of your code.

<img src="file:eac0fc8d-8eb6-46bd-8c46-ae7a229b6571" width="300px"/>


Some things to note:
1. All declarations within this directory are accessible across all files contained in the `baml_src` folder.
2. You can have multiple files, and even nest subdirectories.

You don't need to worry about including this directory when deploying your code. See: [Deploying](/guide/development/deploying/aws)",
    "indexSegmentId": "0",
    "slug": "guide/introduction/baml_src",
    "title": "What is baml_src?",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Introduction",
    ],
    "content": "baml_src is where you keep all your BAML files, and where all the prompt-related code lives. It must be named baml_src for our tooling to pick it up, but it can live wherever you want.
It helps keep your project organized, and makes it easy to separate prompt engineering from the rest of your code.

Some things to note:

All declarations within this directory are accessible across all files contained in the baml_src folder.
You can have multiple files, and even nest subdirectories.

You don't need to worry about including this directory when deploying your code. See: Deploying",
    "indexSegmentId": "0",
    "slug": "guide/introduction/baml_src",
    "title": "What is baml_src?",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/introduction/what-is-baml",
        "title": "Introduction",
      },
    ],
    "description": "**baml_client** is the code that gets generated from your BAML files that transforms your BAML prompts into the same equivalent function in your language, with validated type-safe outputs.
<img src="file:eac0fc8d-8eb6-46bd-8c46-ae7a229b6571" width="300px"/>

```python Python
from baml_client import b
resume_info = b.ExtractResume("....some text...")
```

This has all the boilerplate to:
1. call the LLM endpoint with the right parameters, 
2. parse the output, 
3. fix broken JSON (if any)
4. return the result in a nice typed object.
5. handle errors

In Python, your BAML types get converted to Pydantic models. In Typescript, they get converted to TypeScript types, and so on. **BAML acts like a universal type system that can be used in any language**.",
    "indexSegmentId": "0",
    "slug": "guide/introduction/baml_client",
    "title": "What is baml_client?",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Introduction",
    ],
    "content": "baml_client is the code that gets generated from your BAML files that transforms your BAML prompts into the same equivalent function in your language, with validated type-safe outputs.

from baml_client import b
resume_info = b.ExtractResume("....some text...")

This has all the boilerplate to:

call the LLM endpoint with the right parameters,
parse the output,
fix broken JSON (if any)
return the result in a nice typed object.
handle errors

In Python, your BAML types get converted to Pydantic models. In Typescript, they get converted to TypeScript types, and so on. BAML acts like a universal type system that can be used in any language.
Generating baml_client
Refer to the Installation guides for how to set this up for your language, and how to generate it.
But at a high-level, you just include a generator block in any of your BAML files.
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "python/pydantic"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "sync"
    
    // Version of runtime to generate code for (should match installed baml-py version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "typescript"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "async"
    
    // Version of runtime to generate code for (should match the package @boundaryml/baml version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "ruby/sorbet"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"
    output_type "rest/openapi"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.54.0"

    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client
    // This command will be run from within $output_dir
    on_generate "npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g OPENAPI_CLIENT_TYPE -o ."
}

The baml_client transforms a BAML function into the same equivalent function in your language,",
    "indexSegmentId": "0",
    "slug": "guide/introduction/baml_client",
    "title": "What is baml_client?",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/introduction/what-is-baml",
        "title": "Introduction",
      },
    ],
    "description": "Refer to the Installation guides for how to set this up for your language, and how to generate it.
But at a high-level, you just include a generator block in any of your BAML files.
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "python/pydantic"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "sync"
    
    // Version of runtime to generate code for (should match installed baml-py version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "typescript"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "async"
    
    // Version of runtime to generate code for (should match the package @boundaryml/baml version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "ruby/sorbet"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"
    output_type "rest/openapi"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.54.0"

    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client
    // This command will be run from within $output_dir
    on_generate "npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g OPENAPI_CLIENT_TYPE -o ."
}

The baml_client transforms a BAML function into the same equivalent function in your language,",
    "indexSegmentId": "0",
    "slug": "guide/introduction/baml_client#generating-baml_client",
    "title": "Generating baml_client",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Introduction",
    ],
    "content": "Refer to the Installation guides for how to set this up for your language, and how to generate it.
But at a high-level, you just include a generator block in any of your BAML files.
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "python/pydantic"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "sync"
    
    // Version of runtime to generate code for (should match installed baml-py version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "typescript"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "async"
    
    // Version of runtime to generate code for (should match the package @boundaryml/baml version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "ruby/sorbet"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.54.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"
    output_type "rest/openapi"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.54.0"

    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client
    // This command will be run from within $output_dir
    on_generate "npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g OPENAPI_CLIENT_TYPE -o ."
}

The baml_client transforms a BAML function into the same equivalent function in your language,",
    "indexSegmentId": "0",
    "slug": "guide/introduction/baml_client#generating-baml_client",
    "title": "Generating baml_client",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-editors/vs-code-extension",
        "title": "Installation: Editors",
      },
    ],
    "description": "We provide a BAML VSCode extension:     https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension



| Feature | Supported |
|---------|-----------|
| Syntax highlighting for BAML files | ✅ |
| Code snippets for BAML | ✅ |
| LLM playground for testing BAML functions | ✅ |
| Jump to definition for BAML files | ✅ |
| Jump to definition between Python/TS files and BAML files | ✅ |
| Auto generate `baml_client` on save | ✅ |
| BAML formatter | ❌ |",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension",
    "title": "VSCode Extension",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Editors",
    ],
    "content": "We provide a BAML VSCode extension:     https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension
| Feature | Supported |
|---------|-----------|
| Syntax highlighting for BAML files | ✅ |
| Code snippets for BAML | ✅ |
| LLM playground for testing BAML functions | ✅ |
| Jump to definition for BAML files | ✅ |
| Jump to definition between Python/TS files and BAML files | ✅ |
| Auto generate baml_client on save | ✅ |
| BAML formatter | ❌ |
Opening BAML Playground
Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.

Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.

Setting Env Variables
Click on the Settings button in top right of the playground and set the environment variables.
It should have an indicator saying how many unset variables are there.

The playground should persist the environment variables between closing and opening VSCode.
You can set environment variables lazily. If anything is unset you'll get an error when you run the function.
Environment Variables are stored in VSCode's local storage! We don't save any additional data to disk, or send them across the network.
Running Tests


Click on the Run All Tests button in the playground.


Press the ▶️ button next to an individual test case to run that just that test case.


Switching Functions
The playground will automatically switch to the function you're currently editing.
To manually change it, click on the current function name in the playground (next to the dropdown) and search for your desired function.
Switching Test Cases
The test case with the highlighted background is the currently rendered test case. Clicking on a different test case will render that test case.

You can toggle between seeing the results of all test cases or all test cases for the current function.
",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension",
    "title": "VSCode Extension",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-editors/vs-code-extension",
        "title": "Installation: Editors",
      },
    ],
    "description": "Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.

Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.
",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#opening-baml-playground",
    "title": "Opening BAML Playground",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Editors",
    ],
    "content": "Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.

Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.
",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#opening-baml-playground",
    "title": "Opening BAML Playground",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-editors/vs-code-extension",
        "title": "Installation: Editors",
      },
    ],
    "description": "Click on the Settings button in top right of the playground and set the environment variables.
It should have an indicator saying how many unset variables are there.

The playground should persist the environment variables between closing and opening VSCode.
You can set environment variables lazily. If anything is unset you'll get an error when you run the function.
Environment Variables are stored in VSCode's local storage! We don't save any additional data to disk, or send them across the network.",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#setting-env-variables",
    "title": "Setting Env Variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Editors",
    ],
    "content": "Click on the Settings button in top right of the playground and set the environment variables.
It should have an indicator saying how many unset variables are there.

The playground should persist the environment variables between closing and opening VSCode.
You can set environment variables lazily. If anything is unset you'll get an error when you run the function.
Environment Variables are stored in VSCode's local storage! We don't save any additional data to disk, or send them across the network.",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#setting-env-variables",
    "title": "Setting Env Variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-editors/vs-code-extension",
        "title": "Installation: Editors",
      },
    ],
    "description": "

Click on the Run All Tests button in the playground.


Press the ▶️ button next to an individual test case to run that just that test case.

",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#running-tests",
    "title": "Running Tests",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Editors",
    ],
    "content": "

Click on the Run All Tests button in the playground.


Press the ▶️ button next to an individual test case to run that just that test case.

",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#running-tests",
    "title": "Running Tests",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-editors/vs-code-extension",
        "title": "Installation: Editors",
      },
    ],
    "description": "The playground will automatically switch to the function you're currently editing.
To manually change it, click on the current function name in the playground (next to the dropdown) and search for your desired function.",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#switching-functions",
    "title": "Switching Functions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Editors",
    ],
    "content": "The playground will automatically switch to the function you're currently editing.
To manually change it, click on the current function name in the playground (next to the dropdown) and search for your desired function.",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#switching-functions",
    "title": "Switching Functions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-editors/vs-code-extension",
        "title": "Installation: Editors",
      },
    ],
    "description": "The test case with the highlighted background is the currently rendered test case. Clicking on a different test case will render that test case.

You can toggle between seeing the results of all test cases or all test cases for the current function.
",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#switching-test-cases",
    "title": "Switching Test Cases",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Editors",
    ],
    "content": "The test case with the highlighted background is the currently rendered test case. Clicking on a different test case will render that test case.

You can toggle between seeing the results of all test cases or all test cases for the current function.
",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/vs-code-extension#switching-test-cases",
    "title": "Switching Test Cases",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-editors/vs-code-extension",
        "title": "Installation: Editors",
      },
    ],
    "description": "Refer to the [Cursor Extension Installation Guide](https://www.cursor.com/how-to-install-extension) to install the extension in Cursor.

<Warning>
You may need to update BAML extension manually using the process above. Auto-update does not seem to be working well for many extensions in Cursor.
</Warning>",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/cursor-extension",
    "title": "Cursor",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Editors",
    ],
    "content": "Refer to the Cursor Extension Installation Guide to install the extension in Cursor.
You may need to update BAML extension manually using the process above. Auto-update does not seem to be working well for many extensions in Cursor.",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/cursor-extension",
    "title": "Cursor",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-editors/vs-code-extension",
        "title": "Installation: Editors",
      },
    ],
    "description": "We don't currently have any tier support for any other editors.

* JetBrains IDEs
* Helix
* Zed
* Vim
* Emacs
* Sublime Text
* Atom


Since the extension is a language server, we can technically pull out the language server and syntax highlighter and support any editor supporting the language server protocol.
If you're interested in contributing to the project and supporting another editor, [please reach out](/contact).

An alternative is to edit your files in our [Playground](https://www.promptfiddle.com/), and copy the code into your editor, but we recommend using VSCode to edit BAML files for now.",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/others",
    "title": "Others",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Editors",
    ],
    "content": "We don't currently have any tier support for any other editors.

JetBrains IDEs
Helix
Zed
Vim
Emacs
Sublime Text
Atom

Since the extension is a language server, we can technically pull out the language server and syntax highlighter and support any editor supporting the language server protocol.
If you're interested in contributing to the project and supporting another editor, please reach out.
An alternative is to edit your files in our Playground, and copy the code into your editor, but we recommend using VSCode to edit BAML files for now.",
    "indexSegmentId": "0",
    "slug": "guide/installation-editors/others",
    "title": "Others",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-language/python",
        "title": "Installation: Language",
      },
    ],
    "description": "<Note>You can check out this repo: 
https://github.com/BoundaryML/baml-examples/tree/main/python-fastapi-starter</Note>

To set up BAML with Python do the following:

<Steps>
  ### Install BAML VSCode/Cursor Extension
      https://marketplace.visualstudio.com/items?itemName=boundary.baml-extension

      - syntax highlighting
      - testing playground
      - prompt previews

      <Tip>
      In your VSCode User Settings, highly recommend adding this to get better autocomplete for python in general, not just BAML.

      ```json
      {
        "python.analysis.typeCheckingMode": "basic"
      }
      ```
      </Tip>
  
  ### Install BAML
      <CodeBlocks>
        ```bash pip
        pip install baml-py
        ```

        ```bash poetry
        poetry add baml-py
        ```

         ```bash uv
        uv add baml-py
        ```
        </CodeBlocks>
  
  ### Add BAML to your existing project
      This will give you some starter BAML code in a `baml_src` directory.

      <CodeBlocks>
      ```bash pip
      baml-cli init
      ```

      ```bash poetry
      poetry run baml-cli init
      ```

      ```bash uv
      uv run baml-cli init
      ```
      </CodeBlocks>
  
  ### Generate the `baml_client` python module from `.baml` files

  One of the files in your `baml_src` directory will have a [generator block](/ref/baml/generator). The next commmand will auto-generate the `baml_client` directory, which will have auto-generated python code to call your BAML functions. 
  
  Any types defined in .baml files will be converted into Pydantic models in the `baml_client` directory.


    <CodeBlocks>
    ```bash pip
    baml-cli generate
    ```


    ```bash poetry
    poetry run baml-cli generate
    ```

    ```bash uv
    uv run baml-cli generate
    ```
    </CodeBlocks>

    See [What is baml_client](/guide/introduction/baml_client) to learn more about how this works.
    <img src="file:362a1df5-e3ab-4ef1-9327-82ddc7addaba" />

    <Tip>
      If you set up the [VSCode extension](https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension), it will automatically run `baml-cli generate` on saving a BAML file.
    </Tip>

  
  ### Use a BAML function in Python!
    <Error>If `baml_client` doesn't exist, make sure to run the previous step! </Error>

    <CodeBlocks>
    ```python main.py 
    from baml_client.sync_client import b
    from baml_client.types import Resume

    def example(raw_resume: str) -> Resume: 
      # BAML's internal parser guarantees ExtractResume
      # to be always return a Resume type
      response = b.ExtractResume(raw_resume)
      return response

    def example_stream(raw_resume: str) -> Resume:
      stream = b.stream.ExtractResume(raw_resume)
      for msg in stream:
        print(msg) # This will be a PartialResume type
      
      # This will be a Resume type
      final = stream.get_final_response()

      return final
    ```

    ```python async_main.py
    from baml_client.async_client import b
    from baml_client.types import Resume

    async def example(raw_resume: str) -> Resume: 
      # BAML's internal parser guarantees ExtractResume
      # to be always return a Resume type
      response = await b.ExtractResume(raw_resume)
      return response

    async def example_stream(raw_resume: str) -> Resume:
      stream = b.stream.ExtractResume(raw_resume)
      async for msg in stream:
        print(msg) # This will be a PartialResume type
      
      # This will be a Resume type
      final = stream.get_final_response()

      return final
    ```
    </CodeBlocks>
  
</Steps>",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/python",
    "title": "Python",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Language",
    ],
    "content": "You can check out this repo:
https://github.com/BoundaryML/baml-examples/tree/main/python-fastapi-starter
To set up BAML with Python do the following:
Install BAML VSCode/Cursor Extensionhttps://marketplace.visualstudio.com/items?itemName=boundary.baml-extension
syntax highlighting
testing playground
prompt previews
In your VSCode User Settings, highly recommend adding this to get better autocomplete for python in general, not just BAML.{
  "python.analysis.typeCheckingMode": "basic"
}
Install BAMLpip install baml-py
poetry add baml-py
uv add baml-py
Add BAML to your existing projectThis will give you some starter BAML code in a baml_src directory.baml-cli init
poetry run baml-cli init
uv run baml-cli init
Generate the baml_client python module from .baml filesOne of the files in your baml_src directory will have a generator block. The next commmand will auto-generate the baml_client directory, which will have auto-generated python code to call your BAML functions.Any types defined in .baml files will be converted into Pydantic models in the baml_client directory.baml-cli generate
poetry run baml-cli generate
uv run baml-cli generate
See What is baml_client to learn more about how this works.If you set up the VSCode extension, it will automatically run baml-cli generate on saving a BAML file.Use a BAML function in Python!If baml_client doesn't exist, make sure to run the previous step! from baml_client.sync_client import b
from baml_client.types import Resume

def example(raw_resume: str) -> Resume: 
  # BAML's internal parser guarantees ExtractResume
  # to be always return a Resume type
  response = b.ExtractResume(raw_resume)
  return response

def example_stream(raw_resume: str) -> Resume:
  stream = b.stream.ExtractResume(raw_resume)
  for msg in stream:
    print(msg) # This will be a PartialResume type
  
  # This will be a Resume type
  final = stream.get_final_response()

  return final
from baml_client.async_client import b
from baml_client.types import Resume

async def example(raw_resume: str) -> Resume: 
  # BAML's internal parser guarantees ExtractResume
  # to be always return a Resume type
  response = await b.ExtractResume(raw_resume)
  return response

async def example_stream(raw_resume: str) -> Resume:
  stream = b.stream.ExtractResume(raw_resume)
  async for msg in stream:
    print(msg) # This will be a PartialResume type
  
  # This will be a Resume type
  final = stream.get_final_response()

  return final

BAML with Jupyter Notebooks
You can use the baml_client in a Jupyter notebook.
One of the common problems is making sure your code changes are picked up by the notebook without having to restart the whole kernel (and re-run all the cells)
To make sure your changes in .baml files are reflected in your notebook you must do these steps:
Setup the autoreload extension%load_ext autoreload
%autoreload 2
This will make sure to reload imports, such as baml_client's "b" object before every cell runs.Import baml_client module in your notebookNote it's different from how we import in python.# Assuming your baml_client is inside a dir called app/
import app.baml_client as client # you can name this "llm" or "baml" or whatever you want
Usually we import things as
from baml_client import b, and we can call our functions using b, but the %autoreload notebook extension does not work well with from...import statements.Call BAML functions using the module name as a prefixraw_resume = "Here's some resume text"
client.b.ExtractResume(raw_resume)
Now your changes in .baml files are reflected in your notebook automatically, without needing to restart the Jupyter kernel.If you want to keep using the from baml_client import b style, you'll just need to re-import it everytime you regenerate the baml_client.Pylance will complain about any schema changes you make in .baml files. You can ignore these errors. If you want it to pick up your new types, you'll need to restart the kernel.
This auto-reload approach works best if you're only making changes to the prompts.
You're all set! Continue on to the Deployment Guides for your language to learn how to deploy your BAML code or check out the Interactive Examples to see more examples.",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/python",
    "title": "Python",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-language/python",
        "title": "Installation: Language",
      },
    ],
    "description": "You can use the baml_client in a Jupyter notebook.
One of the common problems is making sure your code changes are picked up by the notebook without having to restart the whole kernel (and re-run all the cells)
To make sure your changes in .baml files are reflected in your notebook you must do these steps:
Setup the autoreload extension%load_ext autoreload
%autoreload 2
This will make sure to reload imports, such as baml_client's "b" object before every cell runs.Import baml_client module in your notebookNote it's different from how we import in python.# Assuming your baml_client is inside a dir called app/
import app.baml_client as client # you can name this "llm" or "baml" or whatever you want
Usually we import things as
from baml_client import b, and we can call our functions using b, but the %autoreload notebook extension does not work well with from...import statements.Call BAML functions using the module name as a prefixraw_resume = "Here's some resume text"
client.b.ExtractResume(raw_resume)
Now your changes in .baml files are reflected in your notebook automatically, without needing to restart the Jupyter kernel.If you want to keep using the from baml_client import b style, you'll just need to re-import it everytime you regenerate the baml_client.Pylance will complain about any schema changes you make in .baml files. You can ignore these errors. If you want it to pick up your new types, you'll need to restart the kernel.
This auto-reload approach works best if you're only making changes to the prompts.
You're all set! Continue on to the Deployment Guides for your language to learn how to deploy your BAML code or check out the Interactive Examples to see more examples.",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/python#baml-with-jupyter-notebooks",
    "title": "BAML with Jupyter Notebooks",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Language",
    ],
    "content": "You can use the baml_client in a Jupyter notebook.
One of the common problems is making sure your code changes are picked up by the notebook without having to restart the whole kernel (and re-run all the cells)
To make sure your changes in .baml files are reflected in your notebook you must do these steps:
Setup the autoreload extension%load_ext autoreload
%autoreload 2
This will make sure to reload imports, such as baml_client's "b" object before every cell runs.Import baml_client module in your notebookNote it's different from how we import in python.# Assuming your baml_client is inside a dir called app/
import app.baml_client as client # you can name this "llm" or "baml" or whatever you want
Usually we import things as
from baml_client import b, and we can call our functions using b, but the %autoreload notebook extension does not work well with from...import statements.Call BAML functions using the module name as a prefixraw_resume = "Here's some resume text"
client.b.ExtractResume(raw_resume)
Now your changes in .baml files are reflected in your notebook automatically, without needing to restart the Jupyter kernel.If you want to keep using the from baml_client import b style, you'll just need to re-import it everytime you regenerate the baml_client.Pylance will complain about any schema changes you make in .baml files. You can ignore these errors. If you want it to pick up your new types, you'll need to restart the kernel.
This auto-reload approach works best if you're only making changes to the prompts.
You're all set! Continue on to the Deployment Guides for your language to learn how to deploy your BAML code or check out the Interactive Examples to see more examples.",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/python#baml-with-jupyter-notebooks",
    "title": "BAML with Jupyter Notebooks",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-language/python",
        "title": "Installation: Language",
      },
    ],
    "description": "<Note>You can check out this repo: https://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter</Note>

To set up BAML with Typescript do the following:

<Steps>
  ### Install BAML VSCode/Cursor Extension
      https://marketplace.visualstudio.com/items?itemName=boundary.baml-extension

      - syntax highlighting
      - testing playground
      - prompt previews
  
  ### Install BAML
      <CodeBlocks>
        ```bash npm
        npm install @boundaryml/baml
        ```
        
        ```bash pnpm
        pnpm add @boundaryml/baml
        ```

        ```bash yarn
        yarn add @boundaryml/baml
        ```

        ```bash deno
        deno install npm:@boundaryml/baml
        ```
    </CodeBlocks>
  
  ### Add BAML to your existing project
      This will give you some starter BAML code in a `baml_src` directory.
      
      <CodeBlocks>
        ```bash npm
        npx baml-cli init
        ```
        
        ```bash pnpm
        pnpx baml-cli init
        ```

        ```bash yarn
        yarn baml-cli init
        ```

        ```bash deno
        dpx baml-cli init
        ```
    </CodeBlocks>
  
  ### Generate the `baml_client` typescript package from `.baml` files

    One of the files in your `baml_src` directory will have a [generator block](/ref/baml/generator). This tells BAML how to generate the `baml_client` directory, which will have auto-generated typescript code to call your BAML functions.

    ```bash
    npx baml-cli generate
    ```

    You can modify your `package.json` so you have a helper prefix in front of your build command.

    ```json package.json
    {
      "scripts": {
        // Add a new command
        "baml-generate": "baml-cli generate",
        // Always call baml-generate on every build.
        "build": "npm run baml-generate && tsc --build",
      }
    }
    ```
    
    See [What is baml_src](/guide/introduction/baml_src) to learn more about how this works.
    <img src="file:793e29f1-ff38-4e00-b009-36b0685a8945" />

   
    <Tip>
      If you set up the [VSCode extension](https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension), it will automatically run `baml-cli generate` on saving a BAML file.
    </Tip>
  
  ### Use a BAML function in Typescript!
    <Error>If `baml_client` doesn't exist, make sure to run the previous step! </Error>

    <CodeBlocks>
    ```typescript index.ts
    import {b} from "baml_client"
    import type {Resume} from "baml_client/types"

    async function Example(raw_resume: string): Resume {
      // BAML's internal parser guarantees ExtractResume
      // to be always return a Resume type
      const response = await b.ExtractResume(raw_resume);
      return response;
    }

    async function ExampleStream(raw_resume: string): Resume {
      const stream = b.stream.ExtractResume(raw_resume);
      for await (const msg of stream) {
        console.log(msg) // This will be a Partial<Resume> type
      }

      // This is guaranteed to be a Resume type.
      return await stream.get_final_response();
    }
    ```

    ```typescript sync_example.ts
    import {b} from "baml_client/sync_client"
    import type {Resume} from "baml_client/types"

    function Example(raw_resume: string): Resume {
      // BAML's internal parser guarantees ExtractResume
      // to be always return a Resume type
      const response = b.ExtractResume(raw_resume);
      return response;
    }

    // Streaming is not available in the sync_client.

    ```
    </CodeBlocks>
</Steps>",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/typescript",
    "title": "Typescript",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Language",
    ],
    "content": "You can check out this repo: https://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter
To set up BAML with Typescript do the following:
Install BAML VSCode/Cursor Extensionhttps://marketplace.visualstudio.com/items?itemName=boundary.baml-extension
syntax highlighting
testing playground
prompt previews
Install BAMLnpm install @boundaryml/baml
pnpm add @boundaryml/baml
yarn add @boundaryml/baml
deno install npm:@boundaryml/baml
Add BAML to your existing projectThis will give you some starter BAML code in a baml_src directory.npx baml-cli init
pnpx baml-cli init
yarn baml-cli init
dpx baml-cli init
Generate the baml_client typescript package from .baml filesOne of the files in your baml_src directory will have a generator block. This tells BAML how to generate the baml_client directory, which will have auto-generated typescript code to call your BAML functions.npx baml-cli generate
You can modify your package.json so you have a helper prefix in front of your build command.{
  "scripts": {
    // Add a new command
    "baml-generate": "baml-cli generate",
    // Always call baml-generate on every build.
    "build": "npm run baml-generate && tsc --build",
  }
}
See What is baml_src to learn more about how this works.If you set up the VSCode extension, it will automatically run baml-cli generate on saving a BAML file.Use a BAML function in Typescript!If baml_client doesn't exist, make sure to run the previous step! import {b} from "baml_client"
import type {Resume} from "baml_client/types"

async function Example(raw_resume: string): Resume {
  // BAML's internal parser guarantees ExtractResume
  // to be always return a Resume type
  const response = await b.ExtractResume(raw_resume);
  return response;
}

async function ExampleStream(raw_resume: string): Resume {
  const stream = b.stream.ExtractResume(raw_resume);
  for await (const msg of stream) {
    console.log(msg) // This will be a Partial<Resume> type
  }

  // This is guaranteed to be a Resume type.
  return await stream.get_final_response();
}
import {b} from "baml_client/sync_client"
import type {Resume} from "baml_client/types"

function Example(raw_resume: string): Resume {
  // BAML's internal parser guarantees ExtractResume
  // to be always return a Resume type
  const response = b.ExtractResume(raw_resume);
  return response;
}

// Streaming is not available in the sync_client.

",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/typescript",
    "title": "Typescript",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-language/python",
        "title": "Installation: Language",
      },
    ],
    "description": "<Note>You can check out this repo: https://github.com/BoundaryML/baml-examples/tree/main/ruby-example</Note>

To set up BAML with Ruby do the following:

<Steps>
  ### Install BAML VSCode Extension
      https://marketplace.visualstudio.com/items?itemName=boundary.baml-extension

      - syntax highlighting
      - testing playground
      - prompt previews
  
  ### Install BAML
      ```bash bundle
      bundle add baml sorbet-runtime
      ```
  
  ### Add BAML to your existing project
      This will give you some starter BAML code in a `baml_src` directory.

      ```bash
      bundle exec baml-cli init
      ```
  
  ### Generate Ruby code from `.baml` files

    ```bash
    bundle exec baml-cli generate
    ```
    `
    See [What is baml_src](/guide/introduction/baml_src) to learn more about how this works.
    <img src="file:465b1613-60ca-43e7-bcb3-9daa124d4ca4" />
    
    As fun as writing BAML is, we want you be able to leverage BAML with existing ruby modules. This command gives you a ruby module that is a type-safe interface to every BAML function.

    <Tip>
      Our [VSCode extension](https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension) automatically runs this command when you save a BAML file.
    </Tip>
  
  ### Use a BAML function in Ruby!
    <Error>If `baml_client` doesn't exist, make sure to run the previous step!</Error>

    <CodeBlocks>
    ```ruby main.rb
    require_relative "baml_client/client"

    def example(raw_resume)
        # r is an instance of Baml::Types::Resume, defined in baml_client/types
        r = Baml.Client.ExtractResume(resume: raw_resume)

        puts "ExtractResume response:"
        puts r.inspect
    end

    def example_stream(raw_resume)
        stream = Baml.Client.stream.ExtractResume(resume: raw_resume)

        stream.each do |msg|
            # msg is an instance of Baml::PartialTypes::Resume
            # defined in baml_client/partial_types
            puts msg.inspect
        end

        stream.get_final_response
    end

    example 'Grace Hopper created COBOL'
    example_stream 'Grace Hopper created COBOL'
    ```
    </CodeBlocks>
</Steps>",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/ruby",
    "title": "Ruby",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Language",
    ],
    "content": "You can check out this repo: https://github.com/BoundaryML/baml-examples/tree/main/ruby-example
To set up BAML with Ruby do the following:
Install BAML VSCode Extensionhttps://marketplace.visualstudio.com/items?itemName=boundary.baml-extension
syntax highlighting
testing playground
prompt previews
Install BAMLbundle add baml sorbet-runtime
Add BAML to your existing projectThis will give you some starter BAML code in a baml_src directory.bundle exec baml-cli init
Generate Ruby code from .baml filesbundle exec baml-cli generate
`
See What is baml_src to learn more about how this works.As fun as writing BAML is, we want you be able to leverage BAML with existing ruby modules. This command gives you a ruby module that is a type-safe interface to every BAML function.Our VSCode extension automatically runs this command when you save a BAML file.Use a BAML function in Ruby!If baml_client doesn't exist, make sure to run the previous step!require_relative "baml_client/client"

def example(raw_resume)
    # r is an instance of Baml::Types::Resume, defined in baml_client/types
    r = Baml.Client.ExtractResume(resume: raw_resume)

    puts "ExtractResume response:"
    puts r.inspect
end

def example_stream(raw_resume)
    stream = Baml.Client.stream.ExtractResume(resume: raw_resume)

    stream.each do |msg|
        # msg is an instance of Baml::PartialTypes::Resume
        # defined in baml_client/partial_types
        puts msg.inspect
    end

    stream.get_final_response
end

example 'Grace Hopper created COBOL'
example_stream 'Grace Hopper created COBOL'
",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/ruby",
    "title": "Ruby",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-language/python",
        "title": "Installation: Language",
      },
    ],
    "description": "<Info>
  Requires BAML version >=0.55
</Info>

<Warning>
  This feature is a preview feature and may change. Please provide feedback either
  in [Discord][discord] or on [GitHub][openapi-feedback-github-issue] so that
  we can stabilize the feature and keep you updated!
</Warning>

BAML allows you to expose your BAML functions as RESTful APIs:

<img src="file:6b35bd0f-00fa-4c89-a93e-ddaf360396d5" />

We integrate with [OpenAPI](openapi) (universal API definitions), so you can get typesafe client libraries for free!

<Steps>
  ### Install BAML VSCode Extension
      https://marketplace.visualstudio.com/items?itemName=boundary.baml-extension

      - syntax highlighting
      - testing playground
      - prompt previews

  ### Install NPX + OpenAPI

     <Tabs>
        <Tab title="macOS (brew)">
          ```bash
          brew install npm openapi-generator
          # 'npm' will install npx
          # 'openapi-generator' will install both Java and openapi-generator-cli
          ```
        </Tab>

        <Tab title="Linux (apt)">
          OpenAPI requires `default-jdk`

          ```bash
          apt install npm default-jdk -y
          # 'npm' will install npx; 'default-jdk' will install java
          ```
        </Tab>

        <Tab title="Linux (yum/dnf)">
          OpenAPI requires Java

          ```bash
          dnf install npm java-21-openjdk -y
          # dnf is the successor to yum
          ```

          Amazon Linux 2023:
          ```bash
          dnf install npm java-21-amazon-corretto -y
          # 'npm' will install npx
          # 'java-21-amazon-corretto' will install java
          ```

          Amazon Linux 2:
          ```bash
          curl -sL https://rpm.nodesource.com/setup_16.x | bash -
          yum install nodejs -y
          # 'nodejs' will install npx
          amazon-linux-extras install java-openjdk11 -y
          # 'java-openjdk11' will install java
          ```
        </Tab>

        <Tab title="Windows">
          To install `npx` and `java` (for OpenAPI):

            1. Use the [Node.js installer](https://nodejs.org/en/download/prebuilt-installer) to install `npx` (default installer settings are fine).
            2. Run `npm install -g npm@latest` to update `npx` (there is currently an [issue][npx-windows-issue] with the default install of `npx` on Windows where it doesn't work out of the box).
            3. Run the [Adoptium OpenJDK `.msi` installer](https://adoptium.net/temurin/releases/?os=windows) (install the JDK; default installer settings are fine).

          You can verify that `npx` and `java` are installed by running:
          
          ```powershell
          npx -version
          java -version
          ```
        </Tab>

        <Tab title="Other">
          To install `npx`, use the [Node.js installer](https://nodejs.org/en/download/prebuilt-installer).

          To install `java` (for OpenAPI), use the [Adoptium OpenJDK packages](https://adoptium.net/installation/linux/).
        </Tab>
      </Tabs>

  ### Add BAML to your existing project
      This will give you some starter BAML code in a `baml_src` directory.
    <Tabs>

      <Tab title="C#">
      ```bash
      npx @boundaryml/baml init \
        --client-type rest/openapi --openapi-client-type csharp
      ```
      </Tab>

      <Tab title="C++">

      <Tip>OpenAPI supports [5 different C++ client types][openapi-client-types];
      any of them will work with BAML.</Tip>

      ```bash
      npx @boundaryml/baml init \
        --client-type rest/openapi --openapi-client-type cpp-restsdk
      ```
      </Tab>

      <Tab title="Go">
      ```bash
      npx @boundaryml/baml init \
        --client-type rest/openapi --openapi-client-type go
      ```
      </Tab>

      <Tab title="Java">
      
      ```bash
      npx @boundaryml/baml init \
        --client-type rest/openapi --openapi-client-type java
      ```

      Notice that `on_generate` has been initialized for you to:

      - run the OpenAPI generator to generate a Java client library, and _also_
      - run `mvn clean install` to install the generated client library to your
        local Maven repository

      <Warning>
        If you only use Maven through an IDE (e.g. IntelliJ IDEA), you should
        remove `&& mvn clean install` from the generated `on_generate` command.
      </Warning>

      </Tab>

      <Tab title="PHP">
      ```bash
      npx @boundaryml/baml init \
        --client-type rest/openapi --openapi-client-type php
      ```
      </Tab>

      <Tab title="Ruby">
      ```bash
      npx @boundaryml/baml init \
        --client-type rest/openapi --openapi-client-type ruby
      ```
      </Tab>

      <Tab title="Rust">
      ```bash
      npx @boundaryml/baml init \
        --client-type rest/openapi --openapi-client-type rust
      ```
      </Tab>

      <Tab title="Other">

      As long as there's an OpenAPI client generator that works with your stack,
      you can use it with BAML. Check out the [full list in the OpenAPI docs][openapi-client-types].

      ```bash
      npx @boundaryml/baml init \
        --client-type rest/openapi --openapi-client-type $OPENAPI_CLIENT_TYPE
      ```
      </Tab>

    </Tabs>

  ### Start the BAML development server

    ```bash
    npx @boundaryml/baml dev --preview
    ```
    
    This will do four things:

    - serve your BAML functions over a RESTful interface on `localhost:2024`
    - generate an OpenAPI schema in `baml_client/openapi.yaml`
    - run `openapi-generator -g $OPENAPI_CLIENT_TYPE` in `baml_client` directory to
      generate an OpenAPI client for you to use
    - re-run the above steps whenever you modify any `.baml` files

  <Note>
    BAML-over-REST is currently a preview feature. Please provide feedback
    either in [Discord][discord] or on [GitHub][openapi-feedback-github-issue]
    so that we can stabilize the feature and keep you updated!
  </Note>

  ### Use a BAML function in any language!
  
  `openapi-generator` will generate a `README` with instructions for installing
  and using your client; we've included snippets for some of the most popular
  languages below. Check out
  [`baml-examples`](https://github.com/BoundaryML/baml-examples) for example
  projects with instructions for running them.

  <Note>
    We've tested the below listed OpenAPI clients, but not all of them. If you run
    into issues with any of the OpenAPI clients, please let us know, either in
    [Discord][discord] or by commenting on
    [GitHub][openapi-feedback-github-issue] so that we can either help you out
    or fix it!
  </Note>

<Tabs>

<Tab title="Go">

Run this with `go run main.go`:

```go main.go
package main

import (
	"context"
	"fmt"
	"log"
  baml "my-golang-app/baml_client"
)

func main() {
	cfg := baml.NewConfiguration()
	b := baml.NewAPIClient(cfg).DefaultAPI
	extractResumeRequest := baml.ExtractResumeRequest{
		Resume: "Ada Lovelace (@gmail.com) was an English mathematician and writer",
	}
	resp, r, err := b.ExtractResume(context.Background()).ExtractResumeRequest(extractResumeRequest).Execute()
	if err != nil {
		fmt.Printf("Error when calling b.ExtractResume: %v\n", err)
		fmt.Printf("Full HTTP response: %v\n", r)
		return
	}
	log.Printf("Response from server: %v\n", resp)
}
```
</Tab>

<Tab title="Java">
First, add the OpenAPI-generated client to your project.

<AccordionGroup>

<Accordion title="If you have 'mvn' in your PATH">

You can use the default `on_generate` command, which will tell `baml dev` to
install the OpenAPI-generated client into your local Maven repository by running
`mvn clean install` every time you save a change to a BAML file.

To depend on the client in your local Maven repo, you can use these configs:

<CodeGroup>
```xml pom.xml
<dependency>
  <groupId>org.openapitools</groupId>
  <artifactId>openapi-java-client</artifactId>
  <version>0.1.0</version>
  <scope>compile</scope>
</dependency>
```

```kotlin settings.gradle.kts
repositories {
    mavenCentral()
    mavenLocal()
}

dependencies {
    implementation("org.openapitools:openapi-java-client:0.1.0")
}
```
</CodeGroup>

</Accordion>

<Accordion title="If you don't have 'mvn' in your PATH">

You'll probably want to comment out `on_generate` and instead use either the [OpenAPI Maven plugin] or [OpenAPI Gradle plugin] to build your OpenAPI client.

[OpenAPI Maven plugin]: https://github.com/OpenAPITools/openapi-generator/tree/master/modules/openapi-generator-maven-plugin
[OpenAPI Gradle plugin]: https://github.com/OpenAPITools/openapi-generator/tree/master/modules/openapi-generator-gradle-plugin

<CodeGroup>
```xml pom.xml
<build>
    <plugins>
        <plugin>
            <groupId>org.openapitools</groupId>
            <artifactId>openapi-generator-maven-plugin</artifactId>
            <version>7.8.0</version> <!-- Use the latest stable version -->
            <executions>
                <execution>
                    <goals>
                        <goal>generate</goal>
                    </goals>
                    <configuration>
                        <inputSpec>${project.basedir}/baml_client/openapi.yaml</inputSpec>
                        <generatorName>baml</generatorName> <!-- or another generator name, e.g. 'kotlin' or 'spring' -->
                        <output>${project.build.directory}/generated-sources/openapi</output>
                        <apiPackage>com.boundaryml.baml_client.api</apiPackage>
                        <modelPackage>com.boundaryml.baml_client.model</modelPackage>
                        <invokerPackage>com.boundaryml.baml_client</invokerPackage>
                        <java8>true</java8>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
```

```kotlin settings.gradle.kts
plugins {
    id("org.openapi.generator") version "7.8.0"
}

openApiGenerate {
    generatorName.set("java") // Change to 'kotlin', 'spring', etc. if needed
    inputSpec.set("${projectDir}/baml_client/openapi.yaml")
    outputDir.set("$buildDir/generated-sources/openapi")
    apiPackage.set("com.boundaryml.baml_client.api")
    modelPackage.set("com.boundaryml.baml_client.model")
    invokerPackage.set("com.boundaryml.baml_client")
    additionalProperties.set(mapOf("java8" to "true"))
}

sourceSets["main"].java {
    srcDir("$buildDir/generated-sources/openapi/src/main/java")
}

tasks.named("compileJava") {
    dependsOn("openApiGenerate")
}
```
</CodeGroup>

</Accordion>
</AccordionGroup>

Then, copy this code into wherever your `main` function is:

```Java
import com.boundaryml.baml_client.ApiClient;
import com.boundaryml.baml_client.ApiException;
import com.boundaryml.baml_client.Configuration;
// NOTE: baml_client/README.md will suggest importing from models.* - that is wrong.
// See https://github.com/OpenAPITools/openapi-generator/issues/19431 for more details.
import com.boundaryml.baml_client.model.*;
import com.boundaryml.baml_client.api.DefaultApi;

public class Example {
  public static void main(String[] args) {
    ApiClient defaultClient = Configuration.getDefaultApiClient();
    DefaultApi apiInstance = new DefaultApi(defaultClient);
    ExtractResumeRequest extractResumeRequest = new ExtractResumeRequest(); // ExtractResumeRequest | 
    try {
      Resume result = apiInstance.extractResume(extractResumeRequest);
      System.out.println(result);
    } catch (ApiException e) {
      System.err.println("Exception when calling DefaultApi#extractResume");
      System.err.println("Status code: " + e.getCode());
      System.err.println("Reason: " + e.getResponseBody());
      System.err.println("Response headers: " + e.getResponseHeaders());
      e.printStackTrace();
    }
  }
}

```
</Tab>

<Tab title="PHP">

<Warning>
  The PHP OpenAPI generator doesn't support OpenAPI's `oneOf` type, which is
  what we map BAML union types to. Please let us know if this is an issue for
  you, and you need help working around it.
</Warning>

First, add the OpenAPI-generated client to your project:

```json composer.json
    "repositories": [
        {
            "type": "path",
            "url": "baml_client"
        }
    ],
    "require": {
        "boundaryml/baml-client": "*@dev"
    }
```

You can now use this code to call a BAML function:

```PHP
<?php
require_once(__DIR__ . '/vendor/autoload.php');

$apiInstance = new BamlClient\Api\DefaultApi(
    new GuzzleHttp\Client()
);
$extract_resume_request = new BamlClient\Model\ExtractResumeRequest();
$extract_resume_request->setResume("Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity");

try {
    $result = $apiInstance->extractResume($extract_resume_request);
    print_r($result);
} catch (Exception $e) {
    echo 'Exception when calling DefaultApi->extractResume: ', $e->getMessage(), PHP_EOL;
}
```
</Tab>

<Tab title="Ruby">

Use `ruby -Ilib/baml_client app.rb` to run this:

```ruby app.rb
require 'baml_client'
require 'pp'

api_client = BamlClient::ApiClient.new
b = BamlClient::DefaultApi.new(api_client)

extract_resume_request = BamlClient::ExtractResumeRequest.new(
  resume: <<~RESUME
    John Doe

    Education
    - University of California, Berkeley
    - B.S. in Computer Science
    - graduated 2020

    Skills
    - Python
    - Java
    - C++
  RESUME
)

begin
  result = b.extract_resume(extract_resume_request)
  pp result

  edu0 = result.education[0]
  puts "Education: #{edu0.school}, #{edu0.degree}, #{edu0.year}"
rescue BamlClient::ApiError => e
  puts "Error when calling DefaultApi#extract_resume"
  pp e
end
```
</Tab>

<Tab title="Rust">

<Tip>
  If you're using `cargo watch -- cargo build` and seeing build failures because it can't find
  the generated `baml_client`, try increasing the delay on `cargo watch` to 1 second like so:

  ```bash
  cargo watch --delay 1 -- cargo build
  ```
</Tip>

First, add the OpenAPI-generated client to your project:

```toml Cargo.toml
[dependencies]
baml-client = { path = "./baml_client" }
```

You can now use `cargo run`:

```rust
use baml_client::models::ExtractResumeRequest;
use baml_client::apis::default_api as b;

#[tokio::main]
async fn main() {
    let config = baml_client::apis::configuration::Configuration::default();

    let resp = b::extract_resume(&config, ExtractResumeRequest {
        resume: "Tony Hoare is a British computer scientist who has made foundational contributions to programming languages, algorithms, operating systems, formal verification, and concurrent computing.".to_string(),
    }).await.unwrap();

    println!("{:#?}", resp);
}
```
</Tab>

</Tabs>

</Steps>

[discord]: https://discord.gg/BTNBeXGuaS
[openapi-feedback-github-issue]: https://github.com/BoundaryML/baml/issues/892
[npx-windows-issue]: https://github.com/nodejs/node/issues/53538
[openapi-client-types]: https://github.com/OpenAPITools/openapi-generator#overview",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/rest-api-other-languages",
    "title": "REST API (other languages)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Language",
    ],
    "content": "Requires BAML version >=0.55
This feature is a preview feature and may change. Please provide feedback either
in Discord or on GitHub so that
we can stabilize the feature and keep you updated!
BAML allows you to expose your BAML functions as RESTful APIs:

We integrate with OpenAPI (universal API definitions), so you can get typesafe client libraries for free!
Install BAML VSCode Extensionhttps://marketplace.visualstudio.com/items?itemName=boundary.baml-extension
syntax highlighting
testing playground
prompt previews
Install NPX + OpenAPIbrew install npm openapi-generator
# 'npm' will install npx
# 'openapi-generator' will install both Java and openapi-generator-cli
OpenAPI requires default-jdkapt install npm default-jdk -y
# 'npm' will install npx; 'default-jdk' will install java
OpenAPI requires Javadnf install npm java-21-openjdk -y
# dnf is the successor to yum
Amazon Linux 2023:dnf install npm java-21-amazon-corretto -y
# 'npm' will install npx
# 'java-21-amazon-corretto' will install java
Amazon Linux 2:curl -sL https://rpm.nodesource.com/setup_16.x | bash -
yum install nodejs -y
# 'nodejs' will install npx
amazon-linux-extras install java-openjdk11 -y
# 'java-openjdk11' will install java
To install npx and java (for OpenAPI):
Use the Node.js installer to install npx (default installer settings are fine).
Run npm install -g npm@latest to update npx (there is currently an issue with the default install of npx on Windows where it doesn't work out of the box).
Run the Adoptium OpenJDK .msi installer (install the JDK; default installer settings are fine).
You can verify that npx and java are installed by running:npx -version
java -version
To install npx, use the Node.js installer.To install java (for OpenAPI), use the Adoptium OpenJDK packages.Add BAML to your existing projectThis will give you some starter BAML code in a baml_src directory.npx @boundaryml/baml init \
  --client-type rest/openapi --openapi-client-type csharp
OpenAPI supports 5 different C++ client types;
any of them will work with BAML.npx @boundaryml/baml init \
  --client-type rest/openapi --openapi-client-type cpp-restsdk
npx @boundaryml/baml init \
  --client-type rest/openapi --openapi-client-type go
npx @boundaryml/baml init \
  --client-type rest/openapi --openapi-client-type java
Notice that on_generate has been initialized for you to:
run the OpenAPI generator to generate a Java client library, and also
run mvn clean install to install the generated client library to your
local Maven repository
If you only use Maven through an IDE (e.g. IntelliJ IDEA), you should
remove && mvn clean install from the generated on_generate command.npx @boundaryml/baml init \
  --client-type rest/openapi --openapi-client-type php
npx @boundaryml/baml init \
  --client-type rest/openapi --openapi-client-type ruby
npx @boundaryml/baml init \
  --client-type rest/openapi --openapi-client-type rust
As long as there's an OpenAPI client generator that works with your stack,
you can use it with BAML. Check out the full list in the OpenAPI docs.npx @boundaryml/baml init \
  --client-type rest/openapi --openapi-client-type $OPENAPI_CLIENT_TYPE
Start the BAML development servernpx @boundaryml/baml dev --preview
This will do four things:
serve your BAML functions over a RESTful interface on localhost:2024
generate an OpenAPI schema in baml_client/openapi.yaml
run openapi-generator -g $OPENAPI_CLIENT_TYPE in baml_client directory to
generate an OpenAPI client for you to use
re-run the above steps whenever you modify any .baml files
BAML-over-REST is currently a preview feature. Please provide feedback
either in Discord or on GitHub
so that we can stabilize the feature and keep you updated!Use a BAML function in any language!openapi-generator will generate a README with instructions for installing
and using your client; we've included snippets for some of the most popular
languages below. Check out
baml-examples for example
projects with instructions for running them.We've tested the below listed OpenAPI clients, but not all of them. If you run
into issues with any of the OpenAPI clients, please let us know, either in
Discord or by commenting on
GitHub so that we can either help you out
or fix it!Run this with go run main.go:package main

import (
	"context"
	"fmt"
	"log"
  baml "my-golang-app/baml_client"
)

func main() {
	cfg := baml.NewConfiguration()
	b := baml.NewAPIClient(cfg).DefaultAPI
	extractResumeRequest := baml.ExtractResumeRequest{
		Resume: "Ada Lovelace (@gmail.com) was an English mathematician and writer",
	}
	resp, r, err := b.ExtractResume(context.Background()).ExtractResumeRequest(extractResumeRequest).Execute()
	if err != nil {
		fmt.Printf("Error when calling b.ExtractResume: %v\n", err)
		fmt.Printf("Full HTTP response: %v\n", r)
		return
	}
	log.Printf("Response from server: %v\n", resp)
}
First, add the OpenAPI-generated client to your project.You can use the default on_generate command, which will tell baml dev to
install the OpenAPI-generated client into your local Maven repository by running
mvn clean install every time you save a change to a BAML file.To depend on the client in your local Maven repo, you can use these configs:<dependency>
  <groupId>org.openapitools</groupId>
  <artifactId>openapi-java-client</artifactId>
  <version>0.1.0</version>
  <scope>compile</scope>
</dependency>
repositories {
    mavenCentral()
    mavenLocal()
}

dependencies {
    implementation("org.openapitools:openapi-java-client:0.1.0")
}
You'll probably want to comment out on_generate and instead use either the OpenAPI Maven plugin or OpenAPI Gradle plugin to build your OpenAPI client.<build>
    <plugins>
        <plugin>
            <groupId>org.openapitools</groupId>
            <artifactId>openapi-generator-maven-plugin</artifactId>
            <version>7.8.0</version> <!-- Use the latest stable version -->
            <executions>
                <execution>
                    <goals>
                        <goal>generate</goal>
                    </goals>
                    <configuration>
                        <inputSpec>${project.basedir}/baml_client/openapi.yaml</inputSpec>
                        <generatorName>baml</generatorName> <!-- or another generator name, e.g. 'kotlin' or 'spring' -->
                        <output>${project.build.directory}/generated-sources/openapi</output>
                        <apiPackage>com.boundaryml.baml_client.api</apiPackage>
                        <modelPackage>com.boundaryml.baml_client.model</modelPackage>
                        <invokerPackage>com.boundaryml.baml_client</invokerPackage>
                        <java8>true</java8>
                    </configuration>
                </execution>
            </executions>
        </plugin>
    </plugins>
</build>
plugins {
    id("org.openapi.generator") version "7.8.0"
}

openApiGenerate {
    generatorName.set("java") // Change to 'kotlin', 'spring', etc. if needed
    inputSpec.set("${projectDir}/baml_client/openapi.yaml")
    outputDir.set("$buildDir/generated-sources/openapi")
    apiPackage.set("com.boundaryml.baml_client.api")
    modelPackage.set("com.boundaryml.baml_client.model")
    invokerPackage.set("com.boundaryml.baml_client")
    additionalProperties.set(mapOf("java8" to "true"))
}

sourceSets["main"].java {
    srcDir("$buildDir/generated-sources/openapi/src/main/java")
}

tasks.named("compileJava") {
    dependsOn("openApiGenerate")
}
Then, copy this code into wherever your main function is:import com.boundaryml.baml_client.ApiClient;
import com.boundaryml.baml_client.ApiException;
import com.boundaryml.baml_client.Configuration;
// NOTE: baml_client/README.md will suggest importing from models.* - that is wrong.
// See https://github.com/OpenAPITools/openapi-generator/issues/19431 for more details.
import com.boundaryml.baml_client.model.*;
import com.boundaryml.baml_client.api.DefaultApi;

public class Example {
  public static void main(String[] args) {
    ApiClient defaultClient = Configuration.getDefaultApiClient();
    DefaultApi apiInstance = new DefaultApi(defaultClient);
    ExtractResumeRequest extractResumeRequest = new ExtractResumeRequest(); // ExtractResumeRequest | 
    try {
      Resume result = apiInstance.extractResume(extractResumeRequest);
      System.out.println(result);
    } catch (ApiException e) {
      System.err.println("Exception when calling DefaultApi#extractResume");
      System.err.println("Status code: " + e.getCode());
      System.err.println("Reason: " + e.getResponseBody());
      System.err.println("Response headers: " + e.getResponseHeaders());
      e.printStackTrace();
    }
  }
}

The PHP OpenAPI generator doesn't support OpenAPI's oneOf type, which is
what we map BAML union types to. Please let us know if this is an issue for
you, and you need help working around it.First, add the OpenAPI-generated client to your project:    "repositories": [
        {
            "type": "path",
            "url": "baml_client"
        }
    ],
    "require": {
        "boundaryml/baml-client": "*@dev"
    }
You can now use this code to call a BAML function:<?php
require_once(__DIR__ . '/vendor/autoload.php');

$apiInstance = new BamlClient\Api\DefaultApi(
    new GuzzleHttp\Client()
);
$extract_resume_request = new BamlClient\Model\ExtractResumeRequest();
$extract_resume_request->setResume("Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity");

try {
    $result = $apiInstance->extractResume($extract_resume_request);
    print_r($result);
} catch (Exception $e) {
    echo 'Exception when calling DefaultApi->extractResume: ', $e->getMessage(), PHP_EOL;
}
Use ruby -Ilib/baml_client app.rb to run this:require 'baml_client'
require 'pp'

api_client = BamlClient::ApiClient.new
b = BamlClient::DefaultApi.new(api_client)

extract_resume_request = BamlClient::ExtractResumeRequest.new(
  resume: <<~RESUME
    John Doe

    Education
    - University of California, Berkeley
    - B.S. in Computer Science
    - graduated 2020

    Skills
    - Python
    - Java
    - C++
  RESUME
)

begin
  result = b.extract_resume(extract_resume_request)
  pp result

  edu0 = result.education[0]
  puts "Education: #{edu0.school}, #{edu0.degree}, #{edu0.year}"
rescue BamlClient::ApiError => e
  puts "Error when calling DefaultApi#extract_resume"
  pp e
end
If you're using cargo watch -- cargo build and seeing build failures because it can't find
the generated baml_client, try increasing the delay on cargo watch to 1 second like so:cargo watch --delay 1 -- cargo build
First, add the OpenAPI-generated client to your project:[dependencies]
baml-client = { path = "./baml_client" }
You can now use cargo run:use baml_client::models::ExtractResumeRequest;
use baml_client::apis::default_api as b;

#[tokio::main]
async fn main() {
    let config = baml_client::apis::configuration::Configuration::default();

    let resp = b::extract_resume(&config, ExtractResumeRequest {
        resume: "Tony Hoare is a British computer scientist who has made foundational contributions to programming languages, algorithms, operating systems, formal verification, and concurrent computing.".to_string(),
    }).await.unwrap();

    println!("{:#?}", resp);
}
",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/rest-api-other-languages",
    "title": "REST API (other languages)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/installation-language/python",
        "title": "Installation: Language",
      },
    ],
    "description": "BAML can be used with Vercel's AI SDK to stream BAML functions to your UI.

The latest example code is found in our [NextJS starter](https://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter), but this tutorial will guide you on how to add BAML step-by-step.

See the [live demo](https://baml-examples.vercel.app/)

<Note>
You will need to use Server Actions, from the App Router, for this tutorial. You can still stream BAML functions from Route Handlers however.
</Note>


<Steps>
### Install BAML, and Generate a BAML client for TypeScript
- Follow [the TS installation guide](/docs/get-started/quickstart/typescript)
- Install the VSCode extension and Save a baml file to generate the client (or use `npx baml-cli generate`).


### Update next.config.mjs

@boundaryml/baml uses a native node addon to run the BAML functions. You need to tell NextJS to use the loader for these files.
```JS
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    serverComponentsExternalPackages: ["@boundaryml/baml"],
  },
  webpack: (config, { dev, isServer, webpack, nextRuntime }) => {
    config.module.rules.push({
      test: /\.node$/,
      use: [
        {
          loader: "nextjs-node-loader",
          options: {
            outputPath: config.output.path,
          },
        },
      ],
    });

    return config;
  },
};

export default nextConfig;
```


### Create some helper utilities to stream BAML functions
Let's add some helpers to export our baml functions as streamable server actions. See the last line in this file, where we export the `extractResume` function.

In `app/utils/streamableObject.tsx` add the following code:
```typescript
import { createStreamableValue, StreamableValue as BaseStreamableValue } from "ai/rsc";
import { BamlStream } from "@boundaryml/baml";
import { b } from "@/baml_client"; // You can change the path of this to wherever your baml_client is located.


// ------------------------------
// Helper functions
// ------------------------------

/**
 * Type alias for defining a StreamableValue based on a BamlStream.
 * It captures either a partial or final result depending on the stream state.
 */
type StreamableValue<T extends BamlStream<any, any>> =
  | { partial: T extends BamlStream<infer StreamRet, any> ? StreamRet : never }
  | { final: T extends BamlStream<any, infer Ret> ? Ret : never };

/**
 * Helper function to manage and handle a BamlStream.
 * It consumes the stream, updates the streamable value for each partial event,
 * and finalizes the stream when complete.
 *
 * @param bamlStream - The BamlStream to be processed.
 * @returns A promise that resolves with an object containing the BaseStreamableValue.
 */
export async function streamHelper<T extends BamlStream<any, any>>(
  bamlStream: T,
): Promise<{
  object: BaseStreamableValue<StreamableValue<T>>;
}> {
  const stream = createStreamableValue<StreamableValue<T>>();

  // Asynchronous function to process the BamlStream events
  (async () => {
    try {
      // Iterate through the stream and update the stream value with partial data
      for await (const event of bamlStream) {
        stream.update({ partial: event });
      }

      // Obtain the final response once all events are processed
      const response = await bamlStream.getFinalResponse();
      stream.done({ final: response });
    } catch (err) {
      // Handle any errors during stream processing
      stream.error(err);
    }
  })();

  return { object: stream.value };
}

/**
 * Utility function to create a streamable function from a BamlStream-producing function.
 * This function returns an asynchronous function that manages the streaming process.
 *
 * @param func - A function that produces a BamlStream when called.
 * @returns An asynchronous function that returns a BaseStreamableValue for the stream.
 */
export function makeStreamable<
  BamlStreamFunc extends (...args: any) => BamlStream<any, any>,
>(
  func: BamlStreamFunc
): (...args: Parameters<BamlStreamFunc>) => Promise<{
  object: BaseStreamableValue<StreamableValue<ReturnType<BamlStreamFunc>>>;
}> {
  return async (...args) => {
    const boundFunc = func.bind(b.stream);
    const stream = boundFunc(...args);
    return streamHelper(stream);
  };
}

```

### Export your BAML functions to streamable server actions

In `app/actions/extract.tsx` add the following code:
```typescript
import { makeStreamable } from "../_baml_utils/streamableObjects";


export const extractResume = makeStreamable(b.stream.ExtractResume);
```

### Create a hook to use the streamable functions in React Components
This hook will work like [react-query](https://react-query.tanstack.com/), but for BAML functions.
It will give you partial data, the loading status, and whether the stream was completed.

In `app/_hooks/useStream.ts` add:
```typescript
import { useState, useEffect } from "react";
import { readStreamableValue, StreamableValue } from "ai/rsc";

/**
 * A hook that streams data from a server action. The server action must return a StreamableValue.
 * See the example actiimport { useState, useEffect } from "react";
import { readStreamableValue, StreamableValue } from "ai/rsc";

/**
 * A hook that streams data from a server action. The server action must return a StreamableValue.
 * See the example action in app/actions/streamable_objects.tsx
 *  **/
export function useStream<PartialRet, Ret, P extends any[]>(
  serverAction: (...args: P) => Promise<{ object: StreamableValue<{ partial: PartialRet } | { final: Ret }, any> }>
) {
  const [isLoading, setIsLoading] = useState(false);
  const [isComplete, setIsComplete] = useState(false);
  const [isError, setIsError] = useState(false);
  const [error, setError] = useState<Error | null>(null);
  const [partialData, setPartialData] = useState<PartialRet | undefined>(undefined); // Initialize data state
  const [streamResult, setData] = useState<Ret  | undefined>(undefined); // full non-partial data

  const mutate = async (
    ...params: Parameters<typeof serverAction>
  ): Promise<Ret | undefined> => {
    console.log("mutate", params);
    setIsLoading(true);
    setIsError(false);
    setError(null);

    try {
      const { object } = await serverAction(...params);
      const asyncIterable = readStreamableValue(object);

      for await (const value of asyncIterable) {
        if (value !== undefined) {

          // could also add a callback here.
          // if (options?.onData) {
          //   options.onData(value as T);
          // }
          console.log("value", value);
          if ("partial" in value) {
            setPartialData(value.partial); // Update data state with the latest value
          } else if ("final" in value) {
            setData(value.final); // Update data state with the latest value
            setIsComplete(true);
            return value.final;
          }
        }
      }

      // // If it completes, it means it's the full data.
      // return streamedData;
    } catch (err) {
      console.log("error", err);

      setIsError(true);
      setError(new Error(JSON.stringify(err) ?? "An error occurred"));
      return undefined;
    } finally {
      setIsLoading(false);
    }
  };

  // If you use the "data" property, your component will re-render when the data gets updated.
  return { data: streamResult, partialData, isLoading, isComplete, isError, error, mutate };
}

```



### Stream your BAML function in a component
In `app/page.tsx` you can use the hook to stream the BAML function and render the result in real-time.

```tsx
"use client";
import {
  extractResume,
  extractUnstructuredResume,
} from "../../actions/streamable_objects";
// import types from baml files like this:
import { Resume } from "@/baml_client";

export default function Home() {
  // you can also rename these fields by using ":", like how we renamed partialData to "partialResume"
  // `mutate` is a function that will start the stream. It takes in the same arguments as the BAML function.
  const { data: completedData, partialData: partialResume, isLoading, isError, error, mutate } = useStream(extractResume);

  return (
    <div>
      <h1>BoundaryML Next.js Example</h1>
      
      <button onClick={() => mutate("Some resume text")}>Stream BAML</button>
      {isLoading && <p>Loading...</p>}
      {isError && <p>Error: {error?.message}</p>}
      {partialData && <pre>{JSON.stringify(partialData, null, 2)}</pre>}
      {data && <pre>{JSON.stringify(data, null, 2)}</pre>}
    </div>
  );
}
```

</Steps>


And now you're all set!

If you have issues with your environment variables not loading, you may want to use [dotenv-cli](https://www.npmjs.com/package/dotenv-cli) to load your env vars before the nextjs process starts:

`dotenv -- npm run dev`",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/next-js",
    "title": "Next.js Integration",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Installation: Language",
    ],
    "content": "BAML can be used with Vercel's AI SDK to stream BAML functions to your UI.
The latest example code is found in our NextJS starter, but this tutorial will guide you on how to add BAML step-by-step.
See the live demo
You will need to use Server Actions, from the App Router, for this tutorial. You can still stream BAML functions from Route Handlers however.
Install BAML, and Generate a BAML client for TypeScript
Follow the TS installation guide
Install the VSCode extension and Save a baml file to generate the client (or use npx baml-cli generate).
Update next.config.mjs@boundaryml/baml uses a native node addon to run the BAML functions. You need to tell NextJS to use the loader for these files./** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    serverComponentsExternalPackages: ["@boundaryml/baml"],
  },
  webpack: (config, { dev, isServer, webpack, nextRuntime }) => {
    config.module.rules.push({
      test: /\.node$/,
      use: [
        {
          loader: "nextjs-node-loader",
          options: {
            outputPath: config.output.path,
          },
        },
      ],
    });

    return config;
  },
};

export default nextConfig;
Create some helper utilities to stream BAML functionsLet's add some helpers to export our baml functions as streamable server actions. See the last line in this file, where we export the extractResume function.In app/utils/streamableObject.tsx add the following code:import { createStreamableValue, StreamableValue as BaseStreamableValue } from "ai/rsc";
import { BamlStream } from "@boundaryml/baml";
import { b } from "@/baml_client"; // You can change the path of this to wherever your baml_client is located.


// ------------------------------
// Helper functions
// ------------------------------

/**
 * Type alias for defining a StreamableValue based on a BamlStream.
 * It captures either a partial or final result depending on the stream state.
 */
type StreamableValue<T extends BamlStream<any, any>> =
  | { partial: T extends BamlStream<infer StreamRet, any> ? StreamRet : never }
  | { final: T extends BamlStream<any, infer Ret> ? Ret : never };

/**
 * Helper function to manage and handle a BamlStream.
 * It consumes the stream, updates the streamable value for each partial event,
 * and finalizes the stream when complete.
 *
 * @param bamlStream - The BamlStream to be processed.
 * @returns A promise that resolves with an object containing the BaseStreamableValue.
 */
export async function streamHelper<T extends BamlStream<any, any>>(
  bamlStream: T,
): Promise<{
  object: BaseStreamableValue<StreamableValue<T>>;
}> {
  const stream = createStreamableValue<StreamableValue<T>>();

  // Asynchronous function to process the BamlStream events
  (async () => {
    try {
      // Iterate through the stream and update the stream value with partial data
      for await (const event of bamlStream) {
        stream.update({ partial: event });
      }

      // Obtain the final response once all events are processed
      const response = await bamlStream.getFinalResponse();
      stream.done({ final: response });
    } catch (err) {
      // Handle any errors during stream processing
      stream.error(err);
    }
  })();

  return { object: stream.value };
}

/**
 * Utility function to create a streamable function from a BamlStream-producing function.
 * This function returns an asynchronous function that manages the streaming process.
 *
 * @param func - A function that produces a BamlStream when called.
 * @returns An asynchronous function that returns a BaseStreamableValue for the stream.
 */
export function makeStreamable<
  BamlStreamFunc extends (...args: any) => BamlStream<any, any>,
>(
  func: BamlStreamFunc
): (...args: Parameters<BamlStreamFunc>) => Promise<{
  object: BaseStreamableValue<StreamableValue<ReturnType<BamlStreamFunc>>>;
}> {
  return async (...args) => {
    const boundFunc = func.bind(b.stream);
    const stream = boundFunc(...args);
    return streamHelper(stream);
  };
}

Export your BAML functions to streamable server actionsIn app/actions/extract.tsx add the following code:import { makeStreamable } from "../_baml_utils/streamableObjects";


export const extractResume = makeStreamable(b.stream.ExtractResume);
Create a hook to use the streamable functions in React ComponentsThis hook will work like react-query, but for BAML functions.
It will give you partial data, the loading status, and whether the stream was completed.In app/_hooks/useStream.ts add:import { useState, useEffect } from "react";
import { readStreamableValue, StreamableValue } from "ai/rsc";

/**
 * A hook that streams data from a server action. The server action must return a StreamableValue.
 * See the example actiimport { useState, useEffect } from "react";
import { readStreamableValue, StreamableValue } from "ai/rsc";

/**
 * A hook that streams data from a server action. The server action must return a StreamableValue.
 * See the example action in app/actions/streamable_objects.tsx
 *  **/
export function useStream<PartialRet, Ret, P extends any[]>(
  serverAction: (...args: P) => Promise<{ object: StreamableValue<{ partial: PartialRet } | { final: Ret }, any> }>
) {
  const [isLoading, setIsLoading] = useState(false);
  const [isComplete, setIsComplete] = useState(false);
  const [isError, setIsError] = useState(false);
  const [error, setError] = useState<Error | null>(null);
  const [partialData, setPartialData] = useState<PartialRet | undefined>(undefined); // Initialize data state
  const [streamResult, setData] = useState<Ret  | undefined>(undefined); // full non-partial data

  const mutate = async (
    ...params: Parameters<typeof serverAction>
  ): Promise<Ret | undefined> => {
    console.log("mutate", params);
    setIsLoading(true);
    setIsError(false);
    setError(null);

    try {
      const { object } = await serverAction(...params);
      const asyncIterable = readStreamableValue(object);

      for await (const value of asyncIterable) {
        if (value !== undefined) {

          // could also add a callback here.
          // if (options?.onData) {
          //   options.onData(value as T);
          // }
          console.log("value", value);
          if ("partial" in value) {
            setPartialData(value.partial); // Update data state with the latest value
          } else if ("final" in value) {
            setData(value.final); // Update data state with the latest value
            setIsComplete(true);
            return value.final;
          }
        }
      }

      // // If it completes, it means it's the full data.
      // return streamedData;
    } catch (err) {
      console.log("error", err);

      setIsError(true);
      setError(new Error(JSON.stringify(err) ?? "An error occurred"));
      return undefined;
    } finally {
      setIsLoading(false);
    }
  };

  // If you use the "data" property, your component will re-render when the data gets updated.
  return { data: streamResult, partialData, isLoading, isComplete, isError, error, mutate };
}

Stream your BAML function in a componentIn app/page.tsx you can use the hook to stream the BAML function and render the result in real-time."use client";
import {
  extractResume,
  extractUnstructuredResume,
} from "../../actions/streamable_objects";
// import types from baml files like this:
import { Resume } from "@/baml_client";

export default function Home() {
  // you can also rename these fields by using ":", like how we renamed partialData to "partialResume"
  // `mutate` is a function that will start the stream. It takes in the same arguments as the BAML function.
  const { data: completedData, partialData: partialResume, isLoading, isError, error, mutate } = useStream(extractResume);

  return (
    <div>
      <h1>BoundaryML Next.js Example</h1>
      
      <button onClick={() => mutate("Some resume text")}>Stream BAML</button>
      {isLoading && <p>Loading...</p>}
      {isError && <p>Error: {error?.message}</p>}
      {partialData && <pre>{JSON.stringify(partialData, null, 2)}</pre>}
      {data && <pre>{JSON.stringify(data, null, 2)}</pre>}
    </div>
  );
}

And now you're all set!
If you have issues with your environment variables not loading, you may want to use dotenv-cli to load your env vars before the nextjs process starts:
dotenv -- npm run dev",
    "indexSegmentId": "0",
    "slug": "guide/installation-language/next-js",
    "title": "Next.js Integration",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
    ],
    "description": undefined,
    "indexSegmentId": "0",
    "slug": "guide/development/environment-variables",
    "title": "Set Environment Variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
    ],
    "content": "Environment Variables in BAML
Sometimes you'll see environment variables used in BAML, like in clients:

client<llm> GPT4o {
  provider baml-openai-chat
  options {
    model gpt-4o
    api_key env.OPENAI_API_KEY
  }
}

To set environment variables:
Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.Then you should be able to set environment variables in the settings tab.Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.BAML will expect these to be set already in your program before you import the baml_client in Python/ TS / etc.Any of the following strategies for setting env vars are compatible with BAML:
setting them in your shell before running your program
in your Dockerfile
in your next.config.js
in your Kubernetes manifest
from secrets-store.csi.k8s.io
from a secrets provider such as Infisical / Doppler
from a .env file (using dotenv cli)
using account credentials for ephemeral token generation (e.g. Vertex AI Auth Tokens)
export MY_SUPER_SECRET_API_KEY="..."
python my_program_using_baml.py
Requires BAML Version 0.57+If you don't want BAML to try to auto-load your env vars, you can call manually reset_baml_env_vars
with the current environment variables.
from baml_client import b
from baml_client import reset_baml_env_vars
import os
import dotenv

dotenv.load_dotenv()
reset_baml_env_vars(dict(os.environ))
import dotenv from 'dotenv'
// Wait to import the BAML client until after loading environment variables
import { b, resetBamlEnvVars } from 'baml-client'

dotenv.config()
resetBamlEnvVars(process.env)
require 'dotenv/load'

# Wait to import the BAML client until after loading environment variables
# reset_baml_env_vars is not yet implemented in the Ruby client
require 'baml_client'

Dynamically setting LLM API Keys
You can set the API key for an LLM dynamically by passing in the key as a header or as a parameter (depending on the provider), using the ClientRegistry.",
    "indexSegmentId": "0",
    "slug": "guide/development/environment-variables",
    "title": "Set Environment Variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
    ],
    "description": "Sometimes you'll see environment variables used in BAML, like in clients:

client<llm> GPT4o {
  provider baml-openai-chat
  options {
    model gpt-4o
    api_key env.OPENAI_API_KEY
  }
}

To set environment variables:
Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.Then you should be able to set environment variables in the settings tab.Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.BAML will expect these to be set already in your program before you import the baml_client in Python/ TS / etc.Any of the following strategies for setting env vars are compatible with BAML:
setting them in your shell before running your program
in your Dockerfile
in your next.config.js
in your Kubernetes manifest
from secrets-store.csi.k8s.io
from a secrets provider such as Infisical / Doppler
from a .env file (using dotenv cli)
using account credentials for ephemeral token generation (e.g. Vertex AI Auth Tokens)
export MY_SUPER_SECRET_API_KEY="..."
python my_program_using_baml.py
Requires BAML Version 0.57+If you don't want BAML to try to auto-load your env vars, you can call manually reset_baml_env_vars
with the current environment variables.
from baml_client import b
from baml_client import reset_baml_env_vars
import os
import dotenv

dotenv.load_dotenv()
reset_baml_env_vars(dict(os.environ))
import dotenv from 'dotenv'
// Wait to import the BAML client until after loading environment variables
import { b, resetBamlEnvVars } from 'baml-client'

dotenv.config()
resetBamlEnvVars(process.env)
require 'dotenv/load'

# Wait to import the BAML client until after loading environment variables
# reset_baml_env_vars is not yet implemented in the Ruby client
require 'baml_client'
",
    "indexSegmentId": "0",
    "slug": "guide/development/environment-variables#environment-variables-in-baml",
    "title": "Environment Variables in BAML",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
    ],
    "content": "Sometimes you'll see environment variables used in BAML, like in clients:

client<llm> GPT4o {
  provider baml-openai-chat
  options {
    model gpt-4o
    api_key env.OPENAI_API_KEY
  }
}

To set environment variables:
Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.Then you should be able to set environment variables in the settings tab.Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.BAML will expect these to be set already in your program before you import the baml_client in Python/ TS / etc.Any of the following strategies for setting env vars are compatible with BAML:
setting them in your shell before running your program
in your Dockerfile
in your next.config.js
in your Kubernetes manifest
from secrets-store.csi.k8s.io
from a secrets provider such as Infisical / Doppler
from a .env file (using dotenv cli)
using account credentials for ephemeral token generation (e.g. Vertex AI Auth Tokens)
export MY_SUPER_SECRET_API_KEY="..."
python my_program_using_baml.py
Requires BAML Version 0.57+If you don't want BAML to try to auto-load your env vars, you can call manually reset_baml_env_vars
with the current environment variables.
from baml_client import b
from baml_client import reset_baml_env_vars
import os
import dotenv

dotenv.load_dotenv()
reset_baml_env_vars(dict(os.environ))
import dotenv from 'dotenv'
// Wait to import the BAML client until after loading environment variables
import { b, resetBamlEnvVars } from 'baml-client'

dotenv.config()
resetBamlEnvVars(process.env)
require 'dotenv/load'

# Wait to import the BAML client until after loading environment variables
# reset_baml_env_vars is not yet implemented in the Ruby client
require 'baml_client'
",
    "indexSegmentId": "0",
    "slug": "guide/development/environment-variables#environment-variables-in-baml",
    "title": "Environment Variables in BAML",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
    ],
    "description": "You can set the API key for an LLM dynamically by passing in the key as a header or as a parameter (depending on the provider), using the ClientRegistry.",
    "indexSegmentId": "0",
    "slug": "guide/development/environment-variables#dynamically-setting-llm-api-keys",
    "title": "Dynamically setting LLM API Keys",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
    ],
    "content": "You can set the API key for an LLM dynamically by passing in the key as a header or as a parameter (depending on the provider), using the ClientRegistry.",
    "indexSegmentId": "0",
    "slug": "guide/development/environment-variables#dynamically-setting-llm-api-keys",
    "title": "Dynamically setting LLM API Keys",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
    ],
    "description": "You can add logging to determine what the BAML runtime is doing when it calls LLM endpoints and parses responses.

To enable logging, set the `BAML_LOG` environment variable:
```sh
# default is warn
BAML_LOG=info
```

| Level | Description |
|-------|-------------|
| `error` | Fatal errors by BAML |
| `warn` | Logs any time a function fails (includes LLM calling failures, parsing failures) |
| `info` | Logs every call to a function (including prompt, raw response, and parsed response) |
| `debug` | Requests and detailed parsing errors (warning: may be a lot of logs) |
| `trace` | Everything and more |
| `off` | No logging |


Example log:
<img src="file:a410f5dc-aff8-430e-bce9-3fec22eca527" />

---

Since `>0.54.0`:

To truncate each log entry to a certain length, set the `BOUNDARY_MAX_LOG_CHUNK_CHARS` environment variable:

```sh
BOUNDARY_MAX_LOG_CHUNK_CHARS=3000
```

This will truncate each part in a log entry to 3000 characters.",
    "indexSegmentId": "0",
    "slug": "guide/development/terminal-logs",
    "title": "Terminal Logs",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
    ],
    "content": "You can add logging to determine what the BAML runtime is doing when it calls LLM endpoints and parses responses.
To enable logging, set the BAML_LOG environment variable:
# default is warn
BAML_LOG=info

| Level | Description |
|-------|-------------|
| error | Fatal errors by BAML |
| warn | Logs any time a function fails (includes LLM calling failures, parsing failures) |
| info | Logs every call to a function (including prompt, raw response, and parsed response) |
| debug | Requests and detailed parsing errors (warning: may be a lot of logs) |
| trace | Everything and more |
| off | No logging |
Example log:


Since >0.54.0:
To truncate each log entry to a certain length, set the BOUNDARY_MAX_LOG_CHUNK_CHARS environment variable:
BOUNDARY_MAX_LOG_CHUNK_CHARS=3000

This will truncate each part in a log entry to 3000 characters.",
    "indexSegmentId": "0",
    "slug": "guide/development/terminal-logs",
    "title": "Terminal Logs",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
    ],
    "description": "Remember that the generated `baml_client` code is generated by your `baml_py` / `@boundaryml/baml` package dependency (using `baml-cli generate`), but can also be generated by the VSCode extension when you save a BAML file.

**To upgrade BAML versions:**
1. Update the `generator` clause in your `generators.baml` file (or wherever you have it defined) to the new version. If you ran `baml-cli init`, one has already been generated for you!
```baml generators.baml
generator TypescriptGenerator {
    output_type "typescript"
    ....
    // Version of runtime to generate code for (should match the package @boundaryml/baml version)
    version "0.62.0"
}
```

2. Update your `baml_py`  / `@boundaryml/baml` package dependency to the same version.


<CodeBlock>
```sh pip
pip install --upgrade baml-py
```
```sh npm
npm install @boundaryml/baml@latest
```

```sh ruby
gem install baml
```
</CodeBlock>

3. Update VSCode BAML extension to point to the same version. Read here for how to keep VSCode in sync with your `baml_py` / `@boundaryml/baml` package dependency: [VSCode BAML Extension reference](/ref/editor-extension-settings/baml-cli-path)

You only need to do this for minor version upgrades (e.g., 0.54.0 -> 0.62.0), not patch versions (e.g., 0.62.0 -> 0.62.1).",
    "indexSegmentId": "0",
    "slug": "guide/development/upgrade-baml-versions",
    "title": "Upgrading BAML / Fixing Version Mismatches",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
    ],
    "content": "Remember that the generated baml_client code is generated by your baml_py / @boundaryml/baml package dependency (using baml-cli generate), but can also be generated by the VSCode extension when you save a BAML file.
To upgrade BAML versions:

Update the generator clause in your generators.baml file (or wherever you have it defined) to the new version. If you ran baml-cli init, one has already been generated for you!

generator TypescriptGenerator {
    output_type "typescript"
    ....
    // Version of runtime to generate code for (should match the package @boundaryml/baml version)
    version "0.62.0"
}


Update your baml_py  / @boundaryml/baml package dependency to the same version.

pip install --upgrade baml-py
npm install @boundaryml/baml@latest
gem install baml


Update VSCode BAML extension to point to the same version. Read here for how to keep VSCode in sync with your baml_py / @boundaryml/baml package dependency: VSCode BAML Extension reference

You only need to do this for minor version upgrades (e.g., 0.54.0 -> 0.62.0), not patch versions (e.g., 0.62.0 -> 0.62.1).
Troubleshooting
See the VSCode BAML Extension reference for more information on how to prevent version mismatches.",
    "indexSegmentId": "0",
    "slug": "guide/development/upgrade-baml-versions",
    "title": "Upgrading BAML / Fixing Version Mismatches",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
    ],
    "description": "See the VSCode BAML Extension reference for more information on how to prevent version mismatches.",
    "indexSegmentId": "0",
    "slug": "guide/development/upgrade-baml-versions#troubleshooting",
    "title": "Troubleshooting",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
    ],
    "content": "See the VSCode BAML Extension reference for more information on how to prevent version mismatches.",
    "indexSegmentId": "0",
    "slug": "guide/development/upgrade-baml-versions#troubleshooting",
    "title": "Troubleshooting",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "You can use [SST](https://sst.dev/) to define the Lambda configuration and deploy it.

The example below builds the BAML x86_64 rust binaries into a Lambda layer and uses the layer in the Lambda function.

[Example Node + SST Project](https://github.com/BoundaryML/baml-examples/tree/main/node-aws-lambda-sst)

Let us know if you want to deploy a python BAML project on AWS. Our example project is coming soon.",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/aws",
    "title": "AWS",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "You can use SST to define the Lambda configuration and deploy it.
The example below builds the BAML x86_64 rust binaries into a Lambda layer and uses the layer in the Lambda function.
Example Node + SST Project
Let us know if you want to deploy a python BAML project on AWS. Our example project is coming soon.
Current limitations
The BAML binaries only support the NodeJS 20.x runtime (or a runtime using Amazon Linux 2023). Let us know if you need a different runtime version.",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/aws",
    "title": "AWS",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "The BAML binaries only support the NodeJS 20.x runtime (or a runtime using Amazon Linux 2023). Let us know if you need a different runtime version.",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/aws#current-limitations",
    "title": "Current limitations",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "The BAML binaries only support the NodeJS 20.x runtime (or a runtime using Amazon Linux 2023). Let us know if you need a different runtime version.",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/aws#current-limitations",
    "title": "Current limitations",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "To deploy a NextJS with BAML, take a look at the starter template:
https://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter

All you need is to modify the `nextjs.config.mjs` to allow BAML to run properly:
```JS
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    serverComponentsExternalPackages: ["@boundaryml/baml"],
  },
  webpack: (config, { dev, isServer, webpack, nextRuntime }) => {
    config.module.rules.push({
      test: /\.node$/,
      use: [
        {
          loader: "nextjs-node-loader",
          options: {
            outputPath: config.output.path,
          },
        },
      ],
    });

    return config;
  },
};

export default nextConfig;
```

and change your `package.json` to build the baml client automatically (and enable logging in dev mode if you want):

```json
 "scripts": {
    "dev": "BAML_LOG=info next dev",
    "build": "pnpm generate && next build",
    "start": "next start",
    "lint": "next lint",
    "generate": "baml-cli generate --from ./baml_src"
  },
```",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/next-js",
    "title": "NextJS",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "To deploy a NextJS with BAML, take a look at the starter template:
https://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter
All you need is to modify the nextjs.config.mjs to allow BAML to run properly:
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    serverComponentsExternalPackages: ["@boundaryml/baml"],
  },
  webpack: (config, { dev, isServer, webpack, nextRuntime }) => {
    config.module.rules.push({
      test: /\.node$/,
      use: [
        {
          loader: "nextjs-node-loader",
          options: {
            outputPath: config.output.path,
          },
        },
      ],
    });

    return config;
  },
};

export default nextConfig;

and change your package.json to build the baml client automatically (and enable logging in dev mode if you want):
 "scripts": {
    "dev": "BAML_LOG=info next dev",
    "build": "pnpm generate && next build",
    "start": "next start",
    "lint": "next lint",
    "generate": "baml-cli generate --from ./baml_src"
  },
",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/next-js",
    "title": "NextJS",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "When you develop with BAML, the BAML VScode extension generates a `baml_client` directory (on every save) with all the generated code you need to use your AI functions in your application.

We recommend you add `baml_client` to your `.gitignore` file to avoid committing generated code to your repository, and re-generate the client code when you build and deploy your application.

You _could_ commit the generated code if you're starting out to not deal with this, just make sure the VSCode extension version matches your baml package dependency version (e.g. `baml-py` for python and `@boundaryml/baml` for TS) so there are no compatibility issues.

To build your client you can use the following command. See also [baml-cli generate](/ref/baml-cli/generate):
  
<CodeBlocks>

```dockerfile python Dockerfile
RUN baml-cli generate --from path-to-baml_src
```

```dockerfile TypeScript Dockerfile
# Do this early on in the dockerfile script before transpiling to JS
RUN npx baml-cli generate --from path-to-baml_src
```

```dockerfile Ruby Dockerfile
RUN bundle add baml
RUN bundle exec baml-cli generate --from path/to/baml_src
```
</CodeBlocks>",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker",
    "title": "Docker",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "When you develop with BAML, the BAML VScode extension generates a baml_client directory (on every save) with all the generated code you need to use your AI functions in your application.
We recommend you add baml_client to your .gitignore file to avoid committing generated code to your repository, and re-generate the client code when you build and deploy your application.
You could commit the generated code if you're starting out to not deal with this, just make sure the VSCode extension version matches your baml package dependency version (e.g. baml-py for python and @boundaryml/baml for TS) so there are no compatibility issues.
To build your client you can use the following command. See also baml-cli generate:
RUN baml-cli generate --from path-to-baml_src
# Do this early on in the dockerfile script before transpiling to JS
RUN npx baml-cli generate --from path-to-baml_src
RUN bundle add baml
RUN bundle exec baml-cli generate --from path/to/baml_src
",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker",
    "title": "Docker",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "<Info>
  This feature was added in: v0.55.0.
</Info>

<Info>
  This page assumes you've gone through the [OpenAPI quickstart].
</Info>

[OpenAPI quickstart]: /docs/get-started/quickstart/openapi

To deploy BAML as a RESTful API, you'll need to do three things:

- host your BAML functions in a Docker container
- update your app to call it
- run BAML and your app side-by-side using `docker-compose`

Read on to learn how to do this with `docker-compose`.

<Tip>
  You can also run `baml-cli` in a subprocess from your app directly, and we
  may recommend this approach in the future. Please let us know if you'd
  like to see instructions for doing so, and in what language, by asking in
  [Discord][discord] or [on the GitHub issue][openapi-feedback-github-issue].
</Tip>",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api",
    "title": "OpenAPI",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "This feature was added in: v0.55.0.
This page assumes you've gone through the OpenAPI quickstart.
To deploy BAML as a RESTful API, you'll need to do three things:

host your BAML functions in a Docker container
update your app to call it
run BAML and your app side-by-side using docker-compose

Read on to learn how to do this with docker-compose.
You can also run baml-cli in a subprocess from your app directly, and we
may recommend this approach in the future. Please let us know if you'd
like to see instructions for doing so, and in what language, by asking in
Discord or on the GitHub issue.
Host your BAML functions in a Docker container
In the directory containing your baml_src/ directory, create a
baml.Dockerfile to host your BAML functions in a Docker container:
BAML-over-HTTP is currently a preview feature. Please provide feedback either
in Discord or on GitHub so that
we can stabilize the feature and keep you updated!
FROM node:20

WORKDIR /app
COPY baml_src/ .

# If you want to pin to a specific version (which we recommend):
# RUN npm install -g @boundaryml/baml@VERSION
RUN npm install -g @boundaryml/baml

CMD baml-cli serve --preview --port 2024

Assuming you intend to run your own application in a container, we recommend
using docker-compose to run your app and BAML-over-HTTP side-by-side:docker compose up --build --force-recreate
services:
  baml-over-http:
    build:
      # This will build baml.Dockerfile when you run docker-compose up
      context: .
      dockerfile: baml.Dockerfile
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:2024/_debug/ping" ]
      interval: 1s
      timeout: 100ms
      retries: 3
    # This allows you to 'curl localhost:2024/_debug/ping' from your machine,
    # i.e. the Docker host
    ports:
      - "2024:2024"

  debug-container:
    image: amazonlinux:latest
    depends_on:
      # Wait until the baml-over-http healthcheck passes to start this container
      baml-over-http:
        condition: service_healthy
    command: "curl -v http://baml-over-http:2024/_debug/ping"
To call the BAML server from your laptop (i.e. the host machine), you must use
localhost:2024. You may only reach it as baml-over-http:2024 from within
another Docker container.If you don't care about using docker-compose, you can just run:docker build -t baml-over-http -f baml.Dockerfile .
docker run -p 2024:2024 baml-over-http

To verify for yourself that BAML-over-HTTP is up and running, you can run:
curl http://localhost:2024/_debug/ping

Update your app to call it
Update your code to use BOUNDARY_ENDPOINT, if set, as the endpoint for your BAML functions.
If you plan to use Boundary Cloud to host your BAML functions, you should also update it to use BOUNDARY_API_KEY.
import (
    "os"
    baml "my-golang-app/baml_client"
)

func main() {
    cfg := baml.NewConfiguration()
    if boundaryEndpoint := os.Getenv("BOUNDARY_ENDPOINT"); boundaryEndpoint != "" {
        cfg.BasePath = boundaryEndpoint
    }
    if boundaryApiKey := os.Getenv("BOUNDARY_API_KEY"); boundaryApiKey != "" {
        cfg.DefaultHeader["Authorization"] = "Bearer " + boundaryApiKey
    }
    b := baml.NewAPIClient(cfg).DefaultAPI
    // Use `b` to make API calls
}
import com.boundaryml.baml_client.ApiClient;
import com.boundaryml.baml_client.ApiException;
import com.boundaryml.baml_client.Configuration;
import com.boundaryml.baml_client.api.DefaultApi;
import com.boundaryml.baml_client.auth.*;

public class ApiExample {
    public static void main(String[] args) {
        ApiClient apiClient = Configuration.getDefaultApiClient();

        String boundaryEndpoint = System.getenv("BOUNDARY_ENDPOINT");
        if (boundaryEndpoint != null && !boundaryEndpoint.isEmpty()) {
            apiClient.setBasePath(boundaryEndpoint);
        }

        String boundaryApiKey = System.getenv("BOUNDARY_API_KEY");
        if (boundaryApiKey != null && !boundaryApiKey.isEmpty()) {
            apiClient.addDefaultHeader("Authorization", "Bearer " + boundaryApiKey);
        }

        DefaultApi apiInstance = new DefaultApi(apiClient);
        // Use `apiInstance` to make API calls
    }
}
require_once(__DIR__ . '/vendor/autoload.php');

$config = BamlClient\Configuration::getDefaultConfiguration();

$boundaryEndpoint = getenv('BOUNDARY_ENDPOINT');
$boundaryApiKey = getenv('BOUNDARY_API_KEY');

if ($boundaryEndpoint) {
    $config->setHost($boundaryEndpoint);
}

if ($boundaryApiKey) {
    $config->setAccessToken($boundaryApiKey);
}

$apiInstance = new OpenAPI\Client\Api\DefaultApi(
    new GuzzleHttp\Client(),
    $config
);

// Use `$apiInstance` to make API calls
require 'baml_client'

api_client = BamlClient::ApiClient.new

boundary_endpoint = ENV['BOUNDARY_ENDPOINT']
if boundary_endpoint
  api_client.host = boundary_endpoint
end

boundary_api_key = ENV['BOUNDARY_API_KEY']
if boundary_api_key
  api_client.default_headers['Authorization'] = "Bearer #{boundary_api_key}"
end
b = BamlClient::DefaultApi.new(api_client)
# Use `b` to make API calls
let mut config = baml_client::apis::configuration::Configuration::default();
if let Some(base_path) = std::env::var("BOUNDARY_ENDPOINT").ok() {
    config.base_path = base_path;
}
if let Some(api_key) = std::env::var("BOUNDARY_API_KEY").ok() {
    config.bearer_access_token = Some(api_key);
}
// Use `config` to make API calls

Run your app with docker-compose
Replace debug-container with the Dockerfile for your app in the
docker-compose.yaml file:
services:
  baml-over-http:
    build:
      context: .
      dockerfile: baml.Dockerfile
    networks:
      - my-app-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:2024/_debug/ping" ]
      interval: 1s
      timeout: 100ms
      retries: 3
    ports:
      - "2024:2024"

  my-app:
    build:
      context: .
      dockerfile: my-app.Dockerfile
    depends_on:
      baml-over-http:
        condition: service_healthy
    environment:
      - BAML_ENDPOINT=http://baml-over-http:2024

  debug-container:
    image: amazonlinux:latest
    depends_on:
      baml-over-http:
        condition: service_healthy
    command: sh -c 'curl -v "$${BAML_ENDPOINT}/_debug/ping"'
    environment:
      - BAML_ENDPOINT=http://baml-over-http:2024

Additionally, you'll want to make sure that you generate the BAML client at
image build time, because baml_client/ should not be checked into your repo.
This means that in the CI workflow you use to push your Docker images, you'll
want to do something like this:
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Build the BAML client
        run: |
          set -eux
          npx @boundaryml/baml generate
          docker build -t my-app .

(Optional) Secure your BAML functions
To secure your BAML server, you can also set a password on it using the
BAML_PASSWORD environment variable:
BAML_PASSWORD=sk-baml-your-secret-password \
  baml-cli serve --preview --port 2024
FROM node:20

WORKDIR /app
RUN npm install -g @boundaryml/baml
COPY baml_src/ .

ENV BAML_PASSWORD=sk-baml-your-secret-password
CMD baml-cli serve --preview --port 2024

This will require incoming requests to attach your specified password as
authorization metadata. You can verify this by confirming that this returns 403 Forbidden:
curl -v "http://localhost:2024/_debug/status"

If you attach your password to the request, you'll see that it now returns 200 OK:
export BAML_PASSWORD=sk-baml-your-secret-password
curl "http://baml:${BAML_PASSWORD}@localhost:2024/_debug/status"
export BAML_PASSWORD=sk-baml-your-secret-password
curl "http://localhost:2024/_debug/status" -H "X-BAML-API-KEY: ${BAML_PASSWORD}"

BAML_PASSWORD will secure all endpoints except /_debug/ping, so that you
can always debug the reachability of your BAML server.",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api",
    "title": "OpenAPI",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "In the directory containing your baml_src/ directory, create a
baml.Dockerfile to host your BAML functions in a Docker container:
BAML-over-HTTP is currently a preview feature. Please provide feedback either
in [Discord][discord] or on [GitHub][openapi-feedback-github-issue] so that
we can stabilize the feature and keep you updated!
FROM node:20

WORKDIR /app
COPY baml_src/ .

# If you want to pin to a specific version (which we recommend):
# RUN npm install -g @boundaryml/baml@VERSION
RUN npm install -g @boundaryml/baml

CMD baml-cli serve --preview --port 2024

Assuming you intend to run your own application in a container, we recommend
using docker-compose to run your app and BAML-over-HTTP side-by-side:docker compose up --build --force-recreate
services:
  baml-over-http:
    build:
      # This will build baml.Dockerfile when you run docker-compose up
      context: .
      dockerfile: baml.Dockerfile
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:2024/_debug/ping" ]
      interval: 1s
      timeout: 100ms
      retries: 3
    # This allows you to 'curl localhost:2024/_debug/ping' from your machine,
    # i.e. the Docker host
    ports:
      - "2024:2024"

  debug-container:
    image: amazonlinux:latest
    depends_on:
      # Wait until the baml-over-http healthcheck passes to start this container
      baml-over-http:
        condition: service_healthy
    command: "curl -v http://baml-over-http:2024/_debug/ping"
To call the BAML server from your laptop (i.e. the host machine), you must use
localhost:2024. You may only reach it as baml-over-http:2024 from within
another Docker container.If you don't care about using docker-compose, you can just run:docker build -t baml-over-http -f baml.Dockerfile .
docker run -p 2024:2024 baml-over-http

To verify for yourself that BAML-over-HTTP is up and running, you can run:
curl http://localhost:2024/_debug/ping
",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api#host-your-baml-functions-in-a-docker-container",
    "title": "Host your BAML functions in a Docker container",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "In the directory containing your baml_src/ directory, create a
baml.Dockerfile to host your BAML functions in a Docker container:
BAML-over-HTTP is currently a preview feature. Please provide feedback either
in [Discord][discord] or on [GitHub][openapi-feedback-github-issue] so that
we can stabilize the feature and keep you updated!
FROM node:20

WORKDIR /app
COPY baml_src/ .

# If you want to pin to a specific version (which we recommend):
# RUN npm install -g @boundaryml/baml@VERSION
RUN npm install -g @boundaryml/baml

CMD baml-cli serve --preview --port 2024

Assuming you intend to run your own application in a container, we recommend
using docker-compose to run your app and BAML-over-HTTP side-by-side:docker compose up --build --force-recreate
services:
  baml-over-http:
    build:
      # This will build baml.Dockerfile when you run docker-compose up
      context: .
      dockerfile: baml.Dockerfile
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:2024/_debug/ping" ]
      interval: 1s
      timeout: 100ms
      retries: 3
    # This allows you to 'curl localhost:2024/_debug/ping' from your machine,
    # i.e. the Docker host
    ports:
      - "2024:2024"

  debug-container:
    image: amazonlinux:latest
    depends_on:
      # Wait until the baml-over-http healthcheck passes to start this container
      baml-over-http:
        condition: service_healthy
    command: "curl -v http://baml-over-http:2024/_debug/ping"
To call the BAML server from your laptop (i.e. the host machine), you must use
localhost:2024. You may only reach it as baml-over-http:2024 from within
another Docker container.If you don't care about using docker-compose, you can just run:docker build -t baml-over-http -f baml.Dockerfile .
docker run -p 2024:2024 baml-over-http

To verify for yourself that BAML-over-HTTP is up and running, you can run:
curl http://localhost:2024/_debug/ping
",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api#host-your-baml-functions-in-a-docker-container",
    "title": "Host your BAML functions in a Docker container",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "Update your code to use BOUNDARY_ENDPOINT, if set, as the endpoint for your BAML functions.
If you plan to use Boundary Cloud to host your BAML functions, you should also update it to use BOUNDARY_API_KEY.
import (
    "os"
    baml "my-golang-app/baml_client"
)

func main() {
    cfg := baml.NewConfiguration()
    if boundaryEndpoint := os.Getenv("BOUNDARY_ENDPOINT"); boundaryEndpoint != "" {
        cfg.BasePath = boundaryEndpoint
    }
    if boundaryApiKey := os.Getenv("BOUNDARY_API_KEY"); boundaryApiKey != "" {
        cfg.DefaultHeader["Authorization"] = "Bearer " + boundaryApiKey
    }
    b := baml.NewAPIClient(cfg).DefaultAPI
    // Use `b` to make API calls
}
import com.boundaryml.baml_client.ApiClient;
import com.boundaryml.baml_client.ApiException;
import com.boundaryml.baml_client.Configuration;
import com.boundaryml.baml_client.api.DefaultApi;
import com.boundaryml.baml_client.auth.*;

public class ApiExample {
    public static void main(String[] args) {
        ApiClient apiClient = Configuration.getDefaultApiClient();

        String boundaryEndpoint = System.getenv("BOUNDARY_ENDPOINT");
        if (boundaryEndpoint != null && !boundaryEndpoint.isEmpty()) {
            apiClient.setBasePath(boundaryEndpoint);
        }

        String boundaryApiKey = System.getenv("BOUNDARY_API_KEY");
        if (boundaryApiKey != null && !boundaryApiKey.isEmpty()) {
            apiClient.addDefaultHeader("Authorization", "Bearer " + boundaryApiKey);
        }

        DefaultApi apiInstance = new DefaultApi(apiClient);
        // Use `apiInstance` to make API calls
    }
}
require_once(__DIR__ . '/vendor/autoload.php');

$config = BamlClient\Configuration::getDefaultConfiguration();

$boundaryEndpoint = getenv('BOUNDARY_ENDPOINT');
$boundaryApiKey = getenv('BOUNDARY_API_KEY');

if ($boundaryEndpoint) {
    $config->setHost($boundaryEndpoint);
}

if ($boundaryApiKey) {
    $config->setAccessToken($boundaryApiKey);
}

$apiInstance = new OpenAPI\Client\Api\DefaultApi(
    new GuzzleHttp\Client(),
    $config
);

// Use `$apiInstance` to make API calls
require 'baml_client'

api_client = BamlClient::ApiClient.new

boundary_endpoint = ENV['BOUNDARY_ENDPOINT']
if boundary_endpoint
  api_client.host = boundary_endpoint
end

boundary_api_key = ENV['BOUNDARY_API_KEY']
if boundary_api_key
  api_client.default_headers['Authorization'] = "Bearer #{boundary_api_key}"
end
b = BamlClient::DefaultApi.new(api_client)
# Use `b` to make API calls
let mut config = baml_client::apis::configuration::Configuration::default();
if let Some(base_path) = std::env::var("BOUNDARY_ENDPOINT").ok() {
    config.base_path = base_path;
}
if let Some(api_key) = std::env::var("BOUNDARY_API_KEY").ok() {
    config.bearer_access_token = Some(api_key);
}
// Use `config` to make API calls
",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api#update-your-app-to-call-it",
    "title": "Update your app to call it",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "Update your code to use BOUNDARY_ENDPOINT, if set, as the endpoint for your BAML functions.
If you plan to use Boundary Cloud to host your BAML functions, you should also update it to use BOUNDARY_API_KEY.
import (
    "os"
    baml "my-golang-app/baml_client"
)

func main() {
    cfg := baml.NewConfiguration()
    if boundaryEndpoint := os.Getenv("BOUNDARY_ENDPOINT"); boundaryEndpoint != "" {
        cfg.BasePath = boundaryEndpoint
    }
    if boundaryApiKey := os.Getenv("BOUNDARY_API_KEY"); boundaryApiKey != "" {
        cfg.DefaultHeader["Authorization"] = "Bearer " + boundaryApiKey
    }
    b := baml.NewAPIClient(cfg).DefaultAPI
    // Use `b` to make API calls
}
import com.boundaryml.baml_client.ApiClient;
import com.boundaryml.baml_client.ApiException;
import com.boundaryml.baml_client.Configuration;
import com.boundaryml.baml_client.api.DefaultApi;
import com.boundaryml.baml_client.auth.*;

public class ApiExample {
    public static void main(String[] args) {
        ApiClient apiClient = Configuration.getDefaultApiClient();

        String boundaryEndpoint = System.getenv("BOUNDARY_ENDPOINT");
        if (boundaryEndpoint != null && !boundaryEndpoint.isEmpty()) {
            apiClient.setBasePath(boundaryEndpoint);
        }

        String boundaryApiKey = System.getenv("BOUNDARY_API_KEY");
        if (boundaryApiKey != null && !boundaryApiKey.isEmpty()) {
            apiClient.addDefaultHeader("Authorization", "Bearer " + boundaryApiKey);
        }

        DefaultApi apiInstance = new DefaultApi(apiClient);
        // Use `apiInstance` to make API calls
    }
}
require_once(__DIR__ . '/vendor/autoload.php');

$config = BamlClient\Configuration::getDefaultConfiguration();

$boundaryEndpoint = getenv('BOUNDARY_ENDPOINT');
$boundaryApiKey = getenv('BOUNDARY_API_KEY');

if ($boundaryEndpoint) {
    $config->setHost($boundaryEndpoint);
}

if ($boundaryApiKey) {
    $config->setAccessToken($boundaryApiKey);
}

$apiInstance = new OpenAPI\Client\Api\DefaultApi(
    new GuzzleHttp\Client(),
    $config
);

// Use `$apiInstance` to make API calls
require 'baml_client'

api_client = BamlClient::ApiClient.new

boundary_endpoint = ENV['BOUNDARY_ENDPOINT']
if boundary_endpoint
  api_client.host = boundary_endpoint
end

boundary_api_key = ENV['BOUNDARY_API_KEY']
if boundary_api_key
  api_client.default_headers['Authorization'] = "Bearer #{boundary_api_key}"
end
b = BamlClient::DefaultApi.new(api_client)
# Use `b` to make API calls
let mut config = baml_client::apis::configuration::Configuration::default();
if let Some(base_path) = std::env::var("BOUNDARY_ENDPOINT").ok() {
    config.base_path = base_path;
}
if let Some(api_key) = std::env::var("BOUNDARY_API_KEY").ok() {
    config.bearer_access_token = Some(api_key);
}
// Use `config` to make API calls
",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api#update-your-app-to-call-it",
    "title": "Update your app to call it",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "Replace debug-container with the Dockerfile for your app in the
docker-compose.yaml file:
services:
  baml-over-http:
    build:
      context: .
      dockerfile: baml.Dockerfile
    networks:
      - my-app-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:2024/_debug/ping" ]
      interval: 1s
      timeout: 100ms
      retries: 3
    ports:
      - "2024:2024"

  my-app:
    build:
      context: .
      dockerfile: my-app.Dockerfile
    depends_on:
      baml-over-http:
        condition: service_healthy
    environment:
      - BAML_ENDPOINT=http://baml-over-http:2024

  debug-container:
    image: amazonlinux:latest
    depends_on:
      baml-over-http:
        condition: service_healthy
    command: sh -c 'curl -v "$${BAML_ENDPOINT}/_debug/ping"'
    environment:
      - BAML_ENDPOINT=http://baml-over-http:2024

Additionally, you'll want to make sure that you generate the BAML client at
image build time, because baml_client/ should not be checked into your repo.
This means that in the CI workflow you use to push your Docker images, you'll
want to do something like this:
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Build the BAML client
        run: |
          set -eux
          npx @boundaryml/baml generate
          docker build -t my-app .
",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api#run-your-app-with-docker-compose",
    "title": "Run your app with docker-compose",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "Replace debug-container with the Dockerfile for your app in the
docker-compose.yaml file:
services:
  baml-over-http:
    build:
      context: .
      dockerfile: baml.Dockerfile
    networks:
      - my-app-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:2024/_debug/ping" ]
      interval: 1s
      timeout: 100ms
      retries: 3
    ports:
      - "2024:2024"

  my-app:
    build:
      context: .
      dockerfile: my-app.Dockerfile
    depends_on:
      baml-over-http:
        condition: service_healthy
    environment:
      - BAML_ENDPOINT=http://baml-over-http:2024

  debug-container:
    image: amazonlinux:latest
    depends_on:
      baml-over-http:
        condition: service_healthy
    command: sh -c 'curl -v "$${BAML_ENDPOINT}/_debug/ping"'
    environment:
      - BAML_ENDPOINT=http://baml-over-http:2024

Additionally, you'll want to make sure that you generate the BAML client at
image build time, because baml_client/ should not be checked into your repo.
This means that in the CI workflow you use to push your Docker images, you'll
want to do something like this:
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Build the BAML client
        run: |
          set -eux
          npx @boundaryml/baml generate
          docker build -t my-app .
",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api#run-your-app-with-docker-compose",
    "title": "Run your app with docker-compose",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/development/environment-variables",
        "title": "Development",
      },
      {
        "slug": "guide/development/deploying/aws",
        "title": "Deploying",
      },
    ],
    "description": "To secure your BAML server, you can also set a password on it using the
BAML_PASSWORD environment variable:
BAML_PASSWORD=sk-baml-your-secret-password \
  baml-cli serve --preview --port 2024
FROM node:20

WORKDIR /app
RUN npm install -g @boundaryml/baml
COPY baml_src/ .

ENV BAML_PASSWORD=sk-baml-your-secret-password
CMD baml-cli serve --preview --port 2024

This will require incoming requests to attach your specified password as
authorization metadata. You can verify this by confirming that this returns 403 Forbidden:
curl -v "http://localhost:2024/_debug/status"

If you attach your password to the request, you'll see that it now returns 200 OK:
export BAML_PASSWORD=sk-baml-your-secret-password
curl "http://baml:${BAML_PASSWORD}@localhost:2024/_debug/status"
export BAML_PASSWORD=sk-baml-your-secret-password
curl "http://localhost:2024/_debug/status" -H "X-BAML-API-KEY: ${BAML_PASSWORD}"

BAML_PASSWORD will secure all endpoints except /_debug/ping, so that you
can always debug the reachability of your BAML server.",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api#optional-secure-your-baml-functions",
    "title": "(Optional) Secure your BAML functions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Development",
      "Deploying",
    ],
    "content": "To secure your BAML server, you can also set a password on it using the
BAML_PASSWORD environment variable:
BAML_PASSWORD=sk-baml-your-secret-password \
  baml-cli serve --preview --port 2024
FROM node:20

WORKDIR /app
RUN npm install -g @boundaryml/baml
COPY baml_src/ .

ENV BAML_PASSWORD=sk-baml-your-secret-password
CMD baml-cli serve --preview --port 2024

This will require incoming requests to attach your specified password as
authorization metadata. You can verify this by confirming that this returns 403 Forbidden:
curl -v "http://localhost:2024/_debug/status"

If you attach your password to the request, you'll see that it now returns 200 OK:
export BAML_PASSWORD=sk-baml-your-secret-password
curl "http://baml:${BAML_PASSWORD}@localhost:2024/_debug/status"
export BAML_PASSWORD=sk-baml-your-secret-password
curl "http://localhost:2024/_debug/status" -H "X-BAML-API-KEY: ${BAML_PASSWORD}"

BAML_PASSWORD will secure all endpoints except /_debug/ping, so that you
can always debug the reachability of your BAML server.",
    "indexSegmentId": "0",
    "slug": "guide/development/deploying/docker-rest-api#optional-secure-your-baml-functions",
    "title": "(Optional) Secure your BAML functions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "<Note>
We recommend reading the [installation](/guide/installation-language/python) instructions first
</Note>

BAML functions are special definitions that get converted into real code (Python, TS, etc) that calls LLMs. Think of them as a way to define AI-powered functions that are type-safe and easy to use in your application.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml",
    "title": "Prompting in BAML",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "We recommend reading the installation instructions first
BAML functions are special definitions that get converted into real code (Python, TS, etc) that calls LLMs. Think of them as a way to define AI-powered functions that are type-safe and easy to use in your application.
What BAML Functions Actually Do
When you write a BAML function like this:
function ExtractResume(resume_text: string) -> Resume {
  client "openai/gpt-4o"
  // The prompt uses Jinja syntax.. more on this soon.
  prompt #"
     Extract info from this text.

    {# special macro to print the output schema + instructions #}
    {{ ctx.output_format }}

    Resume:
    ---
    {{ resume_text }}
    ---
  "#
}

BAML converts it into code that:

Takes your input (resume_text)
Sends a request to OpenAI's GPT-4 API with your prompt.
Parses the JSON response into your Resume type
Returns a type-safe object you can use in your code

Prompt Preview + seeing the CURL request
For maximum transparency, you can see the API request BAML makes to the LLM provider using the VSCode extension.
Below you can see the Prompt Preview, where you see the full rendered prompt (once you add a test case):

Note how the {{ ctx.output_format }} macro is replaced with the output schema instructions.
The Playground will also show you the Raw CURL request (if you click on the "curl" checkbox):

Always include the {{ ctx.output_format }} macro in your prompt. This injects your output schema into the prompt, which helps the LLM output the right thing. You can also customize what it prints.One of our design philosophies is to never hide the prompt from you. You control and can always see the entire prompt.
Calling the function
Recall that BAML will generate a baml_client directory in the language of your choice using the parameters in your generator config. This contains the function and types you defined.
Now we can call the function, which will make a request to the LLM and return the Resume object:
# Import the baml client (We call it `b` for short)
from baml_client import b
# Import the Resume type, which is now a Pydantic model!
from baml_client.types import Resume 

def main():
resume_text = """Jason Doe\nPython, Rust\nUniversity of California, Berkeley, B.S.\nin Computer Science, 2020\nAlso an expert in Tableau, SQL, and C++\n"""

    # this function comes from the autogenerated "baml_client".
    # It calls the LLM you specified and handles the parsing.
    resume = b.ExtractResume(resume_text)

    # Fully type-checked and validated!
    assert isinstance(resume, Resume)

import b from 'baml_client'
import { Resume } from 'baml_client/types'

async function main() {
  const resume_text = `Jason Doe\nPython, Rust\nUniversity of California, Berkeley, B.S.\nin Computer Science, 2020\nAlso an expert in Tableau, SQL, and C++`

  // this function comes from the autogenerated "baml_client".
  // It calls the LLM you specified and handles the parsing.
  const resume = await b.ExtractResume(resume_text)

  // Fully type-checked and validated!
  resume.name === 'Jason Doe'
  if (resume instanceof Resume) {
    console.log('resume is a Resume')
  }
}

require_relative "baml_client/client"
b = Baml.Client

# Note this is not async
res = b.TestFnNamedArgsSingleClass(
    myArg: Baml::Types::Resume.new(
        key: "key",
        key_two: true,
        key_three: 52,
    )
)

Do not modify any code inside baml_client, as it's autogenerated.
Next steps
Checkout PromptFiddle to see various interactive BAML function examples or view the example prompts
Read the next guide to learn more about choosing different LLM providers and running tests in the VSCode extension.
Use any provider or open-source modelTest your functions in the VSCode extensionDefine user or assistant roles in your promptsUse function calling or tools in your prompts",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml",
    "title": "Prompting in BAML",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "When you write a BAML function like this:
function ExtractResume(resume_text: string) -> Resume {
  client "openai/gpt-4o"
  // The prompt uses Jinja syntax.. more on this soon.
  prompt #"
     Extract info from this text.

    {# special macro to print the output schema + instructions #}
    {{ ctx.output_format }}

    Resume:
    ---
    {{ resume_text }}
    ---
  "#
}

BAML converts it into code that:

Takes your input (resume_text)
Sends a request to OpenAI's GPT-4 API with your prompt.
Parses the JSON response into your Resume type
Returns a type-safe object you can use in your code
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml#what-baml-functions-actually-do",
    "title": "What BAML Functions Actually Do",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "When you write a BAML function like this:
function ExtractResume(resume_text: string) -> Resume {
  client "openai/gpt-4o"
  // The prompt uses Jinja syntax.. more on this soon.
  prompt #"
     Extract info from this text.

    {# special macro to print the output schema + instructions #}
    {{ ctx.output_format }}

    Resume:
    ---
    {{ resume_text }}
    ---
  "#
}

BAML converts it into code that:

Takes your input (resume_text)
Sends a request to OpenAI's GPT-4 API with your prompt.
Parses the JSON response into your Resume type
Returns a type-safe object you can use in your code
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml#what-baml-functions-actually-do",
    "title": "What BAML Functions Actually Do",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "For maximum transparency, you can see the API request BAML makes to the LLM provider using the VSCode extension.
Below you can see the Prompt Preview, where you see the full rendered prompt (once you add a test case):

Note how the {{ ctx.output_format }} macro is replaced with the output schema instructions.
The Playground will also show you the Raw CURL request (if you click on the "curl" checkbox):

Always include the {{ ctx.output_format }} macro in your prompt. This injects your output schema into the prompt, which helps the LLM output the right thing. You can also customize what it prints.One of our design philosophies is to never hide the prompt from you. You control and can always see the entire prompt.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml#prompt-preview--seeing-the-curl-request",
    "title": "Prompt Preview + seeing the CURL request",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "For maximum transparency, you can see the API request BAML makes to the LLM provider using the VSCode extension.
Below you can see the Prompt Preview, where you see the full rendered prompt (once you add a test case):

Note how the {{ ctx.output_format }} macro is replaced with the output schema instructions.
The Playground will also show you the Raw CURL request (if you click on the "curl" checkbox):

Always include the {{ ctx.output_format }} macro in your prompt. This injects your output schema into the prompt, which helps the LLM output the right thing. You can also customize what it prints.One of our design philosophies is to never hide the prompt from you. You control and can always see the entire prompt.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml#prompt-preview--seeing-the-curl-request",
    "title": "Prompt Preview + seeing the CURL request",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "Recall that BAML will generate a baml_client directory in the language of your choice using the parameters in your generator config. This contains the function and types you defined.
Now we can call the function, which will make a request to the LLM and return the Resume object:
# Import the baml client (We call it `b` for short)
from baml_client import b
# Import the Resume type, which is now a Pydantic model!
from baml_client.types import Resume 

def main():
resume_text = """Jason Doe\nPython, Rust\nUniversity of California, Berkeley, B.S.\nin Computer Science, 2020\nAlso an expert in Tableau, SQL, and C++\n"""

    # this function comes from the autogenerated "baml_client".
    # It calls the LLM you specified and handles the parsing.
    resume = b.ExtractResume(resume_text)

    # Fully type-checked and validated!
    assert isinstance(resume, Resume)

import b from 'baml_client'
import { Resume } from 'baml_client/types'

async function main() {
  const resume_text = `Jason Doe\nPython, Rust\nUniversity of California, Berkeley, B.S.\nin Computer Science, 2020\nAlso an expert in Tableau, SQL, and C++`

  // this function comes from the autogenerated "baml_client".
  // It calls the LLM you specified and handles the parsing.
  const resume = await b.ExtractResume(resume_text)

  // Fully type-checked and validated!
  resume.name === 'Jason Doe'
  if (resume instanceof Resume) {
    console.log('resume is a Resume')
  }
}

require_relative "baml_client/client"
b = Baml.Client

# Note this is not async
res = b.TestFnNamedArgsSingleClass(
    myArg: Baml::Types::Resume.new(
        key: "key",
        key_two: true,
        key_three: 52,
    )
)

Do not modify any code inside baml_client, as it's autogenerated.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml#calling-the-function",
    "title": "Calling the function",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "Recall that BAML will generate a baml_client directory in the language of your choice using the parameters in your generator config. This contains the function and types you defined.
Now we can call the function, which will make a request to the LLM and return the Resume object:
# Import the baml client (We call it `b` for short)
from baml_client import b
# Import the Resume type, which is now a Pydantic model!
from baml_client.types import Resume 

def main():
resume_text = """Jason Doe\nPython, Rust\nUniversity of California, Berkeley, B.S.\nin Computer Science, 2020\nAlso an expert in Tableau, SQL, and C++\n"""

    # this function comes from the autogenerated "baml_client".
    # It calls the LLM you specified and handles the parsing.
    resume = b.ExtractResume(resume_text)

    # Fully type-checked and validated!
    assert isinstance(resume, Resume)

import b from 'baml_client'
import { Resume } from 'baml_client/types'

async function main() {
  const resume_text = `Jason Doe\nPython, Rust\nUniversity of California, Berkeley, B.S.\nin Computer Science, 2020\nAlso an expert in Tableau, SQL, and C++`

  // this function comes from the autogenerated "baml_client".
  // It calls the LLM you specified and handles the parsing.
  const resume = await b.ExtractResume(resume_text)

  // Fully type-checked and validated!
  resume.name === 'Jason Doe'
  if (resume instanceof Resume) {
    console.log('resume is a Resume')
  }
}

require_relative "baml_client/client"
b = Baml.Client

# Note this is not async
res = b.TestFnNamedArgsSingleClass(
    myArg: Baml::Types::Resume.new(
        key: "key",
        key_two: true,
        key_three: 52,
    )
)

Do not modify any code inside baml_client, as it's autogenerated.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml#calling-the-function",
    "title": "Calling the function",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "Checkout PromptFiddle to see various interactive BAML function examples or view the example prompts
Read the next guide to learn more about choosing different LLM providers and running tests in the VSCode extension.
Use any provider or open-source modelTest your functions in the VSCode extensionDefine user or assistant roles in your promptsUse function calling or tools in your prompts",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml#next-steps",
    "title": "Next steps",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "Checkout PromptFiddle to see various interactive BAML function examples or view the example prompts
Read the next guide to learn more about choosing different LLM providers and running tests in the VSCode extension.
Use any provider or open-source modelTest your functions in the VSCode extensionDefine user or assistant roles in your promptsUse function calling or tools in your prompts",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/prompting-with-baml#next-steps",
    "title": "Next steps",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "BAML Supports getting structured output from **all** major providers as well as all OpenAI-API compatible open-source models. See [LLM Providers Reference](/ref/llm-client-providers/open-ai) for how to set each one up.
<Tip>
BAML can help you get structured output from **any Open-Source model**, with better performance than other techniques, even when it's not officially supported via a Tool-Use API (like o1-preview) or fine-tuned for it! [Read more about how BAML does this](https://www.boundaryml.com/blog/schema-aligned-parsing).
</Tip>",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/switching-llms",
    "title": "Switching LLMs",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "BAML Supports getting structured output from all major providers as well as all OpenAI-API compatible open-source models. See LLM Providers Reference for how to set each one up.
BAML can help you get structured output from any Open-Source model, with better performance than other techniques, even when it's not officially supported via a Tool-Use API (like o1-preview) or fine-tuned for it! Read more about how BAML does this.
Using client "<provider>/<model>"
Using openai/model-name or anthropic/model-name will assume you have the ANTHROPIC_API_KEY or OPENAI_API_KEY environment variables set.
function MakeHaiku(topic: string) -> string {
  client "openai/gpt-4o" // or anthropic/claude-3-5-sonnet-20240620
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}

Using a named client
Use this if you are using open-source models or need customization
The longer form uses a named client, and supports adding any parameters supported by the provider or changing the temperature, top_p, etc.
client<llm> MyClient {
  provider "openai"
  options {
    model "gpt-4o"
    api_key env.OPENAI_API_KEY
    // other params like temperature, top_p, etc.
    temperature 0.0
    base_url "https://my-custom-endpoint.com/v1"
    // add headers
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }

}

function MakeHaiku(topic: string) -> string {
  client MyClient
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}

Consult the provider documentation for a list of supported providers
and models, the default options, and setting retry policies.
If you want to specify which client to use at runtime, in your Python/TS/Ruby code,
you can use the client registry to do so.This can come in handy if you're trying to, say, send 10% of your requests to a
different model.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/switching-llms",
    "title": "Switching LLMs",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "Using openai/model-name or anthropic/model-name will assume you have the ANTHROPIC_API_KEY or OPENAI_API_KEY environment variables set.
function MakeHaiku(topic: string) -> string {
  client "openai/gpt-4o" // or anthropic/claude-3-5-sonnet-20240620
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/switching-llms#using-client-providermodel",
    "title": "Using client "<provider>/<model>"",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "Using openai/model-name or anthropic/model-name will assume you have the ANTHROPIC_API_KEY or OPENAI_API_KEY environment variables set.
function MakeHaiku(topic: string) -> string {
  client "openai/gpt-4o" // or anthropic/claude-3-5-sonnet-20240620
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/switching-llms#using-client-providermodel",
    "title": "Using client "<provider>/<model>"",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "Use this if you are using open-source models or need customization
The longer form uses a named client, and supports adding any parameters supported by the provider or changing the temperature, top_p, etc.
client<llm> MyClient {
  provider "openai"
  options {
    model "gpt-4o"
    api_key env.OPENAI_API_KEY
    // other params like temperature, top_p, etc.
    temperature 0.0
    base_url "https://my-custom-endpoint.com/v1"
    // add headers
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }

}

function MakeHaiku(topic: string) -> string {
  client MyClient
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}

Consult the provider documentation for a list of supported providers
and models, the default options, and setting retry policies.
If you want to specify which client to use at runtime, in your Python/TS/Ruby code,
you can use the client registry to do so.This can come in handy if you're trying to, say, send 10% of your requests to a
different model.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/switching-llms#using-a-named-client",
    "title": "Using a named client",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "Use this if you are using open-source models or need customization
The longer form uses a named client, and supports adding any parameters supported by the provider or changing the temperature, top_p, etc.
client<llm> MyClient {
  provider "openai"
  options {
    model "gpt-4o"
    api_key env.OPENAI_API_KEY
    // other params like temperature, top_p, etc.
    temperature 0.0
    base_url "https://my-custom-endpoint.com/v1"
    // add headers
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }

}

function MakeHaiku(topic: string) -> string {
  client MyClient
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}

Consult the provider documentation for a list of supported providers
and models, the default options, and setting retry policies.
If you want to specify which client to use at runtime, in your Python/TS/Ruby code,
you can use the client registry to do so.This can come in handy if you're trying to, say, send 10% of your requests to a
different model.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/switching-llms#using-a-named-client",
    "title": "Using a named client",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "You can test your BAML functions in the VSCode Playground by adding a `test` snippet into a BAML file:

```baml
enum Category {
    Refund
    CancelOrder
    TechnicalSupport
    AccountIssue
    Question
}

function ClassifyMessage(input: string) -> Category {
  client GPT4Turbo
  prompt #"
    ... truncated ...
  "#
}

test Test1 {
  functions [ClassifyMessage]
  args {
    // input is the first argument of ClassifyMessage
    input "Can't access my account using my usual login credentials, and each attempt results in an error message stating 'Invalid username or password.' I have tried resetting my password using the 'Forgot Password' link, but I haven't received the promised password reset email."
  }
}
```
See the [interactive examples](https://promptfiddle.com)

The BAML playground will give you a starting snippet to copy that will match your function signature.

<Warning>
BAML doesn't use colons `:` between key-value pairs except in function parameters.
</Warning>

<hr />",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions",
    "title": "Testing functions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "You can test your BAML functions in the VSCode Playground by adding a test snippet into a BAML file:
enum Category {
    Refund
    CancelOrder
    TechnicalSupport
    AccountIssue
    Question
}

function ClassifyMessage(input: string) -> Category {
  client GPT4Turbo
  prompt #"
    ... truncated ...
  "#
}

test Test1 {
  functions [ClassifyMessage]
  args {
    // input is the first argument of ClassifyMessage
    input "Can't access my account using my usual login credentials, and each attempt results in an error message stating 'Invalid username or password.' I have tried resetting my password using the 'Forgot Password' link, but I haven't received the promised password reset email."
  }
}

See the interactive examples
The BAML playground will give you a starting snippet to copy that will match your function signature.
BAML doesn't use colons : between key-value pairs except in function parameters.

Complex object inputs
Objects are injected as dictionaries
class Message {
  user string
  content string
}

function ClassifyMessage(messages: Messages[]) -> Category {
...
}

test Test1 {
  functions [ClassifyMessage]
  args {
    messages [
      {
        user "hey there"
        // multi-line string using the #"..."# syntax
        content #"
          You can also add a multi-line
          string with the hashtags
          Instead of ugly json with \n
        "#
      }
    ]
  }
}


Test Image Inputs in the Playground
For a function that takes an image as input, like so:
function MyFunction(myImage: image) -> string {
  client GPT4o
  prompt #"
    Describe this image: {{myImage}}
  "#
}

You can define test cases using image files, URLs, or base64 strings.
Committing a lot of images into your repository can make it slow to clone and
pull your repository. If you expect to commit >500MiB of images, please read
GitHub's size limit documentation and consider setting
up large file storage.test Test1 {
  functions [MyFunction]
  args {
    myImage {
      file "../path/to/image.png"
    }
  }
}
The path to the image file, relative to the directory containing the current BAML file.Image files must be somewhere in baml_src/.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on first, the file extension,
and second, the contents of the file.test Test1 {
  functions [MyFunction]
  args {
    myImage {
      url "https...."
    }
  }
}
The publicly accessible URL from which the image may be downloaded.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.test Test1 {
  args {
    myImage {
      base64 "base64string"
      media_type "image/png"
    }
  }
}
The base64-encoded image data.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.If base64 is a data URL, this field will be ignored.

Test Audio Inputs in the Playground
For a function that takes audio as input, like so:
function MyFunction(myAudio: audio) -> string {
  client GPT4o
  prompt #"
    Describe this audio: {{myAudio}}
  "#
}

You can define test cases using audio files, URLs, or base64 strings.
Committing a lot of audio files into your repository can make it slow to clone and
pull your repository. If you expect to commit >500MiB of audio, please read
GitHub's size limit documentation and consider setting
up large file storage.test Test1 {
  functions [MyFunction]
  args {
    myAudio {
      file "../path/to/audio.mp3"
    }
  }
}
The path to the audio file, relative to the directory containing the current BAML file.audio files must be somewhere in baml_src/.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on first, the file extension,
and second, the contents of the file.test Test1 {
  functions [MyFunction]
  args {
    myAudio {
      url "https...."
    }
  }
}
The publicly accessible URL from which the audio may be downloaded.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.test Test1 {
  args {
    myAudio {
      base64 "base64string"
      media_type "audio/mp3"
    }
  }
}
The base64-encoded audio data.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.If base64 is a data URL, this field will be ignored.
Assertions
This is coming soon! We'll be supporting assertions in test cases. For now -- when you run a test you'll only see errors parsing the output into the right schema, or LLM-provider errors.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions",
    "title": "Testing functions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "Objects are injected as dictionaries
class Message {
  user string
  content string
}

function ClassifyMessage(messages: Messages[]) -> Category {
...
}

test Test1 {
  functions [ClassifyMessage]
  args {
    messages [
      {
        user "hey there"
        // multi-line string using the #"..."# syntax
        content #"
          You can also add a multi-line
          string with the hashtags
          Instead of ugly json with \n
        "#
      }
    ]
  }
}

",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions#complex-object-inputs",
    "title": "Complex object inputs",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "Objects are injected as dictionaries
class Message {
  user string
  content string
}

function ClassifyMessage(messages: Messages[]) -> Category {
...
}

test Test1 {
  functions [ClassifyMessage]
  args {
    messages [
      {
        user "hey there"
        // multi-line string using the #"..."# syntax
        content #"
          You can also add a multi-line
          string with the hashtags
          Instead of ugly json with \n
        "#
      }
    ]
  }
}

",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions#complex-object-inputs",
    "title": "Complex object inputs",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "For a function that takes an image as input, like so:
function MyFunction(myImage: image) -> string {
  client GPT4o
  prompt #"
    Describe this image: {{myImage}}
  "#
}

You can define test cases using image files, URLs, or base64 strings.
Committing a lot of images into your repository can make it slow to clone and
pull your repository. If you expect to commit >500MiB of images, please read
GitHub's size limit documentation and consider setting
up large file storage.test Test1 {
  functions [MyFunction]
  args {
    myImage {
      file "../path/to/image.png"
    }
  }
}
The path to the image file, relative to the directory containing the current BAML file.Image files must be somewhere in baml_src/.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on first, the file extension,
and second, the contents of the file.test Test1 {
  functions [MyFunction]
  args {
    myImage {
      url "https...."
    }
  }
}
The publicly accessible URL from which the image may be downloaded.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.test Test1 {
  args {
    myImage {
      base64 "base64string"
      media_type "image/png"
    }
  }
}
The base64-encoded image data.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.If base64 is a data URL, this field will be ignored.
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions#test-image-inputs-in-the-playground",
    "title": "Test Image Inputs in the Playground",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "For a function that takes an image as input, like so:
function MyFunction(myImage: image) -> string {
  client GPT4o
  prompt #"
    Describe this image: {{myImage}}
  "#
}

You can define test cases using image files, URLs, or base64 strings.
Committing a lot of images into your repository can make it slow to clone and
pull your repository. If you expect to commit >500MiB of images, please read
GitHub's size limit documentation and consider setting
up large file storage.test Test1 {
  functions [MyFunction]
  args {
    myImage {
      file "../path/to/image.png"
    }
  }
}
The path to the image file, relative to the directory containing the current BAML file.Image files must be somewhere in baml_src/.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on first, the file extension,
and second, the contents of the file.test Test1 {
  functions [MyFunction]
  args {
    myImage {
      url "https...."
    }
  }
}
The publicly accessible URL from which the image may be downloaded.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.test Test1 {
  args {
    myImage {
      base64 "base64string"
      media_type "image/png"
    }
  }
}
The base64-encoded image data.The mime-type of the image. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.If base64 is a data URL, this field will be ignored.
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions#test-image-inputs-in-the-playground",
    "title": "Test Image Inputs in the Playground",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "For a function that takes audio as input, like so:
function MyFunction(myAudio: audio) -> string {
  client GPT4o
  prompt #"
    Describe this audio: {{myAudio}}
  "#
}

You can define test cases using audio files, URLs, or base64 strings.
Committing a lot of audio files into your repository can make it slow to clone and
pull your repository. If you expect to commit >500MiB of audio, please read
[GitHub's size limit documentation][github-large-files] and consider setting
up [large file storage][github-lfs].test Test1 {
  functions [MyFunction]
  args {
    myAudio {
      file "../path/to/audio.mp3"
    }
  }
}
The path to the audio file, relative to the directory containing the current BAML file.audio files must be somewhere in baml_src/.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on first, the file extension,
and second, the contents of the file.test Test1 {
  functions [MyFunction]
  args {
    myAudio {
      url "https...."
    }
  }
}
The publicly accessible URL from which the audio may be downloaded.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.test Test1 {
  args {
    myAudio {
      base64 "base64string"
      media_type "audio/mp3"
    }
  }
}
The base64-encoded audio data.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.If base64 is a data URL, this field will be ignored.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions#test-audio-inputs-in-the-playground",
    "title": "Test Audio Inputs in the Playground",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "For a function that takes audio as input, like so:
function MyFunction(myAudio: audio) -> string {
  client GPT4o
  prompt #"
    Describe this audio: {{myAudio}}
  "#
}

You can define test cases using audio files, URLs, or base64 strings.
Committing a lot of audio files into your repository can make it slow to clone and
pull your repository. If you expect to commit >500MiB of audio, please read
[GitHub's size limit documentation][github-large-files] and consider setting
up [large file storage][github-lfs].test Test1 {
  functions [MyFunction]
  args {
    myAudio {
      file "../path/to/audio.mp3"
    }
  }
}
The path to the audio file, relative to the directory containing the current BAML file.audio files must be somewhere in baml_src/.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on first, the file extension,
and second, the contents of the file.test Test1 {
  functions [MyFunction]
  args {
    myAudio {
      url "https...."
    }
  }
}
The publicly accessible URL from which the audio may be downloaded.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.test Test1 {
  args {
    myAudio {
      base64 "base64string"
      media_type "audio/mp3"
    }
  }
}
The base64-encoded audio data.The mime-type of the audio. If not set, and the provider expects a mime-type
to be provided, BAML will try to infer it based on the contents of the file.If base64 is a data URL, this field will be ignored.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions#test-audio-inputs-in-the-playground",
    "title": "Test Audio Inputs in the Playground",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "This is coming soon! We'll be supporting assertions in test cases. For now -- when you run a test you'll only see errors parsing the output into the right schema, or LLM-provider errors.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions#assertions",
    "title": "Assertions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "This is coming soon! We'll be supporting assertions in test cases. For now -- when you run a test you'll only see errors parsing the output into the right schema, or LLM-provider errors.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/testing-functions#assertions",
    "title": "Assertions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "BAML lets you stream in structured JSON output from LLMs as it comes in.

If you tried streaming in a JSON output from an LLM you'd see something like:
```
{"items": [{"name": "Appl
{"items": [{"name": "Apple", "quantity": 2, "price": 1.
{"items": [{"name": "Apple", "quantity": 2, "price": 1.50}], "total_cost":
{"items": [{"name": "Apple", "quantity": 2, "price": 1.50}], "total_cost": 3.00} # Completed
```

BAML automatically fixes this partial JSON, and transforms all your types into `Partial` types with all `Optional` fields only during the stream.

<Tip>You can check out more examples (including streaming in FastAPI and NextJS) in the [BAML Examples] repo.</Tip>

[call BAML functions]: /docs/calling-baml/calling-functions
[BAML Examples]: https://github.com/BoundaryML/baml-examples/tree/main

Lets stream the output of this function `function ExtractReceiptInfo(email: string) -> ReceiptInfo` for our example:

<Accordion title="extract-receipt-info.baml">

```rust
class ReceiptItem {
  name string
  description string?
  quantity int
  price float
}

class ReceiptInfo {
    items ReceiptItem[]
    total_cost float?
}

function ExtractReceiptInfo(email: string) -> ReceiptInfo {
  client GPT4o
  prompt #"
    Given the receipt below:

    {{ email }}

    {{ ctx.output_format }}
  "#
}
```
</Accordion>

<Tabs>

<Tab title="Python">
BAML will generate `b.stream.ExtractReceiptInfo()` for you, which you can use like so:

```python main.py
import asyncio
from baml_client import b, partial_types, types

# Using a stream:
def example1(receipt: str):
    stream = b.stream.ExtractReceiptInfo(receipt)

    # partial is a Partial type with all Optional fields
    for partial in stream:
        print(f"partial: parsed {len(partial.items)} items (object: {partial})")

    # final is the full, original, validated ReceiptInfo type
    final = stream.get_final_response()
    print(f"final: {len(final.items)} items (object: {final})")

# Using only get_final_response() of a stream
#
# In this case, you should just use b.ExtractReceiptInfo(receipt) instead,
# which is slightly faster and more efficient.
def example2(receipt: str):
    final = b.stream.ExtractReceiptInfo(receipt).get_final_response()
    print(f"final: {len(final.items)} items (object: {final})")

# Using the async client:
async def example3(receipt: str):
    # Note the import of the async client
    from baml_client.async_client import b
    stream = b.stream.ExtractReceiptInfo(receipt)
    async for partial in stream:
        print(f"partial: parsed {len(partial.items)} items (object: {partial})")

    final = await stream.get_final_response()
    print(f"final: {len(final.items)} items (object: {final})")

receipt = """
04/14/2024 1:05 pm

Ticket: 220000082489
Register: Shop Counter
Employee: Connor
Customer: Sam
Item	#	Price
Guide leash (1 Pair) uni UNI
1	$34.95
The Index Town Walls
1	$35.00
Boot Punch
3	$60.00
Subtotal	$129.95
Tax ($129.95 @ 9%)	$11.70
Total Tax	$11.70
Total	$141.65
"""

if __name__ == '__main__':
    asyncio.run(example1(receipt))
    asyncio.run(example2(receipt))
    asyncio.run(example3(receipt))
```
</Tab>

<Tab title="TypeScript">
BAML will generate `b.stream.ExtractReceiptInfo()` for you, which you can use like so:

```ts main.ts
import { b } from './baml_client'

// Using both async iteration and getFinalResponse() from a stream
const example1 = async (receipt: string) => {
  const stream = b.stream.ExtractReceiptInfo(receipt)

  // partial is a Partial type with all Optional fields
  for await (const partial of stream) {
    console.log(`partial: ${partial.items?.length} items (object: ${partial})`)
  }

  // final is the full, original, validated ReceiptInfo type
  const final = await stream.getFinalResponse()
  console.log(`final: ${final.items.length} items (object: ${final})`)
}

// Using only async iteration of a stream
const example2 = async (receipt: string) => {
  for await (const partial of b.stream.ExtractReceiptInfo(receipt)) {
    console.log(`partial: ${partial.items?.length} items (object: ${partial})`)
  }
}

// Using only getFinalResponse() of a stream
//
// In this case, you should just use b.ExtractReceiptInfo(receipt) instead,
// which is faster and more efficient.
const example3 = async (receipt: string) => {
  const final = await b.stream.ExtractReceiptInfo(receipt).getFinalResponse()
  console.log(`final: ${final.items.length} items (object: ${final})`)
}

const receipt = `
04/14/2024 1:05 pm

Ticket: 220000082489
Register: Shop Counter
Employee: Connor
Customer: Sam
Item	#	Price
Guide leash (1 Pair) uni UNI
1	$34.95
The Index Town Walls
1	$35.00
Boot Punch
3	$60.00
Subtotal	$129.95
Tax ($129.95 @ 9%)	$11.70
Total Tax	$11.70
Total	$141.65
`

if (require.main === module) {
  example1(receipt)
  example2(receipt)
  example3(receipt)
}
```
</Tab>

<Tab title="Ruby (beta)">
BAML will generate `Baml.Client.stream.ExtractReceiptInfo()` for you,
which you can use like so:

```ruby main.rb
require_relative "baml_client/client"

$b = Baml.Client

# Using both iteration and get_final_response() from a stream
def example1(receipt)
  stream = $b.stream.ExtractReceiptInfo(receipt)

  stream.each do |partial|
    puts "partial: #{partial.items&.length} items"
  end

  final = stream.get_final_response
  puts "final: #{final.items.length} items"
end

# Using only iteration of a stream
def example2(receipt)
  $b.stream.ExtractReceiptInfo(receipt).each do |partial|
    puts "partial: #{partial.items&.length} items"
  end
end

# Using only get_final_response() of a stream
#
# In this case, you should just use BamlClient.ExtractReceiptInfo(receipt) instead,
# which is faster and more efficient.
def example3(receipt)
  final = $b.stream.ExtractReceiptInfo(receipt).get_final_response
  puts "final: #{final.items.length} items"
end

receipt = <<~RECEIPT
  04/14/2024 1:05 pm

  Ticket: 220000082489
  Register: Shop Counter
  Employee: Connor
  Customer: Sam
  Item  #  Price
  Guide leash (1 Pair) uni UNI
  1 $34.95
  The Index Town Walls
  1 $35.00
  Boot Punch
  3 $60.00
  Subtotal $129.95
  Tax ($129.95 @ 9%) $11.70
  Total Tax $11.70
  Total $141.65
RECEIPT

if __FILE__ == $0
  example1(receipt)
  example2(receipt)
  example3(receipt)
end
```

</Tab>
<Tab title="OpenAPI">

Streaming is not yet supported via OpenAPI, but it will be coming soon!

</Tab>
</Tabs>

<Note>
Number fields are always streamed in only when the LLM completes them. E.g. if the final number is 129.95, you'll only see null or 129.95 instead of partial numbers like 1, 12, 129.9, etc.
</Note>",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/streaming",
    "title": "Streaming",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "BAML lets you stream in structured JSON output from LLMs as it comes in.
If you tried streaming in a JSON output from an LLM you'd see something like:
{"items": [{"name": "Appl
{"items": [{"name": "Apple", "quantity": 2, "price": 1.
{"items": [{"name": "Apple", "quantity": 2, "price": 1.50}], "total_cost":
{"items": [{"name": "Apple", "quantity": 2, "price": 1.50}], "total_cost": 3.00} # Completed

BAML automatically fixes this partial JSON, and transforms all your types into Partial types with all Optional fields only during the stream.
You can check out more examples (including streaming in FastAPI and NextJS) in the BAML Examples repo.
Lets stream the output of this function function ExtractReceiptInfo(email: string) -> ReceiptInfo for our example:
class ReceiptItem {
  name string
  description string?
  quantity int
  price float
}

class ReceiptInfo {
    items ReceiptItem[]
    total_cost float?
}

function ExtractReceiptInfo(email: string) -> ReceiptInfo {
  client GPT4o
  prompt #"
    Given the receipt below:

    {{ email }}

    {{ ctx.output_format }}
  "#
}

BAML will generate b.stream.ExtractReceiptInfo() for you, which you can use like so:import asyncio
from baml_client import b, partial_types, types

# Using a stream:
def example1(receipt: str):
    stream = b.stream.ExtractReceiptInfo(receipt)

    # partial is a Partial type with all Optional fields
    for partial in stream:
        print(f"partial: parsed {len(partial.items)} items (object: {partial})")

    # final is the full, original, validated ReceiptInfo type
    final = stream.get_final_response()
    print(f"final: {len(final.items)} items (object: {final})")

# Using only get_final_response() of a stream
#
# In this case, you should just use b.ExtractReceiptInfo(receipt) instead,
# which is slightly faster and more efficient.
def example2(receipt: str):
    final = b.stream.ExtractReceiptInfo(receipt).get_final_response()
    print(f"final: {len(final.items)} items (object: {final})")

# Using the async client:
async def example3(receipt: str):
    # Note the import of the async client
    from baml_client.async_client import b
    stream = b.stream.ExtractReceiptInfo(receipt)
    async for partial in stream:
        print(f"partial: parsed {len(partial.items)} items (object: {partial})")

    final = await stream.get_final_response()
    print(f"final: {len(final.items)} items (object: {final})")

receipt = """
04/14/2024 1:05 pm

Ticket: 220000082489
Register: Shop Counter
Employee: Connor
Customer: Sam
Item	#	Price
Guide leash (1 Pair) uni UNI
1	$34.95
The Index Town Walls
1	$35.00
Boot Punch
3	$60.00
Subtotal	$129.95
Tax ($129.95 @ 9%)	$11.70
Total Tax	$11.70
Total	$141.65
"""

if __name__ == '__main__':
    asyncio.run(example1(receipt))
    asyncio.run(example2(receipt))
    asyncio.run(example3(receipt))
BAML will generate b.stream.ExtractReceiptInfo() for you, which you can use like so:import { b } from './baml_client'

// Using both async iteration and getFinalResponse() from a stream
const example1 = async (receipt: string) => {
  const stream = b.stream.ExtractReceiptInfo(receipt)

  // partial is a Partial type with all Optional fields
  for await (const partial of stream) {
    console.log(`partial: ${partial.items?.length} items (object: ${partial})`)
  }

  // final is the full, original, validated ReceiptInfo type
  const final = await stream.getFinalResponse()
  console.log(`final: ${final.items.length} items (object: ${final})`)
}

// Using only async iteration of a stream
const example2 = async (receipt: string) => {
  for await (const partial of b.stream.ExtractReceiptInfo(receipt)) {
    console.log(`partial: ${partial.items?.length} items (object: ${partial})`)
  }
}

// Using only getFinalResponse() of a stream
//
// In this case, you should just use b.ExtractReceiptInfo(receipt) instead,
// which is faster and more efficient.
const example3 = async (receipt: string) => {
  const final = await b.stream.ExtractReceiptInfo(receipt).getFinalResponse()
  console.log(`final: ${final.items.length} items (object: ${final})`)
}

const receipt = `
04/14/2024 1:05 pm

Ticket: 220000082489
Register: Shop Counter
Employee: Connor
Customer: Sam
Item	#	Price
Guide leash (1 Pair) uni UNI
1	$34.95
The Index Town Walls
1	$35.00
Boot Punch
3	$60.00
Subtotal	$129.95
Tax ($129.95 @ 9%)	$11.70
Total Tax	$11.70
Total	$141.65
`

if (require.main === module) {
  example1(receipt)
  example2(receipt)
  example3(receipt)
}
BAML will generate Baml.Client.stream.ExtractReceiptInfo() for you,
which you can use like so:require_relative "baml_client/client"

$b = Baml.Client

# Using both iteration and get_final_response() from a stream
def example1(receipt)
  stream = $b.stream.ExtractReceiptInfo(receipt)

  stream.each do |partial|
    puts "partial: #{partial.items&.length} items"
  end

  final = stream.get_final_response
  puts "final: #{final.items.length} items"
end

# Using only iteration of a stream
def example2(receipt)
  $b.stream.ExtractReceiptInfo(receipt).each do |partial|
    puts "partial: #{partial.items&.length} items"
  end
end

# Using only get_final_response() of a stream
#
# In this case, you should just use BamlClient.ExtractReceiptInfo(receipt) instead,
# which is faster and more efficient.
def example3(receipt)
  final = $b.stream.ExtractReceiptInfo(receipt).get_final_response
  puts "final: #{final.items.length} items"
end

receipt = <<~RECEIPT
  04/14/2024 1:05 pm

  Ticket: 220000082489
  Register: Shop Counter
  Employee: Connor
  Customer: Sam
  Item  #  Price
  Guide leash (1 Pair) uni UNI
  1 $34.95
  The Index Town Walls
  1 $35.00
  Boot Punch
  3 $60.00
  Subtotal $129.95
  Tax ($129.95 @ 9%) $11.70
  Total Tax $11.70
  Total $141.65
RECEIPT

if __FILE__ == $0
  example1(receipt)
  example2(receipt)
  example3(receipt)
end
Streaming is not yet supported via OpenAPI, but it will be coming soon!
Number fields are always streamed in only when the LLM completes them. E.g. if the final number is 129.95, you'll only see null or 129.95 instead of partial numbers like 1, 12, 129.9, etc.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/streaming",
    "title": "Streaming",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": undefined,
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/multi-modal",
    "title": "Multi-Modal (Images / Audio)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "Multi-modal input
You can use audio or image input types in BAML prompts. Just create an input argument of that type and render it in the prompt.
Check the "raw curl" checkbox in the playground to see how BAML translates multi-modal input into the LLM Request body.
// "image" is a reserved keyword so we name the arg "img"
function DescribeMedia(img: image) -> string {
  client openai/gpt-4o
  // Most LLM providers require images or audio to be sent as "user" messages.
  prompt #"
    {{_.role("user")}}
    Describe this image: {{ img }}
  "#
}

// See the "testing functions" Guide for more on testing Multimodal functions
test Test {
  args {
    img {
      url "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
    }
  }
}

See how to test images in the playground.
Calling Multimodal BAML Functions
Images
Calling a BAML function with an image input argument type (see image types)
The from_url and from_base64 methods create an Image object based on input type.
from baml_py import Image
from baml_client import b

async def test_image_input():
  # from URL
  res = await b.TestImageInput(
      img=Image.from_url(
          "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
      )
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = await b.TestImageInput(
    img=Image.from_base64("image/png", image_b64)
  )
import { b } from '../baml_client'
import { Image } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestImageInput(
    Image.fromUrl('https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png'),
  )

  // Base64
  const image_b64 = "iVB0R..."
  let res = await b.TestImageInput(
    Image.fromBase64('image/png', image_b64),
  )
  
we're working on it!

Audio
Calling functions that have audio types. See audio types
from baml_py import Audio
from baml_client import b

async def run():
  # from URL
  res = await b.TestAudioInput(
      img=Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64
  b64 = "iVBORw0K...."
  res = await b.TestAudioInput(
    audio=Audio.from_base64("audio/ogg", b64)
  )
import { b } from '../baml_client'
import { Audio } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestAudioInput(
    Audio.fromUrl('https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg'),
  )

  // Base64
  const audio_base64 = ".."
  let res = await b.TestAudioInput(
    Audio.fromBase64('audio/ogg', audio_base64),
  )
  
we're working on it!
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/multi-modal",
    "title": "Multi-Modal (Images / Audio)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "You can use audio or image input types in BAML prompts. Just create an input argument of that type and render it in the prompt.
Check the "raw curl" checkbox in the playground to see how BAML translates multi-modal input into the LLM Request body.
// "image" is a reserved keyword so we name the arg "img"
function DescribeMedia(img: image) -> string {
  client openai/gpt-4o
  // Most LLM providers require images or audio to be sent as "user" messages.
  prompt #"
    {{_.role("user")}}
    Describe this image: {{ img }}
  "#
}

// See the "testing functions" Guide for more on testing Multimodal functions
test Test {
  args {
    img {
      url "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
    }
  }
}

See how to test images in the playground.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/multi-modal#multi-modal-input",
    "title": "Multi-modal input",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "You can use audio or image input types in BAML prompts. Just create an input argument of that type and render it in the prompt.
Check the "raw curl" checkbox in the playground to see how BAML translates multi-modal input into the LLM Request body.
// "image" is a reserved keyword so we name the arg "img"
function DescribeMedia(img: image) -> string {
  client openai/gpt-4o
  // Most LLM providers require images or audio to be sent as "user" messages.
  prompt #"
    {{_.role("user")}}
    Describe this image: {{ img }}
  "#
}

// See the "testing functions" Guide for more on testing Multimodal functions
test Test {
  args {
    img {
      url "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
    }
  }
}

See how to test images in the playground.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/multi-modal#multi-modal-input",
    "title": "Multi-modal input",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
      {
        "slug": "guide/baml-basics/multi-modal#calling-multimodal-baml-functions",
        "title": "Calling Multimodal BAML Functions",
      },
    ],
    "description": "Calling a BAML function with an image input argument type (see image types)
The from_url and from_base64 methods create an Image object based on input type.
from baml_py import Image
from baml_client import b

async def test_image_input():
  # from URL
  res = await b.TestImageInput(
      img=Image.from_url(
          "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
      )
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = await b.TestImageInput(
    img=Image.from_base64("image/png", image_b64)
  )
import { b } from '../baml_client'
import { Image } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestImageInput(
    Image.fromUrl('https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png'),
  )

  // Base64
  const image_b64 = "iVB0R..."
  let res = await b.TestImageInput(
    Image.fromBase64('image/png', image_b64),
  )
  
we're working on it!
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/multi-modal#images",
    "title": "Images",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
      "Calling Multimodal BAML Functions",
    ],
    "content": "Calling a BAML function with an image input argument type (see image types)
The from_url and from_base64 methods create an Image object based on input type.
from baml_py import Image
from baml_client import b

async def test_image_input():
  # from URL
  res = await b.TestImageInput(
      img=Image.from_url(
          "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
      )
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = await b.TestImageInput(
    img=Image.from_base64("image/png", image_b64)
  )
import { b } from '../baml_client'
import { Image } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestImageInput(
    Image.fromUrl('https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png'),
  )

  // Base64
  const image_b64 = "iVB0R..."
  let res = await b.TestImageInput(
    Image.fromBase64('image/png', image_b64),
  )
  
we're working on it!
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/multi-modal#images",
    "title": "Images",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
      {
        "slug": "guide/baml-basics/multi-modal#calling-multimodal-baml-functions",
        "title": "Calling Multimodal BAML Functions",
      },
    ],
    "description": "Calling functions that have audio types. See audio types
from baml_py import Audio
from baml_client import b

async def run():
  # from URL
  res = await b.TestAudioInput(
      img=Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64
  b64 = "iVBORw0K...."
  res = await b.TestAudioInput(
    audio=Audio.from_base64("audio/ogg", b64)
  )
import { b } from '../baml_client'
import { Audio } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestAudioInput(
    Audio.fromUrl('https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg'),
  )

  // Base64
  const audio_base64 = ".."
  let res = await b.TestAudioInput(
    Audio.fromBase64('audio/ogg', audio_base64),
  )
  
we're working on it!
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/multi-modal#audio",
    "title": "Audio",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
      "Calling Multimodal BAML Functions",
    ],
    "content": "Calling functions that have audio types. See audio types
from baml_py import Audio
from baml_client import b

async def run():
  # from URL
  res = await b.TestAudioInput(
      img=Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64
  b64 = "iVBORw0K...."
  res = await b.TestAudioInput(
    audio=Audio.from_base64("audio/ogg", b64)
  )
import { b } from '../baml_client'
import { Audio } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestAudioInput(
    Audio.fromUrl('https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg'),
  )

  // Base64
  const audio_base64 = ".."
  let res = await b.TestAudioInput(
    Audio.fromBase64('audio/ogg', audio_base64),
  )
  
we're working on it!
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/multi-modal#audio",
    "title": "Audio",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "When BAML raises an exception, it will be an instance of a subclass of `BamlError`. This allows you to catch all BAML-specific exceptions with a single `except` block.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling",
    "title": "Error Handling",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "When BAML raises an exception, it will be an instance of a subclass of BamlError. This allows you to catch all BAML-specific exceptions with a single except block.
Example
from baml_client import b
from baml_py.errors import BamlError, BamlInvalidArgumentError, BamlClientError, BamlClientHttpError, BamlValidationError

try:
  b.CallFunctionThatRaisesError()
except BamlError as e:
  print(e)


try:
  b.CallFunctionThatRaisesError()
except BamlValidationError as e:
  # The original prompt sent to the LLM
  print(e.prompt)
  # The LLM response string
  print(e.raw_output)
  # A human-readable error message
  print(e.message)
import { b } from './baml_client'
// For catching parsing errors, you can import this
import { BamlValidationError } from '@boundaryml/baml'
// The rest of the BAML errors contain a string that is prefixed with:
// "BamlError:"
// Subclasses are sequentially appended to the string.
// For example, BamlInvalidArgumentError is returned as:
// "BamlError: BamlInvalidArgumentError:"
// Or, BamlClientHttpError is returned as:
// "BamlError: BamlClientError: BamlClientHttpError:"


async function example() {
  try {
    await b.CallFunctionThatRaisesError()
  } catch (e) {
    if (e instanceof BamlValidationError) {
      // You should be lenient to these fields missing.
      // The original prompt sent to the LLM
      console.log(e.prompt)
      // The LLM response string
      console.log(e.raw_output)
      // A human-readable error message
      console.log(e.message)
    } else {
      // Handle other BAML errors
      console.log(e)
    }
  }
}

# Example coming soon

BamlError
Base class for all BAML exceptions.
A human-readable error message.
BamlInvalidArgumentError
Subclass of BamlError.
Raised when one or multiple arguments to a function are invalid.
BamlClientError
Subclass of BamlError.
Raised when a client fails to return a valid response.
In the case of aggregate clients like fallback or those with retry_policy, only the last client's error is raised.
BamlClientHttpError
Subclass of BamlClientError.
Raised when the HTTP request made by a client fails with a non-200 status code.
The status code of the response.Common status codes are:
1: Other
2: Other
400: Bad Request
401: Unauthorized
403: Forbidden
404: Not Found
429: Too Many Requests
500: Internal Server Error

BamlValidationError
Subclass of BamlError.
Raised when BAML fails to parse a string from the LLM into the specified object.
The raw text from the LLM that failed to parse into the expected return type of a function.
The parsing-related error message.
The original prompt that was sent to the LLM, formatted as a plain string. Images sent as base64-encoded strings are not serialized into this field.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling",
    "title": "Error Handling",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "from baml_client import b
from baml_py.errors import BamlError, BamlInvalidArgumentError, BamlClientError, BamlClientHttpError, BamlValidationError

try:
  b.CallFunctionThatRaisesError()
except BamlError as e:
  print(e)


try:
  b.CallFunctionThatRaisesError()
except BamlValidationError as e:
  # The original prompt sent to the LLM
  print(e.prompt)
  # The LLM response string
  print(e.raw_output)
  # A human-readable error message
  print(e.message)
import { b } from './baml_client'
// For catching parsing errors, you can import this
import { BamlValidationError } from '@boundaryml/baml'
// The rest of the BAML errors contain a string that is prefixed with:
// "BamlError:"
// Subclasses are sequentially appended to the string.
// For example, BamlInvalidArgumentError is returned as:
// "BamlError: BamlInvalidArgumentError:"
// Or, BamlClientHttpError is returned as:
// "BamlError: BamlClientError: BamlClientHttpError:"


async function example() {
  try {
    await b.CallFunctionThatRaisesError()
  } catch (e) {
    if (e instanceof BamlValidationError) {
      // You should be lenient to these fields missing.
      // The original prompt sent to the LLM
      console.log(e.prompt)
      // The LLM response string
      console.log(e.raw_output)
      // A human-readable error message
      console.log(e.message)
    } else {
      // Handle other BAML errors
      console.log(e)
    }
  }
}

# Example coming soon
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#example",
    "title": "Example",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "from baml_client import b
from baml_py.errors import BamlError, BamlInvalidArgumentError, BamlClientError, BamlClientHttpError, BamlValidationError

try:
  b.CallFunctionThatRaisesError()
except BamlError as e:
  print(e)


try:
  b.CallFunctionThatRaisesError()
except BamlValidationError as e:
  # The original prompt sent to the LLM
  print(e.prompt)
  # The LLM response string
  print(e.raw_output)
  # A human-readable error message
  print(e.message)
import { b } from './baml_client'
// For catching parsing errors, you can import this
import { BamlValidationError } from '@boundaryml/baml'
// The rest of the BAML errors contain a string that is prefixed with:
// "BamlError:"
// Subclasses are sequentially appended to the string.
// For example, BamlInvalidArgumentError is returned as:
// "BamlError: BamlInvalidArgumentError:"
// Or, BamlClientHttpError is returned as:
// "BamlError: BamlClientError: BamlClientHttpError:"


async function example() {
  try {
    await b.CallFunctionThatRaisesError()
  } catch (e) {
    if (e instanceof BamlValidationError) {
      // You should be lenient to these fields missing.
      // The original prompt sent to the LLM
      console.log(e.prompt)
      // The LLM response string
      console.log(e.raw_output)
      // A human-readable error message
      console.log(e.message)
    } else {
      // Handle other BAML errors
      console.log(e)
    }
  }
}

# Example coming soon
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#example",
    "title": "Example",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "Base class for all BAML exceptions.
A human-readable error message.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlerror",
    "title": "BamlError",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "Base class for all BAML exceptions.
A human-readable error message.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlerror",
    "title": "BamlError",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
      {
        "slug": "guide/baml-basics/error-handling#bamlerror",
        "title": "BamlError",
      },
    ],
    "description": "Subclass of BamlError.
Raised when one or multiple arguments to a function are invalid.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlinvalidargumenterror",
    "title": "BamlInvalidArgumentError",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
      "BamlError",
    ],
    "content": "Subclass of BamlError.
Raised when one or multiple arguments to a function are invalid.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlinvalidargumenterror",
    "title": "BamlInvalidArgumentError",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
      {
        "slug": "guide/baml-basics/error-handling#bamlerror",
        "title": "BamlError",
      },
    ],
    "description": "Subclass of BamlError.
Raised when a client fails to return a valid response.
In the case of aggregate clients like fallback or those with retry_policy, only the last client's error is raised.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlclienterror",
    "title": "BamlClientError",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
      "BamlError",
    ],
    "content": "Subclass of BamlError.
Raised when a client fails to return a valid response.
In the case of aggregate clients like fallback or those with retry_policy, only the last client's error is raised.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlclienterror",
    "title": "BamlClientError",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
      {
        "slug": "guide/baml-basics/error-handling#bamlerror",
        "title": "BamlError",
      },
      {
        "slug": "guide/baml-basics/error-handling#bamlclienterror",
        "title": "BamlClientError",
      },
    ],
    "description": "Subclass of BamlClientError.
Raised when the HTTP request made by a client fails with a non-200 status code.
The status code of the response.Common status codes are:
1: Other
2: Other
400: Bad Request
401: Unauthorized
403: Forbidden
404: Not Found
429: Too Many Requests
500: Internal Server Error
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlclienthttperror",
    "title": "BamlClientHttpError",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
      "BamlError",
      "BamlClientError",
    ],
    "content": "Subclass of BamlClientError.
Raised when the HTTP request made by a client fails with a non-200 status code.
The status code of the response.Common status codes are:
1: Other
2: Other
400: Bad Request
401: Unauthorized
403: Forbidden
404: Not Found
429: Too Many Requests
500: Internal Server Error
",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlclienthttperror",
    "title": "BamlClientHttpError",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
      {
        "slug": "guide/baml-basics/error-handling#bamlerror",
        "title": "BamlError",
      },
    ],
    "description": "Subclass of BamlError.
Raised when BAML fails to parse a string from the LLM into the specified object.
The raw text from the LLM that failed to parse into the expected return type of a function.
The parsing-related error message.
The original prompt that was sent to the LLM, formatted as a plain string. Images sent as base64-encoded strings are not serialized into this field.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlvalidationerror",
    "title": "BamlValidationError",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
      "BamlError",
    ],
    "content": "Subclass of BamlError.
Raised when BAML fails to parse a string from the LLM into the specified object.
The raw text from the LLM that failed to parse into the expected return type of a function.
The parsing-related error message.
The original prompt that was sent to the LLM, formatted as a plain string. Images sent as base64-encoded strings are not serialized into this field.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/error-handling#bamlvalidationerror",
    "title": "BamlValidationError",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-basics/prompting-with-baml",
        "title": "BAML Basics",
      },
    ],
    "description": "We’ll use `function ClassifyMessage(input: string) -> Category` for our example:

<Accordion title="classify-message.baml">
```baml
enum Category {
    Refund
    CancelOrder
    TechnicalSupport
    AccountIssue
    Question
}

function ClassifyMessage(input: string) -> Category {
  client GPT4o
  prompt #"
    Classify the following INPUT into ONE
    of the following categories:

    INPUT: {{ input }}

    {{ ctx.output_format }}

    Response:
  "#
}
```
</Accordion>

<Tabs>
<Tab title="Python">

You can make concurrent `b.ClassifyMessage()` calls like so:

```python main.py
import asyncio

from baml_client import b
from baml_client.types import Category

async def main():
    await asyncio.gather(
        b.ClassifyMessage("I want to cancel my order"),
        b.ClassifyMessage("I want a refund")
    )

if __name__ == '__main__':
    asyncio.run(main())
```
</Tab>

<Tab title="TypeScript">

You can make concurrent `b.ClassifyMessage()` calls like so:

```ts main.ts
import { b } from './baml_client'
import { Category } from './baml_client/types'
import assert from 'assert'

const main = async () => {
  const category = await Promise.all(
    b.ClassifyMessage('I want to cancel my order'),
    b.ClassifyMessage('I want a refund'),
  )
}

if (require.main === module) {
  main()
}

```
</Tab>

<Tab title="Ruby (beta)">

BAML Ruby (beta) does not currently support async/concurrent calls.

Please [contact us](/contact) if this is something you need.

</Tab>
</Tabs>",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/concurrent-calls",
    "title": "Concurrent function calls",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Basics",
    ],
    "content": "We’ll use function ClassifyMessage(input: string) -> Category for our example:
enum Category {
    Refund
    CancelOrder
    TechnicalSupport
    AccountIssue
    Question
}

function ClassifyMessage(input: string) -> Category {
  client GPT4o
  prompt #"
    Classify the following INPUT into ONE
    of the following categories:

    INPUT: {{ input }}

    {{ ctx.output_format }}

    Response:
  "#
}

You can make concurrent b.ClassifyMessage() calls like so:import asyncio

from baml_client import b
from baml_client.types import Category

async def main():
    await asyncio.gather(
        b.ClassifyMessage("I want to cancel my order"),
        b.ClassifyMessage("I want a refund")
    )

if __name__ == '__main__':
    asyncio.run(main())
You can make concurrent b.ClassifyMessage() calls like so:import { b } from './baml_client'
import { Category } from './baml_client/types'
import assert from 'assert'

const main = async () => {
  const category = await Promise.all(
    b.ClassifyMessage('I want to cancel my order'),
    b.ClassifyMessage('I want a refund'),
  )
}

if (require.main === module) {
  main()
}

BAML Ruby (beta) does not currently support async/concurrent calls.Please contact us if this is something you need.",
    "indexSegmentId": "0",
    "slug": "guide/baml-basics/concurrent-calls",
    "title": "Concurrent function calls",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "If you need to modify the model / parameters for an LLM client at runtime, you can modify the `ClientRegistry` for any specified function.

<Tabs>

<Tab title="Python">

```python
import os
from baml_py import ClientRegistry

async def run():
    cr = ClientRegistry()
    # Creates a new client
    cr.add_llm_client(name='MyAmazingClient', provider='openai', options={
        "model": "gpt-4o",
        "temperature": 0.7,
        "api_key": os.environ.get('OPENAI_API_KEY')
    })
    # Sets MyAmazingClient as the primary client
    cr.set_primary('MyAmazingClient')

    # ExtractResume will now use MyAmazingClient as the calling client
    res = await b.ExtractResume("...", { "client_registry": cr })
```

</Tab>

<Tab title="TypeScript">
```typescript
import { ClientRegistry } from '@boundaryml/baml'

async function run() {
    const cr = new ClientRegistry()
    // Creates a new client
    cr.addLlmClient({ name: 'MyAmazingClient', provider: 'openai', options: {
        model: "gpt-4o",
        temperature: 0.7,
        api_key: process.env.OPENAI_API_KEY
    }})
    // Sets MyAmazingClient as the primary client
    cr.setPrimary('MyAmazingClient')

    // ExtractResume will now use MyAmazingClient as the calling client
    const res = await b.ExtractResume("...", { clientRegistry: cr })
}
```
</Tab>

<Tab title="Ruby">

```ruby
require_relative "baml_client/client"

def run
  cr = Baml::ClientRegistry.new

  # Creates a new client
  cr.add_llm_client(
    name: 'MyAmazingClient',
    provider: 'openai',
    options: {
      model: 'gpt-4o',
      temperature: 0.7,
      api_key: ENV['OPENAI_API_KEY']
    }
  )

  # Sets MyAmazingClient as the primary client
  cr.set_primary('MyAmazingClient')

  # ExtractResume will now use MyAmazingClient as the calling client
  res = Baml.Client.extract_resume(input: '...', baml_options: { client_registry: cr })
end

# Call the asynchronous function
run
```
</Tab>

<Tab title="OpenAPI">

The API supports passing client registry as a field on `__baml_options__` in the request body.

Example request body:

```json
{
    "resume": "Vaibhav Gupta",
    "__baml_options__": {
        "client_registry": {
            "clients": [
                {
                    "name": "OpenAI",
                    "provider": "openai",
                    "retry_policy": null,
                    "options": {
                        "model": "gpt-4o-mini",
                        "api_key": "sk-..."
                    }
                }
            ],
            "primary": "OpenAI"
        }
    }
}
```

```sh
curl -X POST http://localhost:2024/call/ExtractResume \
    -H 'Content-Type: application/json' -d @body.json
```

</Tab>

</Tabs>",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/llm-client-registry",
    "title": "Client Registry",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "If you need to modify the model / parameters for an LLM client at runtime, you can modify the ClientRegistry for any specified function.
import os
from baml_py import ClientRegistry

async def run():
    cr = ClientRegistry()
    # Creates a new client
    cr.add_llm_client(name='MyAmazingClient', provider='openai', options={
        "model": "gpt-4o",
        "temperature": 0.7,
        "api_key": os.environ.get('OPENAI_API_KEY')
    })
    # Sets MyAmazingClient as the primary client
    cr.set_primary('MyAmazingClient')

    # ExtractResume will now use MyAmazingClient as the calling client
    res = await b.ExtractResume("...", { "client_registry": cr })
import { ClientRegistry } from '@boundaryml/baml'

async function run() {
    const cr = new ClientRegistry()
    // Creates a new client
    cr.addLlmClient({ name: 'MyAmazingClient', provider: 'openai', options: {
        model: "gpt-4o",
        temperature: 0.7,
        api_key: process.env.OPENAI_API_KEY
    }})
    // Sets MyAmazingClient as the primary client
    cr.setPrimary('MyAmazingClient')

    // ExtractResume will now use MyAmazingClient as the calling client
    const res = await b.ExtractResume("...", { clientRegistry: cr })
}
require_relative "baml_client/client"

def run
  cr = Baml::ClientRegistry.new

  # Creates a new client
  cr.add_llm_client(
    name: 'MyAmazingClient',
    provider: 'openai',
    options: {
      model: 'gpt-4o',
      temperature: 0.7,
      api_key: ENV['OPENAI_API_KEY']
    }
  )

  # Sets MyAmazingClient as the primary client
  cr.set_primary('MyAmazingClient')

  # ExtractResume will now use MyAmazingClient as the calling client
  res = Baml.Client.extract_resume(input: '...', baml_options: { client_registry: cr })
end

# Call the asynchronous function
run
The API supports passing client registry as a field on __baml_options__ in the request body.Example request body:{
    "resume": "Vaibhav Gupta",
    "__baml_options__": {
        "client_registry": {
            "clients": [
                {
                    "name": "OpenAI",
                    "provider": "openai",
                    "retry_policy": null,
                    "options": {
                        "model": "gpt-4o-mini",
                        "api_key": "sk-..."
                    }
                }
            ],
            "primary": "OpenAI"
        }
    }
}
curl -X POST http://localhost:2024/call/ExtractResume \
    -H 'Content-Type: application/json' -d @body.json

ClientRegistry Interface
Note: ClientRegistry is imported from baml_py in Python and @boundaryml/baml in TypeScript, not baml_client.As we mature ClientRegistry, we will add a more type-safe and ergonomic interface directly in baml_client. See Github issue #766.
Methods use snake_case in Python and camelCase in TypeScript.
add_llm_client / addLlmClient
A function to add an LLM client to the registry.
The name of the client.Using the exact same name as a client also defined in .baml files overwrites the existing client whenever the ClientRegistry is used.
This configures which provider to use. The provider is responsible for handling the actual API calls to the LLM service. The provider is a required field.The configuration modifies the URL request BAML runtime makes.| Provider Name    | Docs                                                                | Notes                                                      |
| ---------------- | ------------------------------------------------------------------- | ---------------------------------------------------------- |
| anthropic      | Anthropic             |                                                            |
| aws-bedrock    | AWS Bedrock         |                                                            |
| azure-openai   | Azure OpenAI              |                                                            |
| google-ai      | Google AI                |                                                            |
| openai         | OpenAI                   |                                                            |
| openai-generic | OpenAI (generic) | Any model provider that supports an OpenAI-compatible API  |
| vertex-ai      | Vertex AI                |                                                            |We also have some special providers that allow composing clients together:
| Provider Name  | Docs                             | Notes                                                      |
| -------------- | -------------------------------- | ---------------------------------------------------------- |
| fallback     | Fallback             | Used to chain models conditional on failures               |
| round-robin  | Round Robin       | Used to load balance                                       |
These vary per provider. Please see provider specific documentation for more
information. Generally they are pass through options to the POST request made
to the LLM.
The name of a retry policy that is already defined in a .baml file. See Retry Policies.
set_primary / setPrimary
This sets the client for the function to use. (i.e. replaces the client property in a function)
The name of the client to use.This can be a new client that was added with add_llm_client or an existing client that is already in a .baml file.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/llm-client-registry",
    "title": "Client Registry",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "Note: ClientRegistry is imported from baml_py in Python and @boundaryml/baml in TypeScript, not baml_client.As we mature ClientRegistry, we will add a more type-safe and ergonomic interface directly in baml_client. See Github issue #766.
Methods use snake_case in Python and camelCase in TypeScript.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/llm-client-registry#clientregistry-interface",
    "title": "ClientRegistry Interface",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "Note: ClientRegistry is imported from baml_py in Python and @boundaryml/baml in TypeScript, not baml_client.As we mature ClientRegistry, we will add a more type-safe and ergonomic interface directly in baml_client. See Github issue #766.
Methods use snake_case in Python and camelCase in TypeScript.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/llm-client-registry#clientregistry-interface",
    "title": "ClientRegistry Interface",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
      {
        "slug": "guide/baml-advanced/llm-client-registry#clientregistry-interface",
        "title": "ClientRegistry Interface",
      },
    ],
    "description": "A function to add an LLM client to the registry.
The name of the client.Using the exact same name as a client also defined in .baml files overwrites the existing client whenever the ClientRegistry is used.
This configures which provider to use. The provider is responsible for handling the actual API calls to the LLM service. The provider is a required field.The configuration modifies the URL request BAML runtime makes.| Provider Name    | Docs                                                                | Notes                                                      |
| ---------------- | ------------------------------------------------------------------- | ---------------------------------------------------------- |
| anthropic      | Anthropic             |                                                            |
| aws-bedrock    | AWS Bedrock         |                                                            |
| azure-openai   | Azure OpenAI              |                                                            |
| google-ai      | Google AI                |                                                            |
| openai         | OpenAI                   |                                                            |
| openai-generic | OpenAI (generic) | Any model provider that supports an OpenAI-compatible API  |
| vertex-ai      | Vertex AI                |                                                            |We also have some special providers that allow composing clients together:
| Provider Name  | Docs                             | Notes                                                      |
| -------------- | -------------------------------- | ---------------------------------------------------------- |
| fallback     | Fallback             | Used to chain models conditional on failures               |
| round-robin  | Round Robin       | Used to load balance                                       |
These vary per provider. Please see provider specific documentation for more
information. Generally they are pass through options to the POST request made
to the LLM.
The name of a retry policy that is already defined in a .baml file. See Retry Policies.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/llm-client-registry#add_llm_client--addllmclient",
    "title": "add_llm_client / addLlmClient",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
      "ClientRegistry Interface",
    ],
    "content": "A function to add an LLM client to the registry.
The name of the client.Using the exact same name as a client also defined in .baml files overwrites the existing client whenever the ClientRegistry is used.
This configures which provider to use. The provider is responsible for handling the actual API calls to the LLM service. The provider is a required field.The configuration modifies the URL request BAML runtime makes.| Provider Name    | Docs                                                                | Notes                                                      |
| ---------------- | ------------------------------------------------------------------- | ---------------------------------------------------------- |
| anthropic      | Anthropic             |                                                            |
| aws-bedrock    | AWS Bedrock         |                                                            |
| azure-openai   | Azure OpenAI              |                                                            |
| google-ai      | Google AI                |                                                            |
| openai         | OpenAI                   |                                                            |
| openai-generic | OpenAI (generic) | Any model provider that supports an OpenAI-compatible API  |
| vertex-ai      | Vertex AI                |                                                            |We also have some special providers that allow composing clients together:
| Provider Name  | Docs                             | Notes                                                      |
| -------------- | -------------------------------- | ---------------------------------------------------------- |
| fallback     | Fallback             | Used to chain models conditional on failures               |
| round-robin  | Round Robin       | Used to load balance                                       |
These vary per provider. Please see provider specific documentation for more
information. Generally they are pass through options to the POST request made
to the LLM.
The name of a retry policy that is already defined in a .baml file. See Retry Policies.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/llm-client-registry#add_llm_client--addllmclient",
    "title": "add_llm_client / addLlmClient",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
      {
        "slug": "guide/baml-advanced/llm-client-registry#clientregistry-interface",
        "title": "ClientRegistry Interface",
      },
    ],
    "description": "This sets the client for the function to use. (i.e. replaces the client property in a function)
The name of the client to use.This can be a new client that was added with add_llm_client or an existing client that is already in a .baml file.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/llm-client-registry#set_primary--setprimary",
    "title": "set_primary / setPrimary",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
      "ClientRegistry Interface",
    ],
    "content": "This sets the client for the function to use. (i.e. replaces the client property in a function)
The name of the client to use.This can be a new client that was added with add_llm_client or an existing client that is already in a .baml file.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/llm-client-registry#set_primary--setprimary",
    "title": "set_primary / setPrimary",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "Sometimes you have **output schemas that change at runtime** -- for example if
you have a list of Categories that you need to classify that come from a
database, or your schema is user-provided.

`TypeBuilder` is used to create or modify dynamic types at runtime to achieve this.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types",
    "title": "Dynamic Types - TypeBuilder",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "Sometimes you have output schemas that change at runtime -- for example if
you have a list of Categories that you need to classify that come from a
database, or your schema is user-provided.
TypeBuilder is used to create or modify dynamic types at runtime to achieve this.
Dynamic BAML Enums
Imagine we want to make a categorizer prompt, but the list of categories to output come from a database.

Add @@dynamic to the class or enum definition to mark it as dynamic in BAML.

enum Category {
  VALUE1 // normal static enum values that don't change
  VALUE2
  @@dynamic // this enum can have more values added at runtime
} 

// The Category enum can now be modified at runtime!
function DynamicCategorizer(input: string) -> Category {
  client GPT4
  prompt #"
    Given a string, classify it into a category
    {{ input }}
     
    {{ ctx.output_format }}
  "#
}



Import the TypeBuilder from baml_client in your runtime code and modify Category. All dynamic types you
define in BAML will be available as properties of TypeBuilder. Think of the
typebuilder as a registry of modified runtime types that the baml function will
read from when building the output schema in the prompt.

from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.Category.add_value('VALUE3')
  tb.Category.add_value('VALUE4')
  # Pass the typebuilder in the baml_options argument -- the last argument of the function.
  res = await b.DynamicCategorizer("some input", { "tb": tb })
  # Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import {
  b
} from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.Category.addValue('VALUE3')
  tb.Category.addValue('VALUE4')
  const res = await b.DynamicCategorizer("some input", { tb: tb })
  // Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  console.log(res)
}
require_relative '../baml_client'

def run
  tb = Baml::TypeBuilder.new
  tb.Category.add_value('VALUE3')
  tb.Category.add_value('VALUE4')
  res = Baml.Client.dynamic_categorizer(input: "some input", baml_options: {tb: tb})
  # Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  puts res
end
Dynamic types are not yet supported when used via OpenAPI.Please let us know if you want this feature, either via Discord or GitHub.
Dynamic BAML Classes
Now we'll add some properties to a User class at runtime using @@dynamic.
class User {
  name string
  age int
  @@dynamic
}

function DynamicUserCreator(user_info: string) -> User {
  client GPT4
  prompt #"
    Extract the information from this chunk of text:
    "{{ user_info }}"
     
    {{ ctx.output_format }}
  "#
}

We can then modify the User schema at runtime. Since we marked User with @@dynamic, it'll be available as a property of TypeBuilder.
from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.User.add_property('email', tb.string())
  tb.User.add_property('address', tb.string()).description("The user's address")
  res = await b.DynamicUserCreator("some user info", { "tb": tb })
  # Now res can have email and address fields
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import {
  b
} from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.User.add_property('email', tb.string())
  tb.User.add_property('address', tb.string()).description("The user's address")
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res can have email and address fields
  console.log(res)
}
require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  tb.User.add_property('email', tb.string)
  tb.User.add_property('address', tb.string).description("The user's address")
  
  res = Baml.Client.dynamic_user_creator(input: "some user info", baml_options: {tb: tb})
  # Now res can have email and address fields
  puts res
end

Creating new dynamic classes or enums not in BAML
The previous examples showed how to modify existing types. Here we create a new Hobbies enum, and a new class called Address without having them defined in BAML.
Note that you must attach the new types to the existing Return Type of your BAML function(in this case it's User).
from baml_client.type_builder import TypeBuilder
from baml_client.async_client import b

async def run():
  tb = TypeBuilder()
  hobbies_enum = tb.add_enum("Hobbies")
  hobbies_enum.add_value("Soccer")
  hobbies_enum.add_value("Reading")

  address_class = tb.add_class("Address")
  address_class.add_property("street", tb.string()).description("The user's street address")

  tb.User.add_property("hobby", hobbies_enum.type().optional())
  tb.User.add_property("address", address_class.type().optional())
  res = await b.DynamicUserCreator("some user info", {"tb": tb})
  # Now res might have the hobby property, which can be Soccer or Reading
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import { b } from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  const hobbiesEnum = tb.addEnum('Hobbies')
  hobbiesEnum.addValue('Soccer')
  hobbiesEnum.addValue('Reading')

  const addressClass = tb.addClass('Address')
  addressClass.addProperty('street', tb.string()).description("The user's street address")


  tb.User.addProperty('hobby', hobbiesEnum.type().optional())
  tb.User.addProperty('address', addressClass.type())
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res might have the hobby property, which can be Soccer or Reading
  console.log(res)
}
require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  hobbies_enum = tb.add_enum('Hobbies')
  hobbies_enum.add_value('Soccer')
  hobbies_enum.add_value('Reading')

  address_class = tb.add_class('Address')
  address_class.add_property('street', tb.string)

  tb.User.add_property('hobby', hobbies_enum.type.optional)
  tb.User.add_property('address', address_class.type.optional)
  
  res = Baml::Client.dynamic_user_creator(input: "some user info", baml_options: { tb: tb })
  # Now res might have the hobby property, which can be Soccer or Reading
  puts res
end

TypeBuilder provides methods for building different kinds of types:
| Method | Description | Example |
|--------|-------------|---------|
| string() | Creates a string type | tb.string() |
| int() | Creates an integer type | tb.int() |
| float() | Creates a float type | tb.float() |
| bool() | Creates a boolean type | tb.bool() |
| list() | Makes a type into a list | tb.string().list() |
| optional() | Makes a type optional | tb.string().optional() |
Adding descriptions to dynamic types
tb = TypeBuilder()
tb.User.add_property("email", tb.string()).description("The user's email")
const tb = new TypeBuilder()
tb.User.addProperty("email", tb.string()).description("The user's email")
tb = Baml::TypeBuilder.new
tb.User.add_property("email", tb.string).description("The user's email")

Building dynamic types from JSON schema
We have a working implementation of this, but are waiting for a concrete use case to merge it.
Please chime in on the GitHub issue if this is
something you'd like to use.
import pydantic
from baml_client import b

class Person(pydantic.BaseModel):
    last_name: list[str]
    height: Optional[float] = pydantic.Field(description="Height in meters")

tb = TypeBuilder()
tb.unstable_features.add_json_schema(Person.model_json_schema())

res = await b.ExtractPeople(
    "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
    {"tb": tb},
)
import 'z' from zod
import 'zodToJsonSchema' from zod-to-json-schema
import { b } from '../baml_client'

const personSchema = z.object({
  animalLiked: z.object({
    animal: z.string().describe('The animal mentioned, in singular form.'),
  }),
  hobbies: z.enum(['chess', 'sports', 'music', 'reading']).array(),
  height: z.union([z.string(), z.number().int()]).describe('Height in meters'),
})

let tb = new TypeBuilder()
tb.unstableFeatures.addJsonSchema(zodToJsonSchema(personSchema, 'Person'))

const res = await b.ExtractPeople(
  "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
  { tb },
)
tb = Baml::TypeBuilder.new
tb.unstable_features.add_json_schema(...)

res = Baml::Client.extract_people(
  input: "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
  baml_options: { tb: tb }
)

puts res

Testing dynamic types in BAML
This feature is coming soon! Let us know if you're interested in testing it out!
You can still write tests in Python, TypeScript, Ruby, etc in the meantime.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types",
    "title": "Dynamic Types - TypeBuilder",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "Imagine we want to make a categorizer prompt, but the list of categories to output come from a database.

Add @@dynamic to the class or enum definition to mark it as dynamic in BAML.

enum Category {
  VALUE1 // normal static enum values that don't change
  VALUE2
  @@dynamic // this enum can have more values added at runtime
} 

// The Category enum can now be modified at runtime!
function DynamicCategorizer(input: string) -> Category {
  client GPT4
  prompt #"
    Given a string, classify it into a category
    {{ input }}
     
    {{ ctx.output_format }}
  "#
}



Import the TypeBuilder from baml_client in your runtime code and modify Category. All dynamic types you
define in BAML will be available as properties of TypeBuilder. Think of the
typebuilder as a registry of modified runtime types that the baml function will
read from when building the output schema in the prompt.

from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.Category.add_value('VALUE3')
  tb.Category.add_value('VALUE4')
  # Pass the typebuilder in the baml_options argument -- the last argument of the function.
  res = await b.DynamicCategorizer("some input", { "tb": tb })
  # Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import {
  b
} from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.Category.addValue('VALUE3')
  tb.Category.addValue('VALUE4')
  const res = await b.DynamicCategorizer("some input", { tb: tb })
  // Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  console.log(res)
}
require_relative '../baml_client'

def run
  tb = Baml::TypeBuilder.new
  tb.Category.add_value('VALUE3')
  tb.Category.add_value('VALUE4')
  res = Baml.Client.dynamic_categorizer(input: "some input", baml_options: {tb: tb})
  # Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  puts res
end
Dynamic types are not yet supported when used via OpenAPI.Please let us know if you want this feature, either via Discord or GitHub.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#dynamic-baml-enums",
    "title": "Dynamic BAML Enums",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "Imagine we want to make a categorizer prompt, but the list of categories to output come from a database.

Add @@dynamic to the class or enum definition to mark it as dynamic in BAML.

enum Category {
  VALUE1 // normal static enum values that don't change
  VALUE2
  @@dynamic // this enum can have more values added at runtime
} 

// The Category enum can now be modified at runtime!
function DynamicCategorizer(input: string) -> Category {
  client GPT4
  prompt #"
    Given a string, classify it into a category
    {{ input }}
     
    {{ ctx.output_format }}
  "#
}



Import the TypeBuilder from baml_client in your runtime code and modify Category. All dynamic types you
define in BAML will be available as properties of TypeBuilder. Think of the
typebuilder as a registry of modified runtime types that the baml function will
read from when building the output schema in the prompt.

from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.Category.add_value('VALUE3')
  tb.Category.add_value('VALUE4')
  # Pass the typebuilder in the baml_options argument -- the last argument of the function.
  res = await b.DynamicCategorizer("some input", { "tb": tb })
  # Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import {
  b
} from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.Category.addValue('VALUE3')
  tb.Category.addValue('VALUE4')
  const res = await b.DynamicCategorizer("some input", { tb: tb })
  // Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  console.log(res)
}
require_relative '../baml_client'

def run
  tb = Baml::TypeBuilder.new
  tb.Category.add_value('VALUE3')
  tb.Category.add_value('VALUE4')
  res = Baml.Client.dynamic_categorizer(input: "some input", baml_options: {tb: tb})
  # Now res can be VALUE1, VALUE2, VALUE3, or VALUE4
  puts res
end
Dynamic types are not yet supported when used via OpenAPI.Please let us know if you want this feature, either via Discord or GitHub.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#dynamic-baml-enums",
    "title": "Dynamic BAML Enums",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "Now we'll add some properties to a User class at runtime using @@dynamic.
class User {
  name string
  age int
  @@dynamic
}

function DynamicUserCreator(user_info: string) -> User {
  client GPT4
  prompt #"
    Extract the information from this chunk of text:
    "{{ user_info }}"
     
    {{ ctx.output_format }}
  "#
}

We can then modify the User schema at runtime. Since we marked User with @@dynamic, it'll be available as a property of TypeBuilder.
from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.User.add_property('email', tb.string())
  tb.User.add_property('address', tb.string()).description("The user's address")
  res = await b.DynamicUserCreator("some user info", { "tb": tb })
  # Now res can have email and address fields
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import {
  b
} from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.User.add_property('email', tb.string())
  tb.User.add_property('address', tb.string()).description("The user's address")
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res can have email and address fields
  console.log(res)
}
require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  tb.User.add_property('email', tb.string)
  tb.User.add_property('address', tb.string).description("The user's address")
  
  res = Baml.Client.dynamic_user_creator(input: "some user info", baml_options: {tb: tb})
  # Now res can have email and address fields
  puts res
end
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#dynamic-baml-classes",
    "title": "Dynamic BAML Classes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "Now we'll add some properties to a User class at runtime using @@dynamic.
class User {
  name string
  age int
  @@dynamic
}

function DynamicUserCreator(user_info: string) -> User {
  client GPT4
  prompt #"
    Extract the information from this chunk of text:
    "{{ user_info }}"
     
    {{ ctx.output_format }}
  "#
}

We can then modify the User schema at runtime. Since we marked User with @@dynamic, it'll be available as a property of TypeBuilder.
from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.User.add_property('email', tb.string())
  tb.User.add_property('address', tb.string()).description("The user's address")
  res = await b.DynamicUserCreator("some user info", { "tb": tb })
  # Now res can have email and address fields
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import {
  b
} from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.User.add_property('email', tb.string())
  tb.User.add_property('address', tb.string()).description("The user's address")
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res can have email and address fields
  console.log(res)
}
require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  tb.User.add_property('email', tb.string)
  tb.User.add_property('address', tb.string).description("The user's address")
  
  res = Baml.Client.dynamic_user_creator(input: "some user info", baml_options: {tb: tb})
  # Now res can have email and address fields
  puts res
end
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#dynamic-baml-classes",
    "title": "Dynamic BAML Classes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "The previous examples showed how to modify existing types. Here we create a new Hobbies enum, and a new class called Address without having them defined in BAML.
Note that you must attach the new types to the existing Return Type of your BAML function(in this case it's User).
from baml_client.type_builder import TypeBuilder
from baml_client.async_client import b

async def run():
  tb = TypeBuilder()
  hobbies_enum = tb.add_enum("Hobbies")
  hobbies_enum.add_value("Soccer")
  hobbies_enum.add_value("Reading")

  address_class = tb.add_class("Address")
  address_class.add_property("street", tb.string()).description("The user's street address")

  tb.User.add_property("hobby", hobbies_enum.type().optional())
  tb.User.add_property("address", address_class.type().optional())
  res = await b.DynamicUserCreator("some user info", {"tb": tb})
  # Now res might have the hobby property, which can be Soccer or Reading
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import { b } from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  const hobbiesEnum = tb.addEnum('Hobbies')
  hobbiesEnum.addValue('Soccer')
  hobbiesEnum.addValue('Reading')

  const addressClass = tb.addClass('Address')
  addressClass.addProperty('street', tb.string()).description("The user's street address")


  tb.User.addProperty('hobby', hobbiesEnum.type().optional())
  tb.User.addProperty('address', addressClass.type())
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res might have the hobby property, which can be Soccer or Reading
  console.log(res)
}
require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  hobbies_enum = tb.add_enum('Hobbies')
  hobbies_enum.add_value('Soccer')
  hobbies_enum.add_value('Reading')

  address_class = tb.add_class('Address')
  address_class.add_property('street', tb.string)

  tb.User.add_property('hobby', hobbies_enum.type.optional)
  tb.User.add_property('address', address_class.type.optional)
  
  res = Baml::Client.dynamic_user_creator(input: "some user info", baml_options: { tb: tb })
  # Now res might have the hobby property, which can be Soccer or Reading
  puts res
end

TypeBuilder provides methods for building different kinds of types:
| Method | Description | Example |
|--------|-------------|---------|
| string() | Creates a string type | tb.string() |
| int() | Creates an integer type | tb.int() |
| float() | Creates a float type | tb.float() |
| bool() | Creates a boolean type | tb.bool() |
| list() | Makes a type into a list | tb.string().list() |
| optional() | Makes a type optional | tb.string().optional() |",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#creating-new-dynamic-classes-or-enums-not-in-baml",
    "title": "Creating new dynamic classes or enums not in BAML",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "The previous examples showed how to modify existing types. Here we create a new Hobbies enum, and a new class called Address without having them defined in BAML.
Note that you must attach the new types to the existing Return Type of your BAML function(in this case it's User).
from baml_client.type_builder import TypeBuilder
from baml_client.async_client import b

async def run():
  tb = TypeBuilder()
  hobbies_enum = tb.add_enum("Hobbies")
  hobbies_enum.add_value("Soccer")
  hobbies_enum.add_value("Reading")

  address_class = tb.add_class("Address")
  address_class.add_property("street", tb.string()).description("The user's street address")

  tb.User.add_property("hobby", hobbies_enum.type().optional())
  tb.User.add_property("address", address_class.type().optional())
  res = await b.DynamicUserCreator("some user info", {"tb": tb})
  # Now res might have the hobby property, which can be Soccer or Reading
  print(res)

import TypeBuilder from '../baml_client/type_builder'
import { b } from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  const hobbiesEnum = tb.addEnum('Hobbies')
  hobbiesEnum.addValue('Soccer')
  hobbiesEnum.addValue('Reading')

  const addressClass = tb.addClass('Address')
  addressClass.addProperty('street', tb.string()).description("The user's street address")


  tb.User.addProperty('hobby', hobbiesEnum.type().optional())
  tb.User.addProperty('address', addressClass.type())
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res might have the hobby property, which can be Soccer or Reading
  console.log(res)
}
require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  hobbies_enum = tb.add_enum('Hobbies')
  hobbies_enum.add_value('Soccer')
  hobbies_enum.add_value('Reading')

  address_class = tb.add_class('Address')
  address_class.add_property('street', tb.string)

  tb.User.add_property('hobby', hobbies_enum.type.optional)
  tb.User.add_property('address', address_class.type.optional)
  
  res = Baml::Client.dynamic_user_creator(input: "some user info", baml_options: { tb: tb })
  # Now res might have the hobby property, which can be Soccer or Reading
  puts res
end

TypeBuilder provides methods for building different kinds of types:
| Method | Description | Example |
|--------|-------------|---------|
| string() | Creates a string type | tb.string() |
| int() | Creates an integer type | tb.int() |
| float() | Creates a float type | tb.float() |
| bool() | Creates a boolean type | tb.bool() |
| list() | Makes a type into a list | tb.string().list() |
| optional() | Makes a type optional | tb.string().optional() |",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#creating-new-dynamic-classes-or-enums-not-in-baml",
    "title": "Creating new dynamic classes or enums not in BAML",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "tb = TypeBuilder()
tb.User.add_property("email", tb.string()).description("The user's email")
const tb = new TypeBuilder()
tb.User.addProperty("email", tb.string()).description("The user's email")
tb = Baml::TypeBuilder.new
tb.User.add_property("email", tb.string).description("The user's email")
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#adding-descriptions-to-dynamic-types",
    "title": "Adding descriptions to dynamic types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "tb = TypeBuilder()
tb.User.add_property("email", tb.string()).description("The user's email")
const tb = new TypeBuilder()
tb.User.addProperty("email", tb.string()).description("The user's email")
tb = Baml::TypeBuilder.new
tb.User.add_property("email", tb.string).description("The user's email")
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#adding-descriptions-to-dynamic-types",
    "title": "Adding descriptions to dynamic types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "We have a working implementation of this, but are waiting for a concrete use case to merge it.
Please chime in on the GitHub issue if this is
something you'd like to use.
import pydantic
from baml_client import b

class Person(pydantic.BaseModel):
    last_name: list[str]
    height: Optional[float] = pydantic.Field(description="Height in meters")

tb = TypeBuilder()
tb.unstable_features.add_json_schema(Person.model_json_schema())

res = await b.ExtractPeople(
    "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
    {"tb": tb},
)
import 'z' from zod
import 'zodToJsonSchema' from zod-to-json-schema
import { b } from '../baml_client'

const personSchema = z.object({
  animalLiked: z.object({
    animal: z.string().describe('The animal mentioned, in singular form.'),
  }),
  hobbies: z.enum(['chess', 'sports', 'music', 'reading']).array(),
  height: z.union([z.string(), z.number().int()]).describe('Height in meters'),
})

let tb = new TypeBuilder()
tb.unstableFeatures.addJsonSchema(zodToJsonSchema(personSchema, 'Person'))

const res = await b.ExtractPeople(
  "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
  { tb },
)
tb = Baml::TypeBuilder.new
tb.unstable_features.add_json_schema(...)

res = Baml::Client.extract_people(
  input: "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
  baml_options: { tb: tb }
)

puts res
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#building-dynamic-types-from-json-schema",
    "title": "Building dynamic types from JSON schema",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "We have a working implementation of this, but are waiting for a concrete use case to merge it.
Please chime in on the GitHub issue if this is
something you'd like to use.
import pydantic
from baml_client import b

class Person(pydantic.BaseModel):
    last_name: list[str]
    height: Optional[float] = pydantic.Field(description="Height in meters")

tb = TypeBuilder()
tb.unstable_features.add_json_schema(Person.model_json_schema())

res = await b.ExtractPeople(
    "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
    {"tb": tb},
)
import 'z' from zod
import 'zodToJsonSchema' from zod-to-json-schema
import { b } from '../baml_client'

const personSchema = z.object({
  animalLiked: z.object({
    animal: z.string().describe('The animal mentioned, in singular form.'),
  }),
  hobbies: z.enum(['chess', 'sports', 'music', 'reading']).array(),
  height: z.union([z.string(), z.number().int()]).describe('Height in meters'),
})

let tb = new TypeBuilder()
tb.unstableFeatures.addJsonSchema(zodToJsonSchema(personSchema, 'Person'))

const res = await b.ExtractPeople(
  "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
  { tb },
)
tb = Baml::TypeBuilder.new
tb.unstable_features.add_json_schema(...)

res = Baml::Client.extract_people(
  input: "My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.",
  baml_options: { tb: tb }
)

puts res
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#building-dynamic-types-from-json-schema",
    "title": "Building dynamic types from JSON schema",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "This feature is coming soon! Let us know if you're interested in testing it out!
You can still write tests in Python, TypeScript, Ruby, etc in the meantime.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#testing-dynamic-types-in-baml",
    "title": "Testing dynamic types in BAML",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "This feature is coming soon! Let us know if you're interested in testing it out!
You can still write tests in Python, TypeScript, Ruby, etc in the meantime.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/dynamic-runtime-types#testing-dynamic-types-in-baml",
    "title": "Testing dynamic types in BAML",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "Writing prompts requires a lot of string manipulation. BAML has a `template_string` to let you combine different string templates together. Under-the-hood they use [jinja](/ref/prompt-syntax/what-is-jinja) to evaluate the string and its inputs.

**Template Strings are functions that always return a string.** They can be used to define reusable parts of a prompt, or to make the prompt more readable by breaking it into smaller parts.

Example
```baml BAML
// Inject a list of "system" or "user" messages into the prompt.
// Note the syntax -- there are no curlies. Just a string block.
template_string PrintMessages(messages: Message[]) #"
  {% for m in messages %}
    {{ _.role(m.role) }}
    {{ m.message }}
  {% endfor %}
"#

function ClassifyConversation(messages: Message[]) -> Category[] {
  client GPT4Turbo
  prompt #"
    Classify this conversation:
    {{ PrintMessages(messages) }}

    Use the following categories:
    {{ ctx.output_format}}
  "#
}
```

In this example we can call the template_string `PrintMessages` to subdivide the prompt into "user" or "system" messages using `_.role()` (see [message roles](/ref/prompt-syntax/role)). This allows us to reuse the logic for printing messages in multiple prompts. 

You can nest as many template strings inside each other and call them however many times you want.

<Warning>
  The BAML linter may give you a warning when you use template strings due to a static analysis limitation. You can ignore this warning. If it renders in the playground, you're good!
</Warning>
Use the playground preview to ensure your template string is being evaluated correctly!",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/reusing-prompt-snippets",
    "title": "Reusing Prompt Snippets",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "Writing prompts requires a lot of string manipulation. BAML has a template_string to let you combine different string templates together. Under-the-hood they use jinja to evaluate the string and its inputs.
Template Strings are functions that always return a string. They can be used to define reusable parts of a prompt, or to make the prompt more readable by breaking it into smaller parts.
Example
// Inject a list of "system" or "user" messages into the prompt.
// Note the syntax -- there are no curlies. Just a string block.
template_string PrintMessages(messages: Message[]) #"
  {% for m in messages %}
    {{ _.role(m.role) }}
    {{ m.message }}
  {% endfor %}
"#

function ClassifyConversation(messages: Message[]) -> Category[] {
  client GPT4Turbo
  prompt #"
    Classify this conversation:
    {{ PrintMessages(messages) }}

    Use the following categories:
    {{ ctx.output_format}}
  "#
}

In this example we can call the template_string PrintMessages to subdivide the prompt into "user" or "system" messages using _.role() (see message roles). This allows us to reuse the logic for printing messages in multiple prompts.
You can nest as many template strings inside each other and call them however many times you want.
The BAML linter may give you a warning when you use template strings due to a static analysis limitation. You can ignore this warning. If it renders in the playground, you're good!
Use the playground preview to ensure your template string is being evaluated correctly!",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/reusing-prompt-snippets",
    "title": "Reusing Prompt Snippets",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "Recall that an LLM request usually looks like this, where it sometimes has metadata in each `message`. In this case, Anthropic has a `cache_control` key.

```curl {3,11} Anthropic Request
curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "anthropic-beta: prompt-caching-2024-07-31" \
  -d '{
    "model": "claude-3-5-sonnet-20241022",
    "max_tokens": 1024,
    "messages": [
       {
        "type": "text", 
        "text": "<the entire contents of Pride and Prejudice>",
        "cache_control": {"type": "ephemeral"}
      },
      {
        "role": "user",
        "content": "Analyze the major themes in Pride and Prejudice."
      }
    ]
  }'
```


This is nearly the same as this BAML code, minus the `cache_control` metadata:


Let's add the `cache-control` metadata to each of our messges in BAML now.
There's just 2 steps:

<Steps>
### Allow role metadata and header in the client definition
```baml {5-8} main.baml
client<llm> AnthropicClient {
  provider "anthropic"
  options {
    model "claude-3-5-sonnet-20241022"
    allowed_role_metadata ["cache_control"]
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }
}
```

### Add the metadata to the messages
```baml {2,6} main.baml
function AnalyzeBook(book: string) -> string {
  client<llm> AnthropicClient
  prompt #"
    {{ _.role("user") }}
    {{ book }}
    {{ _.role("user", cache_control={"type": "ephemeral"}) }}
    Analyze the major themes in Pride and Prejudice.
  "#
}
```

</Steps>

We have the "allowed_role_metadata" so that if you swap to other LLM clients, we don't accidentally forward the wrong metadata to the new provider API.


<Tip>
Remember to check the "raw curl" checkbox in the VSCode Playground to see the exact request being sent!
</Tip>",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/prompt-caching-message-role-metadata",
    "title": "Prompt Caching / Message Role Metadata",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "Recall that an LLM request usually looks like this, where it sometimes has metadata in each message. In this case, Anthropic has a cache_control key.
curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "anthropic-beta: prompt-caching-2024-07-31" \
  -d '{
    "model": "claude-3-5-sonnet-20241022",
    "max_tokens": 1024,
    "messages": [
       {
        "type": "text", 
        "text": "<the entire contents of Pride and Prejudice>",
        "cache_control": {"type": "ephemeral"}
      },
      {
        "role": "user",
        "content": "Analyze the major themes in Pride and Prejudice."
      }
    ]
  }'

This is nearly the same as this BAML code, minus the cache_control metadata:
Let's add the cache-control metadata to each of our messges in BAML now.
There's just 2 steps:
Allow role metadata and header in the client definitionclient<llm> AnthropicClient {
  provider "anthropic"
  options {
    model "claude-3-5-sonnet-20241022"
    allowed_role_metadata ["cache_control"]
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }
}
Add the metadata to the messagesfunction AnalyzeBook(book: string) -> string {
  client<llm> AnthropicClient
  prompt #"
    {{ _.role("user") }}
    {{ book }}
    {{ _.role("user", cache_control={"type": "ephemeral"}) }}
    Analyze the major themes in Pride and Prejudice.
  "#
}

We have the "allowed_role_metadata" so that if you swap to other LLM clients, we don't accidentally forward the wrong metadata to the new provider API.
Remember to check the "raw curl" checkbox in the VSCode Playground to see the exact request being sent!",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/prompt-caching-message-role-metadata",
    "title": "Prompt Caching / Message Role Metadata",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "With custom type validations, you can set specific rules to ensure your data's
value falls within an acceptable range.

BAML provides two types of validations:
- **`@assert`** for strict validations. If a type fails an `@assert` validation, it
  will not be returned in the response. If the failing assertion was part of the
  top-level type, it will raise an exception. If it's part of a container, it
  will be removed from the container.
- **`@check`** for non-exception-raising validations. Whether a `@check` passes or
  fails, the data will be returned. You can access the results of invidividual
  checks in the response data.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations",
    "title": "Validations",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "With custom type validations, you can set specific rules to ensure your data's
value falls within an acceptable range.
BAML provides two types of validations:

@assert for strict validations. If a type fails an @assert validation, it
will not be returned in the response. If the failing assertion was part of the
top-level type, it will raise an exception. If it's part of a container, it
will be removed from the container.
@check for non-exception-raising validations. Whether a @check passes or
fails, the data will be returned. You can access the results of invidividual
checks in the response data.

Assertions
Assertions are used to guarantee properties about a type or its components in a response.
They can be written directly as inline attributes next to the field
definition or on the line following the field definition, or on a top-level type used
in a function declaration.
Using @assert
BAML will raise an exception if a function returns a Foo where Foo.bar
is not between 0 and 10.
If the function NextInt8 returns 128, BAML will raise an exception.
class Foo {
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }}) //this = Foo.bar value
}

function NextInt8(a: int) -> int @assert(ok_int8, {{ this >= -128 and this < 127 }}) {
  client GPT4
  prompt #"Return the number after {{ a }}"#
}

Asserts may be applied to a whole class via @@assert.
class Bar {
  baz int
  quux string
  @@assert(length_limit, {{ this.quux|length < this.baz }})
}

Using @assert with Union Types
Note that when using Unions, it is
crucial to specify where the @assert attribute is applied within the union
type, as it is not known until runtime which type the value will be.
class Foo {
  bar (int @assert(positive, {{ this > 0 }}) | bool @assert(is_true, {{ this }}))
}

In the above example, the @assert attribute is applied specifically to the
int and string instances of the Union, rather than to the Foo.bar field
as a whole.
Likewise, the keyword this refers to the value of the type instance it is
directly associated with (e.g., int or string).
Chaining Assertions
You can have multiple assertions on a single field by chaining multiple @assert attributes.
In this example, the asserts on bar and baz are equivalent.
class Foo {
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }})
  baz int @assert(positive, {{ this > 0 }}) @assert(less_than_10, {{ this < 10 }})
}

Chained asserts are evaluated in order from left to right. If the first assert
fails, the second assert will not be evaluated.
Writing Assertions
Assertions are represented as Jinja expressions and can be used to validate
various types of data. Possible constraints include checking the length of a
string, comparing two values, or verifying the presence of a substring with
regular expressions.
In the future, we plan to support shorthand syntax for common assertions to make
writing them easier.
For now, see our Jinja cookbook / guide
or the Minijinja filters docs
for more information on writing expressions.
Expression keywords

this refers to the value of the current field being validated.

this.field is used to refer to a specific field within the context of this.
Access nested fields of a data type by chaining the field names together with a . as shown below.
class Resume {
  name string
  experience string[]

}

class Person {
  resume Resume @assert({{ this.experience|length > 0 }}, "Nonzero experience")
  person_name name
}

Assertion Errors
When validations fail, your BAML function will raise a BamlValidationError
exception, same as when parsing fails. You can catch this exception and handle
it as you see fit.
You can define custom names for each assertion, which will be included
in the exception for that failure case. If you don't define a custom name,
BAML will display the body of the assert expression.
In this example, if the quote field is empty, BAML raises a
BamlValidationError with the message "exact_citation_not_found". If the
website_link field does not contain "https://", it raises a
BamlValidationError with the message invalid_link.
class Citation {
  //@assert(<name>, <expr>)
  quote string @assert(exact_citation_found,
	  {{ this|length > 0 }}
  )

  website_link string @assert(valid_link,
    {{ this|regex_match("https://") }}
  )
}

from baml_client import b
from baml_client.types import Citation

def main():
    try:
        citation: Citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

        # Access the value of the quote field
        quote = citation.quote
        website_link = citation.website_link
        print(f"Quote: {quote} from {website_link}")
        
    except BamlValidationError as e:
        print(f"Validation error: {str(e)}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

import { b, BamlValidationError } from './baml_client';
import { Citation } from './baml_client/types';

const main = () => {
    try {
        const citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...");
        
        const quote = citation.quote.value;
        console.log(`Quote: ${quote}`);

        const checks = citation.quote.checks;
        console.log(`Check exact_citation_found: ${checks.exact_citation_found.status}`);
        for (const check of get_checks(checks)) {
            console.log(`Check ${check.name}: ${check.status}`);
        }

        const author = citation.author;
        console.log(`Author: ${author}`);
    } catch (e) {
        if (e instanceof BamlValidationError) {
            console.log(`Validation error: ${e}`);
        } else {
            console.error(e);
        }
    }
};

Checks
@check attributes add validations without raising exceptions if they fail.
Types with @check attributes allow the validations to be inspected at
runtime.
( bar int @check(less_than_zero, {{ this < 0 }}) )[]

List[Checked[int, Dict[Literal["less_than_zero"]]]]
Checked<int,"less_than_zero">[]

The following example uses both @check and @assert. If line_number fails its
@assert, no Citation will be returned by GetCitation(). However,
exact_citation_not_found can fail without interrupting the result. Because it
was a @check, client code can inspect the result of the check.
class Citation {
  quote string @check(
      exact_citation_match,
	  {{ this|length > 0 }}
  )
  line_number string @assert(
    has_line_number
    {{ this|length >= 0 }}
  )
}

function GetCitation(full_text: string) -> Citation {
  client GPT4 
  prompt #"
    Generate a citation of the text below in MLA format:
    {{full_text}}

    {{ctx.output_format}}
  "#
}


from baml_client import b
from baml_client.types import Citation, get_checks

def main():
    citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

    # Access the value of the quote field
    quote = citation.quote.value 
    print(f"Quote: {quote}")

    # Access a particular check.
    quote_match_check = citation.quote.checks['exact_citation_match'].status
    print(f"Citation match status: {quote_match_check})")

    # Access each check and its status.
    for check in get_checks(citation.quote.checks):
        print(f"Check {check.name}: {check.status}")
import { b, get_checks } from './baml_client'
import { Citation } from './baml_client/types'

const main = () => {
    const citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

    // Access the value of the quote field
    const quote = citation.quote.value
    console.log(`Quote: ${quote}`)

    // Access a particular check.
    const quote_match_check = citation.quote.checks.exact_citation_match.status;
    console.log(`Exact citation status: ${quote_match_check}`);

    // Access each check and its status.
    for (const check of get_checks(citation.quote.checks)) {
        console.log(`Check: ${check.name}, Status: ${check.status}`)
    }
}

You can also chain multiple @check and @assert attributes on a single field.
class Foo {
  bar string @check(bar_nonempty, {{ this|length > 0 }})
  @assert(bar_no_foo, {{ this|contains("foo") }})
  @check(bar_no_fizzle, {{ this|contains("fizzle") }})
  @assert(bar_no_baz, {{ this|contains("baz") }})
}

 When using @check, all checks on the response data are evaluated even if
one fails. In contrast, with @assert, a failure will stop the parsing process
and immediately raise an exception. 
Advanced Example
The following example shows more complex minijinja expressions, see the
Minijinja filters docs
for more information on available operators to use in your assertions.

The Book and Library classes below demonstrate how to validate a book's
title, author, ISBN, publication year, genres, and a library's name and books.
The block-level assertion in the Library class ensures that all books have
unique ISBNs.
class Book {
    title string @assert(this|length > 0)
    author string @assert(this|length > 0)
    isbn string @assert(
        {{ this|regex_match("^(97(8|9))?\d{9}(\d|X)$") }},
        "Invalid ISBN format"
    )
    publication_year int @assert(valid_pub_year, {{ 1000 <= this <= 2100 }})
    genres string[] @assert(valid_length, {{ 1 <= this|length <= 10 }})
}

class Library {
    name string
    books Book[] @assert(nonempty_books, {{ this|length > 0 }})
                 @assert(unique_isbn, {{ this|map(attribute='isbn')|unique()|length == this|length }} )
}

In this example, we use a block-level @@assert to check a dependency across
a pair of fields.
class Person {
    name string @assert(valid_name, {{ this|length >= 2 }})
    age int @assert(valid_age, {{ this >= 0 }})
    address Address

    @@assert(not_usa_minor, {{
        this.age >= 18 or this.address.country != "USA",
    }})
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations",
    "title": "Validations",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "Assertions are used to guarantee properties about a type or its components in a response.
They can be written directly as inline attributes next to the field
definition or on the line following the field definition, or on a top-level type used
in a function declaration.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#assertions",
    "title": "Assertions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "Assertions are used to guarantee properties about a type or its components in a response.
They can be written directly as inline attributes next to the field
definition or on the line following the field definition, or on a top-level type used
in a function declaration.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#assertions",
    "title": "Assertions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
      {
        "slug": "guide/baml-advanced/validations#assertions",
        "title": "Assertions",
      },
    ],
    "description": "BAML will raise an exception if a function returns a Foo where Foo.bar
is not between 0 and 10.
If the function NextInt8 returns 128, BAML will raise an exception.
class Foo {
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }}) //this = Foo.bar value
}

function NextInt8(a: int) -> int @assert(ok_int8, {{ this >= -128 and this < 127 }}) {
  client GPT4
  prompt #"Return the number after {{ a }}"#
}

Asserts may be applied to a whole class via @@assert.
class Bar {
  baz int
  quux string
  @@assert(length_limit, {{ this.quux|length < this.baz }})
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#using-assert",
    "title": "Using @assert",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
      "Assertions",
    ],
    "content": "BAML will raise an exception if a function returns a Foo where Foo.bar
is not between 0 and 10.
If the function NextInt8 returns 128, BAML will raise an exception.
class Foo {
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }}) //this = Foo.bar value
}

function NextInt8(a: int) -> int @assert(ok_int8, {{ this >= -128 and this < 127 }}) {
  client GPT4
  prompt #"Return the number after {{ a }}"#
}

Asserts may be applied to a whole class via @@assert.
class Bar {
  baz int
  quux string
  @@assert(length_limit, {{ this.quux|length < this.baz }})
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#using-assert",
    "title": "Using @assert",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
      {
        "slug": "guide/baml-advanced/validations#assertions",
        "title": "Assertions",
      },
    ],
    "description": "Note that when using Unions, it is
crucial to specify where the @assert attribute is applied within the union
type, as it is not known until runtime which type the value will be.
class Foo {
  bar (int @assert(positive, {{ this > 0 }}) | bool @assert(is_true, {{ this }}))
}

In the above example, the @assert attribute is applied specifically to the
int and string instances of the Union, rather than to the Foo.bar field
as a whole.
Likewise, the keyword this refers to the value of the type instance it is
directly associated with (e.g., int or string).",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#using-assert-with-union-types",
    "title": "Using @assert with Union Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
      "Assertions",
    ],
    "content": "Note that when using Unions, it is
crucial to specify where the @assert attribute is applied within the union
type, as it is not known until runtime which type the value will be.
class Foo {
  bar (int @assert(positive, {{ this > 0 }}) | bool @assert(is_true, {{ this }}))
}

In the above example, the @assert attribute is applied specifically to the
int and string instances of the Union, rather than to the Foo.bar field
as a whole.
Likewise, the keyword this refers to the value of the type instance it is
directly associated with (e.g., int or string).",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#using-assert-with-union-types",
    "title": "Using @assert with Union Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "You can have multiple assertions on a single field by chaining multiple @assert attributes.
In this example, the asserts on bar and baz are equivalent.
class Foo {
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }})
  baz int @assert(positive, {{ this > 0 }}) @assert(less_than_10, {{ this < 10 }})
}

Chained asserts are evaluated in order from left to right. If the first assert
fails, the second assert will not be evaluated.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#chaining-assertions",
    "title": "Chaining Assertions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "You can have multiple assertions on a single field by chaining multiple @assert attributes.
In this example, the asserts on bar and baz are equivalent.
class Foo {
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }})
  baz int @assert(positive, {{ this > 0 }}) @assert(less_than_10, {{ this < 10 }})
}

Chained asserts are evaluated in order from left to right. If the first assert
fails, the second assert will not be evaluated.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#chaining-assertions",
    "title": "Chaining Assertions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "Assertions are represented as Jinja expressions and can be used to validate
various types of data. Possible constraints include checking the length of a
string, comparing two values, or verifying the presence of a substring with
regular expressions.
In the future, we plan to support shorthand syntax for common assertions to make
writing them easier.
For now, see our Jinja cookbook / guide
or the Minijinja filters docs
for more information on writing expressions.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#writing-assertions",
    "title": "Writing Assertions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "Assertions are represented as Jinja expressions and can be used to validate
various types of data. Possible constraints include checking the length of a
string, comparing two values, or verifying the presence of a substring with
regular expressions.
In the future, we plan to support shorthand syntax for common assertions to make
writing them easier.
For now, see our Jinja cookbook / guide
or the Minijinja filters docs
for more information on writing expressions.",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#writing-assertions",
    "title": "Writing Assertions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
      {
        "slug": "guide/baml-advanced/validations#writing-assertions",
        "title": "Writing Assertions",
      },
    ],
    "description": "
this refers to the value of the current field being validated.

this.field is used to refer to a specific field within the context of this.
Access nested fields of a data type by chaining the field names together with a . as shown below.
class Resume {
  name string
  experience string[]

}

class Person {
  resume Resume @assert({{ this.experience|length > 0 }}, "Nonzero experience")
  person_name name
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#expression-keywords",
    "title": "Expression keywords",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
      "Writing Assertions",
    ],
    "content": "
this refers to the value of the current field being validated.

this.field is used to refer to a specific field within the context of this.
Access nested fields of a data type by chaining the field names together with a . as shown below.
class Resume {
  name string
  experience string[]

}

class Person {
  resume Resume @assert({{ this.experience|length > 0 }}, "Nonzero experience")
  person_name name
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#expression-keywords",
    "title": "Expression keywords",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "When validations fail, your BAML function will raise a BamlValidationError
exception, same as when parsing fails. You can catch this exception and handle
it as you see fit.
You can define custom names for each assertion, which will be included
in the exception for that failure case. If you don't define a custom name,
BAML will display the body of the assert expression.
In this example, if the quote field is empty, BAML raises a
BamlValidationError with the message "exact_citation_not_found". If the
website_link field does not contain "https://", it raises a
BamlValidationError with the message invalid_link.
class Citation {
  //@assert(<name>, <expr>)
  quote string @assert(exact_citation_found,
	  {{ this|length > 0 }}
  )

  website_link string @assert(valid_link,
    {{ this|regex_match("https://") }}
  )
}

from baml_client import b
from baml_client.types import Citation

def main():
    try:
        citation: Citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

        # Access the value of the quote field
        quote = citation.quote
        website_link = citation.website_link
        print(f"Quote: {quote} from {website_link}")
        
    except BamlValidationError as e:
        print(f"Validation error: {str(e)}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

import { b, BamlValidationError } from './baml_client';
import { Citation } from './baml_client/types';

const main = () => {
    try {
        const citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...");
        
        const quote = citation.quote.value;
        console.log(`Quote: ${quote}`);

        const checks = citation.quote.checks;
        console.log(`Check exact_citation_found: ${checks.exact_citation_found.status}`);
        for (const check of get_checks(checks)) {
            console.log(`Check ${check.name}: ${check.status}`);
        }

        const author = citation.author;
        console.log(`Author: ${author}`);
    } catch (e) {
        if (e instanceof BamlValidationError) {
            console.log(`Validation error: ${e}`);
        } else {
            console.error(e);
        }
    }
};
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#assertion-errors",
    "title": "Assertion Errors",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "When validations fail, your BAML function will raise a BamlValidationError
exception, same as when parsing fails. You can catch this exception and handle
it as you see fit.
You can define custom names for each assertion, which will be included
in the exception for that failure case. If you don't define a custom name,
BAML will display the body of the assert expression.
In this example, if the quote field is empty, BAML raises a
BamlValidationError with the message "exact_citation_not_found". If the
website_link field does not contain "https://", it raises a
BamlValidationError with the message invalid_link.
class Citation {
  //@assert(<name>, <expr>)
  quote string @assert(exact_citation_found,
	  {{ this|length > 0 }}
  )

  website_link string @assert(valid_link,
    {{ this|regex_match("https://") }}
  )
}

from baml_client import b
from baml_client.types import Citation

def main():
    try:
        citation: Citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

        # Access the value of the quote field
        quote = citation.quote
        website_link = citation.website_link
        print(f"Quote: {quote} from {website_link}")
        
    except BamlValidationError as e:
        print(f"Validation error: {str(e)}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

import { b, BamlValidationError } from './baml_client';
import { Citation } from './baml_client/types';

const main = () => {
    try {
        const citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...");
        
        const quote = citation.quote.value;
        console.log(`Quote: ${quote}`);

        const checks = citation.quote.checks;
        console.log(`Check exact_citation_found: ${checks.exact_citation_found.status}`);
        for (const check of get_checks(checks)) {
            console.log(`Check ${check.name}: ${check.status}`);
        }

        const author = citation.author;
        console.log(`Author: ${author}`);
    } catch (e) {
        if (e instanceof BamlValidationError) {
            console.log(`Validation error: ${e}`);
        } else {
            console.error(e);
        }
    }
};
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#assertion-errors",
    "title": "Assertion Errors",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "@check attributes add validations without raising exceptions if they fail.
Types with @check attributes allow the validations to be inspected at
runtime.
( bar int @check(less_than_zero, {{ this < 0 }}) )[]

List[Checked[int, Dict[Literal["less_than_zero"]]]]
Checked<int,"less_than_zero">[]

The following example uses both @check and @assert. If line_number fails its
@assert, no Citation will be returned by GetCitation(). However,
exact_citation_not_found can fail without interrupting the result. Because it
was a @check, client code can inspect the result of the check.
class Citation {
  quote string @check(
      exact_citation_match,
	  {{ this|length > 0 }}
  )
  line_number string @assert(
    has_line_number
    {{ this|length >= 0 }}
  )
}

function GetCitation(full_text: string) -> Citation {
  client GPT4 
  prompt #"
    Generate a citation of the text below in MLA format:
    {{full_text}}

    {{ctx.output_format}}
  "#
}


from baml_client import b
from baml_client.types import Citation, get_checks

def main():
    citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

    # Access the value of the quote field
    quote = citation.quote.value 
    print(f"Quote: {quote}")

    # Access a particular check.
    quote_match_check = citation.quote.checks['exact_citation_match'].status
    print(f"Citation match status: {quote_match_check})")

    # Access each check and its status.
    for check in get_checks(citation.quote.checks):
        print(f"Check {check.name}: {check.status}")
import { b, get_checks } from './baml_client'
import { Citation } from './baml_client/types'

const main = () => {
    const citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

    // Access the value of the quote field
    const quote = citation.quote.value
    console.log(`Quote: ${quote}`)

    // Access a particular check.
    const quote_match_check = citation.quote.checks.exact_citation_match.status;
    console.log(`Exact citation status: ${quote_match_check}`);

    // Access each check and its status.
    for (const check of get_checks(citation.quote.checks)) {
        console.log(`Check: ${check.name}, Status: ${check.status}`)
    }
}

You can also chain multiple @check and @assert attributes on a single field.
class Foo {
  bar string @check(bar_nonempty, {{ this|length > 0 }})
  @assert(bar_no_foo, {{ this|contains("foo") }})
  @check(bar_no_fizzle, {{ this|contains("fizzle") }})
  @assert(bar_no_baz, {{ this|contains("baz") }})
}

 When using @check, all checks on the response data are evaluated even if
one fails. In contrast, with @assert, a failure will stop the parsing process
and immediately raise an exception. ",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#checks",
    "title": "Checks",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "@check attributes add validations without raising exceptions if they fail.
Types with @check attributes allow the validations to be inspected at
runtime.
( bar int @check(less_than_zero, {{ this < 0 }}) )[]

List[Checked[int, Dict[Literal["less_than_zero"]]]]
Checked<int,"less_than_zero">[]

The following example uses both @check and @assert. If line_number fails its
@assert, no Citation will be returned by GetCitation(). However,
exact_citation_not_found can fail without interrupting the result. Because it
was a @check, client code can inspect the result of the check.
class Citation {
  quote string @check(
      exact_citation_match,
	  {{ this|length > 0 }}
  )
  line_number string @assert(
    has_line_number
    {{ this|length >= 0 }}
  )
}

function GetCitation(full_text: string) -> Citation {
  client GPT4 
  prompt #"
    Generate a citation of the text below in MLA format:
    {{full_text}}

    {{ctx.output_format}}
  "#
}


from baml_client import b
from baml_client.types import Citation, get_checks

def main():
    citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

    # Access the value of the quote field
    quote = citation.quote.value 
    print(f"Quote: {quote}")

    # Access a particular check.
    quote_match_check = citation.quote.checks['exact_citation_match'].status
    print(f"Citation match status: {quote_match_check})")

    # Access each check and its status.
    for check in get_checks(citation.quote.checks):
        print(f"Check {check.name}: {check.status}")
import { b, get_checks } from './baml_client'
import { Citation } from './baml_client/types'

const main = () => {
    const citation = b.GetCitation("SpaceX, is an American spacecraft manufacturer, launch service provider...")

    // Access the value of the quote field
    const quote = citation.quote.value
    console.log(`Quote: ${quote}`)

    // Access a particular check.
    const quote_match_check = citation.quote.checks.exact_citation_match.status;
    console.log(`Exact citation status: ${quote_match_check}`);

    // Access each check and its status.
    for (const check of get_checks(citation.quote.checks)) {
        console.log(`Check: ${check.name}, Status: ${check.status}`)
    }
}

You can also chain multiple @check and @assert attributes on a single field.
class Foo {
  bar string @check(bar_nonempty, {{ this|length > 0 }})
  @assert(bar_no_foo, {{ this|contains("foo") }})
  @check(bar_no_fizzle, {{ this|contains("fizzle") }})
  @assert(bar_no_baz, {{ this|contains("baz") }})
}

 When using @check, all checks on the response data are evaluated even if
one fails. In contrast, with @assert, a failure will stop the parsing process
and immediately raise an exception. ",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#checks",
    "title": "Checks",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/baml-advanced/llm-client-registry",
        "title": "BAML Advanced",
      },
    ],
    "description": "The following example shows more complex minijinja expressions, see the
Minijinja filters docs
for more information on available operators to use in your assertions.

The Book and Library classes below demonstrate how to validate a book's
title, author, ISBN, publication year, genres, and a library's name and books.
The block-level assertion in the Library class ensures that all books have
unique ISBNs.
class Book {
    title string @assert(this|length > 0)
    author string @assert(this|length > 0)
    isbn string @assert(
        {{ this|regex_match("^(97(8|9))?\d{9}(\d|X)$") }},
        "Invalid ISBN format"
    )
    publication_year int @assert(valid_pub_year, {{ 1000 <= this <= 2100 }})
    genres string[] @assert(valid_length, {{ 1 <= this|length <= 10 }})
}

class Library {
    name string
    books Book[] @assert(nonempty_books, {{ this|length > 0 }})
                 @assert(unique_isbn, {{ this|map(attribute='isbn')|unique()|length == this|length }} )
}

In this example, we use a block-level @@assert to check a dependency across
a pair of fields.
class Person {
    name string @assert(valid_name, {{ this|length >= 2 }})
    age int @assert(valid_age, {{ this >= 0 }})
    address Address

    @@assert(not_usa_minor, {{
        this.age >= 18 or this.address.country != "USA",
    }})
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#advanced-example",
    "title": "Advanced Example",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "BAML Advanced",
    ],
    "content": "The following example shows more complex minijinja expressions, see the
Minijinja filters docs
for more information on available operators to use in your assertions.

The Book and Library classes below demonstrate how to validate a book's
title, author, ISBN, publication year, genres, and a library's name and books.
The block-level assertion in the Library class ensures that all books have
unique ISBNs.
class Book {
    title string @assert(this|length > 0)
    author string @assert(this|length > 0)
    isbn string @assert(
        {{ this|regex_match("^(97(8|9))?\d{9}(\d|X)$") }},
        "Invalid ISBN format"
    )
    publication_year int @assert(valid_pub_year, {{ 1000 <= this <= 2100 }})
    genres string[] @assert(valid_length, {{ 1 <= this|length <= 10 }})
}

class Library {
    name string
    books Book[] @assert(nonempty_books, {{ this|length > 0 }})
                 @assert(unique_isbn, {{ this|map(attribute='isbn')|unique()|length == this|length }} )
}

In this example, we use a block-level @@assert to check a dependency across
a pair of fields.
class Person {
    name string @assert(valid_name, {{ this|length >= 2 }})
    age int @assert(valid_age, {{ this >= 0 }})
    address Address

    @@assert(not_usa_minor, {{
        this.age >= 18 or this.address.country != "USA",
    }})
}
",
    "indexSegmentId": "0",
    "slug": "guide/baml-advanced/validations#advanced-example",
    "title": "Advanced Example",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "_Learn how to host your BAML code on Boundary Functions and call it over HTTP._

<Info>
  This is a preview feature, available starting with `baml-cli v0.66.0`.
</Info>

<Note>
  The BAML language, compiler, and runtime will always be 100% free and
  open-source: we will always allow you to run BAML functions directly in your
  own backends.
  
  Boundary Functions' goal is to make it even easier to host and run BAML
  functions, by adding support for features like rate limits, telemetry, and
  end-user feedback.
</Note>

Boundary Functions allows you to host your BAML functions on our infrastructure, exposing
one REST API endpoint per BAML function.

<div class="flex flex-col items-center">
  <img src="file:3f232775-f97b-449b-a76d-293b065c6307" alt="OpenAPI diagram" />
</div>

This guide will walk you through:

  - creating a Boundary Cloud account,
  - deploying your BAML code to Boundary Functions,
  - setting your API keys, and
  - calling your BAML functions.

Once you've deployed your BAML functions, you can use the [OpenAPI client] to
call them.

[OpenAPI client]: /guide/installation-language/rest-api-other-languages",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/get-started",
    "title": "Get Started",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "Learn how to host your BAML code on Boundary Functions and call it over HTTP.
This is a preview feature, available starting with baml-cli v0.66.0.
The BAML language, compiler, and runtime will always be 100% free and
open-source: we will always allow you to run BAML functions directly in your
own backends.Boundary Functions' goal is to make it even easier to host and run BAML
functions, by adding support for features like rate limits, telemetry, and
end-user feedback.
Boundary Functions allows you to host your BAML functions on our infrastructure, exposing
one REST API endpoint per BAML function.

This guide will walk you through:

creating a Boundary Cloud account,
deploying your BAML code to Boundary Functions,
setting your API keys, and
calling your BAML functions.

Once you've deployed your BAML functions, you can use the OpenAPI client to
call them.
Get Started
First, create your account and organization at https://dashboard.boundaryml.com.
Then, log in from your terminal:
baml-cli login

and run this command in your baml_src/ directory:
baml-cli deploy

This will prompt you to create a new Boundary project, deploy your BAML code to it,
and then point you to the dashboard, where you can set environment variables and
create API keys to use to call your BAML functions.

Once you've set the environment variables you need (probably ANTHROPIC_API_KEY
and/or OPENAI_API_KEY), you can call your BAML functions!
If you still have the ExtractResume function that your BAML project was created with,
you can use this command to test it out:
curl https://api2.boundaryml.com/v3/functions/prod/call/ExtractResume \
  -H "Authorization: Bearer $BOUNDARY_API_KEY" \
  -H "Content-Type: application/json" \
  -d @- << EOF
{
  "resume": "
    Grace Hopper
    grace.hopper@example.com

    Experience:
    - Rear Admiral, US Navy
    - Senior Programmer, Eckert-Mauchly Computer Corporation
    - Associate Professor, Vassar College

    Skills:
    - COBOL
    - Compiler development
  "
}
EOF

Congratulations! You've gotten your first BAML functions working on Boundary Functions.
Local development and testing
To test your BAML functions locally, you can use baml-cli dev:
ANTHROPIC_API_KEY=... OPENAI_API_KEY=... baml-cli dev

which will allow you to call your functions at http://localhost:2024/call/<function_name> instead of
https://api2.boundaryml.com/v3/functions/prod/call/<function_name> using the exact same curl command:
curl http://localhost:2024/functions/prod/call/ExtractResume \
  -H "Authorization: Bearer $BOUNDARY_API_KEY" \
  -H "Content-Type: application/json" \
  -d @- << EOF
{
  "resume": "
    Grace Hopper
    grace.hopper@example.com

    Experience:
    - Rear Admiral, US Navy
    - Senior Programmer, Eckert-Mauchly Computer Corporation
    - Associate Professor, Vassar College

    Skills:
    - COBOL
    - Compiler development
  "
}
EOF

",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/get-started",
    "title": "Get Started",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "First, create your account and organization at https://dashboard.boundaryml.com.
Then, log in from your terminal:
baml-cli login

and run this command in your baml_src/ directory:
baml-cli deploy

This will prompt you to create a new Boundary project, deploy your BAML code to it,
and then point you to the dashboard, where you can set environment variables and
create API keys to use to call your BAML functions.

Once you've set the environment variables you need (probably ANTHROPIC_API_KEY
and/or OPENAI_API_KEY), you can call your BAML functions!
If you still have the ExtractResume function that your BAML project was created with,
you can use this command to test it out:
curl https://api2.boundaryml.com/v3/functions/prod/call/ExtractResume \
  -H "Authorization: Bearer $BOUNDARY_API_KEY" \
  -H "Content-Type: application/json" \
  -d @- << EOF
{
  "resume": "
    Grace Hopper
    grace.hopper@example.com

    Experience:
    - Rear Admiral, US Navy
    - Senior Programmer, Eckert-Mauchly Computer Corporation
    - Associate Professor, Vassar College

    Skills:
    - COBOL
    - Compiler development
  "
}
EOF

Congratulations! You've gotten your first BAML functions working on Boundary Functions.",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/get-started#get-started",
    "title": "Get Started",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "First, create your account and organization at https://dashboard.boundaryml.com.
Then, log in from your terminal:
baml-cli login

and run this command in your baml_src/ directory:
baml-cli deploy

This will prompt you to create a new Boundary project, deploy your BAML code to it,
and then point you to the dashboard, where you can set environment variables and
create API keys to use to call your BAML functions.

Once you've set the environment variables you need (probably ANTHROPIC_API_KEY
and/or OPENAI_API_KEY), you can call your BAML functions!
If you still have the ExtractResume function that your BAML project was created with,
you can use this command to test it out:
curl https://api2.boundaryml.com/v3/functions/prod/call/ExtractResume \
  -H "Authorization: Bearer $BOUNDARY_API_KEY" \
  -H "Content-Type: application/json" \
  -d @- << EOF
{
  "resume": "
    Grace Hopper
    grace.hopper@example.com

    Experience:
    - Rear Admiral, US Navy
    - Senior Programmer, Eckert-Mauchly Computer Corporation
    - Associate Professor, Vassar College

    Skills:
    - COBOL
    - Compiler development
  "
}
EOF

Congratulations! You've gotten your first BAML functions working on Boundary Functions.",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/get-started#get-started",
    "title": "Get Started",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "To test your BAML functions locally, you can use baml-cli dev:
ANTHROPIC_API_KEY=... OPENAI_API_KEY=... baml-cli dev

which will allow you to call your functions at http://localhost:2024/call/<function_name> instead of
https://api2.boundaryml.com/v3/functions/prod/call/<function_name> using the exact same curl command:
curl http://localhost:2024/functions/prod/call/ExtractResume \
  -H "Authorization: Bearer $BOUNDARY_API_KEY" \
  -H "Content-Type: application/json" \
  -d @- << EOF
{
  "resume": "
    Grace Hopper
    grace.hopper@example.com

    Experience:
    - Rear Admiral, US Navy
    - Senior Programmer, Eckert-Mauchly Computer Corporation
    - Associate Professor, Vassar College

    Skills:
    - COBOL
    - Compiler development
  "
}
EOF

",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/get-started#local-development-and-testing",
    "title": "Local development and testing",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "To test your BAML functions locally, you can use baml-cli dev:
ANTHROPIC_API_KEY=... OPENAI_API_KEY=... baml-cli dev

which will allow you to call your functions at http://localhost:2024/call/<function_name> instead of
https://api2.boundaryml.com/v3/functions/prod/call/<function_name> using the exact same curl command:
curl http://localhost:2024/functions/prod/call/ExtractResume \
  -H "Authorization: Bearer $BOUNDARY_API_KEY" \
  -H "Content-Type: application/json" \
  -d @- << EOF
{
  "resume": "
    Grace Hopper
    grace.hopper@example.com

    Experience:
    - Rear Admiral, US Navy
    - Senior Programmer, Eckert-Mauchly Computer Corporation
    - Associate Professor, Vassar College

    Skills:
    - COBOL
    - Compiler development
  "
}
EOF

",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/get-started#local-development-and-testing",
    "title": "Local development and testing",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "_Learn how to use your OpenAPI client to call your functions in Boundary Functions._

<Info>
  This page assumes you've already deployed your BAML code to Boundary Functions. If
  you haven't done that yet, check out the guide for [getting started](/guide/cloud/functions/get-started).
</Info>

<Info>
  This page assumes you're using an OpenAPI-generated BAML client. If you
  haven't done that yet, check out the [OpenAPI quickstart](/docs/get-started/quickstart/openapi).
</Info>",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi",
    "title": "Using OpenAPI",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "Learn how to use your OpenAPI client to call your functions in Boundary Functions.
This page assumes you've already deployed your BAML code to Boundary Functions. If
you haven't done that yet, check out the guide for getting started.
This page assumes you're using an OpenAPI-generated BAML client. If you
haven't done that yet, check out the OpenAPI quickstart.
Create an API key
You can create API keys in the Boundary
Dashboard by going to the left sidebar and
clicking on the key icon.

Once you've created a new key, update your application code to use it as BOUNDARY_API_KEY.
Update your application code
You also need to update your application code to use BOUNDARY_ENDPOINT and
BOUNDARY_API_KEY, if set, when constructing the OpenAPI client.
import (
    "os"
    baml "my-golang-app/baml_client"
)

func main() {
    cfg := baml.NewConfiguration()
    if boundaryEndpoint := os.Getenv("BOUNDARY_ENDPOINT"); boundaryEndpoint != "" {
        cfg.BasePath = boundaryEndpoint
    }
    if boundaryApiKey := os.Getenv("BOUNDARY_API_KEY"); boundaryApiKey != "" {
        cfg.DefaultHeader["Authorization"] = "Bearer " + boundaryApiKey
    }
    b := baml.NewAPIClient(cfg).DefaultAPI
    // Use `b` to make API calls
}
import com.boundaryml.baml_client.ApiClient;
import com.boundaryml.baml_client.ApiException;
import com.boundaryml.baml_client.Configuration;
import com.boundaryml.baml_client.api.DefaultApi;
import com.boundaryml.baml_client.auth.*;

public class ApiExample {
    public static void main(String[] args) {
        ApiClient apiClient = Configuration.getDefaultApiClient();

        String boundaryEndpoint = System.getenv("BOUNDARY_ENDPOINT");
        if (boundaryEndpoint != null && !boundaryEndpoint.isEmpty()) {
            apiClient.setBasePath(boundaryEndpoint);
        }

        String boundaryApiKey = System.getenv("BOUNDARY_API_KEY");
        if (boundaryApiKey != null && !boundaryApiKey.isEmpty()) {
            apiClient.addDefaultHeader("Authorization", "Bearer " + boundaryApiKey);
        }

        DefaultApi apiInstance = new DefaultApi(apiClient);
        // Use `apiInstance` to make API calls
    }
}
require_once(__DIR__ . '/vendor/autoload.php');

$config = BamlClient\Configuration::getDefaultConfiguration();

$boundaryEndpoint = getenv('BOUNDARY_ENDPOINT');
$boundaryApiKey = getenv('BOUNDARY_API_KEY');

if ($boundaryEndpoint) {
    $config->setHost($boundaryEndpoint);
}

if ($boundaryApiKey) {
    $config->setAccessToken($boundaryApiKey);
}

$apiInstance = new OpenAPI\Client\Api\DefaultApi(
    new GuzzleHttp\Client(),
    $config
);

// Use `$apiInstance` to make API calls
require 'baml_client'

api_client = BamlClient::ApiClient.new

boundary_endpoint = ENV['BOUNDARY_ENDPOINT']
if boundary_endpoint
  api_client.host = boundary_endpoint
end

boundary_api_key = ENV['BOUNDARY_API_KEY']
if boundary_api_key
  api_client.default_headers['Authorization'] = "Bearer #{boundary_api_key}"
end
b = BamlClient::DefaultApi.new(api_client)
# Use `b` to make API calls
let mut config = baml_client::apis::configuration::Configuration::default();
if let Some(base_path) = std::env::var("BOUNDARY_ENDPOINT").ok() {
    config.base_path = base_path;
}
if let Some(api_key) = std::env::var("BOUNDARY_API_KEY").ok() {
    config.bearer_access_token = Some(api_key);
}
// Use `config` to make API calls

Set your environment variables
You can now set the following environment variables in your application:
BOUNDARY_API_KEY=...
BOUNDARY_ENDPOINT=https://api2.boundaryml.com/v3/functions/prod/

Call your functions
You should now be able to call your deployed BAML functions using your OpenAPI client!",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi",
    "title": "Using OpenAPI",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "You can create API keys in the Boundary
Dashboard by going to the left sidebar and
clicking on the key icon.

Once you've created a new key, update your application code to use it as BOUNDARY_API_KEY.",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi#create-an-api-key",
    "title": "Create an API key",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "You can create API keys in the Boundary
Dashboard by going to the left sidebar and
clicking on the key icon.

Once you've created a new key, update your application code to use it as BOUNDARY_API_KEY.",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi#create-an-api-key",
    "title": "Create an API key",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "You also need to update your application code to use BOUNDARY_ENDPOINT and
BOUNDARY_API_KEY, if set, when constructing the OpenAPI client.
import (
    "os"
    baml "my-golang-app/baml_client"
)

func main() {
    cfg := baml.NewConfiguration()
    if boundaryEndpoint := os.Getenv("BOUNDARY_ENDPOINT"); boundaryEndpoint != "" {
        cfg.BasePath = boundaryEndpoint
    }
    if boundaryApiKey := os.Getenv("BOUNDARY_API_KEY"); boundaryApiKey != "" {
        cfg.DefaultHeader["Authorization"] = "Bearer " + boundaryApiKey
    }
    b := baml.NewAPIClient(cfg).DefaultAPI
    // Use `b` to make API calls
}
import com.boundaryml.baml_client.ApiClient;
import com.boundaryml.baml_client.ApiException;
import com.boundaryml.baml_client.Configuration;
import com.boundaryml.baml_client.api.DefaultApi;
import com.boundaryml.baml_client.auth.*;

public class ApiExample {
    public static void main(String[] args) {
        ApiClient apiClient = Configuration.getDefaultApiClient();

        String boundaryEndpoint = System.getenv("BOUNDARY_ENDPOINT");
        if (boundaryEndpoint != null && !boundaryEndpoint.isEmpty()) {
            apiClient.setBasePath(boundaryEndpoint);
        }

        String boundaryApiKey = System.getenv("BOUNDARY_API_KEY");
        if (boundaryApiKey != null && !boundaryApiKey.isEmpty()) {
            apiClient.addDefaultHeader("Authorization", "Bearer " + boundaryApiKey);
        }

        DefaultApi apiInstance = new DefaultApi(apiClient);
        // Use `apiInstance` to make API calls
    }
}
require_once(__DIR__ . '/vendor/autoload.php');

$config = BamlClient\Configuration::getDefaultConfiguration();

$boundaryEndpoint = getenv('BOUNDARY_ENDPOINT');
$boundaryApiKey = getenv('BOUNDARY_API_KEY');

if ($boundaryEndpoint) {
    $config->setHost($boundaryEndpoint);
}

if ($boundaryApiKey) {
    $config->setAccessToken($boundaryApiKey);
}

$apiInstance = new OpenAPI\Client\Api\DefaultApi(
    new GuzzleHttp\Client(),
    $config
);

// Use `$apiInstance` to make API calls
require 'baml_client'

api_client = BamlClient::ApiClient.new

boundary_endpoint = ENV['BOUNDARY_ENDPOINT']
if boundary_endpoint
  api_client.host = boundary_endpoint
end

boundary_api_key = ENV['BOUNDARY_API_KEY']
if boundary_api_key
  api_client.default_headers['Authorization'] = "Bearer #{boundary_api_key}"
end
b = BamlClient::DefaultApi.new(api_client)
# Use `b` to make API calls
let mut config = baml_client::apis::configuration::Configuration::default();
if let Some(base_path) = std::env::var("BOUNDARY_ENDPOINT").ok() {
    config.base_path = base_path;
}
if let Some(api_key) = std::env::var("BOUNDARY_API_KEY").ok() {
    config.bearer_access_token = Some(api_key);
}
// Use `config` to make API calls
",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi#update-your-application-code",
    "title": "Update your application code",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "You also need to update your application code to use BOUNDARY_ENDPOINT and
BOUNDARY_API_KEY, if set, when constructing the OpenAPI client.
import (
    "os"
    baml "my-golang-app/baml_client"
)

func main() {
    cfg := baml.NewConfiguration()
    if boundaryEndpoint := os.Getenv("BOUNDARY_ENDPOINT"); boundaryEndpoint != "" {
        cfg.BasePath = boundaryEndpoint
    }
    if boundaryApiKey := os.Getenv("BOUNDARY_API_KEY"); boundaryApiKey != "" {
        cfg.DefaultHeader["Authorization"] = "Bearer " + boundaryApiKey
    }
    b := baml.NewAPIClient(cfg).DefaultAPI
    // Use `b` to make API calls
}
import com.boundaryml.baml_client.ApiClient;
import com.boundaryml.baml_client.ApiException;
import com.boundaryml.baml_client.Configuration;
import com.boundaryml.baml_client.api.DefaultApi;
import com.boundaryml.baml_client.auth.*;

public class ApiExample {
    public static void main(String[] args) {
        ApiClient apiClient = Configuration.getDefaultApiClient();

        String boundaryEndpoint = System.getenv("BOUNDARY_ENDPOINT");
        if (boundaryEndpoint != null && !boundaryEndpoint.isEmpty()) {
            apiClient.setBasePath(boundaryEndpoint);
        }

        String boundaryApiKey = System.getenv("BOUNDARY_API_KEY");
        if (boundaryApiKey != null && !boundaryApiKey.isEmpty()) {
            apiClient.addDefaultHeader("Authorization", "Bearer " + boundaryApiKey);
        }

        DefaultApi apiInstance = new DefaultApi(apiClient);
        // Use `apiInstance` to make API calls
    }
}
require_once(__DIR__ . '/vendor/autoload.php');

$config = BamlClient\Configuration::getDefaultConfiguration();

$boundaryEndpoint = getenv('BOUNDARY_ENDPOINT');
$boundaryApiKey = getenv('BOUNDARY_API_KEY');

if ($boundaryEndpoint) {
    $config->setHost($boundaryEndpoint);
}

if ($boundaryApiKey) {
    $config->setAccessToken($boundaryApiKey);
}

$apiInstance = new OpenAPI\Client\Api\DefaultApi(
    new GuzzleHttp\Client(),
    $config
);

// Use `$apiInstance` to make API calls
require 'baml_client'

api_client = BamlClient::ApiClient.new

boundary_endpoint = ENV['BOUNDARY_ENDPOINT']
if boundary_endpoint
  api_client.host = boundary_endpoint
end

boundary_api_key = ENV['BOUNDARY_API_KEY']
if boundary_api_key
  api_client.default_headers['Authorization'] = "Bearer #{boundary_api_key}"
end
b = BamlClient::DefaultApi.new(api_client)
# Use `b` to make API calls
let mut config = baml_client::apis::configuration::Configuration::default();
if let Some(base_path) = std::env::var("BOUNDARY_ENDPOINT").ok() {
    config.base_path = base_path;
}
if let Some(api_key) = std::env::var("BOUNDARY_API_KEY").ok() {
    config.bearer_access_token = Some(api_key);
}
// Use `config` to make API calls
",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi#update-your-application-code",
    "title": "Update your application code",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "You can now set the following environment variables in your application:
BOUNDARY_API_KEY=...
BOUNDARY_ENDPOINT=https://api2.boundaryml.com/v3/functions/prod/
",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi#set-your-environment-variables",
    "title": "Set your environment variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "You can now set the following environment variables in your application:
BOUNDARY_API_KEY=...
BOUNDARY_ENDPOINT=https://api2.boundaryml.com/v3/functions/prod/
",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi#set-your-environment-variables",
    "title": "Set your environment variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "You should now be able to call your deployed BAML functions using your OpenAPI client!",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi#call-your-functions",
    "title": "Call your functions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "You should now be able to call your deployed BAML functions using your OpenAPI client!",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/using-openapi#call-your-functions",
    "title": "Call your functions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "_Learn how to use Boundary Functions Environment Variables, which are key-value pairs
configured outside your source code._

Environment variables are key-value pairs, configured outside your source code,
and used to provide secrets for your deployed BAML functions, such as
`ANTHROPIC_API_KEY` and `OPENAI_API_KEY`.

You can set environment variables in the [Boundary
Dashboard](https://dashboard.boundaryml.com/) by going to the left sidebar and
clicking on the cloud icon.

<div class="flex flex-col items-center">
  <img src="file:e2159ce8-f03f-4fda-9d91-92cd7cc867cf" alt="Boundary Cloud secrets" />
</div>

Changes to environment variables will take effect immediately.",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/environment-variables",
    "title": "Environment Variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "Learn how to use Boundary Functions Environment Variables, which are key-value pairs
configured outside your source code.
Environment variables are key-value pairs, configured outside your source code,
and used to provide secrets for your deployed BAML functions, such as
ANTHROPIC_API_KEY and OPENAI_API_KEY.
You can set environment variables in the Boundary
Dashboard by going to the left sidebar and
clicking on the cloud icon.

Changes to environment variables will take effect immediately.
Limits
See Limits for more information.",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/environment-variables",
    "title": "Environment Variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Functions",
      },
    ],
    "description": "See Limits for more information.",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/environment-variables#limits",
    "title": "Limits",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Functions",
    ],
    "content": "See Limits for more information.",
    "indexSegmentId": "0",
    "slug": "guide/cloud/functions/environment-variables#limits",
    "title": "Limits",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/boundary-cloud/observability/tracking-usage",
        "title": "Observability",
      },
    ],
    "description": "<Tip>
For the remaining of 2024, Boundary Studio is free for new accounts!

Boundary Studio 2 will be released in 2025 with a new pricing model.
</Tip>

To enable observability with BAML, you'll first need to sign up for a [Boundary Studio](https://app.boundaryml.com) account. 

Once you've signed up, you'll be able to create a new project and get your project token.

Then simply add the following environment variables prior to running your application:

```bash
export BOUNDARY_PROJECT_ID=project_uuid
export BOUNDARY_SECRET=your_token
```

There you'll be able to see all the metrics and logs from your application including:

- Cost
- Function calls
- Execution time
- Token Usage
- Prompt Logs
- and more...",
    "indexSegmentId": "0",
    "slug": "guide/boundary-cloud/observability/tracking-usage",
    "title": "Boundary Studio",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Observability",
    ],
    "content": "For the remaining of 2024, Boundary Studio is free for new accounts!Boundary Studio 2 will be released in 2025 with a new pricing model.
To enable observability with BAML, you'll first need to sign up for a Boundary Studio account.
Once you've signed up, you'll be able to create a new project and get your project token.
Then simply add the following environment variables prior to running your application:
export BOUNDARY_PROJECT_ID=project_uuid
export BOUNDARY_SECRET=your_token

There you'll be able to see all the metrics and logs from your application including:

Cost
Function calls
Execution time
Token Usage
Prompt Logs
and more...

Tracing Custom Events
BAML allows you to trace any function with the @trace decorator.
This will make the function's input and output show up in the Boundary dashboard. This works for any python function you define yourself. BAML LLM functions (or any other function declared in a .baml file) are already traced by default. Logs are only sent to the Dashboard if you setup your environment variables correctly.
Example
In the example below, we trace each of the two functions pre_process_text and full_analysis:
from baml_client import baml
from baml_client.types import Book, AuthorInfo
from baml_client.tracing import trace

# You can also add a custom name with trace(name="my_custom_name")
# By default, we use the function's name.
@trace
def pre_process_text(text):
    return text.replace("\n", " ")


@trace
async def full_analysis(book: Book):
    sentiment = await baml.ClassifySentiment(
        pre_process_text(book.content)
    )
    book_analysis = await baml.AnalyzeBook(book)
    return book_analysis


@trace
async def test_book1():
    content = """Before I could reply that he [Gatsby] was my neighbor...
    """
    processed_content = pre_process_text(content)
    return await full_analysis(
        Book(
            title="The Great Gatsby",
            author=AuthorInfo(firstName="F. Scott", lastName="Fitzgerald"),
            content=processed_content,
        ),
    )
import { baml } from 'baml_client';
import { Book, AuthorInfo } from 'baml_client/types';
import { traceSync, traceAsync } from 'baml_client/tracing';

const preProcessText = traceSync(function(text: string): Promise<string> {
    return text.replace(/\n/g, " ");
});

const fullAnalysis = traceAsync(async function(book: Book): Promise<any> {
    const sentiment = await baml.ClassifySentiment(
        preProcessText(book.content)
    );
    const bookAnalysis = await baml.AnalyzeBook(book);
    return bookAnalysis;
});

const testBook1 = traceAsync(async function(): Promise<any> {
    const content = `Before I could reply that he [Gatsby] was my neighbor...`;
    const processedContent = preProcessText(content);
    return await fullAnalysis(
        new Book(
            "The Great Gatsby",
            new AuthorInfo("F. Scott", "Fitzgerald"),
            processedContent
        )
    );
});
Tracing non-baml functions is not yet supported in Ruby.
Tracing non-baml functions is not yet supported in REST (OpenAPI).

This allows us to see each function invocation, as well as all its children in the dashboard:

See running tests for more information on how to run this test.
Adding custom tags
The dashboard view allows you to see custom tags for each of the function calls. This is useful for adding metadata to your traces and allow you to query your generated logs more easily.
To add a custom tag, you can import set_tags(..) as below:
from baml_client.tracing import set_tags, trace
import typing

@trace
async def pre_process_text(text):
    set_tags(userId="1234")

    # You can also create a dictionary and pass it in
    tags_dict: typing.Dict[str, str] = {"userId": "1234"}
    set_tags(**tags_dict) # "**" unpacks the dictionary
    return text.replace("\n", " ")
",
    "indexSegmentId": "0",
    "slug": "guide/boundary-cloud/observability/tracking-usage",
    "title": "Boundary Studio",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/boundary-cloud/observability/tracking-usage",
        "title": "Observability",
      },
    ],
    "description": "BAML allows you to trace any function with the @trace decorator.
This will make the function's input and output show up in the Boundary dashboard. This works for any python function you define yourself. BAML LLM functions (or any other function declared in a .baml file) are already traced by default. Logs are only sent to the Dashboard if you setup your environment variables correctly.",
    "indexSegmentId": "0",
    "slug": "guide/boundary-cloud/observability/tracking-usage#tracing-custom-events",
    "title": "Tracing Custom Events",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Observability",
    ],
    "content": "BAML allows you to trace any function with the @trace decorator.
This will make the function's input and output show up in the Boundary dashboard. This works for any python function you define yourself. BAML LLM functions (or any other function declared in a .baml file) are already traced by default. Logs are only sent to the Dashboard if you setup your environment variables correctly.",
    "indexSegmentId": "0",
    "slug": "guide/boundary-cloud/observability/tracking-usage#tracing-custom-events",
    "title": "Tracing Custom Events",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/boundary-cloud/observability/tracking-usage",
        "title": "Observability",
      },
      {
        "slug": "guide/boundary-cloud/observability/tracking-usage#tracing-custom-events",
        "title": "Tracing Custom Events",
      },
    ],
    "description": "In the example below, we trace each of the two functions pre_process_text and full_analysis:
from baml_client import baml
from baml_client.types import Book, AuthorInfo
from baml_client.tracing import trace

# You can also add a custom name with trace(name="my_custom_name")
# By default, we use the function's name.
@trace
def pre_process_text(text):
    return text.replace("\n", " ")


@trace
async def full_analysis(book: Book):
    sentiment = await baml.ClassifySentiment(
        pre_process_text(book.content)
    )
    book_analysis = await baml.AnalyzeBook(book)
    return book_analysis


@trace
async def test_book1():
    content = """Before I could reply that he [Gatsby] was my neighbor...
    """
    processed_content = pre_process_text(content)
    return await full_analysis(
        Book(
            title="The Great Gatsby",
            author=AuthorInfo(firstName="F. Scott", lastName="Fitzgerald"),
            content=processed_content,
        ),
    )
import { baml } from 'baml_client';
import { Book, AuthorInfo } from 'baml_client/types';
import { traceSync, traceAsync } from 'baml_client/tracing';

const preProcessText = traceSync(function(text: string): Promise<string> {
    return text.replace(/\n/g, " ");
});

const fullAnalysis = traceAsync(async function(book: Book): Promise<any> {
    const sentiment = await baml.ClassifySentiment(
        preProcessText(book.content)
    );
    const bookAnalysis = await baml.AnalyzeBook(book);
    return bookAnalysis;
});

const testBook1 = traceAsync(async function(): Promise<any> {
    const content = `Before I could reply that he [Gatsby] was my neighbor...`;
    const processedContent = preProcessText(content);
    return await fullAnalysis(
        new Book(
            "The Great Gatsby",
            new AuthorInfo("F. Scott", "Fitzgerald"),
            processedContent
        )
    );
});
Tracing non-baml functions is not yet supported in Ruby.
Tracing non-baml functions is not yet supported in REST (OpenAPI).

This allows us to see each function invocation, as well as all its children in the dashboard:

See running tests for more information on how to run this test.",
    "indexSegmentId": "0",
    "slug": "guide/boundary-cloud/observability/tracking-usage#example",
    "title": "Example",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Observability",
      "Tracing Custom Events",
    ],
    "content": "In the example below, we trace each of the two functions pre_process_text and full_analysis:
from baml_client import baml
from baml_client.types import Book, AuthorInfo
from baml_client.tracing import trace

# You can also add a custom name with trace(name="my_custom_name")
# By default, we use the function's name.
@trace
def pre_process_text(text):
    return text.replace("\n", " ")


@trace
async def full_analysis(book: Book):
    sentiment = await baml.ClassifySentiment(
        pre_process_text(book.content)
    )
    book_analysis = await baml.AnalyzeBook(book)
    return book_analysis


@trace
async def test_book1():
    content = """Before I could reply that he [Gatsby] was my neighbor...
    """
    processed_content = pre_process_text(content)
    return await full_analysis(
        Book(
            title="The Great Gatsby",
            author=AuthorInfo(firstName="F. Scott", lastName="Fitzgerald"),
            content=processed_content,
        ),
    )
import { baml } from 'baml_client';
import { Book, AuthorInfo } from 'baml_client/types';
import { traceSync, traceAsync } from 'baml_client/tracing';

const preProcessText = traceSync(function(text: string): Promise<string> {
    return text.replace(/\n/g, " ");
});

const fullAnalysis = traceAsync(async function(book: Book): Promise<any> {
    const sentiment = await baml.ClassifySentiment(
        preProcessText(book.content)
    );
    const bookAnalysis = await baml.AnalyzeBook(book);
    return bookAnalysis;
});

const testBook1 = traceAsync(async function(): Promise<any> {
    const content = `Before I could reply that he [Gatsby] was my neighbor...`;
    const processedContent = preProcessText(content);
    return await fullAnalysis(
        new Book(
            "The Great Gatsby",
            new AuthorInfo("F. Scott", "Fitzgerald"),
            processedContent
        )
    );
});
Tracing non-baml functions is not yet supported in Ruby.
Tracing non-baml functions is not yet supported in REST (OpenAPI).

This allows us to see each function invocation, as well as all its children in the dashboard:

See running tests for more information on how to run this test.",
    "indexSegmentId": "0",
    "slug": "guide/boundary-cloud/observability/tracking-usage#example",
    "title": "Example",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/cloud/functions/get-started",
        "title": "Boundary Cloud",
      },
      {
        "slug": "guide/boundary-cloud/observability/tracking-usage",
        "title": "Observability",
      },
      {
        "slug": "guide/boundary-cloud/observability/tracking-usage#tracing-custom-events",
        "title": "Tracing Custom Events",
      },
    ],
    "description": "The dashboard view allows you to see custom tags for each of the function calls. This is useful for adding metadata to your traces and allow you to query your generated logs more easily.
To add a custom tag, you can import set_tags(..) as below:
from baml_client.tracing import set_tags, trace
import typing

@trace
async def pre_process_text(text):
    set_tags(userId="1234")

    # You can also create a dictionary and pass it in
    tags_dict: typing.Dict[str, str] = {"userId": "1234"}
    set_tags(**tags_dict) # "**" unpacks the dictionary
    return text.replace("\n", " ")
",
    "indexSegmentId": "0",
    "slug": "guide/boundary-cloud/observability/tracking-usage#adding-custom-tags",
    "title": "Adding custom tags",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud",
      "Observability",
      "Tracing Custom Events",
    ],
    "content": "The dashboard view allows you to see custom tags for each of the function calls. This is useful for adding metadata to your traces and allow you to query your generated logs more easily.
To add a custom tag, you can import set_tags(..) as below:
from baml_client.tracing import set_tags, trace
import typing

@trace
async def pre_process_text(text):
    set_tags(userId="1234")

    # You can also create a dictionary and pass it in
    tags_dict: typing.Dict[str, str] = {"userId": "1234"}
    set_tags(**tags_dict) # "**" unpacks the dictionary
    return text.replace("\n", " ")
",
    "indexSegmentId": "0",
    "slug": "guide/boundary-cloud/observability/tracking-usage#adding-custom-tags",
    "title": "Adding custom tags",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
    ],
    "description": "[Marvin](https://github.com/PrefectHQ/marvin) lets developers do extraction or classification tasks in Python as shown below (TypeScript is not supported):


```python
import pydantic

class Location(pydantic.BaseModel):
    city: str
    state: str

marvin.extract("I moved from NY to CHI", target=Location)
```

You can also provide instructions:
```python
marvin.extract(
    "I paid $10 for 3 tacos and got a dollar and 25 cents back.",
    target=float,
    instructions="Only extract money"
)

#  [10.0, 1.25]
```
or using enums to classify
```python
from enum import Enum
import marvin

class RequestType(Enum):
    SUPPORT = "support request"
    ACCOUNT = "account issue"
    INQUIRY = "general inquiry"

request = marvin.classify("Reset my password", RequestType)
assert request == RequestType.ACCOUNT
```


For enum classification, you can add more instructions to each enum, but then you don't get fully typed outputs, nor can reuse the enum in your own code. You're back to working with raw strings.

```python
# Classifying a task based on project specifications
project_specs = {
    "Frontend": "Tasks involving UI design, CSS, and JavaScript.",
    "Backend": "Tasks related to server, database, and application logic.",
    "DevOps": "Tasks involving deployment, CI/CD, and server maintenance."
}

task_description = "Set up the server for the new application."

task_category = marvin.classify(
    task_description,
    labels=list(project_specs.keys()),
    instructions="Match the task to the project category based on the provided specifications."
)
assert task_category == "Backend"
```

Marvin has some inherent limitations for example:
1. How to use a different model?
2. What is the full prompt? Where does it live? What if I want to change it because it doesn't work well for my use-case? How many tokens is it?
3. How do I test this function?
4. How do I visualize results over time in production?",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-marvin",
    "title": "Comparing Marvin",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
    ],
    "content": "Marvin lets developers do extraction or classification tasks in Python as shown below (TypeScript is not supported):
import pydantic

class Location(pydantic.BaseModel):
    city: str
    state: str

marvin.extract("I moved from NY to CHI", target=Location)

You can also provide instructions:
marvin.extract(
    "I paid $10 for 3 tacos and got a dollar and 25 cents back.",
    target=float,
    instructions="Only extract money"
)

#  [10.0, 1.25]

or using enums to classify
from enum import Enum
import marvin

class RequestType(Enum):
    SUPPORT = "support request"
    ACCOUNT = "account issue"
    INQUIRY = "general inquiry"

request = marvin.classify("Reset my password", RequestType)
assert request == RequestType.ACCOUNT

For enum classification, you can add more instructions to each enum, but then you don't get fully typed outputs, nor can reuse the enum in your own code. You're back to working with raw strings.
# Classifying a task based on project specifications
project_specs = {
    "Frontend": "Tasks involving UI design, CSS, and JavaScript.",
    "Backend": "Tasks related to server, database, and application logic.",
    "DevOps": "Tasks involving deployment, CI/CD, and server maintenance."
}

task_description = "Set up the server for the new application."

task_category = marvin.classify(
    task_description,
    labels=list(project_specs.keys()),
    instructions="Match the task to the project category based on the provided specifications."
)
assert task_category == "Backend"

Marvin has some inherent limitations for example:

How to use a different model?
What is the full prompt? Where does it live? What if I want to change it because it doesn't work well for my use-case? How many tokens is it?
How do I test this function?
How do I visualize results over time in production?

Using BAML
Here is the BAML equivalent of this classification task based off the prompt Marvin uses under-the-hood. Note how the prompt becomes transparent to you using BAML. You can easily make it more complex or simpler depending on the model.
enum RequestType {
  SUPPORT @alias("support request")
  ACCOUNT @alias("account issue") @description("A detailed description")
  INQUIRY @alias("general inquiry")
}

function ClassifyRequest(input: string) -> RequestType {
  client GPT4 // choose even open source models
  prompt #"
    You are an expert classifier that always maintains as much semantic meaning
    as possible when labeling text. Classify the provided data,
    text, or information as one of the provided labels:

    TEXT:
    ---
    {{ input }}
    ---

    {{ ctx.output_format }}

    The best label for the text is:
  "#
}

And you can call this function in your code
from baml_client import baml as b

...
requestType = await b.ClassifyRequest("Reset my password")
# fully typed output
assert requestType == RequestType.ACCOUNT

The prompt string may be more wordy, but with BAML you now have

Fully typed responses, guaranteed
Full transparency and flexibility of the prompt string
Full freedom for what model to use
Helper functions to manipulate types in prompts (print_enum)
Testing capabilities using the VSCode playground
Analytics in the Boundary Dashboard
Support for TypeScript
A better understanding of how prompt engineering works

Marvin was a big source of inspiration for us -- their approach is simple and elegant. We recommend checking out Marvin if you're just starting out with prompt engineering or want to do a one-off simple task in Python. But if you'd like a whole added set of features, we'd love for you to give BAML a try and let us know what you think.
Limitations of BAML
BAML does have some limitations we are continuously working on. Here are a few of them:

It is a new language. However, it is fully open source and getting started takes less than 10 minutes. We are on-call 24/7 to help with any issues (and even provide prompt engineering tips)
Developing requires VSCode. You could use vim and we have workarounds but we don't recommend it.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-marvin",
    "title": "Comparing Marvin",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
    ],
    "description": "Here is the BAML equivalent of this classification task based off the prompt Marvin uses under-the-hood. Note how the prompt becomes transparent to you using BAML. You can easily make it more complex or simpler depending on the model.
enum RequestType {
  SUPPORT @alias("support request")
  ACCOUNT @alias("account issue") @description("A detailed description")
  INQUIRY @alias("general inquiry")
}

function ClassifyRequest(input: string) -> RequestType {
  client GPT4 // choose even open source models
  prompt #"
    You are an expert classifier that always maintains as much semantic meaning
    as possible when labeling text. Classify the provided data,
    text, or information as one of the provided labels:

    TEXT:
    ---
    {{ input }}
    ---

    {{ ctx.output_format }}

    The best label for the text is:
  "#
}

And you can call this function in your code
from baml_client import baml as b

...
requestType = await b.ClassifyRequest("Reset my password")
# fully typed output
assert requestType == RequestType.ACCOUNT

The prompt string may be more wordy, but with BAML you now have

Fully typed responses, guaranteed
Full transparency and flexibility of the prompt string
Full freedom for what model to use
Helper functions to manipulate types in prompts (print_enum)
Testing capabilities using the VSCode playground
Analytics in the Boundary Dashboard
Support for TypeScript
A better understanding of how prompt engineering works

Marvin was a big source of inspiration for us -- their approach is simple and elegant. We recommend checking out Marvin if you're just starting out with prompt engineering or want to do a one-off simple task in Python. But if you'd like a whole added set of features, we'd love for you to give BAML a try and let us know what you think.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-marvin#using-baml",
    "title": "Using BAML",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
    ],
    "content": "Here is the BAML equivalent of this classification task based off the prompt Marvin uses under-the-hood. Note how the prompt becomes transparent to you using BAML. You can easily make it more complex or simpler depending on the model.
enum RequestType {
  SUPPORT @alias("support request")
  ACCOUNT @alias("account issue") @description("A detailed description")
  INQUIRY @alias("general inquiry")
}

function ClassifyRequest(input: string) -> RequestType {
  client GPT4 // choose even open source models
  prompt #"
    You are an expert classifier that always maintains as much semantic meaning
    as possible when labeling text. Classify the provided data,
    text, or information as one of the provided labels:

    TEXT:
    ---
    {{ input }}
    ---

    {{ ctx.output_format }}

    The best label for the text is:
  "#
}

And you can call this function in your code
from baml_client import baml as b

...
requestType = await b.ClassifyRequest("Reset my password")
# fully typed output
assert requestType == RequestType.ACCOUNT

The prompt string may be more wordy, but with BAML you now have

Fully typed responses, guaranteed
Full transparency and flexibility of the prompt string
Full freedom for what model to use
Helper functions to manipulate types in prompts (print_enum)
Testing capabilities using the VSCode playground
Analytics in the Boundary Dashboard
Support for TypeScript
A better understanding of how prompt engineering works

Marvin was a big source of inspiration for us -- their approach is simple and elegant. We recommend checking out Marvin if you're just starting out with prompt engineering or want to do a one-off simple task in Python. But if you'd like a whole added set of features, we'd love for you to give BAML a try and let us know what you think.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-marvin#using-baml",
    "title": "Using BAML",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
    ],
    "description": "BAML does have some limitations we are continuously working on. Here are a few of them:

It is a new language. However, it is fully open source and getting started takes less than 10 minutes. We are on-call 24/7 to help with any issues (and even provide prompt engineering tips)
Developing requires VSCode. You could use vim and we have workarounds but we don't recommend it.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-marvin#limitations-of-baml",
    "title": "Limitations of BAML",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
    ],
    "content": "BAML does have some limitations we are continuously working on. Here are a few of them:

It is a new language. However, it is fully open source and getting started takes less than 10 minutes. We are on-call 24/7 to help with any issues (and even provide prompt engineering tips)
Developing requires VSCode. You could use vim and we have workarounds but we don't recommend it.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-marvin#limitations-of-baml",
    "title": "Limitations of BAML",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
    ],
    "description": "Pydantic is a popular library for data validation in Python used by most -- if not all -- LLM frameworks, like [instructor](https://github.com/jxnl/instructor/tree/main).

BAML also uses Pydantic. The BAML Rust compiler can generate Pydantic models from your `.baml` files. But that's not all the compiler does -- it also takes care of fixing common LLM parsing issues, supports more data types, handles retries, and reduces the amount of boilerplate code you have to write.

Let's dive into how Pydantic is used and its limitations.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic",
    "title": "Comparing Pydantic",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
    ],
    "content": "Pydantic is a popular library for data validation in Python used by most -- if not all -- LLM frameworks, like instructor.
BAML also uses Pydantic. The BAML Rust compiler can generate Pydantic models from your .baml files. But that's not all the compiler does -- it also takes care of fixing common LLM parsing issues, supports more data types, handles retries, and reduces the amount of boilerplate code you have to write.
Let's dive into how Pydantic is used and its limitations.
Why working with LLMs requires more than just Pydantic
Pydantic can help you get structured output from an LLM easily at first glance:
class Resume(BaseModel):
    name: str
    skills: List[str]

def create_prompt(input_text: str) -> str:
    PROMPT_TEMPLATE = f"""Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
{input_text}
---

Schema:
{Resume.model_json_schema()['properties']}

Output JSON:
"""
    return PROMPT_TEMPLATE

def extract_resume(input_text: str) -> Union[Resume, None]:
    prompt = create_prompt(input_text)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            return Resume.model_validate_json(output)
        return None
    except Exception as e:
        raise e

That's pretty good, but now we want to add an Education model to the Resume model. We add the following code:
...
+class Education(BaseModel):
+    school: str
+    degree: str
+    year: int

class Resume(BaseModel):
    name: str
    skills: List[str]
+   education: List[Education]

def create_prompt(input_text: str) -> str:
    additional_models = ""
+    if "$defs" in Resume.model_json_schema():
+        additional_models += f"\nUse these other schema definitions as +well:\n{Resume.model_json_schema()['$defs']}"
    PROMPT_TEMPLATE = f"""Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
{input_text}
---

Schema:
{Resume.model_json_schema()['properties']}

+ {additional_models}

Output JSON:
""".strip()
    return PROMPT_TEMPLATE
...

A little ugly, but still readable... But managing all these prompt strings can make your codebase disorganized very quickly.
Then you realize the LLM sometimes outputs some text before giving you the json, like this:
+ The output is:
{
  "name": "John Doe",
  ... // truncated for brevity
}

So you add a regex to address that that extracts everything in {}:
def extract_resume(input_text: str) -> Union[Resume, None]:
    prompt = create_prompt(input_text)
    print(prompt)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        print(output)
        if output:
+            # Extract JSON block using regex
+            json_match = re.search(r"\{.*?\}", output, re.DOTALL)
+            if json_match:
+                json_output = json_match.group(0)
                return Resume.model_validate_json(output)
        return None
    except Exception as e:
        raise e

Next you realize you actually want an array of Resumes, but you can't really use List[Resume] because Pydantic and Python don't work this way, so you have to add another wrapper:
+class ResumeArray(BaseModel):
+    resumes: List[Resume]

Now you need to change the rest of your code to handle different models. That's good longterm, but it is now more boilerplate you have to write, test and maintain.
Next, you notice the LLM sometimes outputs a single resume {...}, and sometimes an array [{...}]...
You must now change your parser to handle both cases:
+def extract_resume(input_text: str) -> Union[List[Resume], None]:
+    prompt = create_prompt(input_text) # Also requires changes
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            # Extract JSON block using regex
            json_match = re.search(r"\{.*?\}", output, re.DOTALL)
            if json_match:
                json_output = json_match.group(0)
                try:
+                  parsed = json.loads(json_output)
+                  if isinstance(parsed, list):
+                      return list(map(Resume.model_validate_json, parsed))
+                  else:
+                      return [ResumeArray(**parsed)]
        return None
    except Exception as e:
        raise e

You could retry the call against the LLM to fix the issue, but that will cost you precious seconds and tokens, so handling this corner case manually is the only solution.

A small tangent -- JSON schemas vs type definitions
Sidenote: At this point your prompt looks like this:
JSON Schema:
{'name': {'title': 'Name', 'type': 'string'}, 'skills': {'items': {'type': 'string'}, 'title': 'Skills', 'type': 'array'}, 'education': {'anyOf': [{'$ref': '#/$defs/Education'}, {'type': 'null'}]}}


Use these other JSON schema definitions as well:
{'Education': {'properties': {'degree': {'title': 'Degree', 'type': 'string'}, 'major': {'title': 'Major', 'type': 'string'}, 'school': {'title': 'School', 'type': 'string'}, 'year': {'title': 'Year', 'type': 'integer'}}, 'required': ['degree', 'major', 'school', 'year'], 'title': 'Education', 'type': 'object'}}

and sometimes even GPT-4 outputs incorrect stuff like this, even though it's technically correct JSON (OpenAI's "JSON mode" will still break you)
{
  "name": 
  {
    "title": "Name", 
    "type": "string", 
    "value": "John Doe"
  }, 
  "skills": 
  {
    "items": 
    {
      "type": "string", 
      "values": 
      [
        "Python", 
        "JavaScript", 
        "React"
      ]
    ... // truncated for brevity

(this is an actual result from GPT-4 before some more prompt engineering)
when all you really want is a prompt that looks like the one below -- with way less tokens (and less likelihood of confusion). :
Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
John Doe
Python, Rust
University of California, Berkeley, B.S. in Computer Science, 2020
---

+JSON Schema:
+{
+  "name": string,
+  "skills": string[]
+  "education": {
+    "school": string,
+    "degree": string,
+    "year": integer
+  }[]
+}

Output JSON:

Ahh, much better. That's 80% less tokens with a simpler prompt, for the same results. (See also Microsoft's TypeChat which uses a similar schema format using typescript types)

But we digress, let's get back to the point. You can see how this can get out of hand quickly, and how Pydantic wasn't really made with LLMs in mind.  We haven't gotten around to adding resilience like retries, or falling back to a different model in the event of an outage. There's still a lot of wrapper code to write.
Pydantic and Enums
There are other core limitations.
Say you want to do a classification task using Pydantic. An Enum is a great fit for modelling this.
Assume this is our prompt:
Classify the company described in this text into the best
of the following categories:

Text:
---
{some_text}
---

Categories:
- Technology: Companies involved in the development and production of technology products or services
- Healthcare: Includes companies in pharmaceuticals, biotechnology, medical devices.
- Real estate: Includes real estate investment trusts (REITs) and companies involved in real estate development.

The best category is:

Since we have descriptions, we need to generate a custom enum we can use to build the prompt:
class FinancialCategory(Enum):
    technology = (
        "Technology",
        "Companies involved in the development and production of technology products or services.",
    )
    ...
    real_estate = (
        "Real Estate",
        "Includes real estate investment trusts (REITs) and companies involved in real estate development.",
    )

    def __init__(self, category, description):
        self._category = category
        self._description = description

    @property
    def category(self):
        return self._category

    @property
    def description(self):
        return self._description


We add a class method to load the right enum from the LLM output string:
    @classmethod
    def from_string(cls, category: str) -> "FinancialCategory":
        for c in cls:
            if c.category == category:
                return c
        raise ValueError(f"Invalid category: {category}")

Update the prompt to use the enum descriptions:
def print_categories_and_descriptions():
    for category in FinancialCategory:
        print(f"{category.category}: {category.description}")

def create_prompt(text: str) -> str:
    additional_models = ""
    print_categories_and_descriptions()
    PROMPT_TEMPLATE = f"""Classify the company described in this text into the best
of the following categories:

Text:
---
{text}
---

Categories:
{print_categories_and_descriptions()}

The best category is:
"""
    return PROMPT_TEMPLATE

And then we use it in our AI function:
def classify_company(text: str) -> FinancialCategory:
    prompt = create_prompt(text)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            # Use our helper function!
            return FinancialCategory.from_string(output)
        return None
    except Exception as e:
        raise e

What gets hairy is if you want to change your types.

What if you want the LLM to return an object instead? You have to change your enum, your prompt, AND your parser.
What if you want to handle cases where the LLM outputs "Real Estate" or "real estate"?
What if you want to save the enum information in a database? str(category) will save FinancialCategory.healthcare into your DB, but your parser only recognizes "Healthcare", so you'll need more boilerplate if you ever want to programmatically analyze your data.

Alternatives
There are libraries like instructor do provide a great amount of boilerplate but you're still:

Using prompts that you cannot control. E.g. a commit may change your results underneath you.
Using more tokens than you may need to to declare schemas (higher costs and latencies)
There are no included testing capabilities.. Developers have to copy-paste JSON blobs everywhere, potentially between their IDEs and other websites. Existing LLM Playgrounds were not made with structured data in mind.
Lack of observability. No automatic tracing of requests.

Enter BAML
The Boundary toolkit helps you iterate seamlessly compared to Pydantic.
Here's all the BAML code you need to solve the Extract Resume problem from earlier (VSCode prompt preview is shown on the right):

Here we use a "GPT4" client, but you can use any model. See client docs

The BAML compiler generates a python client that imports and calls the function:
from baml_client import baml as b

async def main():
  resume = await b.ExtractResume(resume_text="""John Doe
Python, Rust
University of California, Berkeley, B.S. in Computer Science, 2020""")

  assert resume.name == "John Doe"

That's it! No need to write any more code. Since the compiler knows what your function signature is we literally generate a custom deserializer for your own unique usecase that just works.
Converting the Resume into an array of resumes requires a single line change in BAML (vs having to create array wrapper classes and parsing logic).
In this image we change the types and BAML automatically updates the prompt, parser, and the Python types you get back.

Adding retries or resilience requires just a couple of modifications. And best of all, you can test things instantly, without leaving your VSCode.
Conclusion
We built BAML because writing a Python library was just not powerful enough to do everything we envisioned, as we have just explored.
Check out the Hello World tutorial to get started.
Our mission is to make the best DX for AI engineers working with LLMs. Contact us at founders@boundaryml.com or Join us on Discord to stay in touch with the community and influence the roadmap.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic",
    "title": "Comparing Pydantic",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
    ],
    "description": "Pydantic can help you get structured output from an LLM easily at first glance:
class Resume(BaseModel):
    name: str
    skills: List[str]

def create_prompt(input_text: str) -> str:
    PROMPT_TEMPLATE = f"""Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
{input_text}
---

Schema:
{Resume.model_json_schema()['properties']}

Output JSON:
"""
    return PROMPT_TEMPLATE

def extract_resume(input_text: str) -> Union[Resume, None]:
    prompt = create_prompt(input_text)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            return Resume.model_validate_json(output)
        return None
    except Exception as e:
        raise e

That's pretty good, but now we want to add an Education model to the Resume model. We add the following code:
...
+class Education(BaseModel):
+    school: str
+    degree: str
+    year: int

class Resume(BaseModel):
    name: str
    skills: List[str]
+   education: List[Education]

def create_prompt(input_text: str) -> str:
    additional_models = ""
+    if "$defs" in Resume.model_json_schema():
+        additional_models += f"\nUse these other schema definitions as +well:\n{Resume.model_json_schema()['$defs']}"
    PROMPT_TEMPLATE = f"""Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
{input_text}
---

Schema:
{Resume.model_json_schema()['properties']}

+ {additional_models}

Output JSON:
""".strip()
    return PROMPT_TEMPLATE
...

A little ugly, but still readable... But managing all these prompt strings can make your codebase disorganized very quickly.
Then you realize the LLM sometimes outputs some text before giving you the json, like this:
+ The output is:
{
  "name": "John Doe",
  ... // truncated for brevity
}

So you add a regex to address that that extracts everything in {}:
def extract_resume(input_text: str) -> Union[Resume, None]:
    prompt = create_prompt(input_text)
    print(prompt)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        print(output)
        if output:
+            # Extract JSON block using regex
+            json_match = re.search(r"\{.*?\}", output, re.DOTALL)
+            if json_match:
+                json_output = json_match.group(0)
                return Resume.model_validate_json(output)
        return None
    except Exception as e:
        raise e

Next you realize you actually want an array of Resumes, but you can't really use List[Resume] because Pydantic and Python don't work this way, so you have to add another wrapper:
+class ResumeArray(BaseModel):
+    resumes: List[Resume]

Now you need to change the rest of your code to handle different models. That's good longterm, but it is now more boilerplate you have to write, test and maintain.
Next, you notice the LLM sometimes outputs a single resume {...}, and sometimes an array [{...}]...
You must now change your parser to handle both cases:
+def extract_resume(input_text: str) -> Union[List[Resume], None]:
+    prompt = create_prompt(input_text) # Also requires changes
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            # Extract JSON block using regex
            json_match = re.search(r"\{.*?\}", output, re.DOTALL)
            if json_match:
                json_output = json_match.group(0)
                try:
+                  parsed = json.loads(json_output)
+                  if isinstance(parsed, list):
+                      return list(map(Resume.model_validate_json, parsed))
+                  else:
+                      return [ResumeArray(**parsed)]
        return None
    except Exception as e:
        raise e

You could retry the call against the LLM to fix the issue, but that will cost you precious seconds and tokens, so handling this corner case manually is the only solution.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#why-working-with-llms-requires-more-than-just-pydantic",
    "title": "Why working with LLMs requires more than just Pydantic",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
    ],
    "content": "Pydantic can help you get structured output from an LLM easily at first glance:
class Resume(BaseModel):
    name: str
    skills: List[str]

def create_prompt(input_text: str) -> str:
    PROMPT_TEMPLATE = f"""Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
{input_text}
---

Schema:
{Resume.model_json_schema()['properties']}

Output JSON:
"""
    return PROMPT_TEMPLATE

def extract_resume(input_text: str) -> Union[Resume, None]:
    prompt = create_prompt(input_text)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            return Resume.model_validate_json(output)
        return None
    except Exception as e:
        raise e

That's pretty good, but now we want to add an Education model to the Resume model. We add the following code:
...
+class Education(BaseModel):
+    school: str
+    degree: str
+    year: int

class Resume(BaseModel):
    name: str
    skills: List[str]
+   education: List[Education]

def create_prompt(input_text: str) -> str:
    additional_models = ""
+    if "$defs" in Resume.model_json_schema():
+        additional_models += f"\nUse these other schema definitions as +well:\n{Resume.model_json_schema()['$defs']}"
    PROMPT_TEMPLATE = f"""Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
{input_text}
---

Schema:
{Resume.model_json_schema()['properties']}

+ {additional_models}

Output JSON:
""".strip()
    return PROMPT_TEMPLATE
...

A little ugly, but still readable... But managing all these prompt strings can make your codebase disorganized very quickly.
Then you realize the LLM sometimes outputs some text before giving you the json, like this:
+ The output is:
{
  "name": "John Doe",
  ... // truncated for brevity
}

So you add a regex to address that that extracts everything in {}:
def extract_resume(input_text: str) -> Union[Resume, None]:
    prompt = create_prompt(input_text)
    print(prompt)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        print(output)
        if output:
+            # Extract JSON block using regex
+            json_match = re.search(r"\{.*?\}", output, re.DOTALL)
+            if json_match:
+                json_output = json_match.group(0)
                return Resume.model_validate_json(output)
        return None
    except Exception as e:
        raise e

Next you realize you actually want an array of Resumes, but you can't really use List[Resume] because Pydantic and Python don't work this way, so you have to add another wrapper:
+class ResumeArray(BaseModel):
+    resumes: List[Resume]

Now you need to change the rest of your code to handle different models. That's good longterm, but it is now more boilerplate you have to write, test and maintain.
Next, you notice the LLM sometimes outputs a single resume {...}, and sometimes an array [{...}]...
You must now change your parser to handle both cases:
+def extract_resume(input_text: str) -> Union[List[Resume], None]:
+    prompt = create_prompt(input_text) # Also requires changes
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            # Extract JSON block using regex
            json_match = re.search(r"\{.*?\}", output, re.DOTALL)
            if json_match:
                json_output = json_match.group(0)
                try:
+                  parsed = json.loads(json_output)
+                  if isinstance(parsed, list):
+                      return list(map(Resume.model_validate_json, parsed))
+                  else:
+                      return [ResumeArray(**parsed)]
        return None
    except Exception as e:
        raise e

You could retry the call against the LLM to fix the issue, but that will cost you precious seconds and tokens, so handling this corner case manually is the only solution.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#why-working-with-llms-requires-more-than-just-pydantic",
    "title": "Why working with LLMs requires more than just Pydantic",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
    ],
    "description": "Sidenote: At this point your prompt looks like this:
JSON Schema:
{'name': {'title': 'Name', 'type': 'string'}, 'skills': {'items': {'type': 'string'}, 'title': 'Skills', 'type': 'array'}, 'education': {'anyOf': [{'$ref': '#/$defs/Education'}, {'type': 'null'}]}}


Use these other JSON schema definitions as well:
{'Education': {'properties': {'degree': {'title': 'Degree', 'type': 'string'}, 'major': {'title': 'Major', 'type': 'string'}, 'school': {'title': 'School', 'type': 'string'}, 'year': {'title': 'Year', 'type': 'integer'}}, 'required': ['degree', 'major', 'school', 'year'], 'title': 'Education', 'type': 'object'}}

and sometimes even GPT-4 outputs incorrect stuff like this, even though it's technically correct JSON (OpenAI's "JSON mode" will still break you)
{
  "name": 
  {
    "title": "Name", 
    "type": "string", 
    "value": "John Doe"
  }, 
  "skills": 
  {
    "items": 
    {
      "type": "string", 
      "values": 
      [
        "Python", 
        "JavaScript", 
        "React"
      ]
    ... // truncated for brevity

(this is an actual result from GPT-4 before some more prompt engineering)
when all you really want is a prompt that looks like the one below -- with way less tokens (and less likelihood of confusion). :
Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
John Doe
Python, Rust
University of California, Berkeley, B.S. in Computer Science, 2020
---

+JSON Schema:
+{
+  "name": string,
+  "skills": string[]
+  "education": {
+    "school": string,
+    "degree": string,
+    "year": integer
+  }[]
+}

Output JSON:

Ahh, much better. That's 80% less tokens with a simpler prompt, for the same results. (See also Microsoft's TypeChat which uses a similar schema format using typescript types)

But we digress, let's get back to the point. You can see how this can get out of hand quickly, and how Pydantic wasn't really made with LLMs in mind.  We haven't gotten around to adding resilience like retries, or falling back to a different model in the event of an outage. There's still a lot of wrapper code to write.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#a-small-tangent----json-schemas-vs-type-definitions",
    "title": "A small tangent -- JSON schemas vs type definitions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
    ],
    "content": "Sidenote: At this point your prompt looks like this:
JSON Schema:
{'name': {'title': 'Name', 'type': 'string'}, 'skills': {'items': {'type': 'string'}, 'title': 'Skills', 'type': 'array'}, 'education': {'anyOf': [{'$ref': '#/$defs/Education'}, {'type': 'null'}]}}


Use these other JSON schema definitions as well:
{'Education': {'properties': {'degree': {'title': 'Degree', 'type': 'string'}, 'major': {'title': 'Major', 'type': 'string'}, 'school': {'title': 'School', 'type': 'string'}, 'year': {'title': 'Year', 'type': 'integer'}}, 'required': ['degree', 'major', 'school', 'year'], 'title': 'Education', 'type': 'object'}}

and sometimes even GPT-4 outputs incorrect stuff like this, even though it's technically correct JSON (OpenAI's "JSON mode" will still break you)
{
  "name": 
  {
    "title": "Name", 
    "type": "string", 
    "value": "John Doe"
  }, 
  "skills": 
  {
    "items": 
    {
      "type": "string", 
      "values": 
      [
        "Python", 
        "JavaScript", 
        "React"
      ]
    ... // truncated for brevity

(this is an actual result from GPT-4 before some more prompt engineering)
when all you really want is a prompt that looks like the one below -- with way less tokens (and less likelihood of confusion). :
Parse the following resume and return a structured representation of the data in the schema below.
Resume:
---
John Doe
Python, Rust
University of California, Berkeley, B.S. in Computer Science, 2020
---

+JSON Schema:
+{
+  "name": string,
+  "skills": string[]
+  "education": {
+    "school": string,
+    "degree": string,
+    "year": integer
+  }[]
+}

Output JSON:

Ahh, much better. That's 80% less tokens with a simpler prompt, for the same results. (See also Microsoft's TypeChat which uses a similar schema format using typescript types)

But we digress, let's get back to the point. You can see how this can get out of hand quickly, and how Pydantic wasn't really made with LLMs in mind.  We haven't gotten around to adding resilience like retries, or falling back to a different model in the event of an outage. There's still a lot of wrapper code to write.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#a-small-tangent----json-schemas-vs-type-definitions",
    "title": "A small tangent -- JSON schemas vs type definitions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
      {
        "slug": "guide/comparisons/baml-vs-pydantic#a-small-tangent----json-schemas-vs-type-definitions",
        "title": "A small tangent -- JSON schemas vs type definitions",
      },
    ],
    "description": "There are other core limitations.
Say you want to do a classification task using Pydantic. An Enum is a great fit for modelling this.
Assume this is our prompt:
Classify the company described in this text into the best
of the following categories:

Text:
---
{some_text}
---

Categories:
- Technology: Companies involved in the development and production of technology products or services
- Healthcare: Includes companies in pharmaceuticals, biotechnology, medical devices.
- Real estate: Includes real estate investment trusts (REITs) and companies involved in real estate development.

The best category is:

Since we have descriptions, we need to generate a custom enum we can use to build the prompt:
class FinancialCategory(Enum):
    technology = (
        "Technology",
        "Companies involved in the development and production of technology products or services.",
    )
    ...
    real_estate = (
        "Real Estate",
        "Includes real estate investment trusts (REITs) and companies involved in real estate development.",
    )

    def __init__(self, category, description):
        self._category = category
        self._description = description

    @property
    def category(self):
        return self._category

    @property
    def description(self):
        return self._description


We add a class method to load the right enum from the LLM output string:
    @classmethod
    def from_string(cls, category: str) -> "FinancialCategory":
        for c in cls:
            if c.category == category:
                return c
        raise ValueError(f"Invalid category: {category}")

Update the prompt to use the enum descriptions:
def print_categories_and_descriptions():
    for category in FinancialCategory:
        print(f"{category.category}: {category.description}")

def create_prompt(text: str) -> str:
    additional_models = ""
    print_categories_and_descriptions()
    PROMPT_TEMPLATE = f"""Classify the company described in this text into the best
of the following categories:

Text:
---
{text}
---

Categories:
{print_categories_and_descriptions()}

The best category is:
"""
    return PROMPT_TEMPLATE

And then we use it in our AI function:
def classify_company(text: str) -> FinancialCategory:
    prompt = create_prompt(text)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            # Use our helper function!
            return FinancialCategory.from_string(output)
        return None
    except Exception as e:
        raise e

What gets hairy is if you want to change your types.

What if you want the LLM to return an object instead? You have to change your enum, your prompt, AND your parser.
What if you want to handle cases where the LLM outputs "Real Estate" or "real estate"?
What if you want to save the enum information in a database? str(category) will save FinancialCategory.healthcare into your DB, but your parser only recognizes "Healthcare", so you'll need more boilerplate if you ever want to programmatically analyze your data.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#pydantic-and-enums",
    "title": "Pydantic and Enums",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
      "A small tangent -- JSON schemas vs type definitions",
    ],
    "content": "There are other core limitations.
Say you want to do a classification task using Pydantic. An Enum is a great fit for modelling this.
Assume this is our prompt:
Classify the company described in this text into the best
of the following categories:

Text:
---
{some_text}
---

Categories:
- Technology: Companies involved in the development and production of technology products or services
- Healthcare: Includes companies in pharmaceuticals, biotechnology, medical devices.
- Real estate: Includes real estate investment trusts (REITs) and companies involved in real estate development.

The best category is:

Since we have descriptions, we need to generate a custom enum we can use to build the prompt:
class FinancialCategory(Enum):
    technology = (
        "Technology",
        "Companies involved in the development and production of technology products or services.",
    )
    ...
    real_estate = (
        "Real Estate",
        "Includes real estate investment trusts (REITs) and companies involved in real estate development.",
    )

    def __init__(self, category, description):
        self._category = category
        self._description = description

    @property
    def category(self):
        return self._category

    @property
    def description(self):
        return self._description


We add a class method to load the right enum from the LLM output string:
    @classmethod
    def from_string(cls, category: str) -> "FinancialCategory":
        for c in cls:
            if c.category == category:
                return c
        raise ValueError(f"Invalid category: {category}")

Update the prompt to use the enum descriptions:
def print_categories_and_descriptions():
    for category in FinancialCategory:
        print(f"{category.category}: {category.description}")

def create_prompt(text: str) -> str:
    additional_models = ""
    print_categories_and_descriptions()
    PROMPT_TEMPLATE = f"""Classify the company described in this text into the best
of the following categories:

Text:
---
{text}
---

Categories:
{print_categories_and_descriptions()}

The best category is:
"""
    return PROMPT_TEMPLATE

And then we use it in our AI function:
def classify_company(text: str) -> FinancialCategory:
    prompt = create_prompt(text)
    chat_completion = client.chat.completions.create(
        model="gpt-4", messages=[{"role": "system", "content": prompt}]
    )
    try:
        output = chat_completion.choices[0].message.content
        if output:
            # Use our helper function!
            return FinancialCategory.from_string(output)
        return None
    except Exception as e:
        raise e

What gets hairy is if you want to change your types.

What if you want the LLM to return an object instead? You have to change your enum, your prompt, AND your parser.
What if you want to handle cases where the LLM outputs "Real Estate" or "real estate"?
What if you want to save the enum information in a database? str(category) will save FinancialCategory.healthcare into your DB, but your parser only recognizes "Healthcare", so you'll need more boilerplate if you ever want to programmatically analyze your data.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#pydantic-and-enums",
    "title": "Pydantic and Enums",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
      {
        "slug": "guide/comparisons/baml-vs-pydantic#a-small-tangent----json-schemas-vs-type-definitions",
        "title": "A small tangent -- JSON schemas vs type definitions",
      },
    ],
    "description": "There are libraries like instructor do provide a great amount of boilerplate but you're still:

Using prompts that you cannot control. E.g. a commit may change your results underneath you.
Using more tokens than you may need to to declare schemas (higher costs and latencies)
There are no included testing capabilities.. Developers have to copy-paste JSON blobs everywhere, potentially between their IDEs and other websites. Existing LLM Playgrounds were not made with structured data in mind.
Lack of observability. No automatic tracing of requests.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#alternatives",
    "title": "Alternatives",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
      "A small tangent -- JSON schemas vs type definitions",
    ],
    "content": "There are libraries like instructor do provide a great amount of boilerplate but you're still:

Using prompts that you cannot control. E.g. a commit may change your results underneath you.
Using more tokens than you may need to to declare schemas (higher costs and latencies)
There are no included testing capabilities.. Developers have to copy-paste JSON blobs everywhere, potentially between their IDEs and other websites. Existing LLM Playgrounds were not made with structured data in mind.
Lack of observability. No automatic tracing of requests.
",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#alternatives",
    "title": "Alternatives",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
    ],
    "description": "The Boundary toolkit helps you iterate seamlessly compared to Pydantic.
Here's all the BAML code you need to solve the Extract Resume problem from earlier (VSCode prompt preview is shown on the right):

Here we use a "GPT4" client, but you can use any model. See client docs

The BAML compiler generates a python client that imports and calls the function:
from baml_client import baml as b

async def main():
  resume = await b.ExtractResume(resume_text="""John Doe
Python, Rust
University of California, Berkeley, B.S. in Computer Science, 2020""")

  assert resume.name == "John Doe"

That's it! No need to write any more code. Since the compiler knows what your function signature is we literally generate a custom deserializer for your own unique usecase that just works.
Converting the Resume into an array of resumes requires a single line change in BAML (vs having to create array wrapper classes and parsing logic).
In this image we change the types and BAML automatically updates the prompt, parser, and the Python types you get back.

Adding retries or resilience requires just a couple of modifications. And best of all, you can test things instantly, without leaving your VSCode.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#enter-baml",
    "title": "Enter BAML",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
    ],
    "content": "The Boundary toolkit helps you iterate seamlessly compared to Pydantic.
Here's all the BAML code you need to solve the Extract Resume problem from earlier (VSCode prompt preview is shown on the right):

Here we use a "GPT4" client, but you can use any model. See client docs

The BAML compiler generates a python client that imports and calls the function:
from baml_client import baml as b

async def main():
  resume = await b.ExtractResume(resume_text="""John Doe
Python, Rust
University of California, Berkeley, B.S. in Computer Science, 2020""")

  assert resume.name == "John Doe"

That's it! No need to write any more code. Since the compiler knows what your function signature is we literally generate a custom deserializer for your own unique usecase that just works.
Converting the Resume into an array of resumes requires a single line change in BAML (vs having to create array wrapper classes and parsing logic).
In this image we change the types and BAML automatically updates the prompt, parser, and the Python types you get back.

Adding retries or resilience requires just a couple of modifications. And best of all, you can test things instantly, without leaving your VSCode.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#enter-baml",
    "title": "Enter BAML",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "guide/comparisons/baml-vs-marvin",
        "title": "Comparisons",
      },
      {
        "slug": "guide/comparisons/baml-vs-pydantic#enter-baml",
        "title": "Enter BAML",
      },
    ],
    "description": "We built BAML because writing a Python library was just not powerful enough to do everything we envisioned, as we have just explored.
Check out the Hello World tutorial to get started.
Our mission is to make the best DX for AI engineers working with LLMs. Contact us at founders@boundaryml.com or Join us on Discord to stay in touch with the community and influence the roadmap.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#conclusion",
    "title": "Conclusion",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Comparisons",
      "Enter BAML",
    ],
    "content": "We built BAML because writing a Python library was just not powerful enough to do everything we envisioned, as we have just explored.
Check out the Hello World tutorial to get started.
Our mission is to make the best DX for AI engineers working with LLMs. Contact us at founders@boundaryml.com or Join us on Discord to stay in touch with the community and influence the roadmap.",
    "indexSegmentId": "0",
    "slug": "guide/comparisons/baml-vs-pydantic#conclusion",
    "title": "Conclusion",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "We have seen many different prompts for many use-cases. We'd love to hear about your prompt and how you use BAML.

Contact Us at [contact@boundaryml.com](mailto:contact@boundaryml.com)

or join our [Discord](https://discord.gg/BTNBeXGuaS)",
    "indexSegmentId": "0",
    "slug": "guide/contact",
    "title": "Contact",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "We have seen many different prompts for many use-cases. We'd love to hear about your prompt and how you use BAML.
Contact Us at contact@boundaryml.com
or join our Discord",
    "indexSegmentId": "0",
    "slug": "guide/contact",
    "title": "Contact",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "Check out the [live examples](https://baml-examples.vercel.app/) that use NextJS, and the [source code on Github](https://github.com/boundaryml/baml-examples).",
    "indexSegmentId": "0",
    "slug": "examples/interactive-examples",
    "title": "Interactive Examples",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "Check out the live examples that use NextJS, and the source code on Github.",
    "indexSegmentId": "0",
    "slug": "examples/interactive-examples",
    "title": "Interactive Examples",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "We recommend these simple ways to reduce hallucinations:",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations",
    "title": "Reduce Hallucinations",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "We recommend these simple ways to reduce hallucinations:
1. Set temperature to 0.0 (especially if extracting data verbatim)
This will make the model less creative and more likely to just extract the data that you want verbatim.
client<llm> MyClient {
  provider openai
  options {
    temperature 0.0
  }
}

2. Reduce the number of input tokens
Reduce the amount of data you're giving the model to process to reduce confusion.
Prune as much data as possible, or split your prompt into multiple prompts analyzing subsets of the data.
If you're processing images, try cropping the parts of the image that you don't need. LLMs can only handle images of certain sizes, so every pixel counts. Make sure you resize images to the model's input size (even if the provider does the resizing for you), so you can gauge how clear the image is at the model's resolution. You'll notice the blurrier the image is, the higher the hallucination rate.
Let us know if you want more tips for processing images, we have some helper prompts we can share with you, or help debug your prompt.
2. Use reasoning or reflection prompting
Read our chain-of-thought guide for more.
3. Watch out for contradictions and word associations
Each word you add into the prompt will cause it to associate it with something it saw before in its training data. This is why we have techniques like symbol tuning to help control this bias.
Let's say you have a prompt that says:
Answer in this JSON schema:



But when you answer, add some comments in the JSON indicating your reasoning for the field like this:

Example:
---
{
  // I used the name "John" because it's the name of the person who wrote the prompt
  "name": "John"
}

JSON:

The LLM may not write the // comment inline, because it's been trained to associate JSON with actual "valid" JSON.
You can get around this with some more coaxing like:
Answer in this JSON schema:



But when you answer, add some comments in the JSON indicating your reasoning for the field like this:
---
{
  // I used the name "John" because it's the name of the person who wrote the prompt
  "name": "John"
}

It's ok if this isn't fully valid JSON, 
we will fix it afterwards and remove the comments.

JSON:

The LLM made an assumption that you want "JSON" -- which doesn't use comments -- and our instructions were not explicit enough to override that bias originally.
Keep on reading for more tips and tricks! Or reach out in our Discord",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations",
    "title": "Reduce Hallucinations",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "This will make the model less creative and more likely to just extract the data that you want verbatim.
client<llm> MyClient {
  provider openai
  options {
    temperature 0.0
  }
}
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations#1-set-temperature-to-00-especially-if-extracting-data-verbatim",
    "title": "1. Set temperature to 0.0 (especially if extracting data verbatim)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "This will make the model less creative and more likely to just extract the data that you want verbatim.
client<llm> MyClient {
  provider openai
  options {
    temperature 0.0
  }
}
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations#1-set-temperature-to-00-especially-if-extracting-data-verbatim",
    "title": "1. Set temperature to 0.0 (especially if extracting data verbatim)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "Reduce the amount of data you're giving the model to process to reduce confusion.
Prune as much data as possible, or split your prompt into multiple prompts analyzing subsets of the data.
If you're processing images, try cropping the parts of the image that you don't need. LLMs can only handle images of certain sizes, so every pixel counts. Make sure you resize images to the model's input size (even if the provider does the resizing for you), so you can gauge how clear the image is at the model's resolution. You'll notice the blurrier the image is, the higher the hallucination rate.
Let us know if you want more tips for processing images, we have some helper prompts we can share with you, or help debug your prompt.",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations#2-reduce-the-number-of-input-tokens",
    "title": "2. Reduce the number of input tokens",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "Reduce the amount of data you're giving the model to process to reduce confusion.
Prune as much data as possible, or split your prompt into multiple prompts analyzing subsets of the data.
If you're processing images, try cropping the parts of the image that you don't need. LLMs can only handle images of certain sizes, so every pixel counts. Make sure you resize images to the model's input size (even if the provider does the resizing for you), so you can gauge how clear the image is at the model's resolution. You'll notice the blurrier the image is, the higher the hallucination rate.
Let us know if you want more tips for processing images, we have some helper prompts we can share with you, or help debug your prompt.",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations#2-reduce-the-number-of-input-tokens",
    "title": "2. Reduce the number of input tokens",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "Read our chain-of-thought guide for more.",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations#2-use-reasoning-or-reflection-prompting",
    "title": "2. Use reasoning or reflection prompting",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "Read our chain-of-thought guide for more.",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations#2-use-reasoning-or-reflection-prompting",
    "title": "2. Use reasoning or reflection prompting",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "Each word you add into the prompt will cause it to associate it with something it saw before in its training data. This is why we have techniques like symbol tuning to help control this bias.
Let's say you have a prompt that says:
Answer in this JSON schema:



But when you answer, add some comments in the JSON indicating your reasoning for the field like this:

Example:
---
{
  // I used the name "John" because it's the name of the person who wrote the prompt
  "name": "John"
}

JSON:

The LLM may not write the // comment inline, because it's been trained to associate JSON with actual "valid" JSON.
You can get around this with some more coaxing like:
Answer in this JSON schema:



But when you answer, add some comments in the JSON indicating your reasoning for the field like this:
---
{
  // I used the name "John" because it's the name of the person who wrote the prompt
  "name": "John"
}

It's ok if this isn't fully valid JSON, 
we will fix it afterwards and remove the comments.

JSON:

The LLM made an assumption that you want "JSON" -- which doesn't use comments -- and our instructions were not explicit enough to override that bias originally.
Keep on reading for more tips and tricks! Or reach out in our Discord",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations#3-watch-out-for-contradictions-and-word-associations",
    "title": "3. Watch out for contradictions and word associations",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "Each word you add into the prompt will cause it to associate it with something it saw before in its training data. This is why we have techniques like symbol tuning to help control this bias.
Let's say you have a prompt that says:
Answer in this JSON schema:



But when you answer, add some comments in the JSON indicating your reasoning for the field like this:

Example:
---
{
  // I used the name "John" because it's the name of the person who wrote the prompt
  "name": "John"
}

JSON:

The LLM may not write the // comment inline, because it's been trained to associate JSON with actual "valid" JSON.
You can get around this with some more coaxing like:
Answer in this JSON schema:



But when you answer, add some comments in the JSON indicating your reasoning for the field like this:
---
{
  // I used the name "John" because it's the name of the person who wrote the prompt
  "name": "John"
}

It's ok if this isn't fully valid JSON, 
we will fix it afterwards and remove the comments.

JSON:

The LLM made an assumption that you want "JSON" -- which doesn't use comments -- and our instructions were not explicit enough to override that bias originally.
Keep on reading for more tips and tricks! Or reach out in our Discord",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/reducing-hallucinations#3-watch-out-for-contradictions-and-word-associations",
    "title": "3. Watch out for contradictions and word associations",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "In this guide we'll build a small chatbot that takes in user messages and generates responses.


```baml chat-history.baml
class MyUserMessage {
  role "user" | "assistant"
  content string
}

function ChatWithLLM(messages: MyUserMessage[]) -> string {
  client "openai/gpt-4o"
  prompt #"
    Answer the user's questions based on the chat history:
    {% for message in messages %}
      {{ _.role(message.role) }} 
      {{ message.content }}
    {% endfor %}

    Answer:
  "#
}

test TestName {
  functions [ChatWithLLM]
  args {
    messages [
      {
        role "user"
        content "Hello!"
      }
      {
        role "assistant"
        content "Hi!"
      }
    ]
  }
}

```",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/chat",
    "title": "Chat",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "In this guide we'll build a small chatbot that takes in user messages and generates responses.
class MyUserMessage {
  role "user" | "assistant"
  content string
}

function ChatWithLLM(messages: MyUserMessage[]) -> string {
  client "openai/gpt-4o"
  prompt #"
    Answer the user's questions based on the chat history:
    {% for message in messages %}
      {{ _.role(message.role) }} 
      {{ message.content }}
    {% endfor %}

    Answer:
  "#
}

test TestName {
  functions [ChatWithLLM]
  args {
    messages [
      {
        role "user"
        content "Hello!"
      }
      {
        role "assistant"
        content "Hi!"
      }
    ]
  }
}


Code
from baml_client import b
from baml_client.types import MyUserMessage

def main():
    messages: list[MyUserMessage] = []
    
    while True:
        content = input("Enter your message (or 'quit' to exit): ")
        if content.lower() == 'quit':
            break
        
        messages.append(MyUserMessage(role="user", content=content))
        
        agent_response = b.ChatWithLLM(messages=messages)
        print(f"AI: {agent_response}")
        print()
        
        # Add the agent's response to the chat history
        messages.append(MyUserMessage(role="assistant", content=agent_response))

if __name__ == "__main__":
    main()
import { b, MyUserMessage } from 'baml_client';
import * as readline from 'readline';

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

const messages: MyUserMessage[] = [];

function askQuestion(query: string): Promise<string> {
  return new Promise((resolve) => {
    rl.question(query, resolve);
  });
}

async function main() {

  while (true) {
    const content = await askQuestion("Enter your message (or 'quit' to exit): ");
    if (content.toLowerCase() === 'quit') {
      break;
    }

    messages.push({ role: "user", content });

    const agentResponse = await b.ChatWithLLM({ messages });
    console.log(`AI: ${agentResponse}`);
    console.log();

    // Add the agent's response to the chat history
    messages.push({ role: "assistant", content: agentResponse });
  }

  rl.close();
}

main();
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/chat",
    "title": "Chat",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "from baml_client import b
from baml_client.types import MyUserMessage

def main():
    messages: list[MyUserMessage] = []
    
    while True:
        content = input("Enter your message (or 'quit' to exit): ")
        if content.lower() == 'quit':
            break
        
        messages.append(MyUserMessage(role="user", content=content))
        
        agent_response = b.ChatWithLLM(messages=messages)
        print(f"AI: {agent_response}")
        print()
        
        # Add the agent's response to the chat history
        messages.append(MyUserMessage(role="assistant", content=agent_response))

if __name__ == "__main__":
    main()
import { b, MyUserMessage } from 'baml_client';
import * as readline from 'readline';

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

const messages: MyUserMessage[] = [];

function askQuestion(query: string): Promise<string> {
  return new Promise((resolve) => {
    rl.question(query, resolve);
  });
}

async function main() {

  while (true) {
    const content = await askQuestion("Enter your message (or 'quit' to exit): ");
    if (content.toLowerCase() === 'quit') {
      break;
    }

    messages.push({ role: "user", content });

    const agentResponse = await b.ChatWithLLM({ messages });
    console.log(`AI: ${agentResponse}`);
    console.log();

    // Add the agent's response to the chat history
    messages.push({ role: "assistant", content: agentResponse });
  }

  rl.close();
}

main();
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/chat#code",
    "title": "Code",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "from baml_client import b
from baml_client.types import MyUserMessage

def main():
    messages: list[MyUserMessage] = []
    
    while True:
        content = input("Enter your message (or 'quit' to exit): ")
        if content.lower() == 'quit':
            break
        
        messages.append(MyUserMessage(role="user", content=content))
        
        agent_response = b.ChatWithLLM(messages=messages)
        print(f"AI: {agent_response}")
        print()
        
        # Add the agent's response to the chat history
        messages.append(MyUserMessage(role="assistant", content=agent_response))

if __name__ == "__main__":
    main()
import { b, MyUserMessage } from 'baml_client';
import * as readline from 'readline';

const rl = readline.createInterface({
  input: process.stdin,
  output: process.stdout
});

const messages: MyUserMessage[] = [];

function askQuestion(query: string): Promise<string> {
  return new Promise((resolve) => {
    rl.question(query, resolve);
  });
}

async function main() {

  while (true) {
    const content = await askQuestion("Enter your message (or 'quit' to exit): ");
    if (content.toLowerCase() === 'quit') {
      break;
    }

    messages.push({ role: "user", content });

    const agentResponse = await b.ChatWithLLM({ messages });
    console.log(`AI: ${agentResponse}`);
    console.log();

    // Add the agent's response to the chat history
    messages.push({ role: "assistant", content: agentResponse });
  }

  rl.close();
}

main();
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/chat#code",
    "title": "Code",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": ""Function calling" is a technique for getting an LLM to choose a function to call for you.

The way it works is:
1. You define a task with certain function(s)
2. Ask the LLM to **choose which function to call**
3. **Get the function parameters from the LLM** for the appropriate function it choose
4. **Call the functions** in your code with those parameters

In BAML, you can get represent a `tool` or a `function` you want to call as a BAML `class`, and make the function output be that class definition.

```baml BAML
class WeatherAPI {
  city string @description("the user's city")
  timeOfDay string @description("As an ISO8601 timestamp")
}

function UseTool(user_message: string) -> WeatherAPI {
  client GPT4Turbo
  prompt #"
    Extract the info from this message
    ---
    {{ user_message }}
    ---

    {# special macro to print the output schema. #}
    {{ ctx.output_format }}

    JSON:
  "#
}
```
Call the function like this:

<CodeGroup>
```python Python
import asyncio
from baml_client import b
from baml_client.types import WeatherAPI

def main():
    weather_info = b.UseTool("What's the weather like in San Francisco?")
    print(weather_info)
    assert isinstance(weather_info, WeatherAPI)
    print(f"City: {weather_info.city}")
    print(f"Time of Day: {weather_info.timeOfDay}")

if __name__ == '__main__':
    main()
```

```typescript TypeScript
import { b } from './baml_client'
import { WeatherAPI } from './baml_client/types'
import assert from 'assert'

const main = async () => {
  const weatherInfo = await b.UseTool("What's the weather like in San Francisco?")
  console.log(weatherInfo)
  assert(weatherInfo instanceof WeatherAPI)
  console.log(`City: ${weatherInfo.city}`)
  console.log(`Time of Day: ${weatherInfo.timeOfDay}`)
}
```

```ruby Ruby
require_relative "baml_client/client"

$b = Baml.Client

def main
  weather_info = $b.UseTool(user_message: "What's the weather like in San Francisco?")
  puts weather_info
  raise unless weather_info.is_a?(Baml::Types::WeatherAPI)
  puts "City: #{weather_info.city}"
  puts "Time of Day: #{weather_info.timeOfDay}"
end
```
</CodeGroup>",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/tools-function-calling",
    "title": "Tools / Function Calling",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": ""Function calling" is a technique for getting an LLM to choose a function to call for you.
The way it works is:

You define a task with certain function(s)
Ask the LLM to choose which function to call
Get the function parameters from the LLM for the appropriate function it choose
Call the functions in your code with those parameters

In BAML, you can get represent a tool or a function you want to call as a BAML class, and make the function output be that class definition.
class WeatherAPI {
  city string @description("the user's city")
  timeOfDay string @description("As an ISO8601 timestamp")
}

function UseTool(user_message: string) -> WeatherAPI {
  client GPT4Turbo
  prompt #"
    Extract the info from this message
    ---
    {{ user_message }}
    ---

    {# special macro to print the output schema. #}
    {{ ctx.output_format }}

    JSON:
  "#
}

Call the function like this:
import asyncio
from baml_client import b
from baml_client.types import WeatherAPI

def main():
    weather_info = b.UseTool("What's the weather like in San Francisco?")
    print(weather_info)
    assert isinstance(weather_info, WeatherAPI)
    print(f"City: {weather_info.city}")
    print(f"Time of Day: {weather_info.timeOfDay}")

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI } from './baml_client/types'
import assert from 'assert'

const main = async () => {
  const weatherInfo = await b.UseTool("What's the weather like in San Francisco?")
  console.log(weatherInfo)
  assert(weatherInfo instanceof WeatherAPI)
  console.log(`City: ${weatherInfo.city}`)
  console.log(`Time of Day: ${weatherInfo.timeOfDay}`)
}
require_relative "baml_client/client"

$b = Baml.Client

def main
  weather_info = $b.UseTool(user_message: "What's the weather like in San Francisco?")
  puts weather_info
  raise unless weather_info.is_a?(Baml::Types::WeatherAPI)
  puts "City: #{weather_info.city}"
  puts "Time of Day: #{weather_info.timeOfDay}"
end

Choosing multiple Tools
To choose ONE tool out of many, you can use a union:
function UseTool(user_message: string) -> WeatherAPI | MyOtherAPI {
  .... // same thing
}

If you use VSCode Playground, you can see what we inject into the prompt, with full transparency.
Call the function like this:
import asyncio
from baml_client import b
from baml_client.types import WeatherAPI, MyOtherAPI

async def main():
    tool = b.UseTool("What's the weather like in San Francisco?")
    print(tool)
    
    if isinstance(tool, WeatherAPI):
        print(f"Weather API called:")
        print(f"City: {tool.city}")
        print(f"Time of Day: {tool.timeOfDay}")
    elif isinstance(tool, MyOtherAPI):
        print(f"MyOtherAPI called:")
        # Handle MyOtherAPI specific attributes here

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI, MyOtherAPI } from './baml_client/types'

const main = async () => {
  const tool = await b.UseTool("What's the weather like in San Francisco?")
  console.log(tool)
  
  if (tool instanceof WeatherAPI) {
    console.log("Weather API called:")
    console.log(`City: ${tool.city}`)
    console.log(`Time of Day: ${tool.timeOfDay}`)
  } else if (tool instanceof MyOtherAPI) {
    console.log("MyOtherAPI called:")
    // Handle MyOtherAPI specific attributes here
  }
}

main()
require_relative "baml_client/client"

$b = Baml.Client

def main
  tool = $b.UseTool(user_message: "What's the weather like in San Francisco?")
  puts tool
  
  case tool
  when Baml::Types::WeatherAPI
    puts "Weather API called:"
    puts "City: #{tool.city}"
    puts "Time of Day: #{tool.timeOfDay}"
  when Baml::Types::MyOtherAPI
    puts "MyOtherAPI called:"
    # Handle MyOtherAPI specific attributes here
  end
end

main

Choosing N Tools
To choose many tools, you can use a union of a list:
function UseTool(user_message: string) -> (WeatherAPI | MyOtherAPI)[] {
  .... // same thing
}

Call the function like this:
import asyncio
from baml_client import b
from baml_client.types import WeatherAPI, MyOtherAPI

async def main():
    tools = b.UseTool("What's the weather like in San Francisco and New York?")
    print(tools)  
    
    for tool in tools:
        if isinstance(tool, WeatherAPI):
            print(f"Weather API called:")
            print(f"City: {tool.city}")
            print(f"Time of Day: {tool.timeOfDay}")
        elif isinstance(tool, MyOtherAPI):
            print(f"MyOtherAPI called:")
            # Handle MyOtherAPI specific attributes here

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI, MyOtherAPI } from './baml_client/types'

const main = async () => {
  const tools = await b.UseTool("What's the weather like in San Francisco and New York?")
  console.log(tools)
  
  tools.forEach(tool => {
    if (tool instanceof WeatherAPI) {
      console.log("Weather API called:")
      console.log(`City: ${tool.city}`)
      console.log(`Time of Day: ${tool.timeOfDay}`)
    } else if (tool instanceof MyOtherAPI) {
      console.log("MyOtherAPI called:")
      // Handle MyOtherAPI specific attributes here
    }
  })
}

main()
require_relative "baml_client/client"

$b = Baml.Client

def main
  tools = $b.UseTool(user_message: "What's the weather like in San Francisco and New York?")
  puts tools
  
  tools.each do |tool|
    case tool
    when Baml::Types::WeatherAPI
      puts "Weather API called:"
      puts "City: #{tool.city}"
      puts "Time of Day: #{tool.timeOfDay}"
    when Baml::Types::MyOtherAPI
      puts "MyOtherAPI called:"
      # Handle MyOtherAPI specific attributes here
    end
  end
end

main

Function-calling APIs vs Prompting
Injecting your function schemas into the prompt, as BAML does, outperforms function-calling across all benchmarks for major providers (see our Berkeley FC Benchmark results with BAML).
Amongst other limitations, function-calling APIs will at times:

Return a schema when you don't want any (you want an error)
Not work for tools with more than 100 parameters.
Use many more tokens than prompting.

Keep in mind that "JSON mode" is nearly the same thing as "prompting", but it enforces the LLM response is ONLY a JSON blob.
BAML does not use JSON mode since it allows developers to use better prompting techniques like chain-of-thought, to allow the LLM to express its reasoning before printing out the actual schema. BAML's parser can find the json schema(s) out of free-form text for you. Read more about different approaches to structured generation here
BAML will still support native function-calling APIs in the future (please let us know more about your use-case so we can prioritize accordingly)",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/tools-function-calling",
    "title": "Tools / Function Calling",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "To choose ONE tool out of many, you can use a union:
function UseTool(user_message: string) -> WeatherAPI | MyOtherAPI {
  .... // same thing
}

If you use VSCode Playground, you can see what we inject into the prompt, with full transparency.
Call the function like this:
import asyncio
from baml_client import b
from baml_client.types import WeatherAPI, MyOtherAPI

async def main():
    tool = b.UseTool("What's the weather like in San Francisco?")
    print(tool)
    
    if isinstance(tool, WeatherAPI):
        print(f"Weather API called:")
        print(f"City: {tool.city}")
        print(f"Time of Day: {tool.timeOfDay}")
    elif isinstance(tool, MyOtherAPI):
        print(f"MyOtherAPI called:")
        # Handle MyOtherAPI specific attributes here

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI, MyOtherAPI } from './baml_client/types'

const main = async () => {
  const tool = await b.UseTool("What's the weather like in San Francisco?")
  console.log(tool)
  
  if (tool instanceof WeatherAPI) {
    console.log("Weather API called:")
    console.log(`City: ${tool.city}`)
    console.log(`Time of Day: ${tool.timeOfDay}`)
  } else if (tool instanceof MyOtherAPI) {
    console.log("MyOtherAPI called:")
    // Handle MyOtherAPI specific attributes here
  }
}

main()
require_relative "baml_client/client"

$b = Baml.Client

def main
  tool = $b.UseTool(user_message: "What's the weather like in San Francisco?")
  puts tool
  
  case tool
  when Baml::Types::WeatherAPI
    puts "Weather API called:"
    puts "City: #{tool.city}"
    puts "Time of Day: #{tool.timeOfDay}"
  when Baml::Types::MyOtherAPI
    puts "MyOtherAPI called:"
    # Handle MyOtherAPI specific attributes here
  end
end

main
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/tools-function-calling#choosing-multiple-tools",
    "title": "Choosing multiple Tools",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "To choose ONE tool out of many, you can use a union:
function UseTool(user_message: string) -> WeatherAPI | MyOtherAPI {
  .... // same thing
}

If you use VSCode Playground, you can see what we inject into the prompt, with full transparency.
Call the function like this:
import asyncio
from baml_client import b
from baml_client.types import WeatherAPI, MyOtherAPI

async def main():
    tool = b.UseTool("What's the weather like in San Francisco?")
    print(tool)
    
    if isinstance(tool, WeatherAPI):
        print(f"Weather API called:")
        print(f"City: {tool.city}")
        print(f"Time of Day: {tool.timeOfDay}")
    elif isinstance(tool, MyOtherAPI):
        print(f"MyOtherAPI called:")
        # Handle MyOtherAPI specific attributes here

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI, MyOtherAPI } from './baml_client/types'

const main = async () => {
  const tool = await b.UseTool("What's the weather like in San Francisco?")
  console.log(tool)
  
  if (tool instanceof WeatherAPI) {
    console.log("Weather API called:")
    console.log(`City: ${tool.city}`)
    console.log(`Time of Day: ${tool.timeOfDay}`)
  } else if (tool instanceof MyOtherAPI) {
    console.log("MyOtherAPI called:")
    // Handle MyOtherAPI specific attributes here
  }
}

main()
require_relative "baml_client/client"

$b = Baml.Client

def main
  tool = $b.UseTool(user_message: "What's the weather like in San Francisco?")
  puts tool
  
  case tool
  when Baml::Types::WeatherAPI
    puts "Weather API called:"
    puts "City: #{tool.city}"
    puts "Time of Day: #{tool.timeOfDay}"
  when Baml::Types::MyOtherAPI
    puts "MyOtherAPI called:"
    # Handle MyOtherAPI specific attributes here
  end
end

main
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/tools-function-calling#choosing-multiple-tools",
    "title": "Choosing multiple Tools",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "To choose many tools, you can use a union of a list:
function UseTool(user_message: string) -> (WeatherAPI | MyOtherAPI)[] {
  .... // same thing
}

Call the function like this:
import asyncio
from baml_client import b
from baml_client.types import WeatherAPI, MyOtherAPI

async def main():
    tools = b.UseTool("What's the weather like in San Francisco and New York?")
    print(tools)  
    
    for tool in tools:
        if isinstance(tool, WeatherAPI):
            print(f"Weather API called:")
            print(f"City: {tool.city}")
            print(f"Time of Day: {tool.timeOfDay}")
        elif isinstance(tool, MyOtherAPI):
            print(f"MyOtherAPI called:")
            # Handle MyOtherAPI specific attributes here

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI, MyOtherAPI } from './baml_client/types'

const main = async () => {
  const tools = await b.UseTool("What's the weather like in San Francisco and New York?")
  console.log(tools)
  
  tools.forEach(tool => {
    if (tool instanceof WeatherAPI) {
      console.log("Weather API called:")
      console.log(`City: ${tool.city}`)
      console.log(`Time of Day: ${tool.timeOfDay}`)
    } else if (tool instanceof MyOtherAPI) {
      console.log("MyOtherAPI called:")
      // Handle MyOtherAPI specific attributes here
    }
  })
}

main()
require_relative "baml_client/client"

$b = Baml.Client

def main
  tools = $b.UseTool(user_message: "What's the weather like in San Francisco and New York?")
  puts tools
  
  tools.each do |tool|
    case tool
    when Baml::Types::WeatherAPI
      puts "Weather API called:"
      puts "City: #{tool.city}"
      puts "Time of Day: #{tool.timeOfDay}"
    when Baml::Types::MyOtherAPI
      puts "MyOtherAPI called:"
      # Handle MyOtherAPI specific attributes here
    end
  end
end

main
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/tools-function-calling#choosing-n-tools",
    "title": "Choosing N Tools",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "To choose many tools, you can use a union of a list:
function UseTool(user_message: string) -> (WeatherAPI | MyOtherAPI)[] {
  .... // same thing
}

Call the function like this:
import asyncio
from baml_client import b
from baml_client.types import WeatherAPI, MyOtherAPI

async def main():
    tools = b.UseTool("What's the weather like in San Francisco and New York?")
    print(tools)  
    
    for tool in tools:
        if isinstance(tool, WeatherAPI):
            print(f"Weather API called:")
            print(f"City: {tool.city}")
            print(f"Time of Day: {tool.timeOfDay}")
        elif isinstance(tool, MyOtherAPI):
            print(f"MyOtherAPI called:")
            # Handle MyOtherAPI specific attributes here

if __name__ == '__main__':
    main()
import { b } from './baml_client'
import { WeatherAPI, MyOtherAPI } from './baml_client/types'

const main = async () => {
  const tools = await b.UseTool("What's the weather like in San Francisco and New York?")
  console.log(tools)
  
  tools.forEach(tool => {
    if (tool instanceof WeatherAPI) {
      console.log("Weather API called:")
      console.log(`City: ${tool.city}`)
      console.log(`Time of Day: ${tool.timeOfDay}`)
    } else if (tool instanceof MyOtherAPI) {
      console.log("MyOtherAPI called:")
      // Handle MyOtherAPI specific attributes here
    }
  })
}

main()
require_relative "baml_client/client"

$b = Baml.Client

def main
  tools = $b.UseTool(user_message: "What's the weather like in San Francisco and New York?")
  puts tools
  
  tools.each do |tool|
    case tool
    when Baml::Types::WeatherAPI
      puts "Weather API called:"
      puts "City: #{tool.city}"
      puts "Time of Day: #{tool.timeOfDay}"
    when Baml::Types::MyOtherAPI
      puts "MyOtherAPI called:"
      # Handle MyOtherAPI specific attributes here
    end
  end
end

main
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/tools-function-calling#choosing-n-tools",
    "title": "Choosing N Tools",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "Injecting your function schemas into the prompt, as BAML does, outperforms function-calling across all benchmarks for major providers (see our Berkeley FC Benchmark results with BAML).
Amongst other limitations, function-calling APIs will at times:

Return a schema when you don't want any (you want an error)
Not work for tools with more than 100 parameters.
Use many more tokens than prompting.

Keep in mind that "JSON mode" is nearly the same thing as "prompting", but it enforces the LLM response is ONLY a JSON blob.
BAML does not use JSON mode since it allows developers to use better prompting techniques like chain-of-thought, to allow the LLM to express its reasoning before printing out the actual schema. BAML's parser can find the json schema(s) out of free-form text for you. Read more about different approaches to structured generation here
BAML will still support native function-calling APIs in the future (please let us know more about your use-case so we can prioritize accordingly)",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/tools-function-calling#function-calling-apis-vs-prompting",
    "title": "Function-calling APIs vs Prompting",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "Injecting your function schemas into the prompt, as BAML does, outperforms function-calling across all benchmarks for major providers (see our Berkeley FC Benchmark results with BAML).
Amongst other limitations, function-calling APIs will at times:

Return a schema when you don't want any (you want an error)
Not work for tools with more than 100 parameters.
Use many more tokens than prompting.

Keep in mind that "JSON mode" is nearly the same thing as "prompting", but it enforces the LLM response is ONLY a JSON blob.
BAML does not use JSON mode since it allows developers to use better prompting techniques like chain-of-thought, to allow the LLM to express its reasoning before printing out the actual schema. BAML's parser can find the json schema(s) out of free-form text for you. Read more about different approaches to structured generation here
BAML will still support native function-calling APIs in the future (please let us know more about your use-case so we can prioritize accordingly)",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/tools-function-calling#function-calling-apis-vs-prompting",
    "title": "Function-calling APIs vs Prompting",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "Chain of thought prompting is a technique that encourages the language model to think step by step, reasoning through the problem before providing an answer. This can improve the quality of the response and make it easier to understand.


In the below example, we use chain of thought prompting to extract information from an email.

BAML will still parse the response as an `OrderInfo` object, even though there is additional text in the response.
```baml
class Email {
    subject string
    body string
    from_address string
}


class OrderInfo {
    order_status "ORDERED" | "SHIPPED" | "DELIVERED" | "CANCELLED"
    tracking_number string?
    estimated_arrival_date string?
}

function GetOrderInfo(email: Email) -> OrderInfo {
  client GPT4o
  prompt #"
    Extract the info from this email in the INPUT:

    INPUT:
    -------
    from: {{email.from_address}}
    Email Subject: {{email.subject}}
    Email Body: {{email.body}}
    -------

    {{ ctx.output_format }}

    Before you output the JSON, please explain your
    reasoning step-by-step. Here is an example on how to do this:
    'If we think step by step we can see that ...
     therefore the output JSON is:
    {
      ... the json schema ...
    }'
  "#
}

test Test1 {
  functions [GetOrderInfo]
  args {
    email {
      from_address "hello@amazon.com"
      subject "Your Amazon.com order of 'Wood Dowel Rods...' has shipped!"
      body #"
        Hi Sam, your package will arrive:
        Thurs, April 4
        Track your package:
        www.amazon.com/gp/your-account/ship-track?ie=23&orderId123

        On the way:
        Wood Dowel Rods...
        Order #113-7540940
        Ship to:
            Sam
            SEATTLE, WA

        Shipment total:
        $0.00
    "#

    }
  }
}
```",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/chain-of-thought",
    "title": "Chain of Thought Prompting",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "Chain of thought prompting is a technique that encourages the language model to think step by step, reasoning through the problem before providing an answer. This can improve the quality of the response and make it easier to understand.
In the below example, we use chain of thought prompting to extract information from an email.
BAML will still parse the response as an OrderInfo object, even though there is additional text in the response.
class Email {
    subject string
    body string
    from_address string
}


class OrderInfo {
    order_status "ORDERED" | "SHIPPED" | "DELIVERED" | "CANCELLED"
    tracking_number string?
    estimated_arrival_date string?
}

function GetOrderInfo(email: Email) -> OrderInfo {
  client GPT4o
  prompt #"
    Extract the info from this email in the INPUT:

    INPUT:
    -------
    from: {{email.from_address}}
    Email Subject: {{email.subject}}
    Email Body: {{email.body}}
    -------

    {{ ctx.output_format }}

    Before you output the JSON, please explain your
    reasoning step-by-step. Here is an example on how to do this:
    'If we think step by step we can see that ...
     therefore the output JSON is:
    {
      ... the json schema ...
    }'
  "#
}

test Test1 {
  functions [GetOrderInfo]
  args {
    email {
      from_address "hello@amazon.com"
      subject "Your Amazon.com order of 'Wood Dowel Rods...' has shipped!"
      body #"
        Hi Sam, your package will arrive:
        Thurs, April 4
        Track your package:
        www.amazon.com/gp/your-account/ship-track?ie=23&orderId123

        On the way:
        Wood Dowel Rods...
        Order #113-7540940
        Ship to:
            Sam
            SEATTLE, WA

        Shipment total:
        $0.00
    "#

    }
  }
}
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/chain-of-thought",
    "title": "Chain of Thought Prompting",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": "Aliasing field names to abstract symbols like "k1", "k2", etc. can improve classification results. This technique, known as symbol tuning, helps the LLM focus on your descriptions rather than being biased by the enum or property names themselves.

See the paper [Symbol Tuning Improves In-Context Learning in Language Models](https://arxiv.org/abs/2305.08298) for more details.

```baml
enum MyClass {
    Refund @alias("k1")
    @description("Customer wants to refund a product")

    CancelOrder @alias("k2")
    @description("Customer wants to cancel an order")

    TechnicalSupport @alias("k3")
    @description("Customer needs help with a technical issue unrelated to account creation or login")

    AccountIssue @alias("k4")
    @description("Specifically relates to account-login or account-creation")

    Question @alias("k5")
    @description("Customer has a question")
}

function ClassifyMessageWithSymbol(input: string) -> MyClass {
  client GPT4o

  prompt #"
    Classify the following INPUT into ONE
    of the following categories:

    INPUT: {{ input }}

    {{ ctx.output_format }}

    Response:
  "#
}

test Test1 {
  functions [ClassifyMessageWithSymbol]
  args {
    input "I can't access my account using my login credentials. I havent received the promised reset password email. Please help."
  }
}
```",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/symbol-tuning",
    "title": "Creating a Classification Function with Symbol Tuning",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "Aliasing field names to abstract symbols like "k1", "k2", etc. can improve classification results. This technique, known as symbol tuning, helps the LLM focus on your descriptions rather than being biased by the enum or property names themselves.
See the paper Symbol Tuning Improves In-Context Learning in Language Models for more details.
enum MyClass {
    Refund @alias("k1")
    @description("Customer wants to refund a product")

    CancelOrder @alias("k2")
    @description("Customer wants to cancel an order")

    TechnicalSupport @alias("k3")
    @description("Customer needs help with a technical issue unrelated to account creation or login")

    AccountIssue @alias("k4")
    @description("Specifically relates to account-login or account-creation")

    Question @alias("k5")
    @description("Customer has a question")
}

function ClassifyMessageWithSymbol(input: string) -> MyClass {
  client GPT4o

  prompt #"
    Classify the following INPUT into ONE
    of the following categories:

    INPUT: {{ input }}

    {{ ctx.output_format }}

    Response:
  "#
}

test Test1 {
  functions [ClassifyMessageWithSymbol]
  args {
    input "I can't access my account using my login credentials. I havent received the promised reset password email. Please help."
  }
}
",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/symbol-tuning",
    "title": "Creating a Classification Function with Symbol Tuning",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "examples/prompt-engineering/reducing-hallucinations",
        "title": "Prompt Engineering",
      },
    ],
    "description": undefined,
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/retrieval-augmented-generation",
    "title": "Retrieval Augmented Generation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Engineering",
    ],
    "content": "",
    "indexSegmentId": "0",
    "slug": "examples/prompt-engineering/retrieval-augmented-generation",
    "title": "Retrieval Augmented Generation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "Welcome to the BAML reference guide!

Here you can learn about every BAML keyword, feature, and setting.

For more in-depth explanations, we recommend reading the [Guides](/guide) first.

<Cards>
  <Card title="BAML Language" icon="fa-solid fa-language" href="/ref/baml">
    Learn everything about BAML's language features.
  </Card>

  <Card title="Prompt (Jinja) Syntax" icon="fa-solid fa-code" href="/ref/prompt-syntax">
    Learn about BAML's Jinja prompt syntax.
  </Card>

  <Card title="BAML CLI" icon="fa-solid fa-terminal" href="/ref/baml-cli">
    BAML CLI commands and flags.
  </Card>

  <Card title="VSCode Settings" icon="fa-solid fa-gears" href="/ref/editor-extension-settings">
    VSCode BAML Extension settings
  </Card>

  <Card title="LLM Clients" icon="fa-solid fa-brain" href="/ref/llm-client-providers">
    LLM clients and how to configure them.
  </Card>

  <Card title="baml_client" icon="fa-solid fa-running" href="/ref/baml-client">
    API Reference for the `baml_client` object.
  </Card>

</Cards>",
    "indexSegmentId": "0",
    "slug": "ref/overview",
    "title": "BAML Reference",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "Welcome to the BAML reference guide!
Here you can learn about every BAML keyword, feature, and setting.
For more in-depth explanations, we recommend reading the Guides first.
Learn everything about BAML's language features.Learn about BAML's Jinja prompt syntax.BAML CLI commands and flags.VSCode BAML Extension settingsLLM clients and how to configure them.API Reference for the baml_client object.",
    "indexSegmentId": "0",
    "slug": "ref/overview",
    "title": "BAML Reference",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The `init` command is used to initialize a project with BAML. It sets up the necessary directory structure and configuration files to get you started with BAML.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init",
    "title": "init",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The init command is used to initialize a project with BAML. It sets up the necessary directory structure and configuration files to get you started with BAML.
Usage
baml-cli init [OPTIONS]

Options
| Option | Description | Default |
|--------|-------------|---------|
| --dest <PATH> | Specifies where to initialize the BAML project | Current directory (.) |
| --client-type <TYPE> | Type of BAML client to generate | Guesses based on where the CLI was installed from (python/pydantic for pip, typescript for npm, etc.) |
| --openapi-client-type <TYPE> | The OpenAPI client generator to run, if --client-type=openapi | None |
Description
The init command performs the following actions:

Creates a new BAML project structure in ${DEST}/baml_src.
Creates a generators.baml file in the baml_src directory with initial configuration.
Includes some additional examples files in baml_src to get you started.

Client Types
The --client-type option allows you to specify the type of BAML client to generate. Available options include:

python/pydantic: For Python clients using Pydantic
typescript: For TypeScript clients
ruby/sorbet: For Ruby clients using Sorbet
rest/openapi: For REST clients using OpenAPI

If not specified, it uses the default from the runtime CLI configuration.
OpenAPI Client Types
When using --client-type=rest/openai, you can specify the OpenAPI client generator using the --openapi-client-type option. Some examples include:

go
java
php
ruby
rust
csharp

For a full list of supported OpenAPI client types, refer to the OpenAPI Generator documentation.
Examples


Initialize a BAML project in the current directory with default settings:
baml init



Initialize a BAML project in a specific directory:
baml init --dest /path/to/my/project



Initialize a BAML project for Python with Pydantic:
baml init --client-type python/pydantic



Initialize a BAML project for OpenAPI with a Go client:
baml init --client-type openapi --openapi-client-type go



Notes

If the destination directory already contains a baml_src directory, the command will fail to prevent overwriting existing projects.
The command attempts to infer the OpenAPI generator command based on what's available in your system PATH. It checks for openapi-generator, openapi-generator-cli, or falls back to using npx @openapitools/openapi-generator-cli.
After initialization, follow the instructions provided in the console output for language-specific setup steps.

For more information on getting started with BAML, visit the BAML documentation.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init",
    "title": "init",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "baml-cli init [OPTIONS]
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "baml-cli init [OPTIONS]
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "| Option | Description | Default |
|--------|-------------|---------|
| --dest <PATH> | Specifies where to initialize the BAML project | Current directory (.) |
| --client-type <TYPE> | Type of BAML client to generate | Guesses based on where the CLI was installed from (python/pydantic for pip, typescript for npm, etc.) |
| --openapi-client-type <TYPE> | The OpenAPI client generator to run, if --client-type=openapi | None |",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#options",
    "title": "Options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "| Option | Description | Default |
|--------|-------------|---------|
| --dest <PATH> | Specifies where to initialize the BAML project | Current directory (.) |
| --client-type <TYPE> | Type of BAML client to generate | Guesses based on where the CLI was installed from (python/pydantic for pip, typescript for npm, etc.) |
| --openapi-client-type <TYPE> | The OpenAPI client generator to run, if --client-type=openapi | None |",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#options",
    "title": "Options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The init command performs the following actions:

Creates a new BAML project structure in ${DEST}/baml_src.
Creates a generators.baml file in the baml_src directory with initial configuration.
Includes some additional examples files in baml_src to get you started.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#description",
    "title": "Description",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The init command performs the following actions:

Creates a new BAML project structure in ${DEST}/baml_src.
Creates a generators.baml file in the baml_src directory with initial configuration.
Includes some additional examples files in baml_src to get you started.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#description",
    "title": "Description",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The --client-type option allows you to specify the type of BAML client to generate. Available options include:

python/pydantic: For Python clients using Pydantic
typescript: For TypeScript clients
ruby/sorbet: For Ruby clients using Sorbet
rest/openapi: For REST clients using OpenAPI

If not specified, it uses the default from the runtime CLI configuration.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#client-types",
    "title": "Client Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The --client-type option allows you to specify the type of BAML client to generate. Available options include:

python/pydantic: For Python clients using Pydantic
typescript: For TypeScript clients
ruby/sorbet: For Ruby clients using Sorbet
rest/openapi: For REST clients using OpenAPI

If not specified, it uses the default from the runtime CLI configuration.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#client-types",
    "title": "Client Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "When using --client-type=rest/openai, you can specify the OpenAPI client generator using the --openapi-client-type option. Some examples include:

go
java
php
ruby
rust
csharp

For a full list of supported OpenAPI client types, refer to the OpenAPI Generator documentation.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#openapi-client-types",
    "title": "OpenAPI Client Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "When using --client-type=rest/openai, you can specify the OpenAPI client generator using the --openapi-client-type option. Some examples include:

go
java
php
ruby
rust
csharp

For a full list of supported OpenAPI client types, refer to the OpenAPI Generator documentation.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#openapi-client-types",
    "title": "OpenAPI Client Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "

Initialize a BAML project in the current directory with default settings:
baml init



Initialize a BAML project in a specific directory:
baml init --dest /path/to/my/project



Initialize a BAML project for Python with Pydantic:
baml init --client-type python/pydantic



Initialize a BAML project for OpenAPI with a Go client:
baml init --client-type openapi --openapi-client-type go


",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#examples",
    "title": "Examples",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "

Initialize a BAML project in the current directory with default settings:
baml init



Initialize a BAML project in a specific directory:
baml init --dest /path/to/my/project



Initialize a BAML project for Python with Pydantic:
baml init --client-type python/pydantic



Initialize a BAML project for OpenAPI with a Go client:
baml init --client-type openapi --openapi-client-type go


",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#examples",
    "title": "Examples",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "
If the destination directory already contains a baml_src directory, the command will fail to prevent overwriting existing projects.
The command attempts to infer the OpenAPI generator command based on what's available in your system PATH. It checks for openapi-generator, openapi-generator-cli, or falls back to using npx @openapitools/openapi-generator-cli.
After initialization, follow the instructions provided in the console output for language-specific setup steps.

For more information on getting started with BAML, visit the BAML documentation.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#notes",
    "title": "Notes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "
If the destination directory already contains a baml_src directory, the command will fail to prevent overwriting existing projects.
The command attempts to infer the OpenAPI generator command based on what's available in your system PATH. It checks for openapi-generator, openapi-generator-cli, or falls back to using npx @openapitools/openapi-generator-cli.
After initialization, follow the instructions provided in the console output for language-specific setup steps.

For more information on getting started with BAML, visit the BAML documentation.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/init#notes",
    "title": "Notes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The `generate` command is used to generate BAML clients based on your BAML source files. It processes the BAML configurations and creates the necessary client code for your specified output type.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate",
    "title": "generate",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The generate command is used to generate BAML clients based on your BAML source files. It processes the BAML configurations and creates the necessary client code for your specified output type.
Usage
baml-cli generate [OPTIONS]

Options
| Option | Description | Default |
|--------|-------------|---------|
| --from <PATH> | Path to the baml_src directory | ./baml_src |
| --no-version-check | Generate baml_client without checking for version mismatch | false |
Description
The generate command performs the following actions:

Finds all generators in the BAML project (usualy in generators.baml).
Ensure all generators match the CLI version.
Generate each baml_client based on the generator configurations.

Examples


Generate clients using default settings:
baml-cli generate



Generate clients from a specific directory:
baml-cli generate --from /path/to/my/baml_src



Generate clients without version check:
baml-cli generate --no-version-check



Output
The command provides informative output about the generation process:

If no clients were generated, it will suggest a configuration to add to your BAML files.
If clients were generated, it will report the number of clients generated and their locations.

Notes

If no generator configurations are found in the BAML files, the command will generate a default client based on the CLI defaults and provide instructions on how to add a generator configuration to your BAML files.
If generator configurations are found, the command will generate clients according to those configurations.
If one of the generators fails, the command will stop at that point and report the error.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate",
    "title": "generate",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "baml-cli generate [OPTIONS]
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "baml-cli generate [OPTIONS]
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "| Option | Description | Default |
|--------|-------------|---------|
| --from <PATH> | Path to the baml_src directory | ./baml_src |
| --no-version-check | Generate baml_client without checking for version mismatch | false |",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#options",
    "title": "Options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "| Option | Description | Default |
|--------|-------------|---------|
| --from <PATH> | Path to the baml_src directory | ./baml_src |
| --no-version-check | Generate baml_client without checking for version mismatch | false |",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#options",
    "title": "Options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The generate command performs the following actions:

Finds all generators in the BAML project (usualy in generators.baml).
Ensure all generators match the CLI version.
Generate each baml_client based on the generator configurations.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#description",
    "title": "Description",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The generate command performs the following actions:

Finds all generators in the BAML project (usualy in generators.baml).
Ensure all generators match the CLI version.
Generate each baml_client based on the generator configurations.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#description",
    "title": "Description",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "

Generate clients using default settings:
baml-cli generate



Generate clients from a specific directory:
baml-cli generate --from /path/to/my/baml_src



Generate clients without version check:
baml-cli generate --no-version-check


",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#examples",
    "title": "Examples",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "

Generate clients using default settings:
baml-cli generate



Generate clients from a specific directory:
baml-cli generate --from /path/to/my/baml_src



Generate clients without version check:
baml-cli generate --no-version-check


",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#examples",
    "title": "Examples",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The command provides informative output about the generation process:

If no clients were generated, it will suggest a configuration to add to your BAML files.
If clients were generated, it will report the number of clients generated and their locations.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#output",
    "title": "Output",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The command provides informative output about the generation process:

If no clients were generated, it will suggest a configuration to add to your BAML files.
If clients were generated, it will report the number of clients generated and their locations.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#output",
    "title": "Output",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "
If no generator configurations are found in the BAML files, the command will generate a default client based on the CLI defaults and provide instructions on how to add a generator configuration to your BAML files.
If generator configurations are found, the command will generate clients according to those configurations.
If one of the generators fails, the command will stop at that point and report the error.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#notes",
    "title": "Notes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "
If no generator configurations are found in the BAML files, the command will generate a default client based on the CLI defaults and provide instructions on how to add a generator configuration to your BAML files.
If generator configurations are found, the command will generate clients according to those configurations.
If one of the generators fails, the command will stop at that point and report the error.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/generate#notes",
    "title": "Notes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The `serve` command starts a BAML-over-HTTP API server that exposes your BAML functions via HTTP endpoints. This feature allows you to interact with your BAML functions through a RESTful API interface.

<Warning>
  **Warning: Preview Feature**
  
  1. You must include the `--preview` flag when running the `dev` command.
  2. Be aware that this feature is still being stabilized and may change in future releases.
</Warning>",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve",
    "title": "serve",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The serve command starts a BAML-over-HTTP API server that exposes your BAML functions via HTTP endpoints. This feature allows you to interact with your BAML functions through a RESTful API interface.
Warning: Preview Feature
You must include the --preview flag when running the dev command.
Be aware that this feature is still being stabilized and may change in future releases.

Usage
baml-cli serve [OPTIONS] --preview

If you're actively developing, you can use the dev command to include hotreload functionality:baml-cli dev [OPTIONS] --preview
See more
Options
| Option | Description | Default |
|--------|-------------|---------|
| --from <PATH> | Path to the baml_src directory | ./baml_src |
| --port <PORT> | Port to expose BAML on | 2024 |
| --no-version-check | Generate baml_client without checking for version mismatch | false |
| --preview | Enable the preview feature | |
Description
The serve command performs the following actions:

Exposes BAML functions as HTTP endpoints on the specified port.
Provides authentication middleware for secure access.

Endpoints

POST /call/:function_name: Call a BAML function

Debugging

GET /docs: Interactive API documentation (Swagger UI)
GET /openapi.json: OpenAPI specification for the BAML functions
GET /_debug/ping: Health check endpoint
GET /_debug/status: Server status and authentication check

Authentication
We support the header: x-baml-api-key
Set the BAML_PASSWORD environment variable to enable authentication.
Examples


Start the server with default settings:
baml-cli serve --preview



Start the server with a custom source directory and port:
baml-cli serve --from /path/to/my/baml_src --port 3000 --preview



Testing
To test the server, you can use the following curl commands:


Check if the server is running:
curl http://localhost:2024/_debug/ping



Call a function:
curl -X POST http://localhost:2024/call/MyFunctionName -d '{"arg1": "value1", "arg2": "value2"}'

 curl -X POST http://localhost:2024/call/MyFunctionName -H "x-baml-api-key: ${BAML_PASSWORD}" -d '{"arg1": "value1", "arg2": "value2"}'



Access the API documentation:
Open http://localhost:2024/docs in your web browser.

",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve",
    "title": "serve",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "baml-cli serve [OPTIONS] --preview

If you're actively developing, you can use the dev command to include hotreload functionality:baml-cli dev [OPTIONS] --preview
See more",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "baml-cli serve [OPTIONS] --preview

If you're actively developing, you can use the dev command to include hotreload functionality:baml-cli dev [OPTIONS] --preview
See more",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "| Option | Description | Default |
|--------|-------------|---------|
| --from <PATH> | Path to the baml_src directory | ./baml_src |
| --port <PORT> | Port to expose BAML on | 2024 |
| --no-version-check | Generate baml_client without checking for version mismatch | false |
| --preview | Enable the preview feature | |",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#options",
    "title": "Options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "| Option | Description | Default |
|--------|-------------|---------|
| --from <PATH> | Path to the baml_src directory | ./baml_src |
| --port <PORT> | Port to expose BAML on | 2024 |
| --no-version-check | Generate baml_client without checking for version mismatch | false |
| --preview | Enable the preview feature | |",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#options",
    "title": "Options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The serve command performs the following actions:

Exposes BAML functions as HTTP endpoints on the specified port.
Provides authentication middleware for secure access.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#description",
    "title": "Description",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The serve command performs the following actions:

Exposes BAML functions as HTTP endpoints on the specified port.
Provides authentication middleware for secure access.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#description",
    "title": "Description",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "
POST /call/:function_name: Call a BAML function

Debugging

GET /docs: Interactive API documentation (Swagger UI)
GET /openapi.json: OpenAPI specification for the BAML functions
GET /_debug/ping: Health check endpoint
GET /_debug/status: Server status and authentication check
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#endpoints",
    "title": "Endpoints",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "
POST /call/:function_name: Call a BAML function

Debugging

GET /docs: Interactive API documentation (Swagger UI)
GET /openapi.json: OpenAPI specification for the BAML functions
GET /_debug/ping: Health check endpoint
GET /_debug/status: Server status and authentication check
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#endpoints",
    "title": "Endpoints",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "We support the header: x-baml-api-key
Set the BAML_PASSWORD environment variable to enable authentication.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#authentication",
    "title": "Authentication",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "We support the header: x-baml-api-key
Set the BAML_PASSWORD environment variable to enable authentication.",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#authentication",
    "title": "Authentication",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "

Start the server with default settings:
baml-cli serve --preview



Start the server with a custom source directory and port:
baml-cli serve --from /path/to/my/baml_src --port 3000 --preview


",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#examples",
    "title": "Examples",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "

Start the server with default settings:
baml-cli serve --preview



Start the server with a custom source directory and port:
baml-cli serve --from /path/to/my/baml_src --port 3000 --preview


",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#examples",
    "title": "Examples",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "To test the server, you can use the following curl commands:


Check if the server is running:
curl http://localhost:2024/_debug/ping



Call a function:
curl -X POST http://localhost:2024/call/MyFunctionName -d '{"arg1": "value1", "arg2": "value2"}'

 curl -X POST http://localhost:2024/call/MyFunctionName -H "x-baml-api-key: ${BAML_PASSWORD}" -d '{"arg1": "value1", "arg2": "value2"}'



Access the API documentation:
Open http://localhost:2024/docs in your web browser.

",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#testing",
    "title": "Testing",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "To test the server, you can use the following curl commands:


Check if the server is running:
curl http://localhost:2024/_debug/ping



Call a function:
curl -X POST http://localhost:2024/call/MyFunctionName -d '{"arg1": "value1", "arg2": "value2"}'

 curl -X POST http://localhost:2024/call/MyFunctionName -H "x-baml-api-key: ${BAML_PASSWORD}" -d '{"arg1": "value1", "arg2": "value2"}'



Access the API documentation:
Open http://localhost:2024/docs in your web browser.

",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/serve#testing",
    "title": "Testing",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "The `dev` command starts a development server that watches your BAML source files for changes and automatically reloads the BAML runtime. This feature is designed to streamline the development process by providing real-time updates as you modify your BAML configurations.


<Warning>
  **Warning: Preview Feature**
  
  1. You must include the `--preview` flag when running the `dev` command.
  2. Be aware that this feature is still being stabilized and may change in future releases.
</Warning>",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/dev",
    "title": "dev",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "The dev command starts a development server that watches your BAML source files for changes and automatically reloads the BAML runtime. This feature is designed to streamline the development process by providing real-time updates as you modify your BAML configurations.
Warning: Preview Feature
You must include the --preview flag when running the dev command.
Be aware that this feature is still being stabilized and may change in future releases.

Usage
baml-cli dev [OPTIONS] --preview

Details
See the serve command for more information on the arguments.
The dev command performs the exact same functionality, but it additionally:

Watches the BAML source files for changes.
Automatically reloads the server when changes are detected.
Automatically runs any generators when changes are detected.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/dev",
    "title": "dev",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "baml-cli dev [OPTIONS] --preview
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/dev#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "baml-cli dev [OPTIONS] --preview
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/dev#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-cli/init",
        "title": "baml-cli",
      },
    ],
    "description": "See the serve command for more information on the arguments.
The dev command performs the exact same functionality, but it additionally:

Watches the BAML source files for changes.
Automatically reloads the server when changes are detected.
Automatically runs any generators when changes are detected.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/dev#details",
    "title": "Details",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml-cli",
    ],
    "content": "See the serve command for more information on the arguments.
The dev command performs the exact same functionality, but it additionally:

Watches the BAML source files for changes.
Automatically reloads the server when changes are detected.
Automatically runs any generators when changes are detected.
",
    "indexSegmentId": "0",
    "slug": "ref/baml-cli/dev#details",
    "title": "Details",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": undefined,
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/comments",
    "title": "comments",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "Single line / trailing comments
Denoted by //.
// hello there!
foo // this is a trailing comment

Docstrings
We have no special syntax for docstrings. Instead, we use comments.
Eventually, we'll support a /// syntax for docstrings which will
also be used for generating documentation in baml_client.


Comments in block strings
See Block Strings for more information.
#"
    My string. {#
        This is a comment
    #}
    hi!
"#
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/comments",
    "title": "comments",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "Denoted by //.
// hello there!
foo // this is a trailing comment
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/comments#single-line--trailing-comments",
    "title": "Single line / trailing comments",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "Denoted by //.
// hello there!
foo // this is a trailing comment
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/comments#single-line--trailing-comments",
    "title": "Single line / trailing comments",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "We have no special syntax for docstrings. Instead, we use comments.
Eventually, we'll support a /// syntax for docstrings which will
also be used for generating documentation in baml_client.

",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/comments#docstrings",
    "title": "Docstrings",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "We have no special syntax for docstrings. Instead, we use comments.
Eventually, we'll support a /// syntax for docstrings which will
also be used for generating documentation in baml_client.

",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/comments#docstrings",
    "title": "Docstrings",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "See Block Strings for more information.
#"
    My string. {#
        This is a comment
    #}
    hi!
"#
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/comments#comments-in-block-strings",
    "title": "Comments in block strings",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "See Block Strings for more information.
#"
    My string. {#
        This is a comment
    #}
    hi!
"#
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/comments#comments-in-block-strings",
    "title": "Comments in block strings",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "To set a value to an environment variable, use the following syntax:

```baml
env.YOUR_VARIABLE_NAME
```

<Warning>Environment variables with spaces in their names are not supported.</Warning>",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/enviornment-variables",
    "title": "Enviornment Variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "To set a value to an environment variable, use the following syntax:
env.YOUR_VARIABLE_NAME

Environment variables with spaces in their names are not supported.
Example
Using an environment variable for API key:
client<llm> MyCustomClient {
    provider "openai"
    options {
        model "gpt-4o-mini"
        // Set the API key using an environment variable
        api_key env.MY_SUPER_SECRET_API_KEY
    }
}

Setting Environment Variables
To set environment variables:
Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.Then you should be able to set environment variables in the settings tab.Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.BAML will expect these to be set already in your program before you import the baml_client in Python/ TS / etc.Any of the following strategies for setting env vars are compatible with BAML:
setting them in your shell before running your program
in your Dockerfile
in your next.config.js
in your Kubernetes manifest
from secrets-store.csi.k8s.io
from a secrets provider such as Infisical / Doppler
from a .env file (using dotenv cli)
using account credentials for ephemeral token generation (e.g. Vertex AI Auth Tokens)
export MY_SUPER_SECRET_API_KEY="..."
python my_program_using_baml.py
Requires BAML Version 0.57+If you don't want BAML to try to auto-load your env vars, you can call manually reset_baml_env_vars
with the current environment variables.
from baml_client import b
from baml_client import reset_baml_env_vars
import os
import dotenv

dotenv.load_dotenv()
reset_baml_env_vars(dict(os.environ))
import dotenv from 'dotenv'
// Wait to import the BAML client until after loading environment variables
import { b, resetBamlEnvVars } from 'baml-client'

dotenv.config()
resetBamlEnvVars(process.env)
require 'dotenv/load'

# Wait to import the BAML client until after loading environment variables
# reset_baml_env_vars is not yet implemented in the Ruby client
require 'baml_client'

Error Handling
Errors for unset environment variables are only thrown when the variable is accessed. If your BAML project has 15 environment variables and 1 is used for the function you are calling, only that one environment variable will be checked for existence.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/enviornment-variables",
    "title": "Enviornment Variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "Using an environment variable for API key:
client<llm> MyCustomClient {
    provider "openai"
    options {
        model "gpt-4o-mini"
        // Set the API key using an environment variable
        api_key env.MY_SUPER_SECRET_API_KEY
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/enviornment-variables#example",
    "title": "Example",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "Using an environment variable for API key:
client<llm> MyCustomClient {
    provider "openai"
    options {
        model "gpt-4o-mini"
        // Set the API key using an environment variable
        api_key env.MY_SUPER_SECRET_API_KEY
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/enviornment-variables#example",
    "title": "Example",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "To set environment variables:
Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.Then you should be able to set environment variables in the settings tab.Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.BAML will expect these to be set already in your program before you import the baml_client in Python/ TS / etc.Any of the following strategies for setting env vars are compatible with BAML:
setting them in your shell before running your program
in your Dockerfile
in your next.config.js
in your Kubernetes manifest
from secrets-store.csi.k8s.io
from a secrets provider such as Infisical / Doppler
from a .env file (using dotenv cli)
using account credentials for ephemeral token generation (e.g. Vertex AI Auth Tokens)
export MY_SUPER_SECRET_API_KEY="..."
python my_program_using_baml.py
Requires BAML Version 0.57+If you don't want BAML to try to auto-load your env vars, you can call manually reset_baml_env_vars
with the current environment variables.
from baml_client import b
from baml_client import reset_baml_env_vars
import os
import dotenv

dotenv.load_dotenv()
reset_baml_env_vars(dict(os.environ))
import dotenv from 'dotenv'
// Wait to import the BAML client until after loading environment variables
import { b, resetBamlEnvVars } from 'baml-client'

dotenv.config()
resetBamlEnvVars(process.env)
require 'dotenv/load'

# Wait to import the BAML client until after loading environment variables
# reset_baml_env_vars is not yet implemented in the Ruby client
require 'baml_client'
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/enviornment-variables#setting-environment-variables",
    "title": "Setting Environment Variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "To set environment variables:
Once you open a .baml file, in VSCode, you should see a small button over every BAML function: Open Playground.Then you should be able to set environment variables in the settings tab.Or type BAML Playground in the VSCode Command Bar (CMD + Shift + P or CTRL + Shift + P) to open the playground.BAML will expect these to be set already in your program before you import the baml_client in Python/ TS / etc.Any of the following strategies for setting env vars are compatible with BAML:
setting them in your shell before running your program
in your Dockerfile
in your next.config.js
in your Kubernetes manifest
from secrets-store.csi.k8s.io
from a secrets provider such as Infisical / Doppler
from a .env file (using dotenv cli)
using account credentials for ephemeral token generation (e.g. Vertex AI Auth Tokens)
export MY_SUPER_SECRET_API_KEY="..."
python my_program_using_baml.py
Requires BAML Version 0.57+If you don't want BAML to try to auto-load your env vars, you can call manually reset_baml_env_vars
with the current environment variables.
from baml_client import b
from baml_client import reset_baml_env_vars
import os
import dotenv

dotenv.load_dotenv()
reset_baml_env_vars(dict(os.environ))
import dotenv from 'dotenv'
// Wait to import the BAML client until after loading environment variables
import { b, resetBamlEnvVars } from 'baml-client'

dotenv.config()
resetBamlEnvVars(process.env)
require 'dotenv/load'

# Wait to import the BAML client until after loading environment variables
# reset_baml_env_vars is not yet implemented in the Ruby client
require 'baml_client'
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/enviornment-variables#setting-environment-variables",
    "title": "Setting Environment Variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "Errors for unset environment variables are only thrown when the variable is accessed. If your BAML project has 15 environment variables and 1 is used for the function you are calling, only that one environment variable will be checked for existence.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/enviornment-variables#error-handling",
    "title": "Error Handling",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "Errors for unset environment variables are only thrown when the variable is accessed. If your BAML project has 15 environment variables and 1 is used for the function you are calling, only that one environment variable will be checked for existence.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/enviornment-variables#error-handling",
    "title": "Error Handling",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "BAML treats templatized strings as first-class citizens.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string",
    "title": "string",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "BAML treats templatized strings as first-class citizens.
Quoted Strings
These is a valid inline string, which is surrounded by double quotes. They behave like regular strings in most programming languages, and can be escaped with a backslash.
These cannot have template variables or expressions inside them. Use a block string for that.
"Hello World"

"\n"

Unquoted Strings
BAML also supports simple unquoted in-line strings. The string below is valid! These are useful for simple strings such as configuration options.
Hello World

Unquoted strings may not have any of the following since they are reserved characters (note this may change in the future):

Quotes "double" or 'single'
At-signs @
Curlies 
hashtags #
Parentheses ()
Brackets []
commas ,
newlines

When in doubt, use a quoted string or a block string, but the VSCode extension will warn you if there is a parsing issue.
Block Strings
If a string is on multiple lines, it must be surrounded by #" and "#. This is called a block string.
#"
Hello
World
"#

Block strings are automatically dedented and stripped of the first and last newline. This means that the following will render the same thing as above
#"
    Hello
    World
"#

When used for templating, block strings can contain expressions and variables using Jinja syntax.
template_string Greeting(name: string) #"
  Hello {{ name }}!
"#

Escape Characters
Escaped characters are injected as is into the string.
#"\n"#

This will render as \\n in the output.
Adding a "#
To include a "# in a block string, you can prefix it with a different count of #.
###"
  #"Hello"#
"###

This will render as #"Hello"#.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string",
    "title": "string",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "These is a valid inline string, which is surrounded by double quotes. They behave like regular strings in most programming languages, and can be escaped with a backslash.
These cannot have template variables or expressions inside them. Use a block string for that.
"Hello World"

"\n"
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#quoted-strings",
    "title": "Quoted Strings",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "These is a valid inline string, which is surrounded by double quotes. They behave like regular strings in most programming languages, and can be escaped with a backslash.
These cannot have template variables or expressions inside them. Use a block string for that.
"Hello World"

"\n"
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#quoted-strings",
    "title": "Quoted Strings",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "BAML also supports simple unquoted in-line strings. The string below is valid! These are useful for simple strings such as configuration options.
Hello World

Unquoted strings may not have any of the following since they are reserved characters (note this may change in the future):

Quotes "double" or 'single'
At-signs @
Curlies 
hashtags #
Parentheses ()
Brackets []
commas ,
newlines

When in doubt, use a quoted string or a block string, but the VSCode extension will warn you if there is a parsing issue.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#unquoted-strings",
    "title": "Unquoted Strings",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "BAML also supports simple unquoted in-line strings. The string below is valid! These are useful for simple strings such as configuration options.
Hello World

Unquoted strings may not have any of the following since they are reserved characters (note this may change in the future):

Quotes "double" or 'single'
At-signs @
Curlies 
hashtags #
Parentheses ()
Brackets []
commas ,
newlines

When in doubt, use a quoted string or a block string, but the VSCode extension will warn you if there is a parsing issue.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#unquoted-strings",
    "title": "Unquoted Strings",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "If a string is on multiple lines, it must be surrounded by #" and "#. This is called a block string.
#"
Hello
World
"#

Block strings are automatically dedented and stripped of the first and last newline. This means that the following will render the same thing as above
#"
    Hello
    World
"#

When used for templating, block strings can contain expressions and variables using Jinja syntax.
template_string Greeting(name: string) #"
  Hello {{ name }}!
"#
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#block-strings",
    "title": "Block Strings",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "If a string is on multiple lines, it must be surrounded by #" and "#. This is called a block string.
#"
Hello
World
"#

Block strings are automatically dedented and stripped of the first and last newline. This means that the following will render the same thing as above
#"
    Hello
    World
"#

When used for templating, block strings can contain expressions and variables using Jinja syntax.
template_string Greeting(name: string) #"
  Hello {{ name }}!
"#
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#block-strings",
    "title": "Block Strings",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/string#block-strings",
        "title": "Block Strings",
      },
    ],
    "description": "Escaped characters are injected as is into the string.
#"\n"#

This will render as \\n in the output.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#escape-characters",
    "title": "Escape Characters",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Block Strings",
    ],
    "content": "Escaped characters are injected as is into the string.
#"\n"#

This will render as \\n in the output.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#escape-characters",
    "title": "Escape Characters",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/string#block-strings",
        "title": "Block Strings",
      },
    ],
    "description": "To include a "# in a block string, you can prefix it with a different count of #.
###"
  #"Hello"#
"###

This will render as #"Hello"#.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#adding-a-",
    "title": "Adding a "#",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Block Strings",
    ],
    "content": "To include a "# in a block string, you can prefix it with a different count of #.
###"
  #"Hello"#
"###

This will render as #"Hello"#.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/string#adding-a-",
    "title": "Adding a "#",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "Numerical values as denoted more specifically in BAML.

| Value | Description |
| --- | --- |
| `int` | Integer |
| `float` | Floating point number |


We support implicit casting of int -> float, but if you need something to explicitly be a float, use `0.0` instead of `0`.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/int-float",
    "title": "int / float",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "Numerical values as denoted more specifically in BAML.
| Value | Description |
| --- | --- |
| int | Integer |
| float | Floating point number |
We support implicit casting of int -> float, but if you need something to explicitly be a float, use 0.0 instead of 0.
Usage
function DescribeCircle(radius: int | float, pi: float?) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe a circle with a radius of {{ radius }} units.
        Include the area of the circle using pi as {{ pi or 3.14159 }}.
        
        What are some properties of the circle?
    "#
}

test CircleDescription {
    functions [DescribeCircle]
    // will be cast to int
    args { radius 5 }
}

test CircleDescription2 {
    functions [DescribeCircle]
    // will be cast to float
    args { 
        radius 5.0 
        pi 3.14
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/int-float",
    "title": "int / float",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "function DescribeCircle(radius: int | float, pi: float?) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe a circle with a radius of {{ radius }} units.
        Include the area of the circle using pi as {{ pi or 3.14159 }}.
        
        What are some properties of the circle?
    "#
}

test CircleDescription {
    functions [DescribeCircle]
    // will be cast to int
    args { radius 5 }
}

test CircleDescription2 {
    functions [DescribeCircle]
    // will be cast to float
    args { 
        radius 5.0 
        pi 3.14
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/int-float#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "function DescribeCircle(radius: int | float, pi: float?) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe a circle with a radius of {{ radius }} units.
        Include the area of the circle using pi as {{ pi or 3.14159 }}.
        
        What are some properties of the circle?
    "#
}

test CircleDescription {
    functions [DescribeCircle]
    // will be cast to int
    args { radius 5 }
}

test CircleDescription2 {
    functions [DescribeCircle]
    // will be cast to float
    args { 
        radius 5.0 
        pi 3.14
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/int-float#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "`true` or `false`",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/bool",
    "title": "bool",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "true or false
Usage
function CreateStory(long: bool) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Write a story that is {{ "10 paragraphs" if long else "1 paragraph" }} long.
    "#
}

test LongStory {
    functions [CreateStory]
    args { long true }
}

test ShortStory {
    functions [CreateStory]
    args { long false }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/bool",
    "title": "bool",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "function CreateStory(long: bool) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Write a story that is {{ "10 paragraphs" if long else "1 paragraph" }} long.
    "#
}

test LongStory {
    functions [CreateStory]
    args { long true }
}

test ShortStory {
    functions [CreateStory]
    args { long false }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/bool#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "function CreateStory(long: bool) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Write a story that is {{ "10 paragraphs" if long else "1 paragraph" }} long.
    "#
}

test LongStory {
    functions [CreateStory]
    args { long true }
}

test ShortStory {
    functions [CreateStory]
    args { long false }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/bool#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "Allow you to store and manipulate collections of data. They can be declared in a concise and readable manner, supporting both single-line and multi-line formats.",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list",
    "title": "array (list)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "Allow you to store and manipulate collections of data. They can be declared in a concise and readable manner, supporting both single-line and multi-line formats.
Syntax
To declare an array in a BAML file, you can use the following syntax:
{
  key1 [value1, value2, value3],
  key2 [
    value1,
    value2,
    value3
  ],
  key3 [
    {
      subkey1 "valueA",
      subkey2 "valueB"
    },
    {
      subkey1 "valueC",
      subkey2 "valueD"
    }
  ]
}

Key Points:

Commas: Optional for multi-line arrays, but recommended for clarity.
Nested Arrays: Supported, allowing complex data structures.
Key-Value Pairs: Arrays can contain objects with key-value pairs.

Usage Examples
Example 1: Simple Array
function DescriptionGame(items: string[]) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        What 3 words best describe all of these: {{ items }}.
    "#
}

test FruitList {
    functions [DescriptionGame]
    args { items ["apple", "banana", "cherry"] }
}

Example 2: Multi-line Array
test CityDescription {
    functions [DescriptionGame]
    args { items [
            "New York",
            "Los Angeles",
            "Chicago"
        ]
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list",
    "title": "array (list)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "To declare an array in a BAML file, you can use the following syntax:
{
  key1 [value1, value2, value3],
  key2 [
    value1,
    value2,
    value3
  ],
  key3 [
    {
      subkey1 "valueA",
      subkey2 "valueB"
    },
    {
      subkey1 "valueC",
      subkey2 "valueD"
    }
  ]
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list#syntax",
    "title": "Syntax",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "To declare an array in a BAML file, you can use the following syntax:
{
  key1 [value1, value2, value3],
  key2 [
    value1,
    value2,
    value3
  ],
  key3 [
    {
      subkey1 "valueA",
      subkey2 "valueB"
    },
    {
      subkey1 "valueC",
      subkey2 "valueD"
    }
  ]
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list#syntax",
    "title": "Syntax",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/array-list#syntax",
        "title": "Syntax",
      },
    ],
    "description": "
Commas: Optional for multi-line arrays, but recommended for clarity.
Nested Arrays: Supported, allowing complex data structures.
Key-Value Pairs: Arrays can contain objects with key-value pairs.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list#key-points",
    "title": "Key Points:",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Syntax",
    ],
    "content": "
Commas: Optional for multi-line arrays, but recommended for clarity.
Nested Arrays: Supported, allowing complex data structures.
Key-Value Pairs: Arrays can contain objects with key-value pairs.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list#key-points",
    "title": "Key Points:",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/array-list#usage-examples",
        "title": "Usage Examples",
      },
    ],
    "description": "function DescriptionGame(items: string[]) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        What 3 words best describe all of these: {{ items }}.
    "#
}

test FruitList {
    functions [DescriptionGame]
    args { items ["apple", "banana", "cherry"] }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list#example-1-simple-array",
    "title": "Example 1: Simple Array",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Usage Examples",
    ],
    "content": "function DescriptionGame(items: string[]) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        What 3 words best describe all of these: {{ items }}.
    "#
}

test FruitList {
    functions [DescriptionGame]
    args { items ["apple", "banana", "cherry"] }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list#example-1-simple-array",
    "title": "Example 1: Simple Array",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/array-list#usage-examples",
        "title": "Usage Examples",
      },
    ],
    "description": "test CityDescription {
    functions [DescriptionGame]
    args { items [
            "New York",
            "Los Angeles",
            "Chicago"
        ]
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list#example-2-multi-line-array",
    "title": "Example 2: Multi-line Array",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Usage Examples",
    ],
    "content": "test CityDescription {
    functions [DescriptionGame]
    args { items [
            "New York",
            "Los Angeles",
            "Chicago"
        ]
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/array-list#example-2-multi-line-array",
    "title": "Example 2: Multi-line Array",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "Map values (AKA Dictionaries) allow you to store key-value pairs.

<Tip>Most of BAML (clients, tests, classes, etc) is represented as a map.</Tip>",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary",
    "title": "map (dictionary)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "Map values (AKA Dictionaries) allow you to store key-value pairs.
Most of BAML (clients, tests, classes, etc) is represented as a map.
Syntax
To declare a map in a BAML file, you can use the following syntax:
{
  key1 value1,
  key2 {
    nestedKey1 nestedValue1,
    nestedKey2 nestedValue2
  }
}

Key Points:

Colons: Not used in BAML maps; keys and values are separated by spaces.
Value Types: Maps can contain unquoted or quoted strings, booleans, numbers, and nested maps as values.
Classes: Classes in BAML are represented as maps with keys and values.

Usage Examples
Example 1: Simple Map

class Person {
    name string
    age int
    isEmployed bool
}

function DescribePerson(person: Person) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the person with the following details: {{ person }}.
    "#
}

test PersonDescription {
    functions [DescribePerson]
    args { 
        person {
            name "John Doe",
            age 30,
            isEmployed true
        }
    }
}

Example 2: Nested Map

class Company {
    name string
    location map<string, string>
    employeeCount int
}

function DescribeCompany(company: Company) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the company with the following details: {{ company }}.
    "#
}

test CompanyDescription {
    functions [DescribeCompany]
    args { 
        company {
            name "TechCorp",
            location {
                city "San Francisco",
                state "California"
            },
            employeeCount 500
        }
    }
}

Example 3: Map with Multiline String
class Project {
    title string
    description string
}

function DescribeProject(project: Project) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the project with the following details: {{ project }}.
    "#
}

test ProjectDescription {
    functions [DescribeProject]
    args { 
        project {
            title "AI Research",
            description #"
                This project focuses on developing
                advanced AI algorithms to improve
                machine learning capabilities.
            "#
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary",
    "title": "map (dictionary)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
    ],
    "description": "To declare a map in a BAML file, you can use the following syntax:
{
  key1 value1,
  key2 {
    nestedKey1 nestedValue1,
    nestedKey2 nestedValue2
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#syntax",
    "title": "Syntax",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
    ],
    "content": "To declare a map in a BAML file, you can use the following syntax:
{
  key1 value1,
  key2 {
    nestedKey1 nestedValue1,
    nestedKey2 nestedValue2
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#syntax",
    "title": "Syntax",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/map-dictionary#syntax",
        "title": "Syntax",
      },
    ],
    "description": "
Colons: Not used in BAML maps; keys and values are separated by spaces.
Value Types: Maps can contain unquoted or quoted strings, booleans, numbers, and nested maps as values.
Classes: Classes in BAML are represented as maps with keys and values.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#key-points",
    "title": "Key Points:",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Syntax",
    ],
    "content": "
Colons: Not used in BAML maps; keys and values are separated by spaces.
Value Types: Maps can contain unquoted or quoted strings, booleans, numbers, and nested maps as values.
Classes: Classes in BAML are represented as maps with keys and values.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#key-points",
    "title": "Key Points:",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/map-dictionary#usage-examples",
        "title": "Usage Examples",
      },
    ],
    "description": "
class Person {
    name string
    age int
    isEmployed bool
}

function DescribePerson(person: Person) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the person with the following details: {{ person }}.
    "#
}

test PersonDescription {
    functions [DescribePerson]
    args { 
        person {
            name "John Doe",
            age 30,
            isEmployed true
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#example-1-simple-map",
    "title": "Example 1: Simple Map",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Usage Examples",
    ],
    "content": "
class Person {
    name string
    age int
    isEmployed bool
}

function DescribePerson(person: Person) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the person with the following details: {{ person }}.
    "#
}

test PersonDescription {
    functions [DescribePerson]
    args { 
        person {
            name "John Doe",
            age 30,
            isEmployed true
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#example-1-simple-map",
    "title": "Example 1: Simple Map",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/map-dictionary#usage-examples",
        "title": "Usage Examples",
      },
    ],
    "description": "
class Company {
    name string
    location map<string, string>
    employeeCount int
}

function DescribeCompany(company: Company) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the company with the following details: {{ company }}.
    "#
}

test CompanyDescription {
    functions [DescribeCompany]
    args { 
        company {
            name "TechCorp",
            location {
                city "San Francisco",
                state "California"
            },
            employeeCount 500
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#example-2-nested-map",
    "title": "Example 2: Nested Map",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Usage Examples",
    ],
    "content": "
class Company {
    name string
    location map<string, string>
    employeeCount int
}

function DescribeCompany(company: Company) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the company with the following details: {{ company }}.
    "#
}

test CompanyDescription {
    functions [DescribeCompany]
    args { 
        company {
            name "TechCorp",
            location {
                city "San Francisco",
                state "California"
            },
            employeeCount 500
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#example-2-nested-map",
    "title": "Example 2: Nested Map",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "General BAML Syntax",
      },
      {
        "slug": "ref/baml/general-baml-syntax/map-dictionary#usage-examples",
        "title": "Usage Examples",
      },
    ],
    "description": "class Project {
    title string
    description string
}

function DescribeProject(project: Project) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the project with the following details: {{ project }}.
    "#
}

test ProjectDescription {
    functions [DescribeProject]
    args { 
        project {
            title "AI Research",
            description #"
                This project focuses on developing
                advanced AI algorithms to improve
                machine learning capabilities.
            "#
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#example-3-map-with-multiline-string",
    "title": "Example 3: Map with Multiline String",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "General BAML Syntax",
      "Usage Examples",
    ],
    "content": "class Project {
    title string
    description string
}

function DescribeProject(project: Project) -> string {
    client "openai/gpt-4o-mini"
    prompt #"
        Describe the project with the following details: {{ project }}.
    "#
}

test ProjectDescription {
    functions [DescribeProject]
    args { 
        project {
            title "AI Research",
            description #"
                This project focuses on developing
                advanced AI algorithms to improve
                machine learning capabilities.
            "#
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/general-baml-syntax/map-dictionary#example-3-map-with-multiline-string",
    "title": "Example 3: Map with Multiline String",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Here's a list of all the types that can be represented in BAML:",
    "indexSegmentId": "0",
    "slug": "ref/baml/types",
    "title": "Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Here's a list of all the types that can be represented in BAML:
Primitive Types

bool
int
float
string
null

Literal Types
This feature was added in: v0.61.0.
The primitive types string, int and bool can be constrained to a specific value.
For example, you can use literal values as return types:
function ClassifyIssue(issue_description: string) -> "bug" | "enhancement" {
  client GPT4Turbo
  prompt #"
    Classify the issue based on the following description:
    {{ ctx.output_format }}

    {{ _.role("user")}}
    {{ issue_description }}
  "#
}

See Union(|) for more details.
Multimodal Types
See calling a function with multimodal types
and testing image inputs
BAML's multimodal types are designed for ease of use: we have deliberately made it
easy for you to construct a image or audio instance from a URL. Under the
hood, depending on the model you're using, BAML may need to download the image
and transcode it (usually as base64) for the model to consume.This ease-of-use does come with some tradeoffs; namely, if you construct
an image or audio instance using untrusted user input, you may be exposing
yourself to server-side request forgery (SSRF) attacks. Attackers may be
able to fetch files on your internal network, on external networks using your
application's identity, or simply excessively drive up your cloud network
bandwidth bill.To prevent this, we recommend only using URLs from trusted sources/users or
validating them using allowlists or denylists.
image
You can use an image like this for models that support them:
function DescribeImage(myImg: image) -> string {
  client GPT4Turbo
  prompt #"
    {{ _.role("user")}}
    Describe the image in four words:
    {{ myImg }}
  "#
}

You cannot name a variable image at the moment as it is a reserved keyword.
Calling a function with an image type:
from baml_py import Image
from baml_client import b

async def test_image_input():
  # from URL
  res = await b.TestImageInput(
    img=Image.from_url("https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png")
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = await b.TestImageInput(
    img=Image.from_base64("image/png", image_b64)
  )
import { b } from '../baml_client'
import { Image } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestImageInput(
    Image.fromUrl('https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png'),
  )

  // Base64
  let res = await b.TestImageInput(
    Image.fromBase64('image/png', image_b64),
  )
require_relative "baml_client/client"

b = Baml.Client
Image = Baml::Image

def test_image_input
  # from URL
  res = b.TestImageInput(
    img: Image.from_url("https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png")
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = b.TestImageInput(
    img: Image.from_base64("image/png", image_b64)
  )
end

If using Pydantic, the following are valid ways to construct the Image type.{
  "url": "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
}
{
  "url": "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png",
  "media_type": "image/png"
}
{
  "base64": "iVBORw0K....",
}
{
  "base64": "iVBORw0K....",
  "media_type": "image/png"
}

audio
Example
function DescribeSound(myAudio: audio) -> string {
  client GPT4Turbo
  prompt #"
    {{ _.role("user")}}
    Describe the audio in four words:
    {{ myAudio }}
  "#
}

Calling functions that have audio types.
from baml_py import Audio
from baml_client import b

async def run():
  # from URL
  res = await b.TestAudioInput(
      audio=Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64
  b64 = "iVBORw0K...."
  res = await b.TestAudioInput(
    audio=Audio.from_base64("audio/ogg", b64)
  )
import { b } from '../baml_client'
import { Audio } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestAudioInput(
    Audio.fromUrl('https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg'),
  )

  // Base64
  const audio_base64 = ".."
  let res = await b.TestAudioInput(
    Audio.fromBase64('audio/ogg', audio_base64),
  )
  
require_relative "baml_client/client"

b = Baml.Client
Audio = Baml::Audio

def test_audio_input
  # from URL
  res = b.TestAudioInput(
      audio: Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64 image
  audio_b64 = "iVBORw0K...."
  res = b.TestAudioInput(
    audio: Audio.from_base64("audio/mp3", audio_b64)
  )
end

Composite/Structured Types
enum
See also: Enum
A user-defined type consisting of a set of named constants.
Use it when you need a model to choose from a known set of values, like in classification problems
enum Name {
  Value1
  Value2 @description("My optional description annotation")
}

If you need to add new variants, because they need to be loaded from a file or fetched dynamically
from a database, you can do this with Dynamic Types.
class
See also: Class
Classes are for user-defined complex data structures.
Use when you need an LLM to call another function (e.g. OpenAI's function calling), you can model the function's parameters as a class. You can also get models to return complex structured data by using a class.
Example:
Note that properties have no :
class Car {
  model string
  year int @description("Year of manufacture")
}

If you need to add fields to a class because some properties of your class are only
known at runtime, you can do this with Dynamic Types.
Optional (?)
A type that represents a value that might or might not be present.
Useful when a variable might not have a value and you want to explicitly handle its absence.
Syntax: Type?
Example: int? or (MyClass | int)?
Union (|)
A type that can hold one of several specified types.
This can be helpful with function calling, where you want to return different types of data depending on which function should be called.
Syntax: Type1 | Type2
Example: int | string or (int | string) | MyClass or string | MyClass | int[]
Order is important. int | string is not the same as string | int.For example, if you have a "1" string, it will be parsed as an int if
you use int | string, but as a string if you use string | int.
List/Array ([])
A collection of elements of the same type.
Syntax: Type[]
Example: string[] or (int | string)[] or int[][]

Array types can be nested to create multi-dimensional arrays
An array type cannot be optional

Map
A mapping of strings to elements of another type.
Syntax: map<string, ValueType>
Example: map<string, string>

❌ Set

Not yet supported. Use a List instead.

❌ Tuple

Not yet supported. Use a class instead.

Examples and Equivalents
Here are some examples and what their equivalents are in different languages.
Example 1
int? | string[] | MyClass
Union[Optional[int], List[str], MyClass]
(number | null) | string[] | MyClass

Example 2
string[]
List[str]
string[]

Example 3
(int | float)[]
List[Union[int, float]]
number[]

Example 4
(int? | string[] | MyClass)[]
Optional[List[Union[Optional[int], List[str], MyClass]]]
((number | null) | string[] | MyClass)[]

Example 5
"str" | 1 | false
Union[Literal["str"], Literal[1], Literal[False]]
"str" | 1 | false

⚠️ Unsupported

any/json - Not supported. We don't want to encourage its use as it defeats the purpose of having a type system. if you really need it, for now use string and call json.parse yourself or use dynamic types
datetime - Not yet supported. Use a string instead.
duration - Not yet supported. We recommend using string and specifying that it must be an "ISO8601 duration" in the description, which you can parse yourself into a duration.
units (currency, temperature) - Not yet supported. Use a number (int or float) and have the unit be part of the variable name. For example, temperature_fahrenheit and cost_usd (see @alias)
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types",
    "title": "Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "
bool
int
float
string
null
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#primitive-types",
    "title": "Primitive Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "
bool
int
float
string
null
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#primitive-types",
    "title": "Primitive Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "This feature was added in: v0.61.0.
The primitive types string, int and bool can be constrained to a specific value.
For example, you can use literal values as return types:
function ClassifyIssue(issue_description: string) -> "bug" | "enhancement" {
  client GPT4Turbo
  prompt #"
    Classify the issue based on the following description:
    {{ ctx.output_format }}

    {{ _.role("user")}}
    {{ issue_description }}
  "#
}

See Union(|) for more details.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#literal-types",
    "title": "Literal Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "This feature was added in: v0.61.0.
The primitive types string, int and bool can be constrained to a specific value.
For example, you can use literal values as return types:
function ClassifyIssue(issue_description: string) -> "bug" | "enhancement" {
  client GPT4Turbo
  prompt #"
    Classify the issue based on the following description:
    {{ ctx.output_format }}

    {{ _.role("user")}}
    {{ issue_description }}
  "#
}

See Union(|) for more details.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#literal-types",
    "title": "Literal Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "See calling a function with multimodal types
and testing image inputs
BAML's multimodal types are designed for ease of use: we have deliberately made it
easy for you to construct a image or audio instance from a URL. Under the
hood, depending on the model you're using, BAML may need to download the image
and transcode it (usually as base64) for the model to consume.This ease-of-use does come with some tradeoffs; namely, if you construct
an image or audio instance using untrusted user input, you may be exposing
yourself to server-side request forgery (SSRF) attacks. Attackers may be
able to fetch files on your internal network, on external networks using your
application's identity, or simply excessively drive up your cloud network
bandwidth bill.To prevent this, we recommend only using URLs from trusted sources/users or
validating them using allowlists or denylists.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#multimodal-types",
    "title": "Multimodal Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "See calling a function with multimodal types
and testing image inputs
BAML's multimodal types are designed for ease of use: we have deliberately made it
easy for you to construct a image or audio instance from a URL. Under the
hood, depending on the model you're using, BAML may need to download the image
and transcode it (usually as base64) for the model to consume.This ease-of-use does come with some tradeoffs; namely, if you construct
an image or audio instance using untrusted user input, you may be exposing
yourself to server-side request forgery (SSRF) attacks. Attackers may be
able to fetch files on your internal network, on external networks using your
application's identity, or simply excessively drive up your cloud network
bandwidth bill.To prevent this, we recommend only using URLs from trusted sources/users or
validating them using allowlists or denylists.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#multimodal-types",
    "title": "Multimodal Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#multimodal-types",
        "title": "Multimodal Types",
      },
    ],
    "description": "You can use an image like this for models that support them:
function DescribeImage(myImg: image) -> string {
  client GPT4Turbo
  prompt #"
    {{ _.role("user")}}
    Describe the image in four words:
    {{ myImg }}
  "#
}

You cannot name a variable image at the moment as it is a reserved keyword.
Calling a function with an image type:
from baml_py import Image
from baml_client import b

async def test_image_input():
  # from URL
  res = await b.TestImageInput(
    img=Image.from_url("https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png")
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = await b.TestImageInput(
    img=Image.from_base64("image/png", image_b64)
  )
import { b } from '../baml_client'
import { Image } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestImageInput(
    Image.fromUrl('https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png'),
  )

  // Base64
  let res = await b.TestImageInput(
    Image.fromBase64('image/png', image_b64),
  )
require_relative "baml_client/client"

b = Baml.Client
Image = Baml::Image

def test_image_input
  # from URL
  res = b.TestImageInput(
    img: Image.from_url("https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png")
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = b.TestImageInput(
    img: Image.from_base64("image/png", image_b64)
  )
end

If using Pydantic, the following are valid ways to construct the Image type.{
  "url": "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
}
{
  "url": "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png",
  "media_type": "image/png"
}
{
  "base64": "iVBORw0K....",
}
{
  "base64": "iVBORw0K....",
  "media_type": "image/png"
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#image",
    "title": "image",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Multimodal Types",
    ],
    "content": "You can use an image like this for models that support them:
function DescribeImage(myImg: image) -> string {
  client GPT4Turbo
  prompt #"
    {{ _.role("user")}}
    Describe the image in four words:
    {{ myImg }}
  "#
}

You cannot name a variable image at the moment as it is a reserved keyword.
Calling a function with an image type:
from baml_py import Image
from baml_client import b

async def test_image_input():
  # from URL
  res = await b.TestImageInput(
    img=Image.from_url("https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png")
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = await b.TestImageInput(
    img=Image.from_base64("image/png", image_b64)
  )
import { b } from '../baml_client'
import { Image } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestImageInput(
    Image.fromUrl('https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png'),
  )

  // Base64
  let res = await b.TestImageInput(
    Image.fromBase64('image/png', image_b64),
  )
require_relative "baml_client/client"

b = Baml.Client
Image = Baml::Image

def test_image_input
  # from URL
  res = b.TestImageInput(
    img: Image.from_url("https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png")
  )

  # Base64 image
  image_b64 = "iVBORw0K...."
  res = b.TestImageInput(
    img: Image.from_base64("image/png", image_b64)
  )
end

If using Pydantic, the following are valid ways to construct the Image type.{
  "url": "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png"
}
{
  "url": "https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png",
  "media_type": "image/png"
}
{
  "base64": "iVBORw0K....",
}
{
  "base64": "iVBORw0K....",
  "media_type": "image/png"
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#image",
    "title": "image",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#multimodal-types",
        "title": "Multimodal Types",
      },
    ],
    "description": "Example
function DescribeSound(myAudio: audio) -> string {
  client GPT4Turbo
  prompt #"
    {{ _.role("user")}}
    Describe the audio in four words:
    {{ myAudio }}
  "#
}

Calling functions that have audio types.
from baml_py import Audio
from baml_client import b

async def run():
  # from URL
  res = await b.TestAudioInput(
      audio=Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64
  b64 = "iVBORw0K...."
  res = await b.TestAudioInput(
    audio=Audio.from_base64("audio/ogg", b64)
  )
import { b } from '../baml_client'
import { Audio } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestAudioInput(
    Audio.fromUrl('https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg'),
  )

  // Base64
  const audio_base64 = ".."
  let res = await b.TestAudioInput(
    Audio.fromBase64('audio/ogg', audio_base64),
  )
  
require_relative "baml_client/client"

b = Baml.Client
Audio = Baml::Audio

def test_audio_input
  # from URL
  res = b.TestAudioInput(
      audio: Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64 image
  audio_b64 = "iVBORw0K...."
  res = b.TestAudioInput(
    audio: Audio.from_base64("audio/mp3", audio_b64)
  )
end
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#audio",
    "title": "audio",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Multimodal Types",
    ],
    "content": "Example
function DescribeSound(myAudio: audio) -> string {
  client GPT4Turbo
  prompt #"
    {{ _.role("user")}}
    Describe the audio in four words:
    {{ myAudio }}
  "#
}

Calling functions that have audio types.
from baml_py import Audio
from baml_client import b

async def run():
  # from URL
  res = await b.TestAudioInput(
      audio=Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64
  b64 = "iVBORw0K...."
  res = await b.TestAudioInput(
    audio=Audio.from_base64("audio/ogg", b64)
  )
import { b } from '../baml_client'
import { Audio } from "@boundaryml/baml"
...

  // URL
  let res = await b.TestAudioInput(
    Audio.fromUrl('https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg'),
  )

  // Base64
  const audio_base64 = ".."
  let res = await b.TestAudioInput(
    Audio.fromBase64('audio/ogg', audio_base64),
  )
  
require_relative "baml_client/client"

b = Baml.Client
Audio = Baml::Audio

def test_audio_input
  # from URL
  res = b.TestAudioInput(
      audio: Audio.from_url(
          "https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg"
      )
  )

  # Base64 image
  audio_b64 = "iVBORw0K...."
  res = b.TestAudioInput(
    audio: Audio.from_base64("audio/mp3", audio_b64)
  )
end
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#audio",
    "title": "audio",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#compositestructured-types",
        "title": "Composite/Structured Types",
      },
    ],
    "description": "See also: Enum
A user-defined type consisting of a set of named constants.
Use it when you need a model to choose from a known set of values, like in classification problems
enum Name {
  Value1
  Value2 @description("My optional description annotation")
}

If you need to add new variants, because they need to be loaded from a file or fetched dynamically
from a database, you can do this with Dynamic Types.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#enum",
    "title": "enum",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Composite/Structured Types",
    ],
    "content": "See also: Enum
A user-defined type consisting of a set of named constants.
Use it when you need a model to choose from a known set of values, like in classification problems
enum Name {
  Value1
  Value2 @description("My optional description annotation")
}

If you need to add new variants, because they need to be loaded from a file or fetched dynamically
from a database, you can do this with Dynamic Types.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#enum",
    "title": "enum",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#compositestructured-types",
        "title": "Composite/Structured Types",
      },
    ],
    "description": "See also: Class
Classes are for user-defined complex data structures.
Use when you need an LLM to call another function (e.g. OpenAI's function calling), you can model the function's parameters as a class. You can also get models to return complex structured data by using a class.
Example:
Note that properties have no :
class Car {
  model string
  year int @description("Year of manufacture")
}

If you need to add fields to a class because some properties of your class are only
known at runtime, you can do this with Dynamic Types.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#class",
    "title": "class",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Composite/Structured Types",
    ],
    "content": "See also: Class
Classes are for user-defined complex data structures.
Use when you need an LLM to call another function (e.g. OpenAI's function calling), you can model the function's parameters as a class. You can also get models to return complex structured data by using a class.
Example:
Note that properties have no :
class Car {
  model string
  year int @description("Year of manufacture")
}

If you need to add fields to a class because some properties of your class are only
known at runtime, you can do this with Dynamic Types.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#class",
    "title": "class",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#compositestructured-types",
        "title": "Composite/Structured Types",
      },
    ],
    "description": "A type that represents a value that might or might not be present.
Useful when a variable might not have a value and you want to explicitly handle its absence.
Syntax: Type?
Example: int? or (MyClass | int)?",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#optional-",
    "title": "Optional (?)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Composite/Structured Types",
    ],
    "content": "A type that represents a value that might or might not be present.
Useful when a variable might not have a value and you want to explicitly handle its absence.
Syntax: Type?
Example: int? or (MyClass | int)?",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#optional-",
    "title": "Optional (?)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#compositestructured-types",
        "title": "Composite/Structured Types",
      },
    ],
    "description": "A type that can hold one of several specified types.
This can be helpful with function calling, where you want to return different types of data depending on which function should be called.
Syntax: Type1 | Type2
Example: int | string or (int | string) | MyClass or string | MyClass | int[]
Order is important. int | string is not the same as string | int.For example, if you have a "1" string, it will be parsed as an int if
you use int | string, but as a string if you use string | int.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#union-",
    "title": "Union (|)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Composite/Structured Types",
    ],
    "content": "A type that can hold one of several specified types.
This can be helpful with function calling, where you want to return different types of data depending on which function should be called.
Syntax: Type1 | Type2
Example: int | string or (int | string) | MyClass or string | MyClass | int[]
Order is important. int | string is not the same as string | int.For example, if you have a "1" string, it will be parsed as an int if
you use int | string, but as a string if you use string | int.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#union-",
    "title": "Union (|)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#compositestructured-types",
        "title": "Composite/Structured Types",
      },
    ],
    "description": "A collection of elements of the same type.
Syntax: Type[]
Example: string[] or (int | string)[] or int[][]

Array types can be nested to create multi-dimensional arrays
An array type cannot be optional
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#listarray-",
    "title": "List/Array ([])",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Composite/Structured Types",
    ],
    "content": "A collection of elements of the same type.
Syntax: Type[]
Example: string[] or (int | string)[] or int[][]

Array types can be nested to create multi-dimensional arrays
An array type cannot be optional
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#listarray-",
    "title": "List/Array ([])",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#compositestructured-types",
        "title": "Composite/Structured Types",
      },
    ],
    "description": "A mapping of strings to elements of another type.
Syntax: map<string, ValueType>
Example: map<string, string>
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#map",
    "title": "Map",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Composite/Structured Types",
    ],
    "content": "A mapping of strings to elements of another type.
Syntax: map<string, ValueType>
Example: map<string, string>
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#map",
    "title": "Map",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#compositestructured-types",
        "title": "Composite/Structured Types",
      },
    ],
    "description": "
Not yet supported. Use a List instead.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#-set",
    "title": "❌ Set",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Composite/Structured Types",
    ],
    "content": "
Not yet supported. Use a List instead.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#-set",
    "title": "❌ Set",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#compositestructured-types",
        "title": "Composite/Structured Types",
      },
    ],
    "description": "
Not yet supported. Use a class instead.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#-tuple",
    "title": "❌ Tuple",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Composite/Structured Types",
    ],
    "content": "
Not yet supported. Use a class instead.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#-tuple",
    "title": "❌ Tuple",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Here are some examples and what their equivalents are in different languages.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#examples-and-equivalents",
    "title": "Examples and Equivalents",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Here are some examples and what their equivalents are in different languages.",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#examples-and-equivalents",
    "title": "Examples and Equivalents",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#examples-and-equivalents",
        "title": "Examples and Equivalents",
      },
    ],
    "description": "int? | string[] | MyClass
Union[Optional[int], List[str], MyClass]
(number | null) | string[] | MyClass
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-1",
    "title": "Example 1",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Examples and Equivalents",
    ],
    "content": "int? | string[] | MyClass
Union[Optional[int], List[str], MyClass]
(number | null) | string[] | MyClass
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-1",
    "title": "Example 1",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#examples-and-equivalents",
        "title": "Examples and Equivalents",
      },
    ],
    "description": "string[]
List[str]
string[]
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-2",
    "title": "Example 2",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Examples and Equivalents",
    ],
    "content": "string[]
List[str]
string[]
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-2",
    "title": "Example 2",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#examples-and-equivalents",
        "title": "Examples and Equivalents",
      },
    ],
    "description": "(int | float)[]
List[Union[int, float]]
number[]
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-3",
    "title": "Example 3",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Examples and Equivalents",
    ],
    "content": "(int | float)[]
List[Union[int, float]]
number[]
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-3",
    "title": "Example 3",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#examples-and-equivalents",
        "title": "Examples and Equivalents",
      },
    ],
    "description": "(int? | string[] | MyClass)[]
Optional[List[Union[Optional[int], List[str], MyClass]]]
((number | null) | string[] | MyClass)[]
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-4",
    "title": "Example 4",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Examples and Equivalents",
    ],
    "content": "(int? | string[] | MyClass)[]
Optional[List[Union[Optional[int], List[str], MyClass]]]
((number | null) | string[] | MyClass)[]
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-4",
    "title": "Example 4",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/types#examples-and-equivalents",
        "title": "Examples and Equivalents",
      },
    ],
    "description": ""str" | 1 | false
Union[Literal["str"], Literal[1], Literal[False]]
"str" | 1 | false
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-5",
    "title": "Example 5",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Examples and Equivalents",
    ],
    "content": ""str" | 1 | false
Union[Literal["str"], Literal[1], Literal[False]]
"str" | 1 | false
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#example-5",
    "title": "Example 5",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "
any/json - Not supported. We don't want to encourage its use as it defeats the purpose of having a type system. if you really need it, for now use string and call json.parse yourself or use dynamic types
datetime - Not yet supported. Use a string instead.
duration - Not yet supported. We recommend using string and specifying that it must be an "ISO8601 duration" in the description, which you can parse yourself into a duration.
units (currency, temperature) - Not yet supported. Use a number (int or float) and have the unit be part of the variable name. For example, temperature_fahrenheit and cost_usd (see @alias)
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#️-unsupported",
    "title": "⚠️ Unsupported",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "
any/json - Not supported. We don't want to encourage its use as it defeats the purpose of having a type system. if you really need it, for now use string and call json.parse yourself or use dynamic types
datetime - Not yet supported. Use a string instead.
duration - Not yet supported. We recommend using string and specifying that it must be an "ISO8601 duration" in the description, which you can parse yourself into a duration.
units (currency, temperature) - Not yet supported. Use a number (int or float) and have the unit be part of the variable name. For example, temperature_fahrenheit and cost_usd (see @alias)
",
    "indexSegmentId": "0",
    "slug": "ref/baml/types#️-unsupported",
    "title": "⚠️ Unsupported",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Functions in BAML define the contract between your application and AI models, providing type-safe interfaces for AI operations.",
    "indexSegmentId": "0",
    "slug": "ref/baml/function",
    "title": "function",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Functions in BAML define the contract between your application and AI models, providing type-safe interfaces for AI operations.
Overview
A BAML function consists of:

Input parameters with explicit types
A return type specification
An LLM client
A prompt (as a block string)

function FunctionName(param: Type) -> ReturnType {
    client ModelName
    prompt #"
        Template content
    "#
}

Function Declaration
Syntax
function name(parameters) -> return_type {
    client llm_specification
    prompt block_string_specification
}

Parameters

name: The function identifier (must start with a capital letter!)
parameters: One or more typed parameters (e.g., text: string, data: CustomType)
return_type: The type that the function guarantees to return (e.g., string | MyType)
llm_specification: The LLM to use (e.g., "openai/gpt-4o-mini", GPT4Turbo, Claude2)
block_string_specification: The prompt template using Jinja syntax

Type System
Functions leverage BAML's strong type system, supporting:
Built-in Types

string: Text data
int: Integer numbers
float: Decimal numbers
bool: True/false values
array: Denoted with [] suffix (e.g., string[])
map: Key-value pairs (e.g., map<string, int>)
literal: Specific values (e.g., "red" | "green" | "blue")
See all

Custom Types
Custom types can be defined using class declarations:
class CustomType {
    field1 string
    field2 int
    nested NestedType
}

function ProcessCustomType(data: CustomType) -> ResultType {
    // ...
}

Prompt Templates
Jinja Syntax
BAML uses Jinja templating for dynamic prompt generation:
prompt #"
    Input data: {{ input_data }}
    
    {% if condition %}
        Conditional content
    {% endif %}
    
    {{ ctx.output_format }}
"#

Special Variables

ctx.output_format: Automatically generates format instructions based on return type
ctx.client: Selected client and model name
_.role: Define the role of the message chunk

Error Handling
Functions automatically handle common AI model errors and provide type validation:

JSON parsing errors are automatically corrected
Type mismatches are detected and reported
Network and rate limit errors are propagated to the caller

Usage Examples
Basic Function
function ExtractEmail(text: string) -> string {
    client GPT4Turbo
    prompt #"
        Extract the email address from the following text:
        {{ text }}
        
        {{ ctx.output_format }}
    "#
}

Complex Types
class Person {
    name string
    age int
    contacts Contact[]
}

class Contact {
    type "email" | "phone"
    value string
}

function ParsePerson(data: string) -> Person {
    client "openai/gpt-4o"
    prompt #"
        {{ ctx.output_format }}
        
        {{ _.role('user') }}
        {{ data }}
    "#
}

baml_client Integration
from baml_client import b
from baml_client.types import Person

async def process() -> Person:
    result = b.ParsePerson("John Doe, 30 years old...")
    print(result.name)  # Type-safe access
    return result
import { b } from 'baml-client';
import { Person } from 'baml-client/types';

async function process(): Promise<Person> {
    const result = await b.ParsePerson("John Doe, 30 years old...");
    console.log(result.name);  // Type-safe access
    return result;
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function",
    "title": "function",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "A BAML function consists of:

Input parameters with explicit types
A return type specification
An LLM client
A prompt (as a block string)

function FunctionName(param: Type) -> ReturnType {
    client ModelName
    prompt #"
        Template content
    "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#overview",
    "title": "Overview",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "A BAML function consists of:

Input parameters with explicit types
A return type specification
An LLM client
A prompt (as a block string)

function FunctionName(param: Type) -> ReturnType {
    client ModelName
    prompt #"
        Template content
    "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#overview",
    "title": "Overview",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/function#function-declaration",
        "title": "Function Declaration",
      },
    ],
    "description": "function name(parameters) -> return_type {
    client llm_specification
    prompt block_string_specification
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#syntax",
    "title": "Syntax",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Function Declaration",
    ],
    "content": "function name(parameters) -> return_type {
    client llm_specification
    prompt block_string_specification
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#syntax",
    "title": "Syntax",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/function#function-declaration",
        "title": "Function Declaration",
      },
    ],
    "description": "
name: The function identifier (must start with a capital letter!)
parameters: One or more typed parameters (e.g., text: string, data: CustomType)
return_type: The type that the function guarantees to return (e.g., string | MyType)
llm_specification: The LLM to use (e.g., "openai/gpt-4o-mini", GPT4Turbo, Claude2)
block_string_specification: The prompt template using Jinja syntax
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#parameters",
    "title": "Parameters",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Function Declaration",
    ],
    "content": "
name: The function identifier (must start with a capital letter!)
parameters: One or more typed parameters (e.g., text: string, data: CustomType)
return_type: The type that the function guarantees to return (e.g., string | MyType)
llm_specification: The LLM to use (e.g., "openai/gpt-4o-mini", GPT4Turbo, Claude2)
block_string_specification: The prompt template using Jinja syntax
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#parameters",
    "title": "Parameters",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Functions leverage BAML's strong type system, supporting:",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#type-system",
    "title": "Type System",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Functions leverage BAML's strong type system, supporting:",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#type-system",
    "title": "Type System",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/function#type-system",
        "title": "Type System",
      },
    ],
    "description": "
string: Text data
int: Integer numbers
float: Decimal numbers
bool: True/false values
array: Denoted with [] suffix (e.g., string[])
map: Key-value pairs (e.g., map<string, int>)
literal: Specific values (e.g., "red" | "green" | "blue")
See all
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#built-in-types",
    "title": "Built-in Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Type System",
    ],
    "content": "
string: Text data
int: Integer numbers
float: Decimal numbers
bool: True/false values
array: Denoted with [] suffix (e.g., string[])
map: Key-value pairs (e.g., map<string, int>)
literal: Specific values (e.g., "red" | "green" | "blue")
See all
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#built-in-types",
    "title": "Built-in Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/function#type-system",
        "title": "Type System",
      },
    ],
    "description": "Custom types can be defined using class declarations:
class CustomType {
    field1 string
    field2 int
    nested NestedType
}

function ProcessCustomType(data: CustomType) -> ResultType {
    // ...
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#custom-types",
    "title": "Custom Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Type System",
    ],
    "content": "Custom types can be defined using class declarations:
class CustomType {
    field1 string
    field2 int
    nested NestedType
}

function ProcessCustomType(data: CustomType) -> ResultType {
    // ...
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#custom-types",
    "title": "Custom Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/function#prompt-templates",
        "title": "Prompt Templates",
      },
    ],
    "description": "BAML uses Jinja templating for dynamic prompt generation:
prompt #"
    Input data: {{ input_data }}
    
    {% if condition %}
        Conditional content
    {% endif %}
    
    {{ ctx.output_format }}
"#
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#jinja-syntax",
    "title": "Jinja Syntax",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Prompt Templates",
    ],
    "content": "BAML uses Jinja templating for dynamic prompt generation:
prompt #"
    Input data: {{ input_data }}
    
    {% if condition %}
        Conditional content
    {% endif %}
    
    {{ ctx.output_format }}
"#
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#jinja-syntax",
    "title": "Jinja Syntax",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/function#prompt-templates",
        "title": "Prompt Templates",
      },
    ],
    "description": "
ctx.output_format: Automatically generates format instructions based on return type
ctx.client: Selected client and model name
_.role: Define the role of the message chunk
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#special-variables",
    "title": "Special Variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Prompt Templates",
    ],
    "content": "
ctx.output_format: Automatically generates format instructions based on return type
ctx.client: Selected client and model name
_.role: Define the role of the message chunk
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#special-variables",
    "title": "Special Variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Functions automatically handle common AI model errors and provide type validation:

JSON parsing errors are automatically corrected
Type mismatches are detected and reported
Network and rate limit errors are propagated to the caller
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#error-handling",
    "title": "Error Handling",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Functions automatically handle common AI model errors and provide type validation:

JSON parsing errors are automatically corrected
Type mismatches are detected and reported
Network and rate limit errors are propagated to the caller
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#error-handling",
    "title": "Error Handling",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/function#usage-examples",
        "title": "Usage Examples",
      },
    ],
    "description": "function ExtractEmail(text: string) -> string {
    client GPT4Turbo
    prompt #"
        Extract the email address from the following text:
        {{ text }}
        
        {{ ctx.output_format }}
    "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#basic-function",
    "title": "Basic Function",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Usage Examples",
    ],
    "content": "function ExtractEmail(text: string) -> string {
    client GPT4Turbo
    prompt #"
        Extract the email address from the following text:
        {{ text }}
        
        {{ ctx.output_format }}
    "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#basic-function",
    "title": "Basic Function",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/function#usage-examples",
        "title": "Usage Examples",
      },
    ],
    "description": "class Person {
    name string
    age int
    contacts Contact[]
}

class Contact {
    type "email" | "phone"
    value string
}

function ParsePerson(data: string) -> Person {
    client "openai/gpt-4o"
    prompt #"
        {{ ctx.output_format }}
        
        {{ _.role('user') }}
        {{ data }}
    "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#complex-types",
    "title": "Complex Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Usage Examples",
    ],
    "content": "class Person {
    name string
    age int
    contacts Contact[]
}

class Contact {
    type "email" | "phone"
    value string
}

function ParsePerson(data: string) -> Person {
    client "openai/gpt-4o"
    prompt #"
        {{ ctx.output_format }}
        
        {{ _.role('user') }}
        {{ data }}
    "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#complex-types",
    "title": "Complex Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "from baml_client import b
from baml_client.types import Person

async def process() -> Person:
    result = b.ParsePerson("John Doe, 30 years old...")
    print(result.name)  # Type-safe access
    return result
import { b } from 'baml-client';
import { Person } from 'baml-client/types';

async function process(): Promise<Person> {
    const result = await b.ParsePerson("John Doe, 30 years old...");
    console.log(result.name);  // Type-safe access
    return result;
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#baml_client-integration",
    "title": "baml_client Integration",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "from baml_client import b
from baml_client.types import Person

async def process() -> Person:
    result = b.ParsePerson("John Doe, 30 years old...")
    print(result.name)  # Type-safe access
    return result
import { b } from 'baml-client';
import { Person } from 'baml-client/types';

async function process(): Promise<Person> {
    const result = await b.ParsePerson("John Doe, 30 years old...");
    console.log(result.name);  // Type-safe access
    return result;
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/function#baml_client-integration",
    "title": "baml_client Integration",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Tests are first-class citizens in BAML, designed to make testing AI functions straightforward and robust. BAML tests can be written anywhere in your codebase and run with minimal setup.",
    "indexSegmentId": "0",
    "slug": "ref/baml/test",
    "title": "test",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Tests are first-class citizens in BAML, designed to make testing AI functions straightforward and robust. BAML tests can be written anywhere in your codebase and run with minimal setup.
Overview
A BAML test consists of:

Test name and metadata
Functions under test
Input arguments
Optional testing configuration
Optional assertions

test TestName {
    functions [FunctionName]
    args {
        paramName "value"
    }
}

Test Declaration
Syntax
test name {
    functions [function_list]
    args {
        parameter_assignments
    }
}

Components

name: Test identifier (unique per function)
functions: List of functions to test
args: Input parameters for the test case

Input Types
Basic Types
Simple values are provided directly:
test SimpleTest {
    functions [ClassifyMessage]
    args {
        input "Can't access my account"
    }
}

Complex Objects
Objects are specified using nested structures:
test ComplexTest {
    functions [ProcessMessage]
    args {
        message {
            user "john_doe"
            content "Hello world"
            metadata {
                timestamp 1234567890
                priority "high"
            }
        }
    }
}

Arrays
Arrays use bracket notation:
test ArrayTest {
    functions [BatchProcess]
    args {
        messages [
            {
                user "user1"
                content "Message 1"
            }
            {
                user "user2"
                content "Message 2"
            }
        ]
    }
}

Media Inputs
Images
Images can be specified using three methods:

File Reference

test ImageFileTest {
    functions [AnalyzeImage]
    args {
        param {
            file "../images/test.png"
        }
    }
}


URL Reference

test ImageUrlTest {
    functions [AnalyzeImage]
    args {
        param {
            url "https://example.com/image.jpg"
        }
    }
}


Base64 Data

test ImageBase64Test {
    functions [AnalyzeImage]
    args {
        param {
            base64 "a41f..."
            media_type "image/png"
        }
    }
}

Audio
Similar to images, audio can be specified in three ways:

File Reference

test AudioFileTest {
    functions [TranscribeAudio]
    args {
        audio {
            file "../audio/sample.mp3"
        }
    }
}


URL Reference

test AudioUrlTest {
    functions [TranscribeAudio]
    args {
        audio {
            url "https://example.com/audio.mp3"
        }
    }
}


Base64 Data

test AudioBase64Test {
    functions [TranscribeAudio]
    args {
        audio {
            base64 "..."
            media_type "audio/mp3"
        }
    }
}

Multi-line Strings
For long text inputs, use the block string syntax:
test LongTextTest {
    functions [AnalyzeText]
    args {
        content #"
            This is a multi-line
            text input that preserves
            formatting and whitespace
        "#
    }
}

Testing Multiple Functions
This requires each function to have teh exact same parameters:
test EndToEndFlow {
    functions [
        ExtractInfo
        ProcessInfo
        ValidateResult
    ]
    args {
        input "test data"
    }
}

Integration with Development Tools
VSCode Integration

Tests can be run directly from the BAML playground
Real-time syntax validation
Test result visualization
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test",
    "title": "test",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "A BAML test consists of:

Test name and metadata
Functions under test
Input arguments
Optional testing configuration
Optional assertions

test TestName {
    functions [FunctionName]
    args {
        paramName "value"
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#overview",
    "title": "Overview",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "A BAML test consists of:

Test name and metadata
Functions under test
Input arguments
Optional testing configuration
Optional assertions

test TestName {
    functions [FunctionName]
    args {
        paramName "value"
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#overview",
    "title": "Overview",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/test#test-declaration",
        "title": "Test Declaration",
      },
    ],
    "description": "test name {
    functions [function_list]
    args {
        parameter_assignments
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#syntax",
    "title": "Syntax",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Test Declaration",
    ],
    "content": "test name {
    functions [function_list]
    args {
        parameter_assignments
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#syntax",
    "title": "Syntax",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/test#test-declaration",
        "title": "Test Declaration",
      },
    ],
    "description": "
name: Test identifier (unique per function)
functions: List of functions to test
args: Input parameters for the test case
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#components",
    "title": "Components",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Test Declaration",
    ],
    "content": "
name: Test identifier (unique per function)
functions: List of functions to test
args: Input parameters for the test case
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#components",
    "title": "Components",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/test#input-types",
        "title": "Input Types",
      },
    ],
    "description": "Simple values are provided directly:
test SimpleTest {
    functions [ClassifyMessage]
    args {
        input "Can't access my account"
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#basic-types",
    "title": "Basic Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Input Types",
    ],
    "content": "Simple values are provided directly:
test SimpleTest {
    functions [ClassifyMessage]
    args {
        input "Can't access my account"
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#basic-types",
    "title": "Basic Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/test#input-types",
        "title": "Input Types",
      },
    ],
    "description": "Objects are specified using nested structures:
test ComplexTest {
    functions [ProcessMessage]
    args {
        message {
            user "john_doe"
            content "Hello world"
            metadata {
                timestamp 1234567890
                priority "high"
            }
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#complex-objects",
    "title": "Complex Objects",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Input Types",
    ],
    "content": "Objects are specified using nested structures:
test ComplexTest {
    functions [ProcessMessage]
    args {
        message {
            user "john_doe"
            content "Hello world"
            metadata {
                timestamp 1234567890
                priority "high"
            }
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#complex-objects",
    "title": "Complex Objects",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/test#input-types",
        "title": "Input Types",
      },
    ],
    "description": "Arrays use bracket notation:
test ArrayTest {
    functions [BatchProcess]
    args {
        messages [
            {
                user "user1"
                content "Message 1"
            }
            {
                user "user2"
                content "Message 2"
            }
        ]
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#arrays",
    "title": "Arrays",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Input Types",
    ],
    "content": "Arrays use bracket notation:
test ArrayTest {
    functions [BatchProcess]
    args {
        messages [
            {
                user "user1"
                content "Message 1"
            }
            {
                user "user2"
                content "Message 2"
            }
        ]
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#arrays",
    "title": "Arrays",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/test#media-inputs",
        "title": "Media Inputs",
      },
    ],
    "description": "Images can be specified using three methods:

File Reference

test ImageFileTest {
    functions [AnalyzeImage]
    args {
        param {
            file "../images/test.png"
        }
    }
}


URL Reference

test ImageUrlTest {
    functions [AnalyzeImage]
    args {
        param {
            url "https://example.com/image.jpg"
        }
    }
}


Base64 Data

test ImageBase64Test {
    functions [AnalyzeImage]
    args {
        param {
            base64 "a41f..."
            media_type "image/png"
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#images",
    "title": "Images",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Media Inputs",
    ],
    "content": "Images can be specified using three methods:

File Reference

test ImageFileTest {
    functions [AnalyzeImage]
    args {
        param {
            file "../images/test.png"
        }
    }
}


URL Reference

test ImageUrlTest {
    functions [AnalyzeImage]
    args {
        param {
            url "https://example.com/image.jpg"
        }
    }
}


Base64 Data

test ImageBase64Test {
    functions [AnalyzeImage]
    args {
        param {
            base64 "a41f..."
            media_type "image/png"
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#images",
    "title": "Images",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/test#media-inputs",
        "title": "Media Inputs",
      },
    ],
    "description": "Similar to images, audio can be specified in three ways:

File Reference

test AudioFileTest {
    functions [TranscribeAudio]
    args {
        audio {
            file "../audio/sample.mp3"
        }
    }
}


URL Reference

test AudioUrlTest {
    functions [TranscribeAudio]
    args {
        audio {
            url "https://example.com/audio.mp3"
        }
    }
}


Base64 Data

test AudioBase64Test {
    functions [TranscribeAudio]
    args {
        audio {
            base64 "..."
            media_type "audio/mp3"
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#audio",
    "title": "Audio",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Media Inputs",
    ],
    "content": "Similar to images, audio can be specified in three ways:

File Reference

test AudioFileTest {
    functions [TranscribeAudio]
    args {
        audio {
            file "../audio/sample.mp3"
        }
    }
}


URL Reference

test AudioUrlTest {
    functions [TranscribeAudio]
    args {
        audio {
            url "https://example.com/audio.mp3"
        }
    }
}


Base64 Data

test AudioBase64Test {
    functions [TranscribeAudio]
    args {
        audio {
            base64 "..."
            media_type "audio/mp3"
        }
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#audio",
    "title": "Audio",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "For long text inputs, use the block string syntax:
test LongTextTest {
    functions [AnalyzeText]
    args {
        content #"
            This is a multi-line
            text input that preserves
            formatting and whitespace
        "#
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#multi-line-strings",
    "title": "Multi-line Strings",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "For long text inputs, use the block string syntax:
test LongTextTest {
    functions [AnalyzeText]
    args {
        content #"
            This is a multi-line
            text input that preserves
            formatting and whitespace
        "#
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#multi-line-strings",
    "title": "Multi-line Strings",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "This requires each function to have teh exact same parameters:
test EndToEndFlow {
    functions [
        ExtractInfo
        ProcessInfo
        ValidateResult
    ]
    args {
        input "test data"
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#testing-multiple-functions",
    "title": "Testing Multiple Functions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "This requires each function to have teh exact same parameters:
test EndToEndFlow {
    functions [
        ExtractInfo
        ProcessInfo
        ValidateResult
    ]
    args {
        input "test data"
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#testing-multiple-functions",
    "title": "Testing Multiple Functions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/test#integration-with-development-tools",
        "title": "Integration with Development Tools",
      },
    ],
    "description": "
Tests can be run directly from the BAML playground
Real-time syntax validation
Test result visualization
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#vscode-integration",
    "title": "VSCode Integration",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Integration with Development Tools",
    ],
    "content": "
Tests can be run directly from the BAML playground
Real-time syntax validation
Test result visualization
",
    "indexSegmentId": "0",
    "slug": "ref/baml/test#vscode-integration",
    "title": "VSCode Integration",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Writing prompts requires a lot of string manipulation. BAML has a `template_string` to let you combine different string templates together. Under-the-hood they use [jinja](/ref/prompt-syntax/what-is-jinja) to evaluate the string and its inputs.

Think of template strings as functions that have variables, and return a string. They can be used to define reusable parts of a prompt, or to make the prompt more readable by breaking it into smaller parts.

Example
```baml BAML
// Inject a list of "system" or "user" messages into the prompt.
template_string PrintMessages(messages: Message[]) #"
  {% for m in messages %}
    {{ _.role(m.role) }}
    {{ m.message }}
  {% endfor %}
"#

function ClassifyConversation(messages: Message[]) -> Category[] {
  client GPT4Turbo
  prompt #"
    Classify this conversation:
    {{ PrintMessages(messages) }}

    Use the following categories:
    {{ ctx.output_format}}
  "#
}
```

In this example we can call the template_string `PrintMessages` to subdivide the prompt into "user" or "system" messages using `_.role()` (see [message roles](/ref/prompt-syntax/role)). This allows us to reuse the logic for printing messages in multiple prompts. 

You can nest as many template strings inside each other and call them however many times you want.

<Warning>
  The BAML linter may give you a warning when you use template strings due to a static analysis limitation. You can ignore this warning. If it renders in the playground, you're good!
</Warning>
Use the playground preview to ensure your template string is being evaluated correctly!",
    "indexSegmentId": "0",
    "slug": "ref/baml/template-string",
    "title": "template_string",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Writing prompts requires a lot of string manipulation. BAML has a template_string to let you combine different string templates together. Under-the-hood they use jinja to evaluate the string and its inputs.
Think of template strings as functions that have variables, and return a string. They can be used to define reusable parts of a prompt, or to make the prompt more readable by breaking it into smaller parts.
Example
// Inject a list of "system" or "user" messages into the prompt.
template_string PrintMessages(messages: Message[]) #"
  {% for m in messages %}
    {{ _.role(m.role) }}
    {{ m.message }}
  {% endfor %}
"#

function ClassifyConversation(messages: Message[]) -> Category[] {
  client GPT4Turbo
  prompt #"
    Classify this conversation:
    {{ PrintMessages(messages) }}

    Use the following categories:
    {{ ctx.output_format}}
  "#
}

In this example we can call the template_string PrintMessages to subdivide the prompt into "user" or "system" messages using _.role() (see message roles). This allows us to reuse the logic for printing messages in multiple prompts.
You can nest as many template strings inside each other and call them however many times you want.
The BAML linter may give you a warning when you use template strings due to a static analysis limitation. You can ignore this warning. If it renders in the playground, you're good!
Use the playground preview to ensure your template string is being evaluated correctly!",
    "indexSegmentId": "0",
    "slug": "ref/baml/template-string",
    "title": "template_string",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Clients are used to configure how LLMs are called, like so:

```rust BAML
function MakeHaiku(topic: string) -> string {
  client "openai/gpt-4o"
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}
```

This is `<provider>/<model>` shorthand for:

```rust BAML
client<llm> MyClient {
  provider "openai"
  options {
    model "gpt-4o"
    // api_key defaults to env.OPENAI_API_KEY
  }
}

function MakeHaiku(topic: string) -> string {
  client MyClient
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}
```

Consult the [provider documentation](#fields) for a list of supported providers
and models, and the default options.

If you want to override options like `api_key` to use a different environment
variable, or you want to point `base_url` to a different endpoint, you should use
the latter form.

<Tip>
If you want to specify which client to use at runtime, in your Python/TS/Ruby code,
you can use the [client registry](/guide/baml-advanced/llm-client-registry) to do so.

This can come in handy if you're trying to, say, send 10% of your requests to a
different model.
</Tip>",
    "indexSegmentId": "0",
    "slug": "ref/baml/client-llm",
    "title": "client<llm>",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Clients are used to configure how LLMs are called, like so:
function MakeHaiku(topic: string) -> string {
  client "openai/gpt-4o"
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}

This is <provider>/<model> shorthand for:
client<llm> MyClient {
  provider "openai"
  options {
    model "gpt-4o"
    // api_key defaults to env.OPENAI_API_KEY
  }
}

function MakeHaiku(topic: string) -> string {
  client MyClient
  prompt #"
    Write a haiku about {{ topic }}.
  "#
}

Consult the provider documentation for a list of supported providers
and models, and the default options.
If you want to override options like api_key to use a different environment
variable, or you want to point base_url to a different endpoint, you should use
the latter form.
If you want to specify which client to use at runtime, in your Python/TS/Ruby code,
you can use the client registry to do so.This can come in handy if you're trying to, say, send 10% of your requests to a
different model.
Fields
This configures which provider to use. The provider is responsible for handling the actual API calls to the LLM service. The provider is a required field.The configuration modifies the URL request BAML runtime makes.| Provider Name    | Docs                                                                | Notes                                                      |
| ---------------- | ------------------------------------------------------------------- | ---------------------------------------------------------- |
| anthropic      | Anthropic             |                                                            |
| aws-bedrock    | AWS Bedrock         |                                                            |
| azure-openai   | Azure OpenAI              |                                                            |
| google-ai      | Google AI                |                                                            |
| openai         | OpenAI                   |                                                            |
| openai-generic | OpenAI (generic) | Any model provider that supports an OpenAI-compatible API  |
| vertex-ai      | Vertex AI                |                                                            |We also have some special providers that allow composing clients together:
| Provider Name  | Docs                             | Notes                                                      |
| -------------- | -------------------------------- | ---------------------------------------------------------- |
| fallback     | Fallback             | Used to chain models conditional on failures               |
| round-robin  | Round Robin       | Used to load balance                                       |
These vary per provider. Please see provider specific documentation for more
information. Generally they are pass through options to the POST request made
to the LLM.
The name of the retry policy. See Retry
Policy.",
    "indexSegmentId": "0",
    "slug": "ref/baml/client-llm",
    "title": "client<llm>",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "This configures which provider to use. The provider is responsible for handling the actual API calls to the LLM service. The provider is a required field.The configuration modifies the URL request BAML runtime makes.| Provider Name    | Docs                                                                | Notes                                                      |
| ---------------- | ------------------------------------------------------------------- | ---------------------------------------------------------- |
| anthropic      | Anthropic             |                                                            |
| aws-bedrock    | AWS Bedrock         |                                                            |
| azure-openai   | Azure OpenAI              |                                                            |
| google-ai      | Google AI                |                                                            |
| openai         | OpenAI                   |                                                            |
| openai-generic | OpenAI (generic) | Any model provider that supports an OpenAI-compatible API  |
| vertex-ai      | Vertex AI                |                                                            |We also have some special providers that allow composing clients together:
| Provider Name  | Docs                             | Notes                                                      |
| -------------- | -------------------------------- | ---------------------------------------------------------- |
| fallback     | Fallback             | Used to chain models conditional on failures               |
| round-robin  | Round Robin       | Used to load balance                                       |
These vary per provider. Please see provider specific documentation for more
information. Generally they are pass through options to the POST request made
to the LLM.
The name of the retry policy. See Retry
Policy.",
    "indexSegmentId": "0",
    "slug": "ref/baml/client-llm#fields",
    "title": "Fields",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "This configures which provider to use. The provider is responsible for handling the actual API calls to the LLM service. The provider is a required field.The configuration modifies the URL request BAML runtime makes.| Provider Name    | Docs                                                                | Notes                                                      |
| ---------------- | ------------------------------------------------------------------- | ---------------------------------------------------------- |
| anthropic      | Anthropic             |                                                            |
| aws-bedrock    | AWS Bedrock         |                                                            |
| azure-openai   | Azure OpenAI              |                                                            |
| google-ai      | Google AI                |                                                            |
| openai         | OpenAI                   |                                                            |
| openai-generic | OpenAI (generic) | Any model provider that supports an OpenAI-compatible API  |
| vertex-ai      | Vertex AI                |                                                            |We also have some special providers that allow composing clients together:
| Provider Name  | Docs                             | Notes                                                      |
| -------------- | -------------------------------- | ---------------------------------------------------------- |
| fallback     | Fallback             | Used to chain models conditional on failures               |
| round-robin  | Round Robin       | Used to load balance                                       |
These vary per provider. Please see provider specific documentation for more
information. Generally they are pass through options to the POST request made
to the LLM.
The name of the retry policy. See Retry
Policy.",
    "indexSegmentId": "0",
    "slug": "ref/baml/client-llm#fields",
    "title": "Fields",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Classes consist of a name, a list of properties, and their [types](class).
In the context of LLMs, classes describe the type of the variables you can inject into prompts and extract out from the response.

<Warning>
  Note properties have no `:`
</Warning>

<CodeBlocks>
```baml Baml
class Foo {
  property1 string
  property2 int?
  property3 Bar[]
  property4 MyEnum
}
```

```python Python Equivalent
from pydantic import BaseModel
from path.to.bar import Bar
from path.to.my_enum import MyEnum

class Foo(BaseModel):
  property1: str
  property2: Optional[int]= None
  property3: List[Bar]
  property4: MyEnum
```

```typescript Typescript Equivalent
import z from "zod";
import { BarZod } from "./path/to/bar";
import { MyEnumZod } from "./path/to/my_enum";

const FooZod = z.object({
  property1: z.string(),
  property2: z.number().int().nullable().optional(),
  property3: z.array(BarZod),
  property4: MyEnumZod,
});

type Foo = z.infer<typeof FooZod>;
```

</CodeBlocks>",
    "indexSegmentId": "0",
    "slug": "ref/baml/class",
    "title": "class",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Classes consist of a name, a list of properties, and their types.
In the context of LLMs, classes describe the type of the variables you can inject into prompts and extract out from the response.
Note properties have no :
class Foo {
  property1 string
  property2 int?
  property3 Bar[]
  property4 MyEnum
}
from pydantic import BaseModel
from path.to.bar import Bar
from path.to.my_enum import MyEnum

class Foo(BaseModel):
  property1: str
  property2: Optional[int]= None
  property3: List[Bar]
  property4: MyEnum
import z from "zod";
import { BarZod } from "./path/to/bar";
import { MyEnumZod } from "./path/to/my_enum";

const FooZod = z.object({
  property1: z.string(),
  property2: z.number().int().nullable().optional(),
  property3: z.array(BarZod),
  property4: MyEnumZod,
});

type Foo = z.infer<typeof FooZod>;

Field Attributes
When prompt engineering, you can also alias values and add descriptions.
Aliasing renames the field for the llm to potentially "understand" your value better, while keeping the original name in your code, so you don't need to change your downstream code everytime.This will also be used for parsing the output of the LLM back into the original object.
This adds some additional context to the field in the prompt.
class MyClass {
  property1 string @alias("name") @description("The name of the object")
  age int? @description("The age of the object")
}

Class Attributes
If set, will allow you to add fields to the class dynamically at runtime (in your python/ts/etc code). See dynamic classes for more information.
class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows me to later propert3 float[] at runtime
}

Syntax
Classes may have any number of properties.
Property names must follow these rules:

Must start with a letter
Must contain only letters, numbers, and underscores
Must be unique within the class
classes cannot be self-referential (cannot have a property of the same type as the class itself)

The type of a property can be any supported type
Default values

Not yet supported. For optional properties, the default value is None in python.

Dynamic classes
See Dynamic Types.
Inheritance
Never supported. Like rust, we take the stance that composition is better than inheritance.",
    "indexSegmentId": "0",
    "slug": "ref/baml/class",
    "title": "class",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "When prompt engineering, you can also alias values and add descriptions.
Aliasing renames the field for the llm to potentially "understand" your value better, while keeping the original name in your code, so you don't need to change your downstream code everytime.This will also be used for parsing the output of the LLM back into the original object.
This adds some additional context to the field in the prompt.
class MyClass {
  property1 string @alias("name") @description("The name of the object")
  age int? @description("The age of the object")
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#field-attributes",
    "title": "Field Attributes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "When prompt engineering, you can also alias values and add descriptions.
Aliasing renames the field for the llm to potentially "understand" your value better, while keeping the original name in your code, so you don't need to change your downstream code everytime.This will also be used for parsing the output of the LLM back into the original object.
This adds some additional context to the field in the prompt.
class MyClass {
  property1 string @alias("name") @description("The name of the object")
  age int? @description("The age of the object")
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#field-attributes",
    "title": "Field Attributes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "If set, will allow you to add fields to the class dynamically at runtime (in your python/ts/etc code). See dynamic classes for more information.
class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows me to later propert3 float[] at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#class-attributes",
    "title": "Class Attributes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "If set, will allow you to add fields to the class dynamically at runtime (in your python/ts/etc code). See dynamic classes for more information.
class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows me to later propert3 float[] at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#class-attributes",
    "title": "Class Attributes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Classes may have any number of properties.
Property names must follow these rules:

Must start with a letter
Must contain only letters, numbers, and underscores
Must be unique within the class
classes cannot be self-referential (cannot have a property of the same type as the class itself)

The type of a property can be any supported type",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#syntax",
    "title": "Syntax",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Classes may have any number of properties.
Property names must follow these rules:

Must start with a letter
Must contain only letters, numbers, and underscores
Must be unique within the class
classes cannot be self-referential (cannot have a property of the same type as the class itself)

The type of a property can be any supported type",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#syntax",
    "title": "Syntax",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/class#syntax",
        "title": "Syntax",
      },
    ],
    "description": "
Not yet supported. For optional properties, the default value is None in python.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#default-values",
    "title": "Default values",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Syntax",
    ],
    "content": "
Not yet supported. For optional properties, the default value is None in python.
",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#default-values",
    "title": "Default values",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
      {
        "slug": "ref/baml/class#syntax",
        "title": "Syntax",
      },
    ],
    "description": "See Dynamic Types.",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#dynamic-classes",
    "title": "Dynamic classes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
      "Syntax",
    ],
    "content": "See Dynamic Types.",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#dynamic-classes",
    "title": "Dynamic classes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Never supported. Like rust, we take the stance that composition is better than inheritance.",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#inheritance",
    "title": "Inheritance",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Never supported. Like rust, we take the stance that composition is better than inheritance.",
    "indexSegmentId": "0",
    "slug": "ref/baml/class#inheritance",
    "title": "Inheritance",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Enums are useful for classification tasks. BAML has helper functions that can help you serialize an enum into your prompt in a neatly formatted list (more on that later).

To define your own custom enum in BAML:

<CodeBlocks>
```baml BAML
enum MyEnum {
  Value1
  Value2
  Value3
}
```

```python Python Equivalent
from enum import StrEnum

class MyEnum(StrEnum):
  Value1 = "Value1"
  Value2 = "Value2"
  Value3 = "Value3"
```

```typescript Typescript Equivalent
enum MyEnum {
  Value1 = "Value1",
  Value2 = "Value2",
  Value3 = "Value3",
}
```

</CodeBlocks>

- You may have as many values as you'd like.
- Values may not be duplicated or empty.
- Values may not contain spaces or special characters and must not start with a number.",
    "indexSegmentId": "0",
    "slug": "ref/baml/enum",
    "title": "enum",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Enums are useful for classification tasks. BAML has helper functions that can help you serialize an enum into your prompt in a neatly formatted list (more on that later).
To define your own custom enum in BAML:
enum MyEnum {
  Value1
  Value2
  Value3
}
from enum import StrEnum

class MyEnum(StrEnum):
  Value1 = "Value1"
  Value2 = "Value2"
  Value3 = "Value3"
enum MyEnum {
  Value1 = "Value1",
  Value2 = "Value2",
  Value3 = "Value3",
}


You may have as many values as you'd like.
Values may not be duplicated or empty.
Values may not contain spaces or special characters and must not start with a number.

Enum Attributes
This is the name of the enum rendered in the prompt.
If set, will allow you to add/remove/modify values to the enum dynamically at runtime (in your python/ts/etc code). See dynamic enums for more information.
enum MyEnum {
  Value1
  Value2
  Value3

  @@alias("My Custom Enum")
  @@dynamic // allows me to later skip Value2 at runtime
}

Value Attributes
When prompt engineering, you can also alias values and add descriptions, or even skip them.
Aliasing renames the values for the llm to potentially "understand" your value better, while keeping the original name in your code, so you don't need to change your downstream code everytime.This will also be used for parsing the output of the LLM back into the enum.
This adds some additional context to the value in the prompt.
Skip this value in the prompt and during parsing.
enum MyEnum {
  Value1 @alias("complete_summary") @description("Answer in 2 sentences")
  Value2
  Value3 @skip
  Value4 @description(#"
    This is a long description that spans multiple lines.
    It can be useful for providing more context to the value.
  "#)
}

See more in prompt syntax docs",
    "indexSegmentId": "0",
    "slug": "ref/baml/enum",
    "title": "enum",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "This is the name of the enum rendered in the prompt.
If set, will allow you to add/remove/modify values to the enum dynamically at runtime (in your python/ts/etc code). See dynamic enums for more information.
enum MyEnum {
  Value1
  Value2
  Value3

  @@alias("My Custom Enum")
  @@dynamic // allows me to later skip Value2 at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/enum#enum-attributes",
    "title": "Enum Attributes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "This is the name of the enum rendered in the prompt.
If set, will allow you to add/remove/modify values to the enum dynamically at runtime (in your python/ts/etc code). See dynamic enums for more information.
enum MyEnum {
  Value1
  Value2
  Value3

  @@alias("My Custom Enum")
  @@dynamic // allows me to later skip Value2 at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/enum#enum-attributes",
    "title": "Enum Attributes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "When prompt engineering, you can also alias values and add descriptions, or even skip them.
Aliasing renames the values for the llm to potentially "understand" your value better, while keeping the original name in your code, so you don't need to change your downstream code everytime.This will also be used for parsing the output of the LLM back into the enum.
This adds some additional context to the value in the prompt.
Skip this value in the prompt and during parsing.
enum MyEnum {
  Value1 @alias("complete_summary") @description("Answer in 2 sentences")
  Value2
  Value3 @skip
  Value4 @description(#"
    This is a long description that spans multiple lines.
    It can be useful for providing more context to the value.
  "#)
}

See more in prompt syntax docs",
    "indexSegmentId": "0",
    "slug": "ref/baml/enum#value-attributes",
    "title": "Value Attributes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "When prompt engineering, you can also alias values and add descriptions, or even skip them.
Aliasing renames the values for the llm to potentially "understand" your value better, while keeping the original name in your code, so you don't need to change your downstream code everytime.This will also be used for parsing the output of the LLM back into the enum.
This adds some additional context to the value in the prompt.
Skip this value in the prompt and during parsing.
enum MyEnum {
  Value1 @alias("complete_summary") @description("Answer in 2 sentences")
  Value2
  Value3 @skip
  Value4 @description(#"
    This is a long description that spans multiple lines.
    It can be useful for providing more context to the value.
  "#)
}

See more in prompt syntax docs",
    "indexSegmentId": "0",
    "slug": "ref/baml/enum#value-attributes",
    "title": "Value Attributes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml/general-baml-syntax/comments",
        "title": "Language Reference",
      },
    ],
    "description": "Each `generator` that you define in your BAML project will tell `baml-cli
generate` to generate code for a specific target language. You can define
multiple `generator` clauses in your BAML project, and `baml-cli generate` will
generate code for each of them.

<Tip>If you created your project using `baml-cli init`, then one has already been generated for you!</Tip>


<CodeBlocks>

```baml Python
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "python/pydantic"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "sync"
    
    // Version of runtime to generate code for (should match installed baml-py version)
    version "0.63.0"
}
```

```baml TypeScript
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "typescript"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "async"
    
    // Version of runtime to generate code for (should match the package @boundaryml/baml version)
    version "0.63.0"
}
```

```baml Ruby (beta)
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "ruby/sorbet"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.63.0"
}
```

```baml OpenAPI
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"
    output_type "rest/openapi"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.54.0"

    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client
    // This command will be run from within $output_dir
    on_generate "npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g OPENAPI_CLIENT_TYPE -o ."
}
```

</CodeBlocks>",
    "indexSegmentId": "0",
    "slug": "ref/baml/generator",
    "title": "generator",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Language Reference",
    ],
    "content": "Each generator that you define in your BAML project will tell baml-cli generate to generate code for a specific target language. You can define
multiple generator clauses in your BAML project, and baml-cli generate will
generate code for each of them.
If you created your project using baml-cli init, then one has already been generated for you!
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "python/pydantic"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "sync"
    
    // Version of runtime to generate code for (should match installed baml-py version)
    version "0.63.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "typescript"
    
    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // What interface you prefer to use for the generated code (sync/async)
    // Both are generated regardless of the choice, just modifies what is exported
    // at the top level
    default_client_mode "async"
    
    // Version of runtime to generate code for (should match the package @boundaryml/baml version)
    version "0.63.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet"
    output_type "ruby/sorbet"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.63.0"
}
generator target {
    // Valid values: "python/pydantic", "typescript", "ruby/sorbet", "rest/openapi"
    output_type "rest/openapi"

    // Where the generated code will be saved (relative to baml_src/)
    output_dir "../"
    
    // Version of runtime to generate code for (should match installed `baml` package version)
    version "0.54.0"

    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client
    // This command will be run from within $output_dir
    on_generate "npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g OPENAPI_CLIENT_TYPE -o ."
}
",
    "indexSegmentId": "0",
    "slug": "ref/baml/generator",
    "title": "generator",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "In BAML, attributes are used to provide additional metadata or behavior to fields and types. They can be applied at different levels, such as field-level or block-level, depending on their intended use.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes",
    "title": "What are attributes?",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "In BAML, attributes are used to provide additional metadata or behavior to fields and types. They can be applied at different levels, such as field-level or block-level, depending on their intended use.
Field-Level Attributes
Field-level attributes are applied directly to individual fields within a class or enum. They modify the behavior or metadata of that specific field.
Examples of Field-Level Attributes

@alias: Renames a field for better understanding by the LLM.
@description: Provides additional context to a field.
@skip: Excludes a field from prompts or parsing.
@assert: Applies strict validation to a field.
@check: Adds non-exception-raising validation to a field.

class MyClass {
  property1 string @alias("name") @description("The name of the object")
  age int? @check(positive, {{ this > 0 }})
}

Block-Level Attributes
Block-level attributes are applied to an entire class or enum, affecting all fields or values within that block. They are used to modify the behavior or metadata of the entire block.
Examples of Block-Level Attributes

@@dynamic: Allows dynamic modification of fields or values at runtime.

class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows adding fields dynamically at runtime
}

Key Differences

Scope: Field-level attributes affect individual fields, while block-level attributes affect the entire class or enum.
Usage: Field-level attributes are used for specific field modifications, whereas block-level attributes are used for broader modifications affecting the whole block.

Understanding the distinction between these types of attributes is crucial for effectively using BAML to define and manipulate data structures.
For more detailed information on each attribute, refer to the specific attribute pages in this section.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes",
    "title": "What are attributes?",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "Field-level attributes are applied directly to individual fields within a class or enum. They modify the behavior or metadata of that specific field.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#field-level-attributes",
    "title": "Field-Level Attributes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "Field-level attributes are applied directly to individual fields within a class or enum. They modify the behavior or metadata of that specific field.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#field-level-attributes",
    "title": "Field-Level Attributes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/what-are-attributes#field-level-attributes",
        "title": "Field-Level Attributes",
      },
    ],
    "description": "
@alias: Renames a field for better understanding by the LLM.
@description: Provides additional context to a field.
@skip: Excludes a field from prompts or parsing.
@assert: Applies strict validation to a field.
@check: Adds non-exception-raising validation to a field.

class MyClass {
  property1 string @alias("name") @description("The name of the object")
  age int? @check(positive, {{ this > 0 }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#examples-of-field-level-attributes",
    "title": "Examples of Field-Level Attributes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Field-Level Attributes",
    ],
    "content": "
@alias: Renames a field for better understanding by the LLM.
@description: Provides additional context to a field.
@skip: Excludes a field from prompts or parsing.
@assert: Applies strict validation to a field.
@check: Adds non-exception-raising validation to a field.

class MyClass {
  property1 string @alias("name") @description("The name of the object")
  age int? @check(positive, {{ this > 0 }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#examples-of-field-level-attributes",
    "title": "Examples of Field-Level Attributes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "Block-level attributes are applied to an entire class or enum, affecting all fields or values within that block. They are used to modify the behavior or metadata of the entire block.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#block-level-attributes",
    "title": "Block-Level Attributes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "Block-level attributes are applied to an entire class or enum, affecting all fields or values within that block. They are used to modify the behavior or metadata of the entire block.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#block-level-attributes",
    "title": "Block-Level Attributes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/what-are-attributes#block-level-attributes",
        "title": "Block-Level Attributes",
      },
    ],
    "description": "
@@dynamic: Allows dynamic modification of fields or values at runtime.

class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows adding fields dynamically at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#examples-of-block-level-attributes",
    "title": "Examples of Block-Level Attributes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Block-Level Attributes",
    ],
    "content": "
@@dynamic: Allows dynamic modification of fields or values at runtime.

class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows adding fields dynamically at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#examples-of-block-level-attributes",
    "title": "Examples of Block-Level Attributes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "
Scope: Field-level attributes affect individual fields, while block-level attributes affect the entire class or enum.
Usage: Field-level attributes are used for specific field modifications, whereas block-level attributes are used for broader modifications affecting the whole block.

Understanding the distinction between these types of attributes is crucial for effectively using BAML to define and manipulate data structures.
For more detailed information on each attribute, refer to the specific attribute pages in this section.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#key-differences",
    "title": "Key Differences",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "
Scope: Field-level attributes affect individual fields, while block-level attributes affect the entire class or enum.
Usage: Field-level attributes are used for specific field modifications, whereas block-level attributes are used for broader modifications affecting the whole block.

Understanding the distinction between these types of attributes is crucial for effectively using BAML to define and manipulate data structures.
For more detailed information on each attribute, refer to the specific attribute pages in this section.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/what-are-attributes#key-differences",
    "title": "Key Differences",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "The `@alias` attribute in BAML is used to rename fields or values for better understanding by the LLM, while keeping the original name in your code. This is particularly useful for prompt engineering, as it allows you to provide a more intuitive name for the LLM without altering your existing codebase.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias",
    "title": "@alias / @@alias",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "The @alias attribute in BAML is used to rename fields or values for better understanding by the LLM, while keeping the original name in your code. This is particularly useful for prompt engineering, as it allows you to provide a more intuitive name for the LLM without altering your existing codebase.
Prompt Impact (class)
Without @alias
class MyClass {
  property1 string
}

ctx.output_format:
{
  property1: string
}

With @alias
class MyClass {
  property1 string @alias("name")
}

ctx.output_format:
{
  name: string
}

Prompt Impact (enum)
enum MyEnum {
  Value1 
  // Note that @@alias is applied to the enum itself, not the value
  @@alias("My Name")
}

ctx.output_format:
My Name
---
Value1

Prompt Impact (enum value)
enum MyEnum {
  Value1 @alias("Something")
}

ctx.output_format:
MyEnum
---
Something
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias",
    "title": "@alias / @@alias",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/alias-alias#prompt-impact-class",
        "title": "Prompt Impact (class)",
      },
    ],
    "description": "class MyClass {
  property1 string
}

ctx.output_format:
{
  property1: string
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias#without-alias",
    "title": "Without @alias",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Prompt Impact (class)",
    ],
    "content": "class MyClass {
  property1 string
}

ctx.output_format:
{
  property1: string
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias#without-alias",
    "title": "Without @alias",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/alias-alias#prompt-impact-class",
        "title": "Prompt Impact (class)",
      },
    ],
    "description": "class MyClass {
  property1 string @alias("name")
}

ctx.output_format:
{
  name: string
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias#with-alias",
    "title": "With @alias",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Prompt Impact (class)",
    ],
    "content": "class MyClass {
  property1 string @alias("name")
}

ctx.output_format:
{
  name: string
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias#with-alias",
    "title": "With @alias",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "enum MyEnum {
  Value1 
  // Note that @@alias is applied to the enum itself, not the value
  @@alias("My Name")
}

ctx.output_format:
My Name
---
Value1
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias#prompt-impact-enum",
    "title": "Prompt Impact (enum)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "enum MyEnum {
  Value1 
  // Note that @@alias is applied to the enum itself, not the value
  @@alias("My Name")
}

ctx.output_format:
My Name
---
Value1
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias#prompt-impact-enum",
    "title": "Prompt Impact (enum)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "enum MyEnum {
  Value1 @alias("Something")
}

ctx.output_format:
MyEnum
---
Something
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias#prompt-impact-enum-value",
    "title": "Prompt Impact (enum value)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "enum MyEnum {
  Value1 @alias("Something")
}

ctx.output_format:
MyEnum
---
Something
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/alias-alias#prompt-impact-enum-value",
    "title": "Prompt Impact (enum value)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "The `@description` attribute in BAML provides additional context to fields or values in prompts. This can help the LLM understand the intended use or meaning of a field or value.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description",
    "title": "@description / @@description",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "The @description attribute in BAML provides additional context to fields or values in prompts. This can help the LLM understand the intended use or meaning of a field or value.
Prompt Impact
Without @description
class MyClass {
  property1 string
}

ctx.output_format:
{
  property1: string
}

With @description
class MyClass {
  property1 string @description("The name of the object")
}

ctx.output_format:
{
  // The name of the object
  property1: string
}

Prompt Impact (enum - value)
Without @description
enum MyEnum {
  Value1
  Value2
}

ctx.output_format:
MyEnum
---
Value1
Value2

With @description
enum MyEnum {
  Value1 @description("The first value")
  Value2 @description("The second value")
}

ctx.output_format:
MyEnum
---
Value1 // The first value
Value2 // The second value

Prompt Impact (enum)
enum MyEnum {
  Value1
  Value2

  @@description("This enum represents status codes")
}

ctx.output_format:
MyEnum: This enum represents status codes
---
Value1
Value2
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description",
    "title": "@description / @@description",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/description-description#prompt-impact",
        "title": "Prompt Impact",
      },
    ],
    "description": "class MyClass {
  property1 string
}

ctx.output_format:
{
  property1: string
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#without-description",
    "title": "Without @description",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Prompt Impact",
    ],
    "content": "class MyClass {
  property1 string
}

ctx.output_format:
{
  property1: string
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#without-description",
    "title": "Without @description",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/description-description#prompt-impact",
        "title": "Prompt Impact",
      },
    ],
    "description": "class MyClass {
  property1 string @description("The name of the object")
}

ctx.output_format:
{
  // The name of the object
  property1: string
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#with-description",
    "title": "With @description",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Prompt Impact",
    ],
    "content": "class MyClass {
  property1 string @description("The name of the object")
}

ctx.output_format:
{
  // The name of the object
  property1: string
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#with-description",
    "title": "With @description",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/description-description#prompt-impact-enum---value",
        "title": "Prompt Impact (enum - value)",
      },
    ],
    "description": "enum MyEnum {
  Value1
  Value2
}

ctx.output_format:
MyEnum
---
Value1
Value2
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#without-description-1",
    "title": "Without @description",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Prompt Impact (enum - value)",
    ],
    "content": "enum MyEnum {
  Value1
  Value2
}

ctx.output_format:
MyEnum
---
Value1
Value2
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#without-description-1",
    "title": "Without @description",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/description-description#prompt-impact-enum---value",
        "title": "Prompt Impact (enum - value)",
      },
    ],
    "description": "enum MyEnum {
  Value1 @description("The first value")
  Value2 @description("The second value")
}

ctx.output_format:
MyEnum
---
Value1 // The first value
Value2 // The second value
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#with-description-1",
    "title": "With @description",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Prompt Impact (enum - value)",
    ],
    "content": "enum MyEnum {
  Value1 @description("The first value")
  Value2 @description("The second value")
}

ctx.output_format:
MyEnum
---
Value1 // The first value
Value2 // The second value
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#with-description-1",
    "title": "With @description",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "enum MyEnum {
  Value1
  Value2

  @@description("This enum represents status codes")
}

ctx.output_format:
MyEnum: This enum represents status codes
---
Value1
Value2
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#prompt-impact-enum",
    "title": "Prompt Impact (enum)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "enum MyEnum {
  Value1
  Value2

  @@description("This enum represents status codes")
}

ctx.output_format:
MyEnum: This enum represents status codes
---
Value1
Value2
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/description-description#prompt-impact-enum",
    "title": "Prompt Impact (enum)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "The `@skip` attribute in BAML is used to exclude certain fields or values from being included in prompts or parsed responses. This can be useful when certain data is not relevant for the LLM's processing.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/skip",
    "title": "@skip",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "The @skip attribute in BAML is used to exclude certain fields or values from being included in prompts or parsed responses. This can be useful when certain data is not relevant for the LLM's processing.
Prompt Impact
Without @skip
enum MyEnum {
  Value1
  Value2
}

ctx.output_format:
MyEnum
---
Value1
Value2

With @skip
enum MyEnum {
  Value1
  Value2 @skip
}

ctx.output_format:
MyEnum
---
Value1
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/skip",
    "title": "@skip",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/skip#prompt-impact",
        "title": "Prompt Impact",
      },
    ],
    "description": "enum MyEnum {
  Value1
  Value2
}

ctx.output_format:
MyEnum
---
Value1
Value2
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/skip#without-skip",
    "title": "Without @skip",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Prompt Impact",
    ],
    "content": "enum MyEnum {
  Value1
  Value2
}

ctx.output_format:
MyEnum
---
Value1
Value2
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/skip#without-skip",
    "title": "Without @skip",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/skip#prompt-impact",
        "title": "Prompt Impact",
      },
    ],
    "description": "enum MyEnum {
  Value1
  Value2 @skip
}

ctx.output_format:
MyEnum
---
Value1
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/skip#with-skip",
    "title": "With @skip",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Prompt Impact",
    ],
    "content": "enum MyEnum {
  Value1
  Value2 @skip
}

ctx.output_format:
MyEnum
---
Value1
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/skip#with-skip",
    "title": "With @skip",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "The `@assert` attribute in BAML is used for strict validations. If a type fails an `@assert` validation, it will not be returned in the response, and an exception will be raised if it's part of the top-level type.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert",
    "title": "@assert",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "The @assert attribute in BAML is used for strict validations. If a type fails an @assert validation, it will not be returned in the response, and an exception will be raised if it's part of the top-level type.
Usage
Asserts can be named or unnamed.
Field Assertion
class Foo {
  // @assert will be applied to the field with the name "bar"
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }})
}

class Foo {
  // @assert will be applied to the field with no name
  bar int @assert({{ this > 0 and this < 10 }})
}

class MyClass {
  // @assert will be applied to each element in the array
  my_field (string @assert(is_valid_email, {{ this.contains("@") }}))[]
}

Parameter Assertion
Asserts can also be applied to parameters.
function MyFunction(x: int @assert(between_0_and_10, {{ this > 0 and this < 10 }})) {
  client "openai/gpt-4o"
  prompt #"Hello, world!"#
}

Block Assertion
Asserts can be used in a block definition, referencing fields within the block.
class Foo {
  bar int
  baz string
  @@assert(baz_length_limit, {{ this.baz|length < this.bar }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert",
    "title": "@assert",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "Asserts can be named or unnamed.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "Asserts can be named or unnamed.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/assert#usage",
        "title": "Usage",
      },
    ],
    "description": "class Foo {
  // @assert will be applied to the field with the name "bar"
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }})
}

class Foo {
  // @assert will be applied to the field with no name
  bar int @assert({{ this > 0 and this < 10 }})
}

class MyClass {
  // @assert will be applied to each element in the array
  my_field (string @assert(is_valid_email, {{ this.contains("@") }}))[]
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert#field-assertion",
    "title": "Field Assertion",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Usage",
    ],
    "content": "class Foo {
  // @assert will be applied to the field with the name "bar"
  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }})
}

class Foo {
  // @assert will be applied to the field with no name
  bar int @assert({{ this > 0 and this < 10 }})
}

class MyClass {
  // @assert will be applied to each element in the array
  my_field (string @assert(is_valid_email, {{ this.contains("@") }}))[]
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert#field-assertion",
    "title": "Field Assertion",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/assert#usage",
        "title": "Usage",
      },
    ],
    "description": "Asserts can also be applied to parameters.
function MyFunction(x: int @assert(between_0_and_10, {{ this > 0 and this < 10 }})) {
  client "openai/gpt-4o"
  prompt #"Hello, world!"#
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert#parameter-assertion",
    "title": "Parameter Assertion",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Usage",
    ],
    "content": "Asserts can also be applied to parameters.
function MyFunction(x: int @assert(between_0_and_10, {{ this > 0 and this < 10 }})) {
  client "openai/gpt-4o"
  prompt #"Hello, world!"#
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert#parameter-assertion",
    "title": "Parameter Assertion",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/assert#usage",
        "title": "Usage",
      },
    ],
    "description": "Asserts can be used in a block definition, referencing fields within the block.
class Foo {
  bar int
  baz string
  @@assert(baz_length_limit, {{ this.baz|length < this.bar }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert#block-assertion",
    "title": "Block Assertion",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Usage",
    ],
    "content": "Asserts can be used in a block definition, referencing fields within the block.
class Foo {
  bar int
  baz string
  @@assert(baz_length_limit, {{ this.baz|length < this.bar }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/assert#block-assertion",
    "title": "Block Assertion",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "The `@check` attribute in BAML adds validations without raising exceptions if they fail. This allows the validations to be inspected at runtime.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/check",
    "title": "@check",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "The @check attribute in BAML adds validations without raising exceptions if they fail. This allows the validations to be inspected at runtime.
Usage
Field Check
class Foo {
  bar int @check(less_than_zero, {{ this < 0 }})
}

Block check
class Bar {
  baz int
  quux string
  @@check(quux_limit, {{ this.quux|length < this.baz }})
}

Benefits

Non-Intrusive Validation: Allows for validation checks without interrupting the flow of data processing.
Runtime Inspection: Enables inspection of validation results at runtime.

See more in validations guide.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/check",
    "title": "@check",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/check#usage",
        "title": "Usage",
      },
    ],
    "description": "class Foo {
  bar int @check(less_than_zero, {{ this < 0 }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/check#field-check",
    "title": "Field Check",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Usage",
    ],
    "content": "class Foo {
  bar int @check(less_than_zero, {{ this < 0 }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/check#field-check",
    "title": "Field Check",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/check#usage",
        "title": "Usage",
      },
    ],
    "description": "class Bar {
  baz int
  quux string
  @@check(quux_limit, {{ this.quux|length < this.baz }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/check#block-check",
    "title": "Block check",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Usage",
    ],
    "content": "class Bar {
  baz int
  quux string
  @@check(quux_limit, {{ this.quux|length < this.baz }})
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/check#block-check",
    "title": "Block check",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "
Non-Intrusive Validation: Allows for validation checks without interrupting the flow of data processing.
Runtime Inspection: Enables inspection of validation results at runtime.

See more in validations guide.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/check#benefits",
    "title": "Benefits",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "
Non-Intrusive Validation: Allows for validation checks without interrupting the flow of data processing.
Runtime Inspection: Enables inspection of validation results at runtime.

See more in validations guide.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/check#benefits",
    "title": "Benefits",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "The `@@dynamic` attribute in BAML allows for the dynamic modification of fields or values at runtime. This is particularly useful when you need to adapt the structure of your data models based on runtime conditions or external inputs.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic",
    "title": "@@dynamic",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "The @@dynamic attribute in BAML allows for the dynamic modification of fields or values at runtime. This is particularly useful when you need to adapt the structure of your data models based on runtime conditions or external inputs.
Usage
Dynamic Classes
The @@dynamic attribute can be applied to classes, enabling the addition of fields dynamically during runtime.
class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows adding fields dynamically at runtime
}

Dynamic Enums
Similarly, the @@dynamic attribute can be applied to enums, allowing for the modification of enum values at runtime.
enum MyEnum {
  Value1
  Value2

  @@dynamic // allows modifying enum values dynamically at runtime
}

Using @@dynamic with TypeBuilder
To modify dynamic types at runtime, you can use the TypeBuilder from the baml_client. Below are examples for Python, TypeScript, and Ruby.
Python Example
from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.MyClass.add_property('email', tb.string())
  tb.MyClass.add_property('address', tb.string()).description("The user's address")
  res = await b.DynamicUserCreator("some user info", { "tb": tb })
  # Now res can have email and address fields
  print(res)

TypeScript Example
import TypeBuilder from '../baml_client/type_builder'
import { b } from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.MyClass.addProperty('email', tb.string())
  tb.MyClass.addProperty('address', tb.string()).description("The user's address")
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res can have email and address fields
  console.log(res)
}

Ruby Example
require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  tb.MyClass.add_property('email', tb.string)
  tb.MyClass.add_property('address', tb.string).description("The user's address")
  
  res = Baml::Client.dynamic_user_creator(input: "some user info", baml_options: {tb: tb})
  # Now res can have email and address fields
  puts res
end
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic",
    "title": "@@dynamic",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/dynamic#usage",
        "title": "Usage",
      },
    ],
    "description": "The @@dynamic attribute can be applied to classes, enabling the addition of fields dynamically during runtime.
class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows adding fields dynamically at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#dynamic-classes",
    "title": "Dynamic Classes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Usage",
    ],
    "content": "The @@dynamic attribute can be applied to classes, enabling the addition of fields dynamically during runtime.
class MyClass {
  property1 string
  property2 int?

  @@dynamic // allows adding fields dynamically at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#dynamic-classes",
    "title": "Dynamic Classes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/dynamic#usage",
        "title": "Usage",
      },
    ],
    "description": "Similarly, the @@dynamic attribute can be applied to enums, allowing for the modification of enum values at runtime.
enum MyEnum {
  Value1
  Value2

  @@dynamic // allows modifying enum values dynamically at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#dynamic-enums",
    "title": "Dynamic Enums",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Usage",
    ],
    "content": "Similarly, the @@dynamic attribute can be applied to enums, allowing for the modification of enum values at runtime.
enum MyEnum {
  Value1
  Value2

  @@dynamic // allows modifying enum values dynamically at runtime
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#dynamic-enums",
    "title": "Dynamic Enums",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
    ],
    "description": "To modify dynamic types at runtime, you can use the TypeBuilder from the baml_client. Below are examples for Python, TypeScript, and Ruby.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#using-dynamic-with-typebuilder",
    "title": "Using @@dynamic with TypeBuilder",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
    ],
    "content": "To modify dynamic types at runtime, you can use the TypeBuilder from the baml_client. Below are examples for Python, TypeScript, and Ruby.",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#using-dynamic-with-typebuilder",
    "title": "Using @@dynamic with TypeBuilder",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/dynamic#using-dynamic-with-typebuilder",
        "title": "Using @@dynamic with TypeBuilder",
      },
    ],
    "description": "from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.MyClass.add_property('email', tb.string())
  tb.MyClass.add_property('address', tb.string()).description("The user's address")
  res = await b.DynamicUserCreator("some user info", { "tb": tb })
  # Now res can have email and address fields
  print(res)
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#python-example",
    "title": "Python Example",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Using @@dynamic with TypeBuilder",
    ],
    "content": "from baml_client.type_builder import TypeBuilder
from baml_client import b

async def run():
  tb = TypeBuilder()
  tb.MyClass.add_property('email', tb.string())
  tb.MyClass.add_property('address', tb.string()).description("The user's address")
  res = await b.DynamicUserCreator("some user info", { "tb": tb })
  # Now res can have email and address fields
  print(res)
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#python-example",
    "title": "Python Example",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/dynamic#using-dynamic-with-typebuilder",
        "title": "Using @@dynamic with TypeBuilder",
      },
    ],
    "description": "import TypeBuilder from '../baml_client/type_builder'
import { b } from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.MyClass.addProperty('email', tb.string())
  tb.MyClass.addProperty('address', tb.string()).description("The user's address")
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res can have email and address fields
  console.log(res)
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#typescript-example",
    "title": "TypeScript Example",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Using @@dynamic with TypeBuilder",
    ],
    "content": "import TypeBuilder from '../baml_client/type_builder'
import { b } from '../baml_client'

async function run() {
  const tb = new TypeBuilder()
  tb.MyClass.addProperty('email', tb.string())
  tb.MyClass.addProperty('address', tb.string()).description("The user's address")
  const res = await b.DynamicUserCreator("some user info", { tb: tb })
  // Now res can have email and address fields
  console.log(res)
}
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#typescript-example",
    "title": "TypeScript Example",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/attributes/what-are-attributes",
        "title": "Attributes",
      },
      {
        "slug": "ref/attributes/dynamic#using-dynamic-with-typebuilder",
        "title": "Using @@dynamic with TypeBuilder",
      },
    ],
    "description": "require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  tb.MyClass.add_property('email', tb.string)
  tb.MyClass.add_property('address', tb.string).description("The user's address")
  
  res = Baml::Client.dynamic_user_creator(input: "some user info", baml_options: {tb: tb})
  # Now res can have email and address fields
  puts res
end
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#ruby-example",
    "title": "Ruby Example",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Attributes",
      "Using @@dynamic with TypeBuilder",
    ],
    "content": "require_relative 'baml_client/client'

def run
  tb = Baml::TypeBuilder.new
  tb.MyClass.add_property('email', tb.string)
  tb.MyClass.add_property('address', tb.string).description("The user's address")
  
  res = Baml::Client.dynamic_user_creator(input: "some user info", baml_options: {tb: tb})
  # Now res can have email and address fields
  puts res
end
",
    "indexSegmentId": "0",
    "slug": "ref/attributes/dynamic#ruby-example",
    "title": "Ruby Example",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "AWS Bedrock provider for BAML

The `aws-bedrock` provider supports all text-output models available via the
[`Converse` API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).

Example:

```baml BAML
client<llm> MyClient {
  provider aws-bedrock
  options {
    api_key env.MY_OPENAI_KEY
    model "gpt-3.5-turbo"
    temperature 0.1
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock",
    "title": "aws-bedrock",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The aws-bedrock provider supports all text-output models available via the
Converse API.
Example:
client<llm> MyClient {
  provider aws-bedrock
  options {
    api_key env.MY_OPENAI_KEY
    model "gpt-3.5-turbo"
    temperature 0.1
  }
}

Authorization
We use the AWS SDK under the hood, which will respect all authentication
mechanisms supported by the
SDK, including but not
limited to:

AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as set in your environment variables
loading the specified AWS_PROFILE from ~/.aws/config
built-in authn for services running in EC2, ECS, Lambda, etc.

Playground setup
Add these three environment variables to your extension variables to use the AWS Bedrock provider in the playground.

AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_REGION - like us-east-1


Non-forwarded options
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.
Forwarded options
BAML will auto construct this field for you from the prompt
The model to use.| Model           | Description                    |
| --------------- | ------------------------------ |
| anthropic.claude-3-haiku-20240307-v1:0  | Fastest + Cheapest    |
| anthropic.claude-3-sonnet-20240307-v1:0 | Smartest              |
| meta.llama3-8b-instruct-v1:0            |                       |
| meta.llama3-70b-instruct-v1:0           |                       |
| mistral.mistral-7b-instruct-v0:2        |                       |
| mistral.mixtral-8x7b-instruct-v0:1      |                       |Run aws bedrock list-foundation-models | jq '.modelSummaries.[].modelId to get
a list of available foundation models; you can also use any custom models you've
deployed.Note that to use any of these models you'll need to request model access.
Additional inference configuration to send with the request; see AWS Bedrock
documentation.Example:client<llm> MyClient {
  provider aws-bedrock
  options {
    inference_configuration {
      max_tokens 1000
      temperature 1.0
      top_p 0.8
      stop_sequence ["_EOF"]
    }
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock",
    "title": "aws-bedrock",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "We use the AWS SDK under the hood, which will respect all authentication
mechanisms supported by the
SDK, including but not
limited to:

AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as set in your environment variables
loading the specified AWS_PROFILE from ~/.aws/config
built-in authn for services running in EC2, ECS, Lambda, etc.
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock#authorization",
    "title": "Authorization",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "We use the AWS SDK under the hood, which will respect all authentication
mechanisms supported by the
SDK, including but not
limited to:

AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY as set in your environment variables
loading the specified AWS_PROFILE from ~/.aws/config
built-in authn for services running in EC2, ECS, Lambda, etc.
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock#authorization",
    "title": "Authorization",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "Add these three environment variables to your extension variables to use the AWS Bedrock provider in the playground.

AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_REGION - like us-east-1

",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock#playground-setup",
    "title": "Playground setup",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Add these three environment variables to your extension variables to use the AWS Bedrock provider in the playground.

AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
AWS_REGION - like us-east-1

",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock#playground-setup",
    "title": "Playground setup",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "BAML will auto construct this field for you from the prompt
The model to use.| Model           | Description                    |
| --------------- | ------------------------------ |
| anthropic.claude-3-haiku-20240307-v1:0  | Fastest + Cheapest    |
| anthropic.claude-3-sonnet-20240307-v1:0 | Smartest              |
| meta.llama3-8b-instruct-v1:0            |                       |
| meta.llama3-70b-instruct-v1:0           |                       |
| mistral.mistral-7b-instruct-v0:2        |                       |
| mistral.mixtral-8x7b-instruct-v0:1      |                       |Run aws bedrock list-foundation-models | jq '.modelSummaries.[].modelId to get
a list of available foundation models; you can also use any custom models you've
deployed.Note that to use any of these models you'll need to request model access.
Additional inference configuration to send with the request; see AWS Bedrock
documentation.Example:client<llm> MyClient {
  provider aws-bedrock
  options {
    inference_configuration {
      max_tokens 1000
      temperature 1.0
      top_p 0.8
      stop_sequence ["_EOF"]
    }
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "BAML will auto construct this field for you from the prompt
The model to use.| Model           | Description                    |
| --------------- | ------------------------------ |
| anthropic.claude-3-haiku-20240307-v1:0  | Fastest + Cheapest    |
| anthropic.claude-3-sonnet-20240307-v1:0 | Smartest              |
| meta.llama3-8b-instruct-v1:0            |                       |
| meta.llama3-70b-instruct-v1:0           |                       |
| mistral.mistral-7b-instruct-v0:2        |                       |
| mistral.mixtral-8x7b-instruct-v0:1      |                       |Run aws bedrock list-foundation-models | jq '.modelSummaries.[].modelId to get
a list of available foundation models; you can also use any custom models you've
deployed.Note that to use any of these models you'll need to request model access.
Additional inference configuration to send with the request; see AWS Bedrock
documentation.Example:client<llm> MyClient {
  provider aws-bedrock
  options {
    inference_configuration {
      max_tokens 1000
      temperature 1.0
      top_p 0.8
      stop_sequence ["_EOF"]
    }
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/aws-bedrock#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The `anthropic` provider supports all APIs that use the same interface for the `/v1/messages` endpoint.

Example:
```baml BAML
client<llm> MyClient {
  provider anthropic
  options {
    model "claude-3-5-sonnet-20240620"
    temperature 0
  }
}
```

The options are passed through directly to the API, barring a few. Here's a shorthand of the options:",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/anthropic",
    "title": "anthropic",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The anthropic provider supports all APIs that use the same interface for the /v1/messages endpoint.
Example:
client<llm> MyClient {
  provider anthropic
  options {
    model "claude-3-5-sonnet-20240620"
    temperature 0
  }
}

The options are passed through directly to the API, barring a few. Here's a shorthand of the options:
Non-forwarded options
Will be passed as a bearer token. Default: env.ANTHROPIC_API_KEYAuthorization: Bearer $api_key
The base URL for the API. Default: https://api.anthropic.com
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Additional headers to send with the request.Unless specified with a different value, we inject in the following headers:"anthropic-version" "2023-06-01"
Example:client<llm> MyClient {
  provider anthropic
  options {
    api_key env.MY_ANTHROPIC_KEY
    model "claude-3-5-sonnet-20240620"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["cache_control"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> ClaudeWithCaching {
  provider anthropic
  options {
    model claude-3-haiku-20240307
    api_key env.ANTHROPIC_API_KEY
    max_tokens 1000
    allowed_role_metadata ["cache_control"]
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }
}

client<llm> FooWithout {
  provider anthropic
  options {
  }
}

template_string Foo() #"
  {{ _.role('user', cache_control={"type": "ephemeral"}) }}
  This will be cached for ClaudeWithCaching, but not for FooWithout!
  {{ _.role('user') }}
  This will not be cached for Foo or FooWithout!
"#
You can use the playground to see the raw curl request to see what is being sent to the API.
Forwarded options
BAML will auto construct this field for you from the prompt, if necessary.
Only the first system message will be used, all subsequent ones will be cast to the assistant role.
BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model |
| --- |
| claude-3-5-sonnet-20240620 |
| claude-3-opus-20240229 |
| claude-3-sonnet-20240229 |
| claude-3-haiku-20240307 |See anthropic docs for the latest list of all models. You can pass any model name you wish, we will not check if it exists.
The maximum number of tokens to generate. Default: 4069
For all other options, see the official anthropic API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/anthropic",
    "title": "anthropic",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "Will be passed as a bearer token. Default: env.ANTHROPIC_API_KEYAuthorization: Bearer $api_key
The base URL for the API. Default: https://api.anthropic.com
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Additional headers to send with the request.Unless specified with a different value, we inject in the following headers:"anthropic-version" "2023-06-01"
Example:client<llm> MyClient {
  provider anthropic
  options {
    api_key env.MY_ANTHROPIC_KEY
    model "claude-3-5-sonnet-20240620"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["cache_control"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> ClaudeWithCaching {
  provider anthropic
  options {
    model claude-3-haiku-20240307
    api_key env.ANTHROPIC_API_KEY
    max_tokens 1000
    allowed_role_metadata ["cache_control"]
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }
}

client<llm> FooWithout {
  provider anthropic
  options {
  }
}

template_string Foo() #"
  {{ _.role('user', cache_control={"type": "ephemeral"}) }}
  This will be cached for ClaudeWithCaching, but not for FooWithout!
  {{ _.role('user') }}
  This will not be cached for Foo or FooWithout!
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/anthropic#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Will be passed as a bearer token. Default: env.ANTHROPIC_API_KEYAuthorization: Bearer $api_key
The base URL for the API. Default: https://api.anthropic.com
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Additional headers to send with the request.Unless specified with a different value, we inject in the following headers:"anthropic-version" "2023-06-01"
Example:client<llm> MyClient {
  provider anthropic
  options {
    api_key env.MY_ANTHROPIC_KEY
    model "claude-3-5-sonnet-20240620"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["cache_control"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> ClaudeWithCaching {
  provider anthropic
  options {
    model claude-3-haiku-20240307
    api_key env.ANTHROPIC_API_KEY
    max_tokens 1000
    allowed_role_metadata ["cache_control"]
    headers {
      "anthropic-beta" "prompt-caching-2024-07-31"
    }
  }
}

client<llm> FooWithout {
  provider anthropic
  options {
  }
}

template_string Foo() #"
  {{ _.role('user', cache_control={"type": "ephemeral"}) }}
  This will be cached for ClaudeWithCaching, but not for FooWithout!
  {{ _.role('user') }}
  This will not be cached for Foo or FooWithout!
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/anthropic#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "BAML will auto construct this field for you from the prompt, if necessary.
Only the first system message will be used, all subsequent ones will be cast to the assistant role.
BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model |
| --- |
| claude-3-5-sonnet-20240620 |
| claude-3-opus-20240229 |
| claude-3-sonnet-20240229 |
| claude-3-haiku-20240307 |See anthropic docs for the latest list of all models. You can pass any model name you wish, we will not check if it exists.
The maximum number of tokens to generate. Default: 4069
For all other options, see the official anthropic API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/anthropic#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "BAML will auto construct this field for you from the prompt, if necessary.
Only the first system message will be used, all subsequent ones will be cast to the assistant role.
BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model |
| --- |
| claude-3-5-sonnet-20240620 |
| claude-3-opus-20240229 |
| claude-3-sonnet-20240229 |
| claude-3-haiku-20240307 |See anthropic docs for the latest list of all models. You can pass any model name you wish, we will not check if it exists.
The maximum number of tokens to generate. Default: 4069
For all other options, see the official anthropic API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/anthropic#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The `google-ai` provider supports the `https://generativelanguage.googleapis.com/v1beta/models/{model_id}/generateContent` and `https://generativelanguage.googleapis.com/v1beta/models/{model_id}/streamGenerateContent` endpoints.

<Tip>
The use of `v1beta` rather than `v1` aligns with the endpoint conventions established in [Google's SDKs](https://github.com/google-gemini/generative-ai-python/blob/8a29017e9120f0552ee3ad6092e8545d1aa6f803/google/generativeai/client.py#L60) and offers access to both the existing `v1` models and additional models exclusive to `v1beta`.
</Tip>

<Tip>
BAML will automatically pick `streamGenerateContent` if you call the streaming interface.
</Tip>

Example:
```baml BAML
client<llm> MyClient {
  provider google-ai
  options {
    model "gemini-1.5-flash"
  }
}
```

The options are passed through directly to the API, barring a few. Here's a shorthand of the options:",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-ai-gemini",
    "title": "google-ai",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The google-ai provider supports the https://generativelanguage.googleapis.com/v1beta/models/{model_id}/generateContent and https://generativelanguage.googleapis.com/v1beta/models/{model_id}/streamGenerateContent endpoints.
The use of v1beta rather than v1 aligns with the endpoint conventions established in Google's SDKs and offers access to both the existing v1 models and additional models exclusive to v1beta.
BAML will automatically pick streamGenerateContent if you call the streaming interface.
Example:
client<llm> MyClient {
  provider google-ai
  options {
    model "gemini-1.5-flash"
  }
}

The options are passed through directly to the API, barring a few. Here's a shorthand of the options:
Non-forwarded options
Will be passed as the x-goog-api-key header. Default: env.GOOGLE_API_KEYx-goog-api-key: $api_key
The base URL for the API. Default: https://generativelanguage.googleapis.com/v1beta
The default role for any prompts that don't specify a role. Default: userWe don't have any checks for this field, you can pass any string you wish.
The model to use. Default: gemini-1.5-flashWe don't have any checks for this field, you can pass any string you wish.| Model | Input(s) | Optimized for |
| --- | ---  | --- |
| gemini-1.5-pro  | Audio, images, videos, and text | Complex reasoning tasks such as code and text generation, text editing, problem solving, data extraction and generation |
| gemini-1.5-flash  | Audio, images, videos, and text | Fast and versatile performance across a diverse variety of tasks |
| gemini-1.0-pro | Text | Natural language tasks, multi-turn text and code chat, and code generation |See the Google Model Docs for the latest models.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider google-ai
  options {
    model "gemini-1.5-flash"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.
Forwarded options
BAML will auto construct this field for you from the prompt
For all other options, see the official Google Gemini API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-ai-gemini",
    "title": "google-ai",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "Will be passed as the x-goog-api-key header. Default: env.GOOGLE_API_KEYx-goog-api-key: $api_key
The base URL for the API. Default: https://generativelanguage.googleapis.com/v1beta
The default role for any prompts that don't specify a role. Default: userWe don't have any checks for this field, you can pass any string you wish.
The model to use. Default: gemini-1.5-flashWe don't have any checks for this field, you can pass any string you wish.| Model | Input(s) | Optimized for |
| --- | ---  | --- |
| gemini-1.5-pro  | Audio, images, videos, and text | Complex reasoning tasks such as code and text generation, text editing, problem solving, data extraction and generation |
| gemini-1.5-flash  | Audio, images, videos, and text | Fast and versatile performance across a diverse variety of tasks |
| gemini-1.0-pro | Text | Natural language tasks, multi-turn text and code chat, and code generation |See the Google Model Docs for the latest models.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider google-ai
  options {
    model "gemini-1.5-flash"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-ai-gemini#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Will be passed as the x-goog-api-key header. Default: env.GOOGLE_API_KEYx-goog-api-key: $api_key
The base URL for the API. Default: https://generativelanguage.googleapis.com/v1beta
The default role for any prompts that don't specify a role. Default: userWe don't have any checks for this field, you can pass any string you wish.
The model to use. Default: gemini-1.5-flashWe don't have any checks for this field, you can pass any string you wish.| Model | Input(s) | Optimized for |
| --- | ---  | --- |
| gemini-1.5-pro  | Audio, images, videos, and text | Complex reasoning tasks such as code and text generation, text editing, problem solving, data extraction and generation |
| gemini-1.5-flash  | Audio, images, videos, and text | Fast and versatile performance across a diverse variety of tasks |
| gemini-1.0-pro | Text | Natural language tasks, multi-turn text and code chat, and code generation |See the Google Model Docs for the latest models.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider google-ai
  options {
    model "gemini-1.5-flash"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-ai-gemini#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "BAML will auto construct this field for you from the prompt
For all other options, see the official Google Gemini API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-ai-gemini#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "BAML will auto construct this field for you from the prompt
For all other options, see the official Google Gemini API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-ai-gemini#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The `vertex-ai` provider is used to interact with the Google Vertex AI services, specifically the following endpoints:

```
https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent
https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:streamGenerateContent
```




Example: 
```baml BAML
client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex",
    "title": "vertex-ai",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The vertex-ai provider is used to interact with the Google Vertex AI services, specifically the following endpoints:
https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent
https://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:streamGenerateContent

Example:
client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1
  }
}

Authorization
The vertex-ai provider uses the Google Cloud SDK to authenticate with a temporary access token. We generate these Google Cloud Authentication Tokens using Google Cloud service account credentials. We do not store this token, and it is only used for the duration of the request.
Instructions for downloading Google Cloud credentials

Go to the Google Cloud Console.
Click on the project you want to use.
Select the IAM & Admin section, and click on Service Accounts.
Select an existing service account or create a new one.
Click on the service account and select Add Key.
Choose the JSON key type and click Create.
Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of the file.

See the Google Cloud Application Default Credentials Docs for more information.
The project_id of your client object must match the project_id of your credentials file.
The options are passed through directly to the API, barring a few. Here's a shorthand of the options:
Non-forwarded options
The base URL for the API.Default: https://{LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/{LOCATION}/publishers/google/models/ Can be used in lieu of the project_id and location fields, to manually set the request URL.
Vertex requires a Google Cloud project ID for each request. See the Google Cloud Project ID Docs for more information.
Vertex requires a location for each request. Some locations may have different models avaiable.Common locations include:
us-central1
us-west1
us-east1
us-south1
See the Vertex Location Docs for all locations and supported models.
Path to a JSON credentials file or a JSON object containing the credentials.Default: env.GOOGLE_APPLICATION_CREDENTIALSIn this case, the path is resolved relative to the CWD of your process.client<llm> Vertex {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id jane-doe-test-1
    location us-central1
    credentials 'path/to/credentials.json'
  }
}
client<llm> Vertex {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id jane-doe-mycompany-1
    location us-central1
    credentials {
      ...
      private_key "-----BEGIN PRIVATE KEY-----super-duper-secret-string\n-----END PRIVATE KEY-----\n"
      client_email "jane_doe@mycompany.com"
      ...
    }
  }
}
This field cannot be used in the BAML Playground. For the playground, use the credentials_content instead.
Overrides contents of the Google Cloud Application Credentials. Default: env.GOOGLE_APPLICATION_CREDENTIALS_CONTENT    {
      "type": "service_account",
      "project_id": "my-project-id",
      "private_key_id": "string",
      "private_key": "-----BEGIN PRIVATE KEY-----string\n-----END PRIVATE KEY-----\n",
      "client_email": "john_doe@gmail.com",
      "client_id": "123456",
      "auth_uri": "https://accounts.google.com/o/oauth2/auth",
      "token_uri": "https://oauth2.googleapis.com/token",
      "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
      "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/...",
      "universe_domain": "googleapis.com"
    }
Only use this for the BAML Playground only. Use credentials for your runtime code.
Directly set Google Cloud Authentication Token in lieu of token generation via env.GOOGLE_APPLICATION_CREDENTIALS or env.GOOGLE_APPLICATION_CREDENTIALS_CONTENT fields.
The default role for any prompts that don't specify a role. Default: user
The Google model to use for the request.| Model | Input(s) | Optimized for |
| --- | ---  | --- |
| gemini-1.5-pro  | Audio, images, videos, and text | Complex reasoning tasks such as code and text generation, text editing, problem solving, data extraction and generation |
| gemini-1.5-flash  | Audio, images, videos, and text | Fast and versatile performance across a diverse variety of tasks |
| gemini-1.0-pro | Text | Natural language tasks, multi-turn text and code chat, and code generation |See the Google Model Docs for the latest models.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1
    // Additional headers
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.
Forwarded options
Safety settings to apply to the request. You can stack different safety settings with a new safetySettings header for each one. See the Google Vertex API Request Docs for more information on what safety settings can be set.client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1

    safetySettings {
      category HARM_CATEGORY_HATE_SPEECH
      threshold BLOCK_LOW_AND_ABOVE
      method SEVERITY
    }
  }
}

Generation configurations to apply to the request. See the Google Vertex API Request Docs for more information on what properties can be set.client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1
    
    generationConfig {
      maxOutputTokens 100
      temperature 1
    }
  }
}

For all other options, see the official Vertex AI documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex",
    "title": "vertex-ai",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The vertex-ai provider uses the Google Cloud SDK to authenticate with a temporary access token. We generate these Google Cloud Authentication Tokens using Google Cloud service account credentials. We do not store this token, and it is only used for the duration of the request.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex#authorization",
    "title": "Authorization",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The vertex-ai provider uses the Google Cloud SDK to authenticate with a temporary access token. We generate these Google Cloud Authentication Tokens using Google Cloud service account credentials. We do not store this token, and it is only used for the duration of the request.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex#authorization",
    "title": "Authorization",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
      {
        "slug": "ref/llm-client-providers/google-vertex#authorization",
        "title": "Authorization",
      },
    ],
    "description": "
Go to the Google Cloud Console.
Click on the project you want to use.
Select the IAM & Admin section, and click on Service Accounts.
Select an existing service account or create a new one.
Click on the service account and select Add Key.
Choose the JSON key type and click Create.
Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of the file.

See the Google Cloud Application Default Credentials Docs for more information.
The project_id of your client object must match the project_id of your credentials file.
The options are passed through directly to the API, barring a few. Here's a shorthand of the options:",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex#instructions-for-downloading-google-cloud-credentials",
    "title": "Instructions for downloading Google Cloud credentials",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
      "Authorization",
    ],
    "content": "
Go to the Google Cloud Console.
Click on the project you want to use.
Select the IAM & Admin section, and click on Service Accounts.
Select an existing service account or create a new one.
Click on the service account and select Add Key.
Choose the JSON key type and click Create.
Set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of the file.

See the Google Cloud Application Default Credentials Docs for more information.
The project_id of your client object must match the project_id of your credentials file.
The options are passed through directly to the API, barring a few. Here's a shorthand of the options:",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex#instructions-for-downloading-google-cloud-credentials",
    "title": "Instructions for downloading Google Cloud credentials",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The base URL for the API.Default: https://{LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/{LOCATION}/publishers/google/models/ Can be used in lieu of the project_id and location fields, to manually set the request URL.
Vertex requires a Google Cloud project ID for each request. See the Google Cloud Project ID Docs for more information.
Vertex requires a location for each request. Some locations may have different models avaiable.Common locations include:
us-central1
us-west1
us-east1
us-south1
See the Vertex Location Docs for all locations and supported models.
Path to a JSON credentials file or a JSON object containing the credentials.Default: env.GOOGLE_APPLICATION_CREDENTIALSIn this case, the path is resolved relative to the CWD of your process.client<llm> Vertex {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id jane-doe-test-1
    location us-central1
    credentials 'path/to/credentials.json'
  }
}
client<llm> Vertex {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id jane-doe-mycompany-1
    location us-central1
    credentials {
      ...
      private_key "-----BEGIN PRIVATE KEY-----super-duper-secret-string\n-----END PRIVATE KEY-----\n"
      client_email "jane_doe@mycompany.com"
      ...
    }
  }
}
This field cannot be used in the BAML Playground. For the playground, use the credentials_content instead.
Overrides contents of the Google Cloud Application Credentials. Default: env.GOOGLE_APPLICATION_CREDENTIALS_CONTENT    {
      "type": "service_account",
      "project_id": "my-project-id",
      "private_key_id": "string",
      "private_key": "-----BEGIN PRIVATE KEY-----string\n-----END PRIVATE KEY-----\n",
      "client_email": "john_doe@gmail.com",
      "client_id": "123456",
      "auth_uri": "https://accounts.google.com/o/oauth2/auth",
      "token_uri": "https://oauth2.googleapis.com/token",
      "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
      "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/...",
      "universe_domain": "googleapis.com"
    }
Only use this for the BAML Playground only. Use credentials for your runtime code.
Directly set Google Cloud Authentication Token in lieu of token generation via env.GOOGLE_APPLICATION_CREDENTIALS or env.GOOGLE_APPLICATION_CREDENTIALS_CONTENT fields.
The default role for any prompts that don't specify a role. Default: user
The Google model to use for the request.| Model | Input(s) | Optimized for |
| --- | ---  | --- |
| gemini-1.5-pro  | Audio, images, videos, and text | Complex reasoning tasks such as code and text generation, text editing, problem solving, data extraction and generation |
| gemini-1.5-flash  | Audio, images, videos, and text | Fast and versatile performance across a diverse variety of tasks |
| gemini-1.0-pro | Text | Natural language tasks, multi-turn text and code chat, and code generation |See the Google Model Docs for the latest models.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1
    // Additional headers
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The base URL for the API.Default: https://{LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/{LOCATION}/publishers/google/models/ Can be used in lieu of the project_id and location fields, to manually set the request URL.
Vertex requires a Google Cloud project ID for each request. See the Google Cloud Project ID Docs for more information.
Vertex requires a location for each request. Some locations may have different models avaiable.Common locations include:
us-central1
us-west1
us-east1
us-south1
See the Vertex Location Docs for all locations and supported models.
Path to a JSON credentials file or a JSON object containing the credentials.Default: env.GOOGLE_APPLICATION_CREDENTIALSIn this case, the path is resolved relative to the CWD of your process.client<llm> Vertex {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id jane-doe-test-1
    location us-central1
    credentials 'path/to/credentials.json'
  }
}
client<llm> Vertex {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id jane-doe-mycompany-1
    location us-central1
    credentials {
      ...
      private_key "-----BEGIN PRIVATE KEY-----super-duper-secret-string\n-----END PRIVATE KEY-----\n"
      client_email "jane_doe@mycompany.com"
      ...
    }
  }
}
This field cannot be used in the BAML Playground. For the playground, use the credentials_content instead.
Overrides contents of the Google Cloud Application Credentials. Default: env.GOOGLE_APPLICATION_CREDENTIALS_CONTENT    {
      "type": "service_account",
      "project_id": "my-project-id",
      "private_key_id": "string",
      "private_key": "-----BEGIN PRIVATE KEY-----string\n-----END PRIVATE KEY-----\n",
      "client_email": "john_doe@gmail.com",
      "client_id": "123456",
      "auth_uri": "https://accounts.google.com/o/oauth2/auth",
      "token_uri": "https://oauth2.googleapis.com/token",
      "auth_provider_x509_cert_url": "https://www.googleapis.com/oauth2/v1/certs",
      "client_x509_cert_url": "https://www.googleapis.com/robot/v1/metadata/...",
      "universe_domain": "googleapis.com"
    }
Only use this for the BAML Playground only. Use credentials for your runtime code.
Directly set Google Cloud Authentication Token in lieu of token generation via env.GOOGLE_APPLICATION_CREDENTIALS or env.GOOGLE_APPLICATION_CREDENTIALS_CONTENT fields.
The default role for any prompts that don't specify a role. Default: user
The Google model to use for the request.| Model | Input(s) | Optimized for |
| --- | ---  | --- |
| gemini-1.5-pro  | Audio, images, videos, and text | Complex reasoning tasks such as code and text generation, text editing, problem solving, data extraction and generation |
| gemini-1.5-flash  | Audio, images, videos, and text | Fast and versatile performance across a diverse variety of tasks |
| gemini-1.0-pro | Text | Natural language tasks, multi-turn text and code chat, and code generation |See the Google Model Docs for the latest models.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1
    // Additional headers
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "Safety settings to apply to the request. You can stack different safety settings with a new safetySettings header for each one. See the Google Vertex API Request Docs for more information on what safety settings can be set.client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1

    safetySettings {
      category HARM_CATEGORY_HATE_SPEECH
      threshold BLOCK_LOW_AND_ABOVE
      method SEVERITY
    }
  }
}

Generation configurations to apply to the request. See the Google Vertex API Request Docs for more information on what properties can be set.client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1
    
    generationConfig {
      maxOutputTokens 100
      temperature 1
    }
  }
}

For all other options, see the official Vertex AI documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Safety settings to apply to the request. You can stack different safety settings with a new safetySettings header for each one. See the Google Vertex API Request Docs for more information on what safety settings can be set.client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1

    safetySettings {
      category HARM_CATEGORY_HATE_SPEECH
      threshold BLOCK_LOW_AND_ABOVE
      method SEVERITY
    }
  }
}

Generation configurations to apply to the request. See the Google Vertex API Request Docs for more information on what properties can be set.client<llm> MyClient {
  provider vertex-ai
  options {
    model gemini-1.5-pro
    project_id my-project-id
    location us-central1
    
    generationConfig {
      maxOutputTokens 100
      temperature 1
    }
  }
}

For all other options, see the official Vertex AI documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/google-vertex#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The `openai` provider supports the OpenAI `/chat` endpoint, setting OpenAI-specific
default configuration options.

<Tip>
  For Azure, we recommend using [`azure-openai`](azure) instead.

  For all other OpenAI-compatible API providers, such as Groq, HuggingFace,
  Ollama, OpenRouter, Together AI, and others, we recommend using
 [`openai-generic`](openai-generic) instead.
</Tip>

Example:

```baml BAML
client<llm> MyClient {
  provider "openai"
  options {
    api_key env.MY_OPENAI_KEY
    model "gpt-3.5-turbo"
    temperature 0.1
  }
}
```

The options are passed through directly to the API, barring a few. Here's a shorthand of the options:",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai",
    "title": "openai",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The openai provider supports the OpenAI /chat endpoint, setting OpenAI-specific
default configuration options.
For Azure, we recommend using azure-openai instead.For all other OpenAI-compatible API providers, such as Groq, HuggingFace,
Ollama, OpenRouter, Together AI, and others, we recommend using
openai-generic instead.
Example:
client<llm> MyClient {
  provider "openai"
  options {
    api_key env.MY_OPENAI_KEY
    model "gpt-3.5-turbo"
    temperature 0.1
  }
}

The options are passed through directly to the API, barring a few. Here's a shorthand of the options:
Non-forwarded options
Will be used to build the Authorization header, like so: Authorization: Bearer $api_keyDefault: env.OPENAI_API_KEY
The base URL for the API.Default: https://api.openai.com/v1
The default role for any prompts that don't specify a role.We don't do any validation of this field, so you can pass any string you wish.Default: system
Additional headers to send with the request.Example:client<llm> MyClient {
  provider openai
  options {
    api_key env.MY_OPENAI_KEY
    model "gpt-3.5-turbo"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.
Forwarded options
BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model           | Description                    |
| --------------- | ------------------------------ |
| gpt-3.5-turbo | Fastest                        |
| gpt-4o        | Fast + text + image            |
| gpt-4-turbo   | Smartest + text + image + code |
| gpt-4o-mini   | Cheapest + text + image        |See openai docs for the list of openai models. You can pass any model name you wish, we will not check if it exists.
For all other options, see the official OpenAI API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai",
    "title": "openai",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "Will be used to build the Authorization header, like so: Authorization: Bearer $api_keyDefault: env.OPENAI_API_KEY
The base URL for the API.Default: https://api.openai.com/v1
The default role for any prompts that don't specify a role.We don't do any validation of this field, so you can pass any string you wish.Default: system
Additional headers to send with the request.Example:client<llm> MyClient {
  provider openai
  options {
    api_key env.MY_OPENAI_KEY
    model "gpt-3.5-turbo"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Will be used to build the Authorization header, like so: Authorization: Bearer $api_keyDefault: env.OPENAI_API_KEY
The base URL for the API.Default: https://api.openai.com/v1
The default role for any prompts that don't specify a role.We don't do any validation of this field, so you can pass any string you wish.Default: system
Additional headers to send with the request.Example:client<llm> MyClient {
  provider openai
  options {
    api_key env.MY_OPENAI_KEY
    model "gpt-3.5-turbo"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model           | Description                    |
| --------------- | ------------------------------ |
| gpt-3.5-turbo | Fastest                        |
| gpt-4o        | Fast + text + image            |
| gpt-4-turbo   | Smartest + text + image + code |
| gpt-4o-mini   | Cheapest + text + image        |See openai docs for the list of openai models. You can pass any model name you wish, we will not check if it exists.
For all other options, see the official OpenAI API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model           | Description                    |
| --------------- | ------------------------------ |
| gpt-3.5-turbo | Fastest                        |
| gpt-4o        | Fast + text + image            |
| gpt-4-turbo   | Smartest + text + image + code |
| gpt-4o-mini   | Cheapest + text + image        |See openai docs for the list of openai models. You can pass any model name you wish, we will not check if it exists.
For all other options, see the official OpenAI API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "For `azure-openai`, we provide a client that can be used to interact with the OpenAI API hosted on Azure using the `/chat/completions` endpoint.

Example:
```baml BAML
client<llm> MyClient {
  provider azure-openai
  options {
    resource_name "my-resource-name"
    deployment_id "my-deployment-id"
    // Alternatively, you can use the base_url field
    // base_url "https://my-resource-name.openai.azure.com/openai/deployments/my-deployment-id"
    api_version "2024-02-01"
    api_key env.AZURE_OPENAI_API_KEY
  }
}
```

<Warning>
  `api_version` is required. Azure will return not found if the version is not specified.
</Warning>


The options are passed through directly to the API, barring a few. Here's a shorthand of the options:",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai-from-azure",
    "title": "azure-openai",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "For azure-openai, we provide a client that can be used to interact with the OpenAI API hosted on Azure using the /chat/completions endpoint.
Example:
client<llm> MyClient {
  provider azure-openai
  options {
    resource_name "my-resource-name"
    deployment_id "my-deployment-id"
    // Alternatively, you can use the base_url field
    // base_url "https://my-resource-name.openai.azure.com/openai/deployments/my-deployment-id"
    api_version "2024-02-01"
    api_key env.AZURE_OPENAI_API_KEY
  }
}

api_version is required. Azure will return not found if the version is not specified.
The options are passed through directly to the API, barring a few. Here's a shorthand of the options:
Non-forwarded options
Will be injected via the header API-KEY. Default: env.AZURE_OPENAI_API_KEYAPI-KEY: $api_key
The base URL for the API. Default: https://${resource_name}.openai.azure.com/openai/deployments/${deployment_id}May be used instead of resource_name and deployment_id.
See the base_url field.
See the base_url field.
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Will be passed via a query parameter api-version.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider azure-openai
  options {
    resource_name "my-resource-name"
    deployment_id "my-deployment-id"
    api_version "2024-02-01"
    api_key env.AZURE_OPENAI_API_KEY
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.
Forwarded options
BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
For all other options, see the official Azure API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai-from-azure",
    "title": "azure-openai",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "Will be injected via the header API-KEY. Default: env.AZURE_OPENAI_API_KEYAPI-KEY: $api_key
The base URL for the API. Default: https://${resource_name}.openai.azure.com/openai/deployments/${deployment_id}May be used instead of resource_name and deployment_id.
See the base_url field.
See the base_url field.
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Will be passed via a query parameter api-version.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider azure-openai
  options {
    resource_name "my-resource-name"
    deployment_id "my-deployment-id"
    api_version "2024-02-01"
    api_key env.AZURE_OPENAI_API_KEY
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai-from-azure#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Will be injected via the header API-KEY. Default: env.AZURE_OPENAI_API_KEYAPI-KEY: $api_key
The base URL for the API. Default: https://${resource_name}.openai.azure.com/openai/deployments/${deployment_id}May be used instead of resource_name and deployment_id.
See the base_url field.
See the base_url field.
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Will be passed via a query parameter api-version.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider azure-openai
  options {
    resource_name "my-resource-name"
    deployment_id "my-deployment-id"
    api_version "2024-02-01"
    api_key env.AZURE_OPENAI_API_KEY
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai-from-azure#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
For all other options, see the official Azure API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai-from-azure#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
For all other options, see the official Azure API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/open-ai-from-azure#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The `openai-generic` provider supports all APIs that use OpenAI's request and
response formats, such as Groq, HuggingFace, Ollama, OpenRouter, and Together AI.

Example:

```baml BAML
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://api.provider.com"
    model "<provider-specified-format>"
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic",
    "title": "openai-generic",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The openai-generic provider supports all APIs that use OpenAI's request and
response formats, such as Groq, HuggingFace, Ollama, OpenRouter, and Together AI.
Example:
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://api.provider.com"
    model "<provider-specified-format>"
  }
}

Non-forwarded options
The base URL for the API.Default: https://api.openai.com/v1
The default role for any prompts that don't specify a role.We don't do any validation of this field, so you can pass any string you wish.Default: system
Will be used to build the Authorization header, like so: Authorization: Bearer $api_key
If api_key is not set, or is set to an empty string, the Authorization header will not be sent.Default: <none>
Additional headers to send with the request.Example:client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://api.provider.com"
    model "<provider-specified-format>"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Forwarded options
BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.For OpenAI, this might be "gpt-4o-mini"; for Ollama, this might be "llama2". The exact
syntax will depend on your API provider's documentation: we'll just forward it to them as-is.
For all other options, see the official OpenAI API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic",
    "title": "openai-generic",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The base URL for the API.Default: https://api.openai.com/v1
The default role for any prompts that don't specify a role.We don't do any validation of this field, so you can pass any string you wish.Default: system
Will be used to build the Authorization header, like so: Authorization: Bearer $api_key
If api_key is not set, or is set to an empty string, the Authorization header will not be sent.Default: <none>
Additional headers to send with the request.Example:client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://api.provider.com"
    model "<provider-specified-format>"
    headers {
      "X-My-Header" "my-value"
    }
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The base URL for the API.Default: https://api.openai.com/v1
The default role for any prompts that don't specify a role.We don't do any validation of this field, so you can pass any string you wish.Default: system
Will be used to build the Authorization header, like so: Authorization: Bearer $api_key
If api_key is not set, or is set to an empty string, the Authorization header will not be sent.Default: <none>
Additional headers to send with the request.Example:client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://api.provider.com"
    model "<provider-specified-format>"
    headers {
      "X-My-Header" "my-value"
    }
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.For OpenAI, this might be "gpt-4o-mini"; for Ollama, this might be "llama2". The exact
syntax will depend on your API provider's documentation: we'll just forward it to them as-is.
For all other options, see the official OpenAI API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.For OpenAI, this might be "gpt-4o-mini"; for Ollama, this might be "llama2". The exact
syntax will depend on your API provider's documentation: we'll just forward it to them as-is.
For all other options, see the official OpenAI API documentation.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "[Groq](https://groq.com) supports the OpenAI client, allowing you to use the
[`openai-generic`](/docs/snippets/clients/providers/openai) provider with an
overridden `base_url`.

See https://console.groq.com/docs/openai for more information.

```baml BAML
client<llm> MyClient {
  provider openai-generic
  options {
    base_url "https://api.groq.com/openai/v1"
    api_key env.GROQ_API_KEY
    model "llama3-70b-8192"
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-groq",
    "title": "groq",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Groq supports the OpenAI client, allowing you to use the
openai-generic provider with an
overridden base_url.
See https://console.groq.com/docs/openai for more information.
client<llm> MyClient {
  provider openai-generic
  options {
    base_url "https://api.groq.com/openai/v1"
    api_key env.GROQ_API_KEY
    model "llama3-70b-8192"
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-groq",
    "title": "groq",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "[HuggingFace](https://huggingface.co/) supports the OpenAI client, allowing you to use the
[`openai-generic`](/docs/snippets/clients/providers/openai) provider with an
overridden `base_url`.

See https://huggingface.co/docs/inference-endpoints/index for more information on their Inference Endpoints.

```baml BAML
client<llm> MyClient {
  provider openai-generic
  options {
    base_url "https://api-inference.huggingface.co/v1"
    api_key env.HUGGINGFACE_API_KEY
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-hugging-face",
    "title": "huggingface",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "HuggingFace supports the OpenAI client, allowing you to use the
openai-generic provider with an
overridden base_url.
See https://huggingface.co/docs/inference-endpoints/index for more information on their Inference Endpoints.
client<llm> MyClient {
  provider openai-generic
  options {
    base_url "https://api-inference.huggingface.co/v1"
    api_key env.HUGGINGFACE_API_KEY
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-hugging-face",
    "title": "huggingface",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "Keywords AI is a proxying layer that allows you to route requests to hundreds of models.

Follow the [Keywords AI + BAML Installation Guide](https://docs.keywordsai.co/integration/development-frameworks/baml) to get started!",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-keywords-ai",
    "title": "Keywords AI",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Keywords AI is a proxying layer that allows you to route requests to hundreds of models.
Follow the Keywords AI + BAML Installation Guide to get started!",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-keywords-ai",
    "title": "Keywords AI",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "[LMStudio](https://lmstudio.ai/docs) supports the OpenAI client, allowing you
to use the [`openai-generic`](/docs/snippets/clients/providers/openai) provider
with an overridden `base_url`.


See https://lmstudio.ai/docs/local-server#make-an-inferencing-request-using-openais-chat-completions-format for more information.

```baml BAML
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "http://localhost:1234/v1"
    model "TheBloke/phi-2-GGUF"
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-lm-studio",
    "title": "vLLM",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "LMStudio supports the OpenAI client, allowing you
to use the openai-generic provider
with an overridden base_url.
See https://lmstudio.ai/docs/local-server#make-an-inferencing-request-using-openais-chat-completions-format for more information.
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "http://localhost:1234/v1"
    model "TheBloke/phi-2-GGUF"
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-lm-studio",
    "title": "vLLM",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "[Ollama](https://ollama.com/) supports the OpenAI client, allowing you to use the
[`openai-generic`](/docs/snippets/clients/providers/openai) provider with an
overridden `base_url`.

<Tip>
  Note that to call Ollama, you must use its OpenAI-compatible
  `/v1` endpoint. See [Ollama's OpenAI compatibility
  documentation](https://ollama.com/blog/openai-compatibility).
</Tip>
<Tip>You can try out BAML with Ollama at promptfiddle.com, by running `OLLAMA_ORIGINS='*' ollama serve`. Learn more in [here](https://www.boundaryml.com/blog/ollama-structured-output)</Tip>

```baml BAML
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "http://localhost:11434/v1"
    model llama3
  }
}
```

The options are passed through directly to the API, barring a few. Here's a shorthand of the options:",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-ollama",
    "title": "ollama",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Ollama supports the OpenAI client, allowing you to use the
openai-generic provider with an
overridden base_url.
Note that to call Ollama, you must use its OpenAI-compatible
/v1 endpoint. See Ollama's OpenAI compatibility
documentation.
You can try out BAML with Ollama at promptfiddle.com, by running OLLAMA_ORIGINS='*' ollama serve. Learn more in here
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "http://localhost:11434/v1"
    model llama3
  }
}

The options are passed through directly to the API, barring a few. Here's a shorthand of the options:
Non-forwarded options
The base URL for the API. Default: http://localhost:11434/v1
Note the /v1 at the end of the URL. See Ollama's OpenAI compatability
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider ollama
  options {
    model "llama3"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.
Forwarded options
BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model | Description |
| --- | --- |
| llama3 | Meta Llama 3: The most capable openly available LLM to date |
| qwen2 | Qwen2 is a new series of large language models from Alibaba group |
| phi3 | Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft |
| aya | Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. |
| mistral | The 7B model released by Mistral AI, updated to version 0.3. |
| gemma | Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1 |
| mixtral | A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes. |For the most up-to-date list of models supported by Ollama, see their Model Library.To use a specific version you would do: "mixtral:8x22b"",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-ollama",
    "title": "ollama",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "The base URL for the API. Default: http://localhost:11434/v1
Note the /v1 at the end of the URL. See Ollama's OpenAI compatability
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider ollama
  options {
    model "llama3"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-ollama#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "The base URL for the API. Default: http://localhost:11434/v1
Note the /v1 at the end of the URL. See Ollama's OpenAI compatability
The default role for any prompts that don't specify a role. Default: systemWe don't have any checks for this field, you can pass any string you wish.
Additional headers to send with the request.Example:client<llm> MyClient {
  provider ollama
  options {
    model "llama3"
    headers {
      "X-My-Header" "my-value"
    }
  }
}

Which role metadata should we forward to the API? Default: []For example you can set this to ["foo", "bar"] to forward the cache policy to the API.If you do not set allowed_role_metadata, we will not forward any role metadata to the API even if it is set in the prompt.Then in your prompt you can use something like:client<llm> Foo {
  provider openai
  options {
    allowed_role_metadata: ["foo", "bar"]
  }
}

client<llm> FooWithout {
  provider openai
  options {
  }
}
template_string Foo() #"
  {{ _.role('user', foo={"type": "ephemeral"}, bar="1", cat=True) }}
  This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.
  {{ _.role('user') }}
  This will have none of the role metadata for Foo or FooWithout.
"#
You can use the playground to see the raw curl request to see what is being sent to the API.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-ollama#non-forwarded-options",
    "title": "Non-forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model | Description |
| --- | --- |
| llama3 | Meta Llama 3: The most capable openly available LLM to date |
| qwen2 | Qwen2 is a new series of large language models from Alibaba group |
| phi3 | Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft |
| aya | Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. |
| mistral | The 7B model released by Mistral AI, updated to version 0.3. |
| gemma | Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1 |
| mixtral | A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes. |For the most up-to-date list of models supported by Ollama, see their Model Library.To use a specific version you would do: "mixtral:8x22b"",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-ollama#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "BAML will auto construct this field for you from the prompt
BAML will auto construct this field for you based on how you call the client in your code
The model to use.| Model | Description |
| --- | --- |
| llama3 | Meta Llama 3: The most capable openly available LLM to date |
| qwen2 | Qwen2 is a new series of large language models from Alibaba group |
| phi3 | Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft |
| aya | Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. |
| mistral | The 7B model released by Mistral AI, updated to version 0.3. |
| gemma | Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1 |
| mixtral | A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes. |For the most up-to-date list of models supported by Ollama, see their Model Library.To use a specific version you would do: "mixtral:8x22b"",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-ollama#forwarded-options",
    "title": "Forwarded options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "[OpenRouter](https://openrouter.ai) supports the OpenAI client, allowing you to use the
[`openai-generic`](/docs/snippets/clients/providers/openai) provider with an
overridden `base_url`.



```baml BAML
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://openrouter.ai/api/v1"
    api_key env.OPENROUTER_API_KEY
    model "openai/gpt-3.5-turbo"
    headers {
      "HTTP-Referer" "YOUR-SITE-URL" // Optional
      "X-Title" "YOUR-TITLE" // Optional
    }
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-open-router",
    "title": "openrouter",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "OpenRouter supports the OpenAI client, allowing you to use the
openai-generic provider with an
overridden base_url.
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://openrouter.ai/api/v1"
    api_key env.OPENROUTER_API_KEY
    model "openai/gpt-3.5-turbo"
    headers {
      "HTTP-Referer" "YOUR-SITE-URL" // Optional
      "X-Title" "YOUR-TITLE" // Optional
    }
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-open-router",
    "title": "openrouter",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "[Together AI](https://www.together.ai/) supports the OpenAI client, allowing you
to use the [`openai-generic`](/docs/snippets/clients/providers/openai) provider
with an overridden `base_url`.

See https://docs.together.ai/docs/openai-api-compatibility for more information.

```baml BAML
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://api.together.ai/v1"
    api_key env.TOGETHER_API_KEY
    model "meta-llama/Llama-3-70b-chat-hf"
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-together-ai",
    "title": "Together AI",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Together AI supports the OpenAI client, allowing you
to use the openai-generic provider
with an overridden base_url.
See https://docs.together.ai/docs/openai-api-compatibility for more information.
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "https://api.together.ai/v1"
    api_key env.TOGETHER_API_KEY
    model "meta-llama/Llama-3-70b-chat-hf"
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-together-ai",
    "title": "Together AI",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "[Unify AI](https://www.unify.ai/) supports the OpenAI client, allowing you
to use the [`openai-generic`](/docs/snippets/clients/providers/openai) provider
with an overridden `base_url`.

See https://docs.unify.ai/universal_api/making_queries#openai-python-package for more information.

```baml BAML
client<llm> UnifyClient {
    provider "openai-generic"
    options {
        base_url "https://api.unify.ai/v0"
        api_key env.MY_UNIFY_API_KEY
        model "llama-3.1-405b-chat@together-ai"
    }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-unify-ai",
    "title": "Unify AI",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "Unify AI supports the OpenAI client, allowing you
to use the openai-generic provider
with an overridden base_url.
See https://docs.unify.ai/universal_api/making_queries#openai-python-package for more information.
client<llm> UnifyClient {
    provider "openai-generic"
    options {
        base_url "https://api.unify.ai/v0"
        api_key env.MY_UNIFY_API_KEY
        model "llama-3.1-405b-chat@together-ai"
    }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-unify-ai",
    "title": "Unify AI",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-providers/aws-bedrock",
        "title": "LLM Client Providers",
      },
    ],
    "description": "[vLLM](https://docs.vllm.ai/) supports the OpenAI client, allowing you
to use the [`openai-generic`](/docs/snippets/clients/providers/openai) provider
with an overridden `base_url`.


See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html for more information.

```baml BAML
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "http://localhost:8000/v1"
    api_key "token-abc123"
    model "NousResearch/Meta-Llama-3-8B-Instruct"
    default_role "user" // Required for using VLLM
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-v-llm",
    "title": "vLLM",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Providers",
    ],
    "content": "vLLM supports the OpenAI client, allowing you
to use the openai-generic provider
with an overridden base_url.
See https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html for more information.
client<llm> MyClient {
  provider "openai-generic"
  options {
    base_url "http://localhost:8000/v1"
    api_key "token-abc123"
    model "NousResearch/Meta-Llama-3-8B-Instruct"
    default_role "user" // Required for using VLLM
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-providers/openai-generic-v-llm",
    "title": "vLLM",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "A retry policy can be attached to any `client<llm>` and will attempt to retry requests that fail due to a network error.

```baml BAML
retry_policy MyPolicyName {
  max_retries 3
}
```

Usage:
```baml BAML
client<llm> MyClient {
  provider anthropic
  retry_policy MyPolicyName
  options {
    model "claude-3-sonnet-20240229"
    api_key env.ANTHROPIC_API_KEY
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/retry-policy",
    "title": "retry_policy",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "A retry policy can be attached to any client<llm> and will attempt to retry requests that fail due to a network error.
retry_policy MyPolicyName {
  max_retries 3
}

Usage:
client<llm> MyClient {
  provider anthropic
  retry_policy MyPolicyName
  options {
    model "claude-3-sonnet-20240229"
    api_key env.ANTHROPIC_API_KEY
  }
}

Fields
Number of additional retries to attempt after the initial request fails.
The strategy to use for retrying requests. Default is constant_delay(delay_ms=200).| Strategy | Docs | Notes |
| --- | --- | --- |
| constant_delay | Docs | |
| exponential_backoff | Docs | |Example:retry_policy MyPolicyName {
  max_retries 3
  strategy {
    type constant_delay
    delay_ms 200
  }
}

Strategies
constant_delay
Configures to the constant delay strategy.
The delay in milliseconds to wait between retries. Default: 200
exponential_backoff
Configures to the exponential backoff strategy.
The initial delay in milliseconds to wait between retries. Default: 200
The multiplier to apply to the delay after each retry. Default: 1.5
The maximum delay in milliseconds to wait between retries. Default: 10000",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/retry-policy",
    "title": "retry_policy",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "Number of additional retries to attempt after the initial request fails.
The strategy to use for retrying requests. Default is constant_delay(delay_ms=200).| Strategy | Docs | Notes |
| --- | --- | --- |
| constant_delay | Docs | |
| exponential_backoff | Docs | |Example:retry_policy MyPolicyName {
  max_retries 3
  strategy {
    type constant_delay
    delay_ms 200
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/retry-policy#fields",
    "title": "Fields",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "Number of additional retries to attempt after the initial request fails.
The strategy to use for retrying requests. Default is constant_delay(delay_ms=200).| Strategy | Docs | Notes |
| --- | --- | --- |
| constant_delay | Docs | |
| exponential_backoff | Docs | |Example:retry_policy MyPolicyName {
  max_retries 3
  strategy {
    type constant_delay
    delay_ms 200
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/retry-policy#fields",
    "title": "Fields",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
      {
        "slug": "ref/llm-client-strategies/retry-policy#strategies",
        "title": "Strategies",
      },
    ],
    "description": "Configures to the constant delay strategy.
The delay in milliseconds to wait between retries. Default: 200",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/retry-policy#constant_delay",
    "title": "constant_delay",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
      "Strategies",
    ],
    "content": "Configures to the constant delay strategy.
The delay in milliseconds to wait between retries. Default: 200",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/retry-policy#constant_delay",
    "title": "constant_delay",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
      {
        "slug": "ref/llm-client-strategies/retry-policy#strategies",
        "title": "Strategies",
      },
    ],
    "description": "Configures to the exponential backoff strategy.
The initial delay in milliseconds to wait between retries. Default: 200
The multiplier to apply to the delay after each retry. Default: 1.5
The maximum delay in milliseconds to wait between retries. Default: 10000",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/retry-policy#exponential_backoff",
    "title": "exponential_backoff",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
      "Strategies",
    ],
    "content": "Configures to the exponential backoff strategy.
The initial delay in milliseconds to wait between retries. Default: 200
The multiplier to apply to the delay after each retry. Default: 1.5
The maximum delay in milliseconds to wait between retries. Default: 10000",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/retry-policy#exponential_backoff",
    "title": "exponential_backoff",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "You can use the `fallback` provider to add more resilliancy to your application.

A fallback will attempt to use the first client, and if it fails, it will try the second client, and so on.

<Tip>You can nest fallbacks inside of other fallbacks.</Tip>

```baml BAML
client<llm> SuperDuperClient {
  provider fallback
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/fallback",
    "title": "fallback",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "You can use the fallback provider to add more resilliancy to your application.
A fallback will attempt to use the first client, and if it fails, it will try the second client, and so on.
You can nest fallbacks inside of other fallbacks.
client<llm> SuperDuperClient {
  provider fallback
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

Options
The list of client names to try in order. Cannot be empty.
retry_policy
Like any other client, you can specify a retry policy for the fallback client. See retry_policy for more information.
The retry policy will test the fallback itself, after the entire strategy has failed.
client<llm> SuperDuperClient {
  provider fallback
  retry_policy MyRetryPolicy
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

Nesting multiple fallbacks
You can nest multiple fallbacks inside of each other. The fallbacks will just chain as you would expect.
client<llm> SuperDuperClient {
  provider fallback
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

client<llm> MegaClient {
  provider fallback
  options {
    strategy [
      SuperDuperClient
      ClientD
    ]
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/fallback",
    "title": "fallback",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "The list of client names to try in order. Cannot be empty.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/fallback#options",
    "title": "Options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "The list of client names to try in order. Cannot be empty.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/fallback#options",
    "title": "Options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "Like any other client, you can specify a retry policy for the fallback client. See retry_policy for more information.
The retry policy will test the fallback itself, after the entire strategy has failed.
client<llm> SuperDuperClient {
  provider fallback
  retry_policy MyRetryPolicy
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/fallback#retry_policy",
    "title": "retry_policy",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "Like any other client, you can specify a retry policy for the fallback client. See retry_policy for more information.
The retry policy will test the fallback itself, after the entire strategy has failed.
client<llm> SuperDuperClient {
  provider fallback
  retry_policy MyRetryPolicy
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/fallback#retry_policy",
    "title": "retry_policy",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "You can nest multiple fallbacks inside of each other. The fallbacks will just chain as you would expect.
client<llm> SuperDuperClient {
  provider fallback
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

client<llm> MegaClient {
  provider fallback
  options {
    strategy [
      SuperDuperClient
      ClientD
    ]
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/fallback#nesting-multiple-fallbacks",
    "title": "Nesting multiple fallbacks",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "You can nest multiple fallbacks inside of each other. The fallbacks will just chain as you would expect.
client<llm> SuperDuperClient {
  provider fallback
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

client<llm> MegaClient {
  provider fallback
  options {
    strategy [
      SuperDuperClient
      ClientD
    ]
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/fallback#nesting-multiple-fallbacks",
    "title": "Nesting multiple fallbacks",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "The `round_robin` provider allows you to distribute requests across multiple clients in a round-robin fashion. After each call, the next client in the list will be used.

```baml BAML
client<llm> MyClient {
  provider round-robin
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}
```",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/round-robin",
    "title": "round-robin",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "The round_robin provider allows you to distribute requests across multiple clients in a round-robin fashion. After each call, the next client in the list will be used.
client<llm> MyClient {
  provider round-robin
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

Options
The list of client names to try in order. Cannot be empty.
The index of the client to start with.Default is random(0, len(strategy))In the BAML Playground, Default is 0.
retry_policy
When using a retry_policy with a round-robin client, it will rotate the strategy list after each retry.
client<llm> MyClient {
  provider round-robin
  retry_policy MyRetryPolicy
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

Nesting multiple round-robin clients
You can nest multiple round-robin clients inside of each other. The round-robin as you would expect.
client<llm> MyClient {
  provider round-robin
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

client<llm> MegaClient {
  provider round-robin
  options {
    strategy [
      MyClient
      ClientD
      ClientE
    ]
  }
}

// Calling MegaClient will call:
// MyClient(ClientA)
// ClientD
// ClientE
// MyClient(ClientB)
// etc.
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/round-robin",
    "title": "round-robin",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "The list of client names to try in order. Cannot be empty.
The index of the client to start with.Default is random(0, len(strategy))In the BAML Playground, Default is 0.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/round-robin#options",
    "title": "Options",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "The list of client names to try in order. Cannot be empty.
The index of the client to start with.Default is random(0, len(strategy))In the BAML Playground, Default is 0.",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/round-robin#options",
    "title": "Options",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "When using a retry_policy with a round-robin client, it will rotate the strategy list after each retry.
client<llm> MyClient {
  provider round-robin
  retry_policy MyRetryPolicy
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/round-robin#retry_policy",
    "title": "retry_policy",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "When using a retry_policy with a round-robin client, it will rotate the strategy list after each retry.
client<llm> MyClient {
  provider round-robin
  retry_policy MyRetryPolicy
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/round-robin#retry_policy",
    "title": "retry_policy",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/llm-client-strategies/retry-policy",
        "title": "LLM Client Strategies",
      },
    ],
    "description": "You can nest multiple round-robin clients inside of each other. The round-robin as you would expect.
client<llm> MyClient {
  provider round-robin
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

client<llm> MegaClient {
  provider round-robin
  options {
    strategy [
      MyClient
      ClientD
      ClientE
    ]
  }
}

// Calling MegaClient will call:
// MyClient(ClientA)
// ClientD
// ClientE
// MyClient(ClientB)
// etc.
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/round-robin#nesting-multiple-round-robin-clients",
    "title": "Nesting multiple round-robin clients",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "LLM Client Strategies",
    ],
    "content": "You can nest multiple round-robin clients inside of each other. The round-robin as you would expect.
client<llm> MyClient {
  provider round-robin
  options {
    strategy [
      ClientA
      ClientB
      ClientC
    ]
  }
}

client<llm> MegaClient {
  provider round-robin
  options {
    strategy [
      MyClient
      ClientD
      ClientE
    ]
  }
}

// Calling MegaClient will call:
// MyClient(ClientA)
// ClientD
// ClientE
// MyClient(ClientB)
// etc.
",
    "indexSegmentId": "0",
    "slug": "ref/llm-client-strategies/round-robin#nesting-multiple-round-robin-clients",
    "title": "Nesting multiple round-robin clients",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-client/type-builder",
        "title": "baml_client",
      },
    ],
    "description": "`TypeBuilder` is used to create or modify output schemas at runtime. It's particularly useful when you have dynamic output structures that can't be determined at compile time - like categories from a database or user-provided schemas.

Here's a simple example of using TypeBuilder to add new enum values before calling a BAML function:

**BAML Code**
```baml {4}
enum Category {
  RED
  BLUE
  @@dynamic  // Makes this enum modifiable at runtime
}

function Categorize(text: string) -> Category {
  prompt #"
    Categorize this text:
    {{ text }}

    {{ ctx.output_format }}
  "#
}
```

**Runtime Usage**
<CodeBlocks>
```python Python
from baml_client.type_builder import TypeBuilder
from baml_client import b

# Create a TypeBuilder instance
tb = TypeBuilder()

# Add new values to the Category enum
tb.Category.add_value('GREEN') 
tb.Category.add_value('YELLOW')

# Pass the typebuilder when calling the function
result = await b.Categorize("The sun is bright", {"tb": tb})
# result can now be RED, BLUE, GREEN, or YELLOW
```
```typescript TypeScript
import { TypeBuilder } from '../baml_client/type_builder'
import { b } from '../baml_client'

// Create a TypeBuilder instance
const tb = new TypeBuilder()

// Add new values to the Category enum
tb.Category.addValue('GREEN')
tb.Category.addValue('YELLOW')

// Pass the typebuilder when calling the function
const result = await b.Categorize("The sun is bright", { tb })
// result can now be RED, BLUE, GREEN, or YELLOW
```
```ruby Ruby
require_relative 'baml_client/client'

# Create a TypeBuilder instance
tb = Baml::TypeBuilder.new

# Add new values to the Category enum
tb.Category.add_value('GREEN')
tb.Category.add_value('YELLOW')

# Pass the typebuilder when calling the function
result = Baml::Client.categorize(text: "The sun is bright", baml_options: { tb: tb })
# result can now be RED, BLUE, GREEN, or YELLOW
```
</CodeBlocks>",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder",
    "title": "TypeBuilder",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml_client",
    ],
    "content": "TypeBuilder is used to create or modify output schemas at runtime. It's particularly useful when you have dynamic output structures that can't be determined at compile time - like categories from a database or user-provided schemas.
Here's a simple example of using TypeBuilder to add new enum values before calling a BAML function:
BAML Code
enum Category {
  RED
  BLUE
  @@dynamic  // Makes this enum modifiable at runtime
}

function Categorize(text: string) -> Category {
  prompt #"
    Categorize this text:
    {{ text }}

    {{ ctx.output_format }}
  "#
}

Runtime Usage
from baml_client.type_builder import TypeBuilder
from baml_client import b

# Create a TypeBuilder instance
tb = TypeBuilder()

# Add new values to the Category enum
tb.Category.add_value('GREEN') 
tb.Category.add_value('YELLOW')

# Pass the typebuilder when calling the function
result = await b.Categorize("The sun is bright", {"tb": tb})
# result can now be RED, BLUE, GREEN, or YELLOW
import { TypeBuilder } from '../baml_client/type_builder'
import { b } from '../baml_client'

// Create a TypeBuilder instance
const tb = new TypeBuilder()

// Add new values to the Category enum
tb.Category.addValue('GREEN')
tb.Category.addValue('YELLOW')

// Pass the typebuilder when calling the function
const result = await b.Categorize("The sun is bright", { tb })
// result can now be RED, BLUE, GREEN, or YELLOW
require_relative 'baml_client/client'

# Create a TypeBuilder instance
tb = Baml::TypeBuilder.new

# Add new values to the Category enum
tb.Category.add_value('GREEN')
tb.Category.add_value('YELLOW')

# Pass the typebuilder when calling the function
result = Baml::Client.categorize(text: "The sun is bright", baml_options: { tb: tb })
# result can now be RED, BLUE, GREEN, or YELLOW

Dynamic Types
There are two ways to use TypeBuilder:

Modifying existing BAML types marked with @@dynamic
Creating entirely new types at runtime

Modifying Existing Types
To modify an existing BAML type, mark it with @@dynamic:
class User {
  name string
  age int
  @@dynamic  // Allow adding more properties
}
Runtime Usagetb = TypeBuilder()
tb.User.add_property('email', tb.string())
tb.User.add_property('address', tb.string())
const tb = new TypeBuilder()
tb.User.addProperty('email', tb.string())
tb.User.addProperty('address', tb.string())
tb = Baml::TypeBuilder.new
tb.User.add_property('email', tb.string)
tb.User.add_property('address', tb.string)

enum Category {
  VALUE1
  VALUE2
  @@dynamic  // Allow adding more values
}
Runtime Usagetb = TypeBuilder()
tb.Category.add_value('VALUE3')
tb.Category.add_value('VALUE4')
const tb = new TypeBuilder()
tb.Category.addValue('VALUE3')
tb.Category.addValue('VALUE4')
tb = Baml::TypeBuilder.new
tb.Category.add_value('VALUE3')
tb.Category.add_value('VALUE4')

Creating New Types
You can also create entirely new types at runtime:
tb = TypeBuilder()

# Create a new enum
hobbies = tb.add_enum("Hobbies")
hobbies.add_value("Soccer")
hobbies.add_value("Reading")

# Create a new class
address = tb.add_class("Address") 
address.add_property("street", tb.string())
address.add_property("city", tb.string())

# Attach new types to existing BAML type
tb.User.add_property("hobbies", hobbies.type().list())
tb.User.add_property("address", address.type())
const tb = new TypeBuilder()

// Create a new enum
const hobbies = tb.addEnum("Hobbies")
hobbies.addValue("Soccer")
hobbies.addValue("Reading")

// Create a new class
const address = tb.addClass("Address")
address.addProperty("street", tb.string())
address.addProperty("city", tb.string())

// Attach new types to existing BAML type
tb.User.addProperty("hobbies", hobbies.type().list())
tb.User.addProperty("address", address.type())
tb = Baml::TypeBuilder.new

# Create a new enum
hobbies = tb.add_enum("Hobbies")
hobbies.add_value("Soccer")
hobbies.add_value("Reading")

# Create a new class
address = tb.add_class("Address")
address.add_property("street", tb.string)
address.add_property("city", tb.string)

# Attach new types to existing BAML type
tb.User.add_property("hobbies", hobbies.type.list)
tb.User.add_property("address", address.type)

Type Builders
TypeBuilder provides methods for building different kinds of types:
| Method | Description | Example |
|--------|-------------|---------|
| string() | Creates a string type | tb.string() |
| int() | Creates an integer type | tb.int() |
| float() | Creates a float type | tb.float() |
| bool() | Creates a boolean type | tb.bool() |
| list() | Makes a type into a list | tb.string().list() |
| optional() | Makes a type optional | tb.string().optional() |
Adding Descriptions
You can add descriptions to properties and enum values to help guide the LLM:
tb = TypeBuilder()

# Add description to a property
tb.User.add_property("email", tb.string()) \
   .description("User's primary email address")

# Add description to an enum value 
tb.Category.add_value("URGENT") \
   .description("Needs immediate attention")
const tb = new TypeBuilder()

// Add description to a property
tb.User.addProperty("email", tb.string())
   .description("User's primary email address")

// Add description to an enum value
tb.Category.addValue("URGENT")
   .description("Needs immediate attention")
tb = Baml::TypeBuilder.new

# Add description to a property
tb.User.add_property("email", tb.string)
   .description("User's primary email address")

# Add description to an enum value
tb.Category.add_value("URGENT")
   .description("Needs immediate attention")

Common Patterns
Here are some common patterns when using TypeBuilder:

Dynamic Categories: When categories come from a database or external source

categories = fetch_categories_from_db()
tb = TypeBuilder()
for category in categories:
    tb.Category.add_value(category)
const categories = await fetchCategoriesFromDb()
const tb = new TypeBuilder()
categories.forEach(category => {
    tb.Category.addValue(category)
})
categories = fetch_categories_from_db
tb = Baml::TypeBuilder.new
categories.each do |category|
    tb.Category.add_value(category)
end


Form Fields: When extracting dynamic form fields

fields = get_form_fields()
tb = TypeBuilder()
form = tb.add_class("Form")
for field in fields:
    form.add_property(field.name, tb.string())
const fields = getFormFields()
const tb = new TypeBuilder()
const form = tb.addClass("Form")
fields.forEach(field => {
    form.addProperty(field.name, tb.string())
})
fields = get_form_fields
tb = Baml::TypeBuilder.new
form = tb.add_class("Form")
fields.each do |field|
    form.add_property(field.name, tb.string)
end


Optional Properties: When some fields might not be present

tb = TypeBuilder()
tb.User.add_property("middle_name", tb.string().optional())
const tb = new TypeBuilder()
tb.User.addProperty("middle_name", tb.string().optional())
tb = Baml::TypeBuilder.new
tb.User.add_property("middle_name", tb.string.optional)

All types added through TypeBuilder must be connected to the return type of your BAML function. Standalone types that aren't referenced won't affect the output schema.
Future Features
We're working on additional features for TypeBuilder:

JSON Schema support (awaiting use cases)
OpenAPI schema integration
Pydantic model support

If you're interested in these features, please join the discussion in our GitHub issues.",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder",
    "title": "TypeBuilder",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-client/type-builder",
        "title": "baml_client",
      },
    ],
    "description": "There are two ways to use TypeBuilder:

Modifying existing BAML types marked with @@dynamic
Creating entirely new types at runtime
",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#dynamic-types",
    "title": "Dynamic Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml_client",
    ],
    "content": "There are two ways to use TypeBuilder:

Modifying existing BAML types marked with @@dynamic
Creating entirely new types at runtime
",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#dynamic-types",
    "title": "Dynamic Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-client/type-builder",
        "title": "baml_client",
      },
      {
        "slug": "ref/baml-client/type-builder#dynamic-types",
        "title": "Dynamic Types",
      },
    ],
    "description": "To modify an existing BAML type, mark it with @@dynamic:
class User {
  name string
  age int
  @@dynamic  // Allow adding more properties
}
Runtime Usagetb = TypeBuilder()
tb.User.add_property('email', tb.string())
tb.User.add_property('address', tb.string())
const tb = new TypeBuilder()
tb.User.addProperty('email', tb.string())
tb.User.addProperty('address', tb.string())
tb = Baml::TypeBuilder.new
tb.User.add_property('email', tb.string)
tb.User.add_property('address', tb.string)

enum Category {
  VALUE1
  VALUE2
  @@dynamic  // Allow adding more values
}
Runtime Usagetb = TypeBuilder()
tb.Category.add_value('VALUE3')
tb.Category.add_value('VALUE4')
const tb = new TypeBuilder()
tb.Category.addValue('VALUE3')
tb.Category.addValue('VALUE4')
tb = Baml::TypeBuilder.new
tb.Category.add_value('VALUE3')
tb.Category.add_value('VALUE4')
",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#modifying-existing-types",
    "title": "Modifying Existing Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml_client",
      "Dynamic Types",
    ],
    "content": "To modify an existing BAML type, mark it with @@dynamic:
class User {
  name string
  age int
  @@dynamic  // Allow adding more properties
}
Runtime Usagetb = TypeBuilder()
tb.User.add_property('email', tb.string())
tb.User.add_property('address', tb.string())
const tb = new TypeBuilder()
tb.User.addProperty('email', tb.string())
tb.User.addProperty('address', tb.string())
tb = Baml::TypeBuilder.new
tb.User.add_property('email', tb.string)
tb.User.add_property('address', tb.string)

enum Category {
  VALUE1
  VALUE2
  @@dynamic  // Allow adding more values
}
Runtime Usagetb = TypeBuilder()
tb.Category.add_value('VALUE3')
tb.Category.add_value('VALUE4')
const tb = new TypeBuilder()
tb.Category.addValue('VALUE3')
tb.Category.addValue('VALUE4')
tb = Baml::TypeBuilder.new
tb.Category.add_value('VALUE3')
tb.Category.add_value('VALUE4')
",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#modifying-existing-types",
    "title": "Modifying Existing Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-client/type-builder",
        "title": "baml_client",
      },
      {
        "slug": "ref/baml-client/type-builder#dynamic-types",
        "title": "Dynamic Types",
      },
    ],
    "description": "You can also create entirely new types at runtime:
tb = TypeBuilder()

# Create a new enum
hobbies = tb.add_enum("Hobbies")
hobbies.add_value("Soccer")
hobbies.add_value("Reading")

# Create a new class
address = tb.add_class("Address") 
address.add_property("street", tb.string())
address.add_property("city", tb.string())

# Attach new types to existing BAML type
tb.User.add_property("hobbies", hobbies.type().list())
tb.User.add_property("address", address.type())
const tb = new TypeBuilder()

// Create a new enum
const hobbies = tb.addEnum("Hobbies")
hobbies.addValue("Soccer")
hobbies.addValue("Reading")

// Create a new class
const address = tb.addClass("Address")
address.addProperty("street", tb.string())
address.addProperty("city", tb.string())

// Attach new types to existing BAML type
tb.User.addProperty("hobbies", hobbies.type().list())
tb.User.addProperty("address", address.type())
tb = Baml::TypeBuilder.new

# Create a new enum
hobbies = tb.add_enum("Hobbies")
hobbies.add_value("Soccer")
hobbies.add_value("Reading")

# Create a new class
address = tb.add_class("Address")
address.add_property("street", tb.string)
address.add_property("city", tb.string)

# Attach new types to existing BAML type
tb.User.add_property("hobbies", hobbies.type.list)
tb.User.add_property("address", address.type)
",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#creating-new-types",
    "title": "Creating New Types",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml_client",
      "Dynamic Types",
    ],
    "content": "You can also create entirely new types at runtime:
tb = TypeBuilder()

# Create a new enum
hobbies = tb.add_enum("Hobbies")
hobbies.add_value("Soccer")
hobbies.add_value("Reading")

# Create a new class
address = tb.add_class("Address") 
address.add_property("street", tb.string())
address.add_property("city", tb.string())

# Attach new types to existing BAML type
tb.User.add_property("hobbies", hobbies.type().list())
tb.User.add_property("address", address.type())
const tb = new TypeBuilder()

// Create a new enum
const hobbies = tb.addEnum("Hobbies")
hobbies.addValue("Soccer")
hobbies.addValue("Reading")

// Create a new class
const address = tb.addClass("Address")
address.addProperty("street", tb.string())
address.addProperty("city", tb.string())

// Attach new types to existing BAML type
tb.User.addProperty("hobbies", hobbies.type().list())
tb.User.addProperty("address", address.type())
tb = Baml::TypeBuilder.new

# Create a new enum
hobbies = tb.add_enum("Hobbies")
hobbies.add_value("Soccer")
hobbies.add_value("Reading")

# Create a new class
address = tb.add_class("Address")
address.add_property("street", tb.string)
address.add_property("city", tb.string)

# Attach new types to existing BAML type
tb.User.add_property("hobbies", hobbies.type.list)
tb.User.add_property("address", address.type)
",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#creating-new-types",
    "title": "Creating New Types",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-client/type-builder",
        "title": "baml_client",
      },
    ],
    "description": "TypeBuilder provides methods for building different kinds of types:
| Method | Description | Example |
|--------|-------------|---------|
| string() | Creates a string type | tb.string() |
| int() | Creates an integer type | tb.int() |
| float() | Creates a float type | tb.float() |
| bool() | Creates a boolean type | tb.bool() |
| list() | Makes a type into a list | tb.string().list() |
| optional() | Makes a type optional | tb.string().optional() |",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#type-builders",
    "title": "Type Builders",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml_client",
    ],
    "content": "TypeBuilder provides methods for building different kinds of types:
| Method | Description | Example |
|--------|-------------|---------|
| string() | Creates a string type | tb.string() |
| int() | Creates an integer type | tb.int() |
| float() | Creates a float type | tb.float() |
| bool() | Creates a boolean type | tb.bool() |
| list() | Makes a type into a list | tb.string().list() |
| optional() | Makes a type optional | tb.string().optional() |",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#type-builders",
    "title": "Type Builders",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-client/type-builder",
        "title": "baml_client",
      },
    ],
    "description": "You can add descriptions to properties and enum values to help guide the LLM:
tb = TypeBuilder()

# Add description to a property
tb.User.add_property("email", tb.string()) \
   .description("User's primary email address")

# Add description to an enum value 
tb.Category.add_value("URGENT") \
   .description("Needs immediate attention")
const tb = new TypeBuilder()

// Add description to a property
tb.User.addProperty("email", tb.string())
   .description("User's primary email address")

// Add description to an enum value
tb.Category.addValue("URGENT")
   .description("Needs immediate attention")
tb = Baml::TypeBuilder.new

# Add description to a property
tb.User.add_property("email", tb.string)
   .description("User's primary email address")

# Add description to an enum value
tb.Category.add_value("URGENT")
   .description("Needs immediate attention")
",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#adding-descriptions",
    "title": "Adding Descriptions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml_client",
    ],
    "content": "You can add descriptions to properties and enum values to help guide the LLM:
tb = TypeBuilder()

# Add description to a property
tb.User.add_property("email", tb.string()) \
   .description("User's primary email address")

# Add description to an enum value 
tb.Category.add_value("URGENT") \
   .description("Needs immediate attention")
const tb = new TypeBuilder()

// Add description to a property
tb.User.addProperty("email", tb.string())
   .description("User's primary email address")

// Add description to an enum value
tb.Category.addValue("URGENT")
   .description("Needs immediate attention")
tb = Baml::TypeBuilder.new

# Add description to a property
tb.User.add_property("email", tb.string)
   .description("User's primary email address")

# Add description to an enum value
tb.Category.add_value("URGENT")
   .description("Needs immediate attention")
",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#adding-descriptions",
    "title": "Adding Descriptions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-client/type-builder",
        "title": "baml_client",
      },
    ],
    "description": "Here are some common patterns when using TypeBuilder:

Dynamic Categories: When categories come from a database or external source

categories = fetch_categories_from_db()
tb = TypeBuilder()
for category in categories:
    tb.Category.add_value(category)
const categories = await fetchCategoriesFromDb()
const tb = new TypeBuilder()
categories.forEach(category => {
    tb.Category.addValue(category)
})
categories = fetch_categories_from_db
tb = Baml::TypeBuilder.new
categories.each do |category|
    tb.Category.add_value(category)
end


Form Fields: When extracting dynamic form fields

fields = get_form_fields()
tb = TypeBuilder()
form = tb.add_class("Form")
for field in fields:
    form.add_property(field.name, tb.string())
const fields = getFormFields()
const tb = new TypeBuilder()
const form = tb.addClass("Form")
fields.forEach(field => {
    form.addProperty(field.name, tb.string())
})
fields = get_form_fields
tb = Baml::TypeBuilder.new
form = tb.add_class("Form")
fields.each do |field|
    form.add_property(field.name, tb.string)
end


Optional Properties: When some fields might not be present

tb = TypeBuilder()
tb.User.add_property("middle_name", tb.string().optional())
const tb = new TypeBuilder()
tb.User.addProperty("middle_name", tb.string().optional())
tb = Baml::TypeBuilder.new
tb.User.add_property("middle_name", tb.string.optional)

All types added through TypeBuilder must be connected to the return type of your BAML function. Standalone types that aren't referenced won't affect the output schema.",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#common-patterns",
    "title": "Common Patterns",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml_client",
    ],
    "content": "Here are some common patterns when using TypeBuilder:

Dynamic Categories: When categories come from a database or external source

categories = fetch_categories_from_db()
tb = TypeBuilder()
for category in categories:
    tb.Category.add_value(category)
const categories = await fetchCategoriesFromDb()
const tb = new TypeBuilder()
categories.forEach(category => {
    tb.Category.addValue(category)
})
categories = fetch_categories_from_db
tb = Baml::TypeBuilder.new
categories.each do |category|
    tb.Category.add_value(category)
end


Form Fields: When extracting dynamic form fields

fields = get_form_fields()
tb = TypeBuilder()
form = tb.add_class("Form")
for field in fields:
    form.add_property(field.name, tb.string())
const fields = getFormFields()
const tb = new TypeBuilder()
const form = tb.addClass("Form")
fields.forEach(field => {
    form.addProperty(field.name, tb.string())
})
fields = get_form_fields
tb = Baml::TypeBuilder.new
form = tb.add_class("Form")
fields.each do |field|
    form.add_property(field.name, tb.string)
end


Optional Properties: When some fields might not be present

tb = TypeBuilder()
tb.User.add_property("middle_name", tb.string().optional())
const tb = new TypeBuilder()
tb.User.addProperty("middle_name", tb.string().optional())
tb = Baml::TypeBuilder.new
tb.User.add_property("middle_name", tb.string.optional)

All types added through TypeBuilder must be connected to the return type of your BAML function. Standalone types that aren't referenced won't affect the output schema.",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#common-patterns",
    "title": "Common Patterns",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/baml-client/type-builder",
        "title": "baml_client",
      },
    ],
    "description": "We're working on additional features for TypeBuilder:

JSON Schema support (awaiting use cases)
OpenAPI schema integration
Pydantic model support

If you're interested in these features, please join the discussion in our GitHub issues.",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#future-features",
    "title": "Future Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "baml_client",
    ],
    "content": "We're working on additional features for TypeBuilder:

JSON Schema support (awaiting use cases)
OpenAPI schema integration
Pydantic model support

If you're interested in these features, please join the discussion in our GitHub issues.",
    "indexSegmentId": "0",
    "slug": "ref/baml-client/type-builder#future-features",
    "title": "Future Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "BAML Prompt strings are essentially [Minijinja](https://docs.rs/minijinja/latest/minijinja/filters/index.html#functions) templates, which offer the ability to express logic and data manipulation within strings. Jinja is a very popular and mature templating language amongst Python developers, so Github Copilot or another LLM can already help you write most of the logic you want.",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja",
    "title": "What is Jinja / Cookbook",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "BAML Prompt strings are essentially Minijinja templates, which offer the ability to express logic and data manipulation within strings. Jinja is a very popular and mature templating language amongst Python developers, so Github Copilot or another LLM can already help you write most of the logic you want.
Jinja Cookbook
When in doubt -- use the BAML VSCode Playground preview. It will show you the fully rendered prompt, even when it has complex logic.
Basic Syntax

{% ... %}: Use for executing statements such as for-loops or conditionals.
{{ ... }}: Use for outputting expressions or variables.
{# ... #}: Use for comments within the template, which will not be rendered.

Loops / Iterating Over Lists
Here's how you can iterate over a list of items, accessing each item's attributes:
function MyFunc(messages: Message[]) -> string {
  prompt #"
    {% for message in messages %}
      {{ message.user_name }}: {{ message.content }}
    {% endfor %}
  "#
}

Conditional Statements
Use conditional statements to control the flow and output of your templates based on conditions:
function MyFunc(user: User) -> string {
  prompt #"
    {% if user.is_active %}
      Welcome back, {{ user.name }}!
    {% else %}
      Please activate your account.
    {% endif %}
  "#
}

Setting Variables
You can define and use variables within your templates to simplify expressions or manage data:
function MyFunc(items: Item[]) -> string {
  prompt #"
    {% set total_price = 0 %}
    {% for item in items %}
      {% set total_price = total_price + item.price %}
    {% endfor %}
    Total price: {{ total_price }}
  "#
}

Including other Templates
To promote reusability, you can include other templates within a template. See template strings:
template_string PrintUserInfo(arg1: string, arg2: User) #"
  {{ arg1 }}
  The user's name is: {{ arg2.name }}
"#

function MyFunc(arg1: string, user: User) -> string {
  prompt #"
    Here is the user info:
    {{ PrintUserInfo(arg1, user) }}
  "#
}

Built-in filters
See jinja docs",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja",
    "title": "What is Jinja / Cookbook",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "When in doubt -- use the BAML VSCode Playground preview. It will show you the fully rendered prompt, even when it has complex logic.",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#jinja-cookbook",
    "title": "Jinja Cookbook",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "When in doubt -- use the BAML VSCode Playground preview. It will show you the fully rendered prompt, even when it has complex logic.",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#jinja-cookbook",
    "title": "Jinja Cookbook",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
      {
        "slug": "ref/prompt-syntax/what-is-jinja#jinja-cookbook",
        "title": "Jinja Cookbook",
      },
    ],
    "description": "
{% ... %}: Use for executing statements such as for-loops or conditionals.
{{ ... }}: Use for outputting expressions or variables.
{# ... #}: Use for comments within the template, which will not be rendered.
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#basic-syntax",
    "title": "Basic Syntax",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
      "Jinja Cookbook",
    ],
    "content": "
{% ... %}: Use for executing statements such as for-loops or conditionals.
{{ ... }}: Use for outputting expressions or variables.
{# ... #}: Use for comments within the template, which will not be rendered.
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#basic-syntax",
    "title": "Basic Syntax",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
      {
        "slug": "ref/prompt-syntax/what-is-jinja#jinja-cookbook",
        "title": "Jinja Cookbook",
      },
    ],
    "description": "Here's how you can iterate over a list of items, accessing each item's attributes:
function MyFunc(messages: Message[]) -> string {
  prompt #"
    {% for message in messages %}
      {{ message.user_name }}: {{ message.content }}
    {% endfor %}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#loops--iterating-over-lists",
    "title": "Loops / Iterating Over Lists",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
      "Jinja Cookbook",
    ],
    "content": "Here's how you can iterate over a list of items, accessing each item's attributes:
function MyFunc(messages: Message[]) -> string {
  prompt #"
    {% for message in messages %}
      {{ message.user_name }}: {{ message.content }}
    {% endfor %}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#loops--iterating-over-lists",
    "title": "Loops / Iterating Over Lists",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
      {
        "slug": "ref/prompt-syntax/what-is-jinja#jinja-cookbook",
        "title": "Jinja Cookbook",
      },
    ],
    "description": "Use conditional statements to control the flow and output of your templates based on conditions:
function MyFunc(user: User) -> string {
  prompt #"
    {% if user.is_active %}
      Welcome back, {{ user.name }}!
    {% else %}
      Please activate your account.
    {% endif %}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#conditional-statements",
    "title": "Conditional Statements",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
      "Jinja Cookbook",
    ],
    "content": "Use conditional statements to control the flow and output of your templates based on conditions:
function MyFunc(user: User) -> string {
  prompt #"
    {% if user.is_active %}
      Welcome back, {{ user.name }}!
    {% else %}
      Please activate your account.
    {% endif %}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#conditional-statements",
    "title": "Conditional Statements",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
      {
        "slug": "ref/prompt-syntax/what-is-jinja#jinja-cookbook",
        "title": "Jinja Cookbook",
      },
    ],
    "description": "You can define and use variables within your templates to simplify expressions or manage data:
function MyFunc(items: Item[]) -> string {
  prompt #"
    {% set total_price = 0 %}
    {% for item in items %}
      {% set total_price = total_price + item.price %}
    {% endfor %}
    Total price: {{ total_price }}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#setting-variables",
    "title": "Setting Variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
      "Jinja Cookbook",
    ],
    "content": "You can define and use variables within your templates to simplify expressions or manage data:
function MyFunc(items: Item[]) -> string {
  prompt #"
    {% set total_price = 0 %}
    {% for item in items %}
      {% set total_price = total_price + item.price %}
    {% endfor %}
    Total price: {{ total_price }}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#setting-variables",
    "title": "Setting Variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
      {
        "slug": "ref/prompt-syntax/what-is-jinja#jinja-cookbook",
        "title": "Jinja Cookbook",
      },
    ],
    "description": "To promote reusability, you can include other templates within a template. See template strings:
template_string PrintUserInfo(arg1: string, arg2: User) #"
  {{ arg1 }}
  The user's name is: {{ arg2.name }}
"#

function MyFunc(arg1: string, user: User) -> string {
  prompt #"
    Here is the user info:
    {{ PrintUserInfo(arg1, user) }}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#including-other-templates",
    "title": "Including other Templates",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
      "Jinja Cookbook",
    ],
    "content": "To promote reusability, you can include other templates within a template. See template strings:
template_string PrintUserInfo(arg1: string, arg2: User) #"
  {{ arg1 }}
  The user's name is: {{ arg2.name }}
"#

function MyFunc(arg1: string, user: User) -> string {
  prompt #"
    Here is the user info:
    {{ PrintUserInfo(arg1, user) }}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#including-other-templates",
    "title": "Including other Templates",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
      {
        "slug": "ref/prompt-syntax/what-is-jinja#jinja-cookbook",
        "title": "Jinja Cookbook",
      },
    ],
    "description": "See jinja docs",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#built-in-filters",
    "title": "Built-in filters",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
      "Jinja Cookbook",
    ],
    "content": "See jinja docs",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/what-is-jinja#built-in-filters",
    "title": "Built-in filters",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "`{{ ctx.output_format }}` is used within a prompt template (or in any template_string) to print out the function's output schema into the prompt. It describes to the LLM how to generate a structure BAML can parse (usually JSON).

Here's an example of a function with `{{ ctx.output_format }}`, and how it gets rendered by BAML before sending it to the LLM.

**BAML Prompt**

```baml
class Resume {
  name string
  education Education[]
}
function ExtractResume(resume_text: string) -> Resume {
  prompt #"
    Extract this resume:
    ---
    {{ resume_text }}
    ---

    {{ ctx.output_format }}
  "#
}
```

**Rendered prompt**

```text
Extract this resume
---
Aaron V.
Bachelors CS, 2015
UT Austin
---

Answer in JSON using this schema: 
{
  name: string
  education: [
    {
      school: string
      graduation_year: string
    }
  ]
}
```",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-output-format",
    "title": "ctx.output_format",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "{{ ctx.output_format }} is used within a prompt template (or in any template_string) to print out the function's output schema into the prompt. It describes to the LLM how to generate a structure BAML can parse (usually JSON).
Here's an example of a function with {{ ctx.output_format }}, and how it gets rendered by BAML before sending it to the LLM.
BAML Prompt
class Resume {
  name string
  education Education[]
}
function ExtractResume(resume_text: string) -> Resume {
  prompt #"
    Extract this resume:
    ---
    {{ resume_text }}
    ---

    {{ ctx.output_format }}
  "#
}

Rendered prompt
Extract this resume
---
Aaron V.
Bachelors CS, 2015
UT Austin
---

Answer in JSON using this schema: 
{
  name: string
  education: [
    {
      school: string
      graduation_year: string
    }
  ]
}

Controlling the output_format
ctx.output_format can also be called as a function with parameters to customize how the schema is printed, like this:

{{ ctx.output_format(prefix="If you use this schema correctly and I'll tip $400:\n", always_hoist_enums=true)}}

Here's the parameters:
The prefix instruction to use before printing out the schema.Answer in this schema correctly I'll tip $400:
{
  ...
}
BAML's default prefix varies based on the function's return type.| Fuction return type | Default Prefix |
| --- | --- |
| Primitive (String) |  |
| Primitive (Other) | Answer as a:  |
| Enum | Answer with any of the categories:\n |
| Class | Answer in JSON using this schema:\n |
| List | Answer with a JSON Array using this schema:\n |
| Union | Answer in JSON using any of these schemas:\n |
| Optional | Answer in JSON using this schema:\n |
Whether to inline the enum definitions in the schema, or print them above. Default: falseInlined
Answer in this json schema:
{
  categories: "ONE" | "TWO" | "THREE"
}
hoistedMyCategory
---
ONE
TWO
THREE

Answer in this json schema:
{
  categories: MyCategory
}
BAML will always hoist if you add a description to any of the enum values.
Default: orIf a type is a union like string | int or an optional like string?, this indicates how it's rendered.BAML renders it as property: string or null as we have observed some LLMs have trouble identifying what property: string | null means (and are better with plain english).You can always set it to | or something else for a specific model you use.
Why BAML doesn't use JSON schema format in prompts
BAML uses "type definitions" or "jsonish" format instead of the long-winded json-schema format.
The tl;dr is that json schemas are

4x more inefficient than "type definitions".
very unreadable by humans (and hence models)
perform worse than type definitions (especially on deeper nested objects or smaller models)

Read our full article on json schema vs type definitions",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-output-format",
    "title": "ctx.output_format",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "ctx.output_format can also be called as a function with parameters to customize how the schema is printed, like this:

{{ ctx.output_format(prefix="If you use this schema correctly and I'll tip $400:\n", always_hoist_enums=true)}}

Here's the parameters:
The prefix instruction to use before printing out the schema.Answer in this schema correctly I'll tip $400:
{
  ...
}
BAML's default prefix varies based on the function's return type.| Fuction return type | Default Prefix |
| --- | --- |
| Primitive (String) |  |
| Primitive (Other) | Answer as a:  |
| Enum | Answer with any of the categories:\n |
| Class | Answer in JSON using this schema:\n |
| List | Answer with a JSON Array using this schema:\n |
| Union | Answer in JSON using any of these schemas:\n |
| Optional | Answer in JSON using this schema:\n |
Whether to inline the enum definitions in the schema, or print them above. Default: falseInlined
Answer in this json schema:
{
  categories: "ONE" | "TWO" | "THREE"
}
hoistedMyCategory
---
ONE
TWO
THREE

Answer in this json schema:
{
  categories: MyCategory
}
BAML will always hoist if you add a description to any of the enum values.
Default: orIf a type is a union like string | int or an optional like string?, this indicates how it's rendered.BAML renders it as property: string or null as we have observed some LLMs have trouble identifying what property: string | null means (and are better with plain english).You can always set it to | or something else for a specific model you use.",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-output-format#controlling-the-output_format",
    "title": "Controlling the output_format",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "ctx.output_format can also be called as a function with parameters to customize how the schema is printed, like this:

{{ ctx.output_format(prefix="If you use this schema correctly and I'll tip $400:\n", always_hoist_enums=true)}}

Here's the parameters:
The prefix instruction to use before printing out the schema.Answer in this schema correctly I'll tip $400:
{
  ...
}
BAML's default prefix varies based on the function's return type.| Fuction return type | Default Prefix |
| --- | --- |
| Primitive (String) |  |
| Primitive (Other) | Answer as a:  |
| Enum | Answer with any of the categories:\n |
| Class | Answer in JSON using this schema:\n |
| List | Answer with a JSON Array using this schema:\n |
| Union | Answer in JSON using any of these schemas:\n |
| Optional | Answer in JSON using this schema:\n |
Whether to inline the enum definitions in the schema, or print them above. Default: falseInlined
Answer in this json schema:
{
  categories: "ONE" | "TWO" | "THREE"
}
hoistedMyCategory
---
ONE
TWO
THREE

Answer in this json schema:
{
  categories: MyCategory
}
BAML will always hoist if you add a description to any of the enum values.
Default: orIf a type is a union like string | int or an optional like string?, this indicates how it's rendered.BAML renders it as property: string or null as we have observed some LLMs have trouble identifying what property: string | null means (and are better with plain english).You can always set it to | or something else for a specific model you use.",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-output-format#controlling-the-output_format",
    "title": "Controlling the output_format",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "BAML uses "type definitions" or "jsonish" format instead of the long-winded json-schema format.
The tl;dr is that json schemas are

4x more inefficient than "type definitions".
very unreadable by humans (and hence models)
perform worse than type definitions (especially on deeper nested objects or smaller models)

Read our full article on json schema vs type definitions",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-output-format#why-baml-doesnt-use-json-schema-format-in-prompts",
    "title": "Why BAML doesn't use JSON schema format in prompts",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "BAML uses "type definitions" or "jsonish" format instead of the long-winded json-schema format.
The tl;dr is that json schemas are

4x more inefficient than "type definitions".
very unreadable by humans (and hence models)
perform worse than type definitions (especially on deeper nested objects or smaller models)

Read our full article on json schema vs type definitions",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-output-format#why-baml-doesnt-use-json-schema-format-in-prompts",
    "title": "Why BAML doesn't use JSON schema format in prompts",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "If you try rendering `{{ ctx }}` into the prompt (literally just write that out!), you'll see all the metadata we inject to run this prompt within the playground preview.

In the earlier tutorial we mentioned `ctx.output_format`, which contains the schema, but you can also access client information:",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-client",
    "title": "ctx (accessing metadata)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "If you try rendering {{ ctx }} into the prompt (literally just write that out!), you'll see all the metadata we inject to run this prompt within the playground preview.
In the earlier tutorial we mentioned ctx.output_format, which contains the schema, but you can also access client information:
Usecase: Conditionally render based on client provider
In this example, we render the list of messages in XML tags if the provider is Anthropic (as they recommend using them as delimiters). See also  template_string as it's used in here.
template_string RenderConditionally(messages: Message[]) #"
  {% for message in messages %}
    {%if ctx.client.provider == "anthropic" %}
      <Message>{{ message.user_name }}: {{ message.content }}</Message>
    {% else %}
      {{ message.user_name }}: {{ message.content }}
    {% endif %}
  {% endfor %}
"#

function MyFuncWithGPT4(messages: Message[]) -> string {
  client GPT4o
  prompt #"
    {{ RenderConditionally(messages)}}
  "#
}

function MyFuncWithAnthropic(messages: Message[]) -> string {
  client Claude35
  prompt #"
    {{ RenderConditionally(messages )}}
  #"
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-client",
    "title": "ctx (accessing metadata)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "In this example, we render the list of messages in XML tags if the provider is Anthropic (as they recommend using them as delimiters). See also  template_string as it's used in here.
template_string RenderConditionally(messages: Message[]) #"
  {% for message in messages %}
    {%if ctx.client.provider == "anthropic" %}
      <Message>{{ message.user_name }}: {{ message.content }}</Message>
    {% else %}
      {{ message.user_name }}: {{ message.content }}
    {% endif %}
  {% endfor %}
"#

function MyFuncWithGPT4(messages: Message[]) -> string {
  client GPT4o
  prompt #"
    {{ RenderConditionally(messages)}}
  "#
}

function MyFuncWithAnthropic(messages: Message[]) -> string {
  client Claude35
  prompt #"
    {{ RenderConditionally(messages )}}
  #"
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-client#usecase-conditionally-render-based-on-client-provider",
    "title": "Usecase: Conditionally render based on client provider",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "In this example, we render the list of messages in XML tags if the provider is Anthropic (as they recommend using them as delimiters). See also  template_string as it's used in here.
template_string RenderConditionally(messages: Message[]) #"
  {% for message in messages %}
    {%if ctx.client.provider == "anthropic" %}
      <Message>{{ message.user_name }}: {{ message.content }}</Message>
    {% else %}
      {{ message.user_name }}: {{ message.content }}
    {% endif %}
  {% endfor %}
"#

function MyFuncWithGPT4(messages: Message[]) -> string {
  client GPT4o
  prompt #"
    {{ RenderConditionally(messages)}}
  "#
}

function MyFuncWithAnthropic(messages: Message[]) -> string {
  client Claude35
  prompt #"
    {{ RenderConditionally(messages )}}
  #"
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/ctx-client#usecase-conditionally-render-based-on-client-provider",
    "title": "Usecase: Conditionally render based on client provider",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "BAML prompts are compiled into a `messages` array (or equivalent) that most LLM providers use:

BAML Prompt -> `[{ role: "user": content: "hi there"}, { role: "assistant", ...}]`

By default, BAML puts everything into a single message with the `system` role if available (or whichever one is best for the provider you have selected). 
When in doubt, the playground always shows you the current role for each message.

To specify a role explicitly, add the `{{ _.role("user")}}` syntax to the prompt
```rust
prompt #"
  {{ _.role("system") }} Everything after
  this element will be a system prompt!

  {{ _.role("user")}} 
  And everything after this
  will be a user role
"#
```
Try it out in [PromptFiddle](https://www.promptfiddle.com)

<Note>
  BAML may change the default role to `user` if using specific APIs that only support user prompts, like when using prompts with images.
</Note>

We use `_` as the prefix of `_.role()` since we plan on adding more helpers here in the future.",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/role",
    "title": "_.role",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "BAML prompts are compiled into a messages array (or equivalent) that most LLM providers use:
BAML Prompt -> [{ role: "user": content: "hi there"}, { role: "assistant", ...}]
By default, BAML puts everything into a single message with the system role if available (or whichever one is best for the provider you have selected).
When in doubt, the playground always shows you the current role for each message.
To specify a role explicitly, add the {{ _.role("user")}} syntax to the prompt
prompt #"
  {{ _.role("system") }} Everything after
  this element will be a system prompt!

  {{ _.role("user")}} 
  And everything after this
  will be a user role
"#

Try it out in PromptFiddle
BAML may change the default role to user if using specific APIs that only support user prompts, like when using prompts with images.
We use _ as the prefix of _.role() since we plan on adding more helpers here in the future.
Example -- Using _.role() in for-loops
Here's how you can inject a list of user/assistant messages and mark each as a user or assistant role:
class Message {
  role string
  message string
}

function ChatWithAgent(input: Message[]) -> string {
  client GPT4o
  prompt #"
    {% for m in messages %}
      {{ _.role(m.role) }}
      {{ m.message }}
    {% endfor %}
  "#
}

function ChatMessages(messages: string[]) -> string {
  client GPT4o
  prompt #"
    {% for m in messages %}
      {{ _.role("user" if loop.index % 2 == 1 else "assistant") }}
      {{ m }}
    {% endfor %}
  "#
}

Example -- Using _.role() in a template string
template_string YouAreA(name: string, job: string) #"
  {{ _.role("system") }} 
  You are an expert {{ name }}. {{ job }}

  {{ ctx.output_format }}
  {{ _.role("user") }}
"#

function CheckJobPosting(post: string) -> bool {
  client GPT4o
  prompt #"
    {{ YouAreA("hr admin", "You're role is to ensure every job posting is bias free.") }}

    {{ post }}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/role",
    "title": "_.role",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "Here's how you can inject a list of user/assistant messages and mark each as a user or assistant role:
class Message {
  role string
  message string
}

function ChatWithAgent(input: Message[]) -> string {
  client GPT4o
  prompt #"
    {% for m in messages %}
      {{ _.role(m.role) }}
      {{ m.message }}
    {% endfor %}
  "#
}

function ChatMessages(messages: string[]) -> string {
  client GPT4o
  prompt #"
    {% for m in messages %}
      {{ _.role("user" if loop.index % 2 == 1 else "assistant") }}
      {{ m }}
    {% endfor %}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/role#example----using-_role-in-for-loops",
    "title": "Example -- Using _.role() in for-loops",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "Here's how you can inject a list of user/assistant messages and mark each as a user or assistant role:
class Message {
  role string
  message string
}

function ChatWithAgent(input: Message[]) -> string {
  client GPT4o
  prompt #"
    {% for m in messages %}
      {{ _.role(m.role) }}
      {{ m.message }}
    {% endfor %}
  "#
}

function ChatMessages(messages: string[]) -> string {
  client GPT4o
  prompt #"
    {% for m in messages %}
      {{ _.role("user" if loop.index % 2 == 1 else "assistant") }}
      {{ m }}
    {% endfor %}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/role#example----using-_role-in-for-loops",
    "title": "Example -- Using _.role() in for-loops",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "template_string YouAreA(name: string, job: string) #"
  {{ _.role("system") }} 
  You are an expert {{ name }}. {{ job }}

  {{ ctx.output_format }}
  {{ _.role("user") }}
"#

function CheckJobPosting(post: string) -> bool {
  client GPT4o
  prompt #"
    {{ YouAreA("hr admin", "You're role is to ensure every job posting is bias free.") }}

    {{ post }}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/role#example----using-_role-in-a-template-string",
    "title": "Example -- Using _.role() in a template string",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "template_string YouAreA(name: string, job: string) #"
  {{ _.role("system") }} 
  You are an expert {{ name }}. {{ job }}

  {{ ctx.output_format }}
  {{ _.role("user") }}
"#

function CheckJobPosting(post: string) -> bool {
  client GPT4o
  prompt #"
    {{ YouAreA("hr admin", "You're role is to ensure every job posting is bias free.") }}

    {{ post }}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/role#example----using-_role-in-a-template-string",
    "title": "Example -- Using _.role() in a template string",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "See [template_string](/ref/baml/template-string) to learn how to add variables in .baml files",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/variables",
    "title": "Variables",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "See template_string to learn how to add variables in .baml files",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/variables",
    "title": "Variables",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "Use conditional statements to control the flow and output of your templates based on conditions:

```jinja
function MyFunc(user: User) -> string {
  prompt #"
    {% if user.is_active %}
      Welcome back, {{ user.name }}!
    {% else %}
      Please activate your account.
    {% endif %}
  "#
}
```",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/conditionals",
    "title": "Conditionals",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "Use conditional statements to control the flow and output of your templates based on conditions:
function MyFunc(user: User) -> string {
  prompt #"
    {% if user.is_active %}
      Welcome back, {{ user.name }}!
    {% else %}
      Please activate your account.
    {% endif %}
  "#
}
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/conditionals",
    "title": "Conditionals",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "Here's how you can iterate over a list of items, accessing each item's attributes:

```jinja
function MyFunc(messages: Message[]) -> string {
  prompt #"
    {% for message in messages %}
      {{ message.user_name }}: {{ message.content }}
    {% endfor %}
  "#
}
```",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/loops",
    "title": "Loops",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "Here's how you can iterate over a list of items, accessing each item's attributes:
function MyFunc(messages: Message[]) -> string {
  prompt #"
    {% for message in messages %}
      {{ message.user_name }}: {{ message.content }}
    {% endfor %}
  "#
}

loop
Jinja provides a loop object that can be used to access information about the loop. Here are some of the attributes of the loop object:
| Variable         | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| loop.index       | The current iteration of the loop. (1 indexed)                              |
| loop.index0      | The current iteration of the loop. (0 indexed)                              |
| loop.revindex    | The number of iterations from the end of the loop (1 indexed)               |
| loop.revindex0   | The number of iterations from the end of the loop (0 indexed)               |
| loop.first       | True if first iteration.                                                    |
| loop.last        | True if last iteration.                                                     |
| loop.length      | The number of items in the sequence.                                        |
| loop.cycle       | A helper function to cycle between a list of sequences. See the explanation below. |
| loop.depth       | Indicates how deep in a recursive loop the rendering currently is. Starts at level 1 |
| loop.depth0      | Indicates how deep in a recursive loop the rendering currently is. Starts at level 0 |
| loop.previtem    | The item from the previous iteration of the loop. Undefined during the first iteration. |
| loop.nextitem    | The item from the following iteration of the loop. Undefined during the last iteration. |
| loop.changed(*val) | True if previously called with a different value (or not called at all).  |
prompt #"
  {% for item in items %}
    {{ loop.index }}: {{ item }}
  {% endfor %}
"#
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/loops",
    "title": "Loops",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/prompt-syntax/what-is-jinja",
        "title": "Prompt Syntax",
      },
    ],
    "description": "Jinja provides a loop object that can be used to access information about the loop. Here are some of the attributes of the loop object:
| Variable         | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| loop.index       | The current iteration of the loop. (1 indexed)                              |
| loop.index0      | The current iteration of the loop. (0 indexed)                              |
| loop.revindex    | The number of iterations from the end of the loop (1 indexed)               |
| loop.revindex0   | The number of iterations from the end of the loop (0 indexed)               |
| loop.first       | True if first iteration.                                                    |
| loop.last        | True if last iteration.                                                     |
| loop.length      | The number of items in the sequence.                                        |
| loop.cycle       | A helper function to cycle between a list of sequences. See the explanation below. |
| loop.depth       | Indicates how deep in a recursive loop the rendering currently is. Starts at level 1 |
| loop.depth0      | Indicates how deep in a recursive loop the rendering currently is. Starts at level 0 |
| loop.previtem    | The item from the previous iteration of the loop. Undefined during the first iteration. |
| loop.nextitem    | The item from the following iteration of the loop. Undefined during the last iteration. |
| loop.changed(*val) | True if previously called with a different value (or not called at all).  |
prompt #"
  {% for item in items %}
    {{ loop.index }}: {{ item }}
  {% endfor %}
"#
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/loops#loop",
    "title": "loop",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Prompt Syntax",
    ],
    "content": "Jinja provides a loop object that can be used to access information about the loop. Here are some of the attributes of the loop object:
| Variable         | Description                                                                 |
|------------------|-----------------------------------------------------------------------------|
| loop.index       | The current iteration of the loop. (1 indexed)                              |
| loop.index0      | The current iteration of the loop. (0 indexed)                              |
| loop.revindex    | The number of iterations from the end of the loop (1 indexed)               |
| loop.revindex0   | The number of iterations from the end of the loop (0 indexed)               |
| loop.first       | True if first iteration.                                                    |
| loop.last        | True if last iteration.                                                     |
| loop.length      | The number of items in the sequence.                                        |
| loop.cycle       | A helper function to cycle between a list of sequences. See the explanation below. |
| loop.depth       | Indicates how deep in a recursive loop the rendering currently is. Starts at level 1 |
| loop.depth0      | Indicates how deep in a recursive loop the rendering currently is. Starts at level 0 |
| loop.previtem    | The item from the previous iteration of the loop. Undefined during the first iteration. |
| loop.nextitem    | The item from the following iteration of the loop. Undefined during the last iteration. |
| loop.changed(*val) | True if previously called with a different value (or not called at all).  |
prompt #"
  {% for item in items %}
    {{ loop.index }}: {{ item }}
  {% endfor %}
"#
",
    "indexSegmentId": "0",
    "slug": "ref/prompt-syntax/loops#loop",
    "title": "loop",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/editor-extension-settings/baml-cli-path",
        "title": "Editor Extension Settings",
      },
    ],
    "description": "| Type | Value |
| --- | --- |
| `string \| null` | null |



If set, all generated code will use this instead of the packaged generator shipped with the extension.

<Tip>
We recommend this setting! This prevents mismatches between the VSCode Extension and the installed BAML package.
</Tip>",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-cli-path",
    "title": "baml.cliPath",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Editor Extension Settings",
    ],
    "content": "| Type | Value |
| --- | --- |
| string \| null | null |
If set, all generated code will use this instead of the packaged generator shipped with the extension.
We recommend this setting! This prevents mismatches between the VSCode Extension and the installed BAML package.
Usage
If you use unix, you can run where baml-cli in your project to figure out what the path is.
{
  "baml.cliPath": "/path/to/baml-cli"
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-cli-path",
    "title": "baml.cliPath",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/editor-extension-settings/baml-cli-path",
        "title": "Editor Extension Settings",
      },
    ],
    "description": "If you use unix, you can run where baml-cli in your project to figure out what the path is.
{
  "baml.cliPath": "/path/to/baml-cli"
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-cli-path#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Editor Extension Settings",
    ],
    "content": "If you use unix, you can run where baml-cli in your project to figure out what the path is.
{
  "baml.cliPath": "/path/to/baml-cli"
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-cli-path#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/editor-extension-settings/baml-cli-path",
        "title": "Editor Extension Settings",
      },
    ],
    "description": "| Type | Default Value |
| --- | --- |
| `"always" \| "never"` | "always" |


- `always`: Generate code for `baml_client` on every save
- `never`: Do not generate `baml_client` on any save

If you have a generator of type `rest/*`, `"always"` will not do any code generation. You will have to manually run:

```
path/to/baml-cli generate
```",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-generate-code-on-save",
    "title": "baml.generateCodeOnSave",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Editor Extension Settings",
    ],
    "content": "| Type | Default Value |
| --- | --- |
| "always" \| "never" | "always" |

always: Generate code for baml_client on every save
never: Do not generate baml_client on any save

If you have a generator of type rest/*, "always" will not do any code generation. You will have to manually run:
path/to/baml-cli generate

Usage
{
  "baml.generateCodeOnSave": "never",
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-generate-code-on-save",
    "title": "baml.generateCodeOnSave",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/editor-extension-settings/baml-cli-path",
        "title": "Editor Extension Settings",
      },
    ],
    "description": "{
  "baml.generateCodeOnSave": "never",
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-generate-code-on-save#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Editor Extension Settings",
    ],
    "content": "{
  "baml.generateCodeOnSave": "never",
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-generate-code-on-save#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/editor-extension-settings/baml-cli-path",
        "title": "Editor Extension Settings",
      },
    ],
    "description": "| Type | Default Value |
| --- | --- |
| `boolean` | `true` |

- `true`: Automatically restarts the TypeScript Language Server in VSCode when the BAML extension generates the TypeScript `baml_client` files. This is a workaround for VSCode's issues with recognizing newly added directories and files in the TypeScript Language Server. No-op if not generating TypeScript files.
- `false`: Does not automatically restart the TypeScript Server. You may need to manually reload the TS server to ensure it recognizes the new types.",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-restart-ts-server-on-save",
    "title": "baml.restartTSServerOnSave",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Editor Extension Settings",
    ],
    "content": "| Type | Default Value |
| --- | --- |
| boolean | true |

true: Automatically restarts the TypeScript Language Server in VSCode when the BAML extension generates the TypeScript baml_client files. This is a workaround for VSCode's issues with recognizing newly added directories and files in the TypeScript Language Server. No-op if not generating TypeScript files.
false: Does not automatically restart the TypeScript Server. You may need to manually reload the TS server to ensure it recognizes the new types.

Usage
{
  "baml.restartTSServerOnSave": true
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-restart-ts-server-on-save",
    "title": "baml.restartTSServerOnSave",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/editor-extension-settings/baml-cli-path",
        "title": "Editor Extension Settings",
      },
    ],
    "description": "{
  "baml.restartTSServerOnSave": true
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-restart-ts-server-on-save#usage",
    "title": "Usage",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Editor Extension Settings",
    ],
    "content": "{
  "baml.restartTSServerOnSave": true
}
",
    "indexSegmentId": "0",
    "slug": "ref/editor-extension-settings/baml-restart-ts-server-on-save#usage",
    "title": "Usage",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
    ],
    "description": "We leveraged our expertise in structured data extraction to create a general purpose extraction API that is independent of BAML.

If you are interested in converting PDF documents, invoices, images into readable structured data, this API is for you.


To try it out visit our [Dashboard v2](https://dashboard.boundaryml.com). Note that this is a different website from the current tracing/observability dashboard (app.boundaryml.com). We are working on unifying the two.",
    "indexSegmentId": "0",
    "slug": "ref/boundary-extraction-api/extract",
    "title": "Overview",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Extraction API",
    ],
    "content": "We leveraged our expertise in structured data extraction to create a general purpose extraction API that is independent of BAML.
If you are interested in converting PDF documents, invoices, images into readable structured data, this API is for you.
To try it out visit our Dashboard v2. Note that this is a different website from the current tracing/observability dashboard (app.boundaryml.com). We are working on unifying the two.",
    "indexSegmentId": "0",
    "slug": "ref/boundary-extraction-api/extract",
    "title": "Overview",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
    ],
    "description": undefined,
    "indexSegmentId": "0",
    "slug": "ref/boundary-extraction-api/extract/extraction-examples",
    "title": "Examples",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Extraction API",
      "API Reference",
    ],
    "content": "Upload a File (PDF, images)
import requests
from typing import List, Dict, Any

def extract_data(api_key: str, file_paths: List[str], prompt: str) -> Dict[str, Any]:
    url = "https://api2.boundaryml.com/v3/extract"
    headers = {
        "Authorization": f"Bearer {api_key}"
    }
    files = [('files', open(file_path, 'rb')) for file_path in file_paths]
    data = {
        'prompt': prompt
    }
    response = requests.post(url, headers=headers, files=files, data=data)
    response.raise_for_status()
    return response.json()

# Usage example
api_key = 'your_api_key_here'
file_paths = ['path/to/file1.pdf', 'path/to/file2.png']
prompt = 'Please extract the text content.'

result = extract_data(api_key, file_paths, prompt)
print(result)
import axios, { AxiosResponse } from 'axios';
import * as FormData from 'form-data';
import * as fs from 'fs';

interface ExtractResponse {
  extractions: Extraction[];
  usage: Usage;
  request_id: string;
}

interface Extraction {
  source: Source;
  output: any;
}

interface Source {
  type: string;
  name?: string;
  page?: number;
}

interface Usage {
  consumed_chars: number;
  produced_chars: number;
  consumed_megapixels: number;
}

async function extractData(apiKey: string, filePaths: string[], prompt: string): Promise<ExtractResponse> {
  const url = 'https://api2.boundaryml.com/v3/extract';
  const formData = new FormData();

  filePaths.forEach(filePath => {
    formData.append('files', fs.createReadStream(filePath));
  });
  formData.append('prompt', prompt);

  const headers = {
    ...formData.getHeaders(),
    'Authorization': `Bearer ${apiKey}`,
  };

  const response: AxiosResponse<ExtractResponse> = await axios.post(url, formData, { headers });
  return response.data;
}

// Usage example
const apiKey = 'your_api_key_here';
const filePaths = ['path/to/file1.pdf', 'path/to/file2.png'];
const prompt = 'Please extract the text content.';

extractData(apiKey, filePaths, prompt)
  .then(result => console.log(result))
  .catch(error => console.error(error));
require 'net/http'
require 'uri'
require 'json'

def extract_data(api_key, file_paths, prompt)
  uri = URI.parse('https://api2.boundaryml.com/v3/extract')
  request = Net::HTTP::Post.new(uri)
  request['Authorization'] = "Bearer #{api_key}"

  form_data = [['prompt', prompt]]
  file_paths.each do |file_path|
    form_data << ['files', File.open(file_path)]
  end

  request.set_form(form_data, 'multipart/form-data')

  req_options = {
    use_ssl: uri.scheme == 'https',
  }

  response = Net::HTTP.start(uri.hostname, uri.port, req_options) do |http|
    http.request(request)
  end

  if response.is_a?(Net::HTTPSuccess)
    JSON.parse(response.body)
  else
    raise "Request failed: #{response.code} #{response.message}"
  end
end

# Usage example
api_key = 'your_api_key_here'
file_paths = ['path/to/file1.pdf', 'path/to/file2.png']
prompt = 'Please extract the text content.'

result = extract_data(api_key, file_paths, prompt)
puts result
package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"mime/multipart"
	"net/http"
	"os"
)

type ExtractResponse struct {
	Extractions []Extraction `json:"extractions"`
	Usage       Usage        `json:"usage"`
	RequestID   string       `json:"request_id"`
}

type Extraction struct {
	Source Source      `json:"source"`
	Output interface{} `json:"output"`
}

type Source struct {
	Type string `json:"type"`
	Name string `json:"name,omitempty"`
	Page int    `json:"page,omitempty"`
}

type Usage struct {
	ConsumedChars      int     `json:"consumed_chars"`
	ProducedChars      int     `json:"produced_chars"`
	ConsumedMegapixels float64 `json:"consumed_megapixels"`
}

func extractData(apiKey string, filePaths []string, prompt string) (ExtractResponse, error) {
	url := "https://api2.boundaryml.com/v3/extract"
	body := &bytes.Buffer{}
	writer := multipart.NewWriter(body)

	for _, filePath := range filePaths {
		file, err := os.Open(filePath)
		if err != nil {
			return ExtractResponse{}, err
		}
		defer file.Close()

		part, err := writer.CreateFormFile("files", file.Name())
		if err != nil {
			return ExtractResponse{}, err
		}
		_, err = io.Copy(part, file)
		if err != nil {
			return ExtractResponse{}, err
		}
	}

	_ = writer.WriteField("prompt", prompt)
	err := writer.Close()
	if err != nil {
		return ExtractResponse{}, err
	}

	req, err := http.NewRequest("POST", url, body)
	if err != nil {
		return ExtractResponse{}, err
	}
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", apiKey))
	req.Header.Set("Content-Type", writer.FormDataContentType())

	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		return ExtractResponse{}, err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return ExtractResponse{}, fmt.Errorf("Request failed with status %s", resp.Status)
	}

	var extractResponse ExtractResponse
	err = json.NewDecoder(resp.Body).Decode(&extractResponse)
	if err != nil {
		return ExtractResponse{}, err
	}

	return extractResponse, nil
}

func main() {
	apiKey := "your_api_key_here"
	filePaths := []string{"path/to/file1.pdf", "path/to/file2.png"}
	prompt := "Please extract the text content."

	result, err := extractData(apiKey, filePaths, prompt)
	if err != nil {
		fmt.Println("Error:", err)
		return
	}

	fmt.Printf("Result: %+v\n", result)
}

",
    "indexSegmentId": "0",
    "slug": "ref/boundary-extraction-api/extract/extraction-examples",
    "title": "Examples",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
    ],
    "description": "import requests
from typing import List, Dict, Any

def extract_data(api_key: str, file_paths: List[str], prompt: str) -> Dict[str, Any]:
    url = "https://api2.boundaryml.com/v3/extract"
    headers = {
        "Authorization": f"Bearer {api_key}"
    }
    files = [('files', open(file_path, 'rb')) for file_path in file_paths]
    data = {
        'prompt': prompt
    }
    response = requests.post(url, headers=headers, files=files, data=data)
    response.raise_for_status()
    return response.json()

# Usage example
api_key = 'your_api_key_here'
file_paths = ['path/to/file1.pdf', 'path/to/file2.png']
prompt = 'Please extract the text content.'

result = extract_data(api_key, file_paths, prompt)
print(result)
import axios, { AxiosResponse } from 'axios';
import * as FormData from 'form-data';
import * as fs from 'fs';

interface ExtractResponse {
  extractions: Extraction[];
  usage: Usage;
  request_id: string;
}

interface Extraction {
  source: Source;
  output: any;
}

interface Source {
  type: string;
  name?: string;
  page?: number;
}

interface Usage {
  consumed_chars: number;
  produced_chars: number;
  consumed_megapixels: number;
}

async function extractData(apiKey: string, filePaths: string[], prompt: string): Promise<ExtractResponse> {
  const url = 'https://api2.boundaryml.com/v3/extract';
  const formData = new FormData();

  filePaths.forEach(filePath => {
    formData.append('files', fs.createReadStream(filePath));
  });
  formData.append('prompt', prompt);

  const headers = {
    ...formData.getHeaders(),
    'Authorization': `Bearer ${apiKey}`,
  };

  const response: AxiosResponse<ExtractResponse> = await axios.post(url, formData, { headers });
  return response.data;
}

// Usage example
const apiKey = 'your_api_key_here';
const filePaths = ['path/to/file1.pdf', 'path/to/file2.png'];
const prompt = 'Please extract the text content.';

extractData(apiKey, filePaths, prompt)
  .then(result => console.log(result))
  .catch(error => console.error(error));
require 'net/http'
require 'uri'
require 'json'

def extract_data(api_key, file_paths, prompt)
  uri = URI.parse('https://api2.boundaryml.com/v3/extract')
  request = Net::HTTP::Post.new(uri)
  request['Authorization'] = "Bearer #{api_key}"

  form_data = [['prompt', prompt]]
  file_paths.each do |file_path|
    form_data << ['files', File.open(file_path)]
  end

  request.set_form(form_data, 'multipart/form-data')

  req_options = {
    use_ssl: uri.scheme == 'https',
  }

  response = Net::HTTP.start(uri.hostname, uri.port, req_options) do |http|
    http.request(request)
  end

  if response.is_a?(Net::HTTPSuccess)
    JSON.parse(response.body)
  else
    raise "Request failed: #{response.code} #{response.message}"
  end
end

# Usage example
api_key = 'your_api_key_here'
file_paths = ['path/to/file1.pdf', 'path/to/file2.png']
prompt = 'Please extract the text content.'

result = extract_data(api_key, file_paths, prompt)
puts result
package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"mime/multipart"
	"net/http"
	"os"
)

type ExtractResponse struct {
	Extractions []Extraction `json:"extractions"`
	Usage       Usage        `json:"usage"`
	RequestID   string       `json:"request_id"`
}

type Extraction struct {
	Source Source      `json:"source"`
	Output interface{} `json:"output"`
}

type Source struct {
	Type string `json:"type"`
	Name string `json:"name,omitempty"`
	Page int    `json:"page,omitempty"`
}

type Usage struct {
	ConsumedChars      int     `json:"consumed_chars"`
	ProducedChars      int     `json:"produced_chars"`
	ConsumedMegapixels float64 `json:"consumed_megapixels"`
}

func extractData(apiKey string, filePaths []string, prompt string) (ExtractResponse, error) {
	url := "https://api2.boundaryml.com/v3/extract"
	body := &bytes.Buffer{}
	writer := multipart.NewWriter(body)

	for _, filePath := range filePaths {
		file, err := os.Open(filePath)
		if err != nil {
			return ExtractResponse{}, err
		}
		defer file.Close()

		part, err := writer.CreateFormFile("files", file.Name())
		if err != nil {
			return ExtractResponse{}, err
		}
		_, err = io.Copy(part, file)
		if err != nil {
			return ExtractResponse{}, err
		}
	}

	_ = writer.WriteField("prompt", prompt)
	err := writer.Close()
	if err != nil {
		return ExtractResponse{}, err
	}

	req, err := http.NewRequest("POST", url, body)
	if err != nil {
		return ExtractResponse{}, err
	}
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", apiKey))
	req.Header.Set("Content-Type", writer.FormDataContentType())

	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		return ExtractResponse{}, err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return ExtractResponse{}, fmt.Errorf("Request failed with status %s", resp.Status)
	}

	var extractResponse ExtractResponse
	err = json.NewDecoder(resp.Body).Decode(&extractResponse)
	if err != nil {
		return ExtractResponse{}, err
	}

	return extractResponse, nil
}

func main() {
	apiKey := "your_api_key_here"
	filePaths := []string{"path/to/file1.pdf", "path/to/file2.png"}
	prompt := "Please extract the text content."

	result, err := extractData(apiKey, filePaths, prompt)
	if err != nil {
		fmt.Println("Error:", err)
		return
	}

	fmt.Printf("Result: %+v\n", result)
}

",
    "indexSegmentId": "0",
    "slug": "ref/boundary-extraction-api/extract/extraction-examples#upload-a-file-pdf-images",
    "title": "Upload a File (PDF, images)",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Extraction API",
      "API Reference",
    ],
    "content": "import requests
from typing import List, Dict, Any

def extract_data(api_key: str, file_paths: List[str], prompt: str) -> Dict[str, Any]:
    url = "https://api2.boundaryml.com/v3/extract"
    headers = {
        "Authorization": f"Bearer {api_key}"
    }
    files = [('files', open(file_path, 'rb')) for file_path in file_paths]
    data = {
        'prompt': prompt
    }
    response = requests.post(url, headers=headers, files=files, data=data)
    response.raise_for_status()
    return response.json()

# Usage example
api_key = 'your_api_key_here'
file_paths = ['path/to/file1.pdf', 'path/to/file2.png']
prompt = 'Please extract the text content.'

result = extract_data(api_key, file_paths, prompt)
print(result)
import axios, { AxiosResponse } from 'axios';
import * as FormData from 'form-data';
import * as fs from 'fs';

interface ExtractResponse {
  extractions: Extraction[];
  usage: Usage;
  request_id: string;
}

interface Extraction {
  source: Source;
  output: any;
}

interface Source {
  type: string;
  name?: string;
  page?: number;
}

interface Usage {
  consumed_chars: number;
  produced_chars: number;
  consumed_megapixels: number;
}

async function extractData(apiKey: string, filePaths: string[], prompt: string): Promise<ExtractResponse> {
  const url = 'https://api2.boundaryml.com/v3/extract';
  const formData = new FormData();

  filePaths.forEach(filePath => {
    formData.append('files', fs.createReadStream(filePath));
  });
  formData.append('prompt', prompt);

  const headers = {
    ...formData.getHeaders(),
    'Authorization': `Bearer ${apiKey}`,
  };

  const response: AxiosResponse<ExtractResponse> = await axios.post(url, formData, { headers });
  return response.data;
}

// Usage example
const apiKey = 'your_api_key_here';
const filePaths = ['path/to/file1.pdf', 'path/to/file2.png'];
const prompt = 'Please extract the text content.';

extractData(apiKey, filePaths, prompt)
  .then(result => console.log(result))
  .catch(error => console.error(error));
require 'net/http'
require 'uri'
require 'json'

def extract_data(api_key, file_paths, prompt)
  uri = URI.parse('https://api2.boundaryml.com/v3/extract')
  request = Net::HTTP::Post.new(uri)
  request['Authorization'] = "Bearer #{api_key}"

  form_data = [['prompt', prompt]]
  file_paths.each do |file_path|
    form_data << ['files', File.open(file_path)]
  end

  request.set_form(form_data, 'multipart/form-data')

  req_options = {
    use_ssl: uri.scheme == 'https',
  }

  response = Net::HTTP.start(uri.hostname, uri.port, req_options) do |http|
    http.request(request)
  end

  if response.is_a?(Net::HTTPSuccess)
    JSON.parse(response.body)
  else
    raise "Request failed: #{response.code} #{response.message}"
  end
end

# Usage example
api_key = 'your_api_key_here'
file_paths = ['path/to/file1.pdf', 'path/to/file2.png']
prompt = 'Please extract the text content.'

result = extract_data(api_key, file_paths, prompt)
puts result
package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"mime/multipart"
	"net/http"
	"os"
)

type ExtractResponse struct {
	Extractions []Extraction `json:"extractions"`
	Usage       Usage        `json:"usage"`
	RequestID   string       `json:"request_id"`
}

type Extraction struct {
	Source Source      `json:"source"`
	Output interface{} `json:"output"`
}

type Source struct {
	Type string `json:"type"`
	Name string `json:"name,omitempty"`
	Page int    `json:"page,omitempty"`
}

type Usage struct {
	ConsumedChars      int     `json:"consumed_chars"`
	ProducedChars      int     `json:"produced_chars"`
	ConsumedMegapixels float64 `json:"consumed_megapixels"`
}

func extractData(apiKey string, filePaths []string, prompt string) (ExtractResponse, error) {
	url := "https://api2.boundaryml.com/v3/extract"
	body := &bytes.Buffer{}
	writer := multipart.NewWriter(body)

	for _, filePath := range filePaths {
		file, err := os.Open(filePath)
		if err != nil {
			return ExtractResponse{}, err
		}
		defer file.Close()

		part, err := writer.CreateFormFile("files", file.Name())
		if err != nil {
			return ExtractResponse{}, err
		}
		_, err = io.Copy(part, file)
		if err != nil {
			return ExtractResponse{}, err
		}
	}

	_ = writer.WriteField("prompt", prompt)
	err := writer.Close()
	if err != nil {
		return ExtractResponse{}, err
	}

	req, err := http.NewRequest("POST", url, body)
	if err != nil {
		return ExtractResponse{}, err
	}
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", apiKey))
	req.Header.Set("Content-Type", writer.FormDataContentType())

	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		return ExtractResponse{}, err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return ExtractResponse{}, fmt.Errorf("Request failed with status %s", resp.Status)
	}

	var extractResponse ExtractResponse
	err = json.NewDecoder(resp.Body).Decode(&extractResponse)
	if err != nil {
		return ExtractResponse{}, err
	}

	return extractResponse, nil
}

func main() {
	apiKey := "your_api_key_here"
	filePaths := []string{"path/to/file1.pdf", "path/to/file2.png"}
	prompt := "Please extract the text content."

	result, err := extractData(apiKey, filePaths, prompt)
	if err != nil {
		fmt.Println("Error:", err)
		return
	}

	fmt.Printf("Result: %+v\n", result)
}

",
    "indexSegmentId": "0",
    "slug": "ref/boundary-extraction-api/extract/extraction-examples#upload-a-file-pdf-images",
    "title": "Upload a File (PDF, images)",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/cloud/limits",
        "title": "Boundary Cloud API",
      },
    ],
    "description": undefined,
    "indexSegmentId": "0",
    "slug": "ref/cloud/limits",
    "title": "Limits",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud API",
    ],
    "content": "Boundary Functions
baml_src, per-project:

You may deploy up to 100MiB of baml_src.

Environment variables, per-project:

You may have up to 1000 environment variables.
You may have up to 64KiB of data across all environment variables combined.
",
    "indexSegmentId": "0",
    "slug": "ref/cloud/limits",
    "title": "Limits",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "ref/cloud/limits",
        "title": "Boundary Cloud API",
      },
    ],
    "description": "baml_src, per-project:

You may deploy up to 100MiB of baml_src.

Environment variables, per-project:

You may have up to 1000 environment variables.
You may have up to 64KiB of data across all environment variables combined.
",
    "indexSegmentId": "0",
    "slug": "ref/cloud/limits#boundary-functions",
    "title": "Boundary Functions",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Cloud API",
    ],
    "content": "baml_src, per-project:

You may deploy up to 100MiB of baml_src.

Environment variables, per-project:

You may have up to 1000 environment variables.
You may have up to 64KiB of data across all environment variables combined.
",
    "indexSegmentId": "0",
    "slug": "ref/cloud/limits#boundary-functions",
    "title": "Boundary Functions",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "All notable changes to this project will be documented in this file. See [conventional commits](https://www.conventionalcommits.org/) for commit guidelines.",
    "indexSegmentId": "0",
    "slug": "changelog/changelog",
    "title": "Changelog",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "All notable changes to this project will be documented in this file. See conventional commits for commit guidelines.
0.65.0 - 2024-10-31
Documentation

New Documentation Structure: Introduced version 3 of the documentation, enhancing clarity and organization. (#1118)

Bug Fixes

Python Type Handling: Moved Python Checked and Check types into baml_client for better type management. (#1122)
Literal Input Type Checking: Fixed an issue where literal inputs were not being type-checked correctly. (#1121)

0.64.0 - 2024-10-29
Bug Fixes

Playground Stability: Prevented crashes in the playground due to malformed vertex credentials (#1107) - Samuel Lijin
Union Handling: Addressed an issue with union types in the schema (#1096) - Greg Hale
WASM Function Signatures: Resolved stack overflow when computing WASM function signatures (#1100) - aaronvg
VSCode Extension: Fixed crashes in the VSCode extension that caused the output panel to open unexpectedly (#1103) - hellovai
Static Analysis Improvements: Enhanced static analysis on Jinja expressions and regex_match functions (#1102, #1104) - hellovai
Codegen Enhancements: Fixed code generation for Python boolean literals and updated integration tests (#1099) - Antonio Sarosi
Enum Handling: Improved substring alias handling for enums (#1098) - Miguel Cárdenas
Syntax Highlighting: Refined span calculations for Jinja expressions and improved VSCode syntax highlighting with Lezer (#1110) - hellovai
Ruby Support: Fixed literal boolean tests for Ruby (#1109) - Antonio Sarosi

Features

Constraint Support: Added the ability to define constraints using Jinja expressions (#1006) - Greg Hale
VSCode & Fiddle UI: Introduced a new "Intro to Checks" UI for easier onboarding (#1106) - Samuel Lijin
Dev Container Configurations: Added Dev Container configurations for streamlined development environments (#1112) - Antonio Sarosi

Documentation

Constraints Documentation: Published new documentation for defining constraints in BAML (#1113) - Greg Hale
Dynamic Types Linking: Added cross-links to dynamic types documentation for easier navigation (#1116) - Greg Hale

Miscellaneous

Code Quality: Improved style and fixed typos in the codebase (#1115) - Greg Hale
Parsing Stability: Added logic to prevent assertions from parsing errors and ensured checks no longer affect parsing (#1101) - hellovai
Version Bump: Bumped version to 0.64.0 (#1114, #ff7e152) - Vaibhav Gupta

0.63.0 - 2024-10-23
Bug Fixes

Fix dynamic enums which already are defined in BAML (#1080) - (22d0f1c) - hellovai

Features

Updated clients.baml to use the latest sonnet model (#1081) - (71df0b7) - aaronvg
Improved clients.baml generated via baml init (#1089) - (682dd66) - hellovai


0.62.0 - 2024-10-21
Features

Support serializing/deserializing baml_py.Image, baml_py.Audio for pydantic (#1062) - (11cb699) - Samuel Lijin
Support rendering input classes with aliases (#1045) - (3824cda) - aaronvg
Add unstable_internal_repr on FunctionResult in python (#1068) - (00082e8) - hellovai
Add literal support for type_builder (#1069) - (c0085d9) - hellovai

Bug Fixes

Surface errors in fallbacks containing only erroneous clients (#1061) - (b69ef79) - Greg Hale
Fix parser so that we are able to correctly detect sequences of empty strings. (#1048) - (977e277) - hellovai
Make substring match algorithm case insensitive (#1056) - (fa2c477) - Antonio Sarosi
Fix vertex-ai citation data being optional (#1058) - (5eae0a7) - aaronvg
Fix bug to correctly cast to pydantic types in ambiguous scenarios where BAML knows better (#1059) - (830b0cb) - hellovai
Parser: Prefer case sensitive match over case insensitive (#1063) - (cd6b141) - Antonio Sarosi
Only popup the vscode env var dialog once (#1066) - (1951474) - aaronvg

Documentation

Docs for literal types (#1030) - (55e5964) - Antonio Sarosi
Contribution guide (#1055) - (f09d943) - aaronvg

Misc

Fix VSCode metrics (#1044) - (a131336) - hellovai
Add more test cases for unquoted strings in objects (#1054) - (2d1b700) - hellovai


0.61.1 - 2024-10-15
Bug Fixes

add musl to the ts release artifacts (#1042) - (e74f3e9) - Samuel Lijin


0.61.0 - 2024-10-14
Features

Implement literal types (#978) - (9e7431f) - Antonio Sarosi
allow installing the TS library on node-alpine (#1029) - (1c37a0d) - Samuel Lijin
Add WYSIWYG UI (Swagger UI) to baml-cli dev (#1019) - (0c73cab) - Greg Hale
Suppress streaming for Numbers (#1032) - (3f4621b) - Greg Hale

Bug Fixes

Add limit on connection pool to prevent stalling issues in pyo3 and other ffi boundaries (#1027) - (eb90e62) - hellovai
Update docs (#1025) - (2dd1bb6) - Farookh Zaheer Siddiqui
Fix parsing for streaming of objects more stable (#1031) - (8aa9c00) - hellovai
Fix python BamlValidationError type (#1036) - (59a9510) - aaronvg

Miscellaneous

Popup settings dialog when no env vars set (#1033) - (b9fa52a) - aaronvg
Bump version to 0.61.0 - (ca2242b) - Aaron Villalpando

0.60.0 - 2024-10-09
Miscellaneous Chores

update Dockerfile (#1017) - (51539b7) - Ikko Eltociear Ashimine
Revert "feat: add a WYSIWYG UI (Swagger UI) to baml-cli dev (#1011)" (#1018) - (f235050) - Greg Hale

Bug fixes

Fix python types for BamlValidationError (#1020) - (520a09c) - aaronvg
coerce floats and ints with commas and other special cases (#1023) - (904492e) - aaronvg

Docs

Add Docs for Jupyter notebook usage (#1008) - (c51d918) - aaronvg

0.59.0 - 2024-10-04
Features

(vertex) allow specifying creds as JSON object (#1009) - (98868da) - Samuel Lijin
Add prompt, raw_output and error message to BamlValidationError in TS and Python (#1005) - (447dbf4) - aaronvg
Add BamlValidationError to baml-cli serve (#1007) - (3b8cf16) - aaronvg
Include a WYSIWYG UI (Swagger UI) to baml-cli dev (#1011) - (fe9dde4) - imalsogreg

0.58.0 - 2024-10-02
Features

Add client registry support for BAML over Rest (OpenAPI) (#1000) - (abe70bf) - Lorenz Ohly

Bug Fixes

Improve performance of parsing escaped characters in strings during streaming. (#1002) - (b35ae2c) - hellovai

Documentation

Add Docs for Document Extraction API (#996) - (da1a5e8) - aaronvg

0.57.1 - 2024-09-29
Bug Fixes

[BUGFIX] Parser should require a space between class keyword and class name (#990) - (7528247) - Greg Hale
Remove dynamic string attributes (#991) - (0960ab2) - Greg Hale
ts fixes (#992) - (36af43f) - aaronvg
Bump version to 0.57.1 - (0aa71dd) - Aaron Villalpando

0.57.0 - 2024-09-27
Documentation

Fix Python dynamic types example (#979) - (eade116) - lorenzoh

Features

teach vscode/fiddle to explain when we drop information (#897) - (93e2b9b) - Samuel Lijin
Add ability for users to reset env vars to their desire. (#984) - (69e6c29) - hellovai

Bug Fixes

Fixed panic during logging for splitting on UTF-8 strings. (#987) - (c27a64f) - hellovai
Improve SAP for triple quoted strings along with unions (#977) - (44202ab) - hellovai
Add more unit tests for parsing logic inspired by user (#980) - (48dd09f) - hellovai
Improve syntax errors e.g. class / enum parsing and also update pestmodel to handle traling comments (#981) - (adbb6ae) - hellovai
Updating docs for env vars (#985) - (305d6b3) - hellovai
When using openai-generic, use a string as the content type in the api request if theres no media (#988) - (e8fa739) - aaronvg

0.56.1 - 2024-09-21
Bug Fixes

Improved parser for unions (#975) - (b390521) - hellovai
[syntax] Allow lists to contain trailing comma (#974) - (9e3dc6c) - Greg Hale

0.56.0 - 2024-09-20
Shout outs to Nico for fixing some internal Rust dependencies, and to Lorenz for correcting our documentation! We really appreciate it :)
Features

use better default for openapi/rust client (#958) - (b74ef15) - Samuel Lijin

Bug Fixes

push optional-list and optional-map validation to post-parse (#959) - (c0480d5) - Samuel Lijin
improve OpenAPI instructions for windows/java (#962) - (6010efb) - Samuel Lijin
assorted fixes: unquoted strings, openai-generic add api_key for bearer auth, support escape characters in quoted strings (#965) - (847f3a9) - hellovai
serde-serialize can cause a package dependency cycle (#967) - (109ae09) - Nico
make anthropic work in fiddle/vscode (#970) - (32eccae) - Samuel Lijin
make dynamic enums work as outputs in Ruby (#972) - (7530402) - Samuel Lijin

Documentation

suggest correct python init command in vscode readme (#954) - (e99c5dd) - Samuel Lijin
add more vscode debugging instructions (#955) - (342b657) - Samuel Lijin
NextJS hook needs to be bound to the correct context (#957) - (ee80451) - aaronvg
update nextjs hooks and docs (#952) - (01cf855) - aaronvg
Fix some documentation typos (#966) - (5193cd7) - Greg Hale
Keywords AI router (#953) - (1c6f975) - aaronvg
Fix post_generate comment (#968) - (919c79f) - lorenzoh

Bug Fixes

show actionable errors for string[]? and map<...>? type validation (#946) - (48879c0) - Samuel Lijin

Documentation

add reference docs about env vars (#945) - (dd43bc5) - Samuel Lijin

0.55.2 - 2024-09-11
Bug Fixes

use correct locking strategy inside baml-cli serve (#943) - (fcb694d) - Samuel Lijin

Features

allow using DANGER_ACCEPT_INVALID_CERTS to disable https verification (#901) - (8873fe7) - Samuel Lijin

0.55.1 - 2024-09-10
Bug Fixes

in generated TS code, put eslint-disable before ts-nocheck - (16d04c6) - Sam Lijin
baml-cli in python works again - (b57ca0f) - Sam Lijin

Documentation

update java install instructions (#933) - (b497003) - Samuel Lijin

Miscellaneous Chores

add version headers to the openapi docs (#931) - (21545f2) - Samuel Lijin

0.55.0 - 2024-09-09
With this release, we're announcing support for BAML in all languages: we now
allow you to call your functions over an HTTP interface, and will generate an
OpenAPI specification for your BAML functions, so you can now generate a client
in any language of your choice, be it Golang, Java, PHP, Ruby, Rust, or any of
the other languages which OpenAPI supports.
Start here to learn more: https://docs.boundaryml.com/docs/get-started/quickstart/openapi
Features

implement BAML-over-HTTP (#908) - (484fa93) - Samuel Lijin
Add anonymous telemetry about playground actions (#925) - (6f58c9e) - hellovai

0.54.2 - 2024-09-05
Features

Add a setting to disable restarting TS server in VSCode (#920) - (628f236) - aaronvg
Add prompt prefix for map types in ctx.output_format and add more type validation for map params (#919) - (4d304c5) - hellovai

Bug fixes

Fix glibC issues for python linux-x86_64 (#922) - (9161bec) - Samuel Lijin

Documentation

Add nextjs hooks (#921) - (fe14f5a) - aaronvg

0.54.1 - 2024-09-03
BREAKING CHANGE

Fix escape characters in quoted strings (#905) - (9ba6eb8) - hellovai

Prior "\n" was interpreted as "\\n" in quoted strings. This has been fixed to interpret "\n" as newline characters and true for other escape characters.
Documentation

updated dead vs-code-extension link (#914) - (b12f164) - Christian Warmuth
Update docs for setting env vars (#904) - (ec1ca94) - hellovai
Add docs for LMStudio (#906) - (ea4c187) - hellovai
Fix docs for anthropic (#910) - (aba2764) - hellovai
Update discord links on docs (#911) - (927357d) - hellovai

Features

BAML_LOG will truncate messages to 1000 characters (modify using env var BOUNDARY_MAX_LOG_CHUNK_SIZE) (#907) - (d266e5c) - hellovai

Bug Fixes

Improve parsing parsing when there are initial closing ] or } (#903) - (46b0cde) - hellovai
Update build script for ruby to build all platforms (#915) - (df2f51e) - hellovai
Add unit-test for openai-generic provider and ensure it compiles (#916) - (fde7c50) - hellovai

0.54.0 - 2024-08-27
BREAKING CHANGE

Update Default Gemini Base URL to v1beta (#891) - (a5d8c58) - gleed

The default base URL for the Gemini provider has been updated to v1beta. This change is should have no impact on existing users as v1beta is the default version for the Gemini python library, we are mirroring this change in BAML.
Bug Fixes

Allow promptfiddle to talk to localhost ollama (#886) - (5f02b2a) - Samuel Lijin
Update Parser for unions so they handle nested objects better (#900) - (c5b9a75) - hellovai

Documentation

Add ollama to default prompt fiddle example (#888) - (49146c0) - Samuel Lijin
Adding improved docs + unit tests for caching (#895) - (ff7be44) - hellovai

Features

Allow local filepaths to be used in tests in BAML files (image and audio) (#871) - (fa6dc03) - Samuel Lijin
Add support for absolute file paths in the file specifier (#881) - (fcd189e) - hellovai
Implement shorthand clients (You can now use "openai/gpt-4o" as short for creating a complete client.) (#879) - (ddd15c9) - Samuel Lijin
Add support for arbritrary metadata (e.g. cache_policy for anthropic) (#893) - (0d63a70) - hellovai
Expose Exceptions to user code: BamlError, BamlInvalidArgumentError, BamlClientError, BamlClientHttpError, BamlValidationError (#770) - (7da14c4) - hellovai

Internal

AST Restructuring (#857) - (75b51cb) - Anish Palakurthi

0.53.1 - 2024-08-11
Bug Fixes


fix github release not passing params to napi script causing issues in x86_64 (#872)


(06b962b) - aaronvg


Features

Add Client orchestration graph in playground (#801) - (24b5895) - Anish Palakurthi
increase range of python FFI support (#870) - (ec9b66c) - Samuel Lijin

Misc

Bump version to 0.53.1 - (e4301e3) - Aaron Villalpando

0.53.0 - 2024-08-05
Bug Fixes

make image[] render correctly in prompts (#855) - (4a17dce) - Samuel Lijin

Features

(ruby) implement dynamic types, dynamic clients, images, and audio (#842) - (4a21eed) - Samuel Lijin
Codelenses for test cases (#812) - (7cd8794) - Anish Palakurthi

Issue

removed vertex auth token printing (#846) - (b839316) - Anish Palakurthi
Fix google type deserialization issue - (a55b9a1) - Aaron Villalpando

Miscellaneous Chores

clean up release stuff (#836) - (eed41b7) - Samuel Lijin
Add bfcl results to readme, fix links icons (#856) - (5ef7f3d) - aaronvg
Fix prompt fiddle and playground styles, add more logging, and add stop-reason to playground (#858) - (38e3153) - aaronvg
Bump version to 0.53.0 - (fd16839) - Aaron Villalpando

0.52.1 - 2024-07-24
Bug Fixes

build python x86_64-linux with an older glibc (#834) - (db12540) - Samuel Lijin

0.52.0 - 2024-07-24
Features

Add official support for ruby (#823) - (e81cc79) - Samuel Lijin

Bug Fixes

Fix ClientRegistry for Typescript code-gen (#828) - (b69921f) - hellovai

0.51.2 - 2024-07-24
Features

Add support for unions / maps / null in TypeBuilder. (#820) - (8d9e92d) - hellovai

Bug Fixes

[Playground] Add a feedback button (#818) - (f749f2b) - Samuel Lijin

Documentation

Improvements across docs (#807) - (bc0c176) - Anish Palakurthi

0.51.1 - 2024-07-21
Features

Add a feedback button to VSCode Extension (#811) - (f371912) - Samuel Lijin

Bug

Allow default_client_mode in the generator #813 (#815) - (6df7fca) - hellovai

0.51.0 - 2024-07-19
Bug Fixes

Improve BAML Parser for numbers and single-key objects (#785) - (c5af7b0) - hellovai
Add docs for VLLM (#792) - (79e8773) - hellovai
LLVM install and rebuild script (#794) - (9ee66ed) - Anish Palakurthi
Prevent version mismatches when generating baml_client (#791) - (d793603) - aaronvg
fiddle build fix (#800) - (d304203) - aaronvg
Dont drop extra fields in dynamic classes when passing them as inputs to a function (#802) - (4264c9b) - aaronvg
Adding support for a sync client for Python + Typescript (#803) - (62085e7) - hellovai
Fix WASM-related issues introduced in #803 (#804) - (0a950e0) - hellovai
Adding various fixes (#806) - (e8c1a61) - hellovai

Features

implement maps in BAML (#797) - (97d7e62) - Samuel Lijin
Support Vertex AI (Google Cloud SDK) (#790) - (d98ee81) - Anish Palakurthi
Add copy buttons to test results in playground (#799) - (b5eee3d) - aaronvg

Miscellaneous Chores

in fern config, defer to installed version (#789) - (479f1b2) - fern
publish docs on every push to the default branch (#796) - (180824a) - Samuel Lijin
🌿 introducing fern docs (#779) - (46f06a9) - fern
Add test for dynamic list input (#798) - (7528d6a) - aaronvg

0.50.0 - 2024-07-11
Bug Fixes

[Playground] Environment variable button is now visible on all themes (#762) - (adc4da1) - aaronvg
[Playground] Fix to cURL rendering and mime_type overriding (#763) - (67f9c6a) - Anish Palakurthi

Features

[Runtime] Add support for clients that change at runtime using ClientRegistry (#683) - (c0fb454) - hellovai
https://docs.boundaryml.com/docs/calling-baml/client-registry

Documentation

Add more documentation for TypeBuilder (#767) - (85dc8ab) - Samuel Lijin

0.49.0 - 2024-07-08
Bug Fixes

Fixed Azure / Ollama clients. Removing stream_options from azure and ollama clients (#760) - (30bf88f) - hellovai

Features

Add support for arm64-linux (#751) - (adb8ee3) - Samuel Lijin

0.48.0 - 2024-07-04
Bug Fixes

Fix env variables dialoge on VSCode (#750)
Playground selects correct function after loading (#757) - (09963a0) - aaronvg

Miscellaneous Chores

Better error messages on logging failures to Boundary Studio (#754) - (49c768f) - aaronvg

0.47.0 - 2024-07-03
Bug Fixes

make settings dialog work in vscode again (#750) (c94e355) - aaronvg
restore releases on arm64-linux (#751) - (adb8ee3) - Samuel Lijin

0.46.0 - 2024-07-03
Bug Fixes

Fixed tracing issues for Boundary Studio (#740) - (77a4db7) - Samuel Lijin
Fixed flush() to be more reliable (#744) - (9dd5fda) - Samuel Lijin
Remove error when user passes in extra fields in a class (#746) - (2755b43) - aaronvg

Features

Add support for base_url for the google-ai provider (#747) - (005b1d9) - hellovai
Playground UX improvements (#742) - (5cb56fd) - hellovai
Prompt Fiddle now auto-switches functions when to change files (#745)

Documentation

Added a large example project on promptfiddle.com (#741) - (f80da1e) - aaronvg
Mark ruby as in beta (#743) - (901109d) - Samuel Lijin

0.45.0 - 2024-06-29
Bug Fixes

Fixed streaming in Python Client which didn't show result until later (#726) - (e4f2daa) - Anish Palakurthi
Improve playground stability on first load (#732) - (2ac7b32) - Anish Palakurthi
Add improved static analysis for jinja (#734) - (423faa1) - hellovai

Documentation

Docs for Dynamic Types (#722) https://docs.boundaryml.com/docs/calling-baml/dynamic-types

Features

Show raw cURL request in Playground (#723) - (57928e1) - Anish Palakurthi
Support bedrock as a provider (#725) - (c64c665) - Samuel Lijin

0.44.0 - 2024-06-26
Bug Fixes

Fix typebuilder for random enums (#721)

0.43.0 - 2024-06-26
Bug Fixes

fix pnpm lockfile issue (#720)

0.42.0 - 2024-06-26
Bug Fixes

correctly propagate LICENSE to baml-py (#695) - (3fda880) - Samuel Lijin

Miscellaneous Chores

update jsonish readme (#685) - (b19f04a) - Samuel Lijin

Vscode

add link to tracing, show token counts (#703) - (64aa18a) - Samuel Lijin

[0.41.0] - 2024-06-20
Bug Fixes

rollback git lfs, images broken in docs rn (#534) - (6945506) - Samuel Lijin
search for markdown blocks correctly (#641) - (6b8abf1) - Samuel Lijin
restore one-workspace-per-folder (#656) - (a464bde) - Samuel Lijin
ruby generator should be ruby/sorbet (#661) - (0019f39) - Samuel Lijin
ruby compile error snuck in (#663) - (0cb2583) - Samuel Lijin

Documentation

add typescript examples (#477) - (532481c) - Samuel Lijin
add titles to code blocks for all CodeGroup elems (#483) - (76c6b68) - Samuel Lijin
add docs for round-robin clients (#500) - (221f902) - Samuel Lijin
add ruby example (#689) - (16e187f) - Samuel Lijin

Features

implement baml version --check --output json (#444) - (5f076ac) - Samuel Lijin
show update prompts in vscode (#451) - (b66da3e) - Samuel Lijin
add tests to check that baml version --check works (#454) - (be1499d) - Samuel Lijin
parse typescript versions in version --check (#473) - (b4b2250) - Samuel Lijin
implement round robin client strategies (#494) - (599fcdd) - Samuel Lijin
add integ-tests support to build (#542) - (f59cf2e) - Samuel Lijin
make ruby work again (#650) - (6472bec) - Samuel Lijin
Add RB2B tracking script (#682) - (54547a3) - hellovai

Miscellaneous Chores

add nodemon config to typescript/ (#435) - (231b396) - Samuel Lijin
finish gloo to BoundaryML renames (#452) - (88a7fda) - Samuel Lijin
set up lfs (#511) - (3a43143) - Samuel Lijin
add internal build tooling for sam (#512) - (9ebacca) - Samuel Lijin
delete clients dir, this is now dead code (#652) - (ec2627f) - Samuel Lijin
consolidate vscode workspace, bump a bunch of deps (#654) - (82bf6ab) - Samuel Lijin
Add RB2B tracking script to propmt fiddle (#681) - (4cf806b) - hellovai
Adding better release script (#688) - (5bec282) - hellovai

[AUTO

patch] Version bump for nightly release [NIGHTLY:cli] [NIGHTLY:vscode_ext] [NIGHTLY:client-python] - (d05a22c) - GitHub Action

Build

fix baml-core-ffi script (#521) - (b1b7f4a) - Samuel Lijin
fix engine/ (#522) - (154f646) - Samuel Lijin

Integ-tests

add ruby test - (c0bc101) - Sam Lijin

Readme

add function calling, collapse the table (#505) - (2f9024c) - Samuel Lijin

Release

bump versions for everything (#662) - (c0254ae) - Samuel Lijin

Vscode

check for updates on the hour (#434) - (c70a3b3) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog",
    "title": "Changelog",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0650---2024-10-31",
        "title": "0.65.0 - 2024-10-31",
      },
    ],
    "description": "
New Documentation Structure: Introduced version 3 of the documentation, enhancing clarity and organization. (#1118)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.65.0 - 2024-10-31",
    ],
    "content": "
New Documentation Structure: Introduced version 3 of the documentation, enhancing clarity and organization. (#1118)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0650---2024-10-31",
        "title": "0.65.0 - 2024-10-31",
      },
    ],
    "description": "
Python Type Handling: Moved Python Checked and Check types into baml_client for better type management. (#1122)
Literal Input Type Checking: Fixed an issue where literal inputs were not being type-checked correctly. (#1121)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.65.0 - 2024-10-31",
    ],
    "content": "
Python Type Handling: Moved Python Checked and Check types into baml_client for better type management. (#1122)
Literal Input Type Checking: Fixed an issue where literal inputs were not being type-checked correctly. (#1121)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0640---2024-10-29",
        "title": "0.64.0 - 2024-10-29",
      },
    ],
    "description": "
Playground Stability: Prevented crashes in the playground due to malformed vertex credentials (#1107) - Samuel Lijin
Union Handling: Addressed an issue with union types in the schema (#1096) - Greg Hale
WASM Function Signatures: Resolved stack overflow when computing WASM function signatures (#1100) - aaronvg
VSCode Extension: Fixed crashes in the VSCode extension that caused the output panel to open unexpectedly (#1103) - hellovai
Static Analysis Improvements: Enhanced static analysis on Jinja expressions and regex_match functions (#1102, #1104) - hellovai
Codegen Enhancements: Fixed code generation for Python boolean literals and updated integration tests (#1099) - Antonio Sarosi
Enum Handling: Improved substring alias handling for enums (#1098) - Miguel Cárdenas
Syntax Highlighting: Refined span calculations for Jinja expressions and improved VSCode syntax highlighting with Lezer (#1110) - hellovai
Ruby Support: Fixed literal boolean tests for Ruby (#1109) - Antonio Sarosi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-1",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.64.0 - 2024-10-29",
    ],
    "content": "
Playground Stability: Prevented crashes in the playground due to malformed vertex credentials (#1107) - Samuel Lijin
Union Handling: Addressed an issue with union types in the schema (#1096) - Greg Hale
WASM Function Signatures: Resolved stack overflow when computing WASM function signatures (#1100) - aaronvg
VSCode Extension: Fixed crashes in the VSCode extension that caused the output panel to open unexpectedly (#1103) - hellovai
Static Analysis Improvements: Enhanced static analysis on Jinja expressions and regex_match functions (#1102, #1104) - hellovai
Codegen Enhancements: Fixed code generation for Python boolean literals and updated integration tests (#1099) - Antonio Sarosi
Enum Handling: Improved substring alias handling for enums (#1098) - Miguel Cárdenas
Syntax Highlighting: Refined span calculations for Jinja expressions and improved VSCode syntax highlighting with Lezer (#1110) - hellovai
Ruby Support: Fixed literal boolean tests for Ruby (#1109) - Antonio Sarosi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-1",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0640---2024-10-29",
        "title": "0.64.0 - 2024-10-29",
      },
    ],
    "description": "
Constraint Support: Added the ability to define constraints using Jinja expressions (#1006) - Greg Hale
VSCode & Fiddle UI: Introduced a new "Intro to Checks" UI for easier onboarding (#1106) - Samuel Lijin
Dev Container Configurations: Added Dev Container configurations for streamlined development environments (#1112) - Antonio Sarosi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.64.0 - 2024-10-29",
    ],
    "content": "
Constraint Support: Added the ability to define constraints using Jinja expressions (#1006) - Greg Hale
VSCode & Fiddle UI: Introduced a new "Intro to Checks" UI for easier onboarding (#1106) - Samuel Lijin
Dev Container Configurations: Added Dev Container configurations for streamlined development environments (#1112) - Antonio Sarosi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0640---2024-10-29",
        "title": "0.64.0 - 2024-10-29",
      },
    ],
    "description": "
Constraints Documentation: Published new documentation for defining constraints in BAML (#1113) - Greg Hale
Dynamic Types Linking: Added cross-links to dynamic types documentation for easier navigation (#1116) - Greg Hale
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-1",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.64.0 - 2024-10-29",
    ],
    "content": "
Constraints Documentation: Published new documentation for defining constraints in BAML (#1113) - Greg Hale
Dynamic Types Linking: Added cross-links to dynamic types documentation for easier navigation (#1116) - Greg Hale
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-1",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0640---2024-10-29",
        "title": "0.64.0 - 2024-10-29",
      },
    ],
    "description": "
Code Quality: Improved style and fixed typos in the codebase (#1115) - Greg Hale
Parsing Stability: Added logic to prevent assertions from parsing errors and ensured checks no longer affect parsing (#1101) - hellovai
Version Bump: Bumped version to 0.64.0 (#1114, #ff7e152) - Vaibhav Gupta
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous",
    "title": "Miscellaneous",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.64.0 - 2024-10-29",
    ],
    "content": "
Code Quality: Improved style and fixed typos in the codebase (#1115) - Greg Hale
Parsing Stability: Added logic to prevent assertions from parsing errors and ensured checks no longer affect parsing (#1101) - hellovai
Version Bump: Bumped version to 0.64.0 (#1114, #ff7e152) - Vaibhav Gupta
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous",
    "title": "Miscellaneous",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0630---2024-10-23",
        "title": "0.63.0 - 2024-10-23",
      },
    ],
    "description": "
Fix dynamic enums which already are defined in BAML (#1080) - (22d0f1c) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-2",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.63.0 - 2024-10-23",
    ],
    "content": "
Fix dynamic enums which already are defined in BAML (#1080) - (22d0f1c) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-2",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0630---2024-10-23",
        "title": "0.63.0 - 2024-10-23",
      },
    ],
    "description": "
Updated clients.baml to use the latest sonnet model (#1081) - (71df0b7) - aaronvg
Improved clients.baml generated via baml init (#1089) - (682dd66) - hellovai

",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-1",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.63.0 - 2024-10-23",
    ],
    "content": "
Updated clients.baml to use the latest sonnet model (#1081) - (71df0b7) - aaronvg
Improved clients.baml generated via baml init (#1089) - (682dd66) - hellovai

",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-1",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0620---2024-10-21",
        "title": "0.62.0 - 2024-10-21",
      },
    ],
    "description": "
Support serializing/deserializing baml_py.Image, baml_py.Audio for pydantic (#1062) - (11cb699) - Samuel Lijin
Support rendering input classes with aliases (#1045) - (3824cda) - aaronvg
Add unstable_internal_repr on FunctionResult in python (#1068) - (00082e8) - hellovai
Add literal support for type_builder (#1069) - (c0085d9) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-2",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.62.0 - 2024-10-21",
    ],
    "content": "
Support serializing/deserializing baml_py.Image, baml_py.Audio for pydantic (#1062) - (11cb699) - Samuel Lijin
Support rendering input classes with aliases (#1045) - (3824cda) - aaronvg
Add unstable_internal_repr on FunctionResult in python (#1068) - (00082e8) - hellovai
Add literal support for type_builder (#1069) - (c0085d9) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-2",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0620---2024-10-21",
        "title": "0.62.0 - 2024-10-21",
      },
    ],
    "description": "
Surface errors in fallbacks containing only erroneous clients (#1061) - (b69ef79) - Greg Hale
Fix parser so that we are able to correctly detect sequences of empty strings. (#1048) - (977e277) - hellovai
Make substring match algorithm case insensitive (#1056) - (fa2c477) - Antonio Sarosi
Fix vertex-ai citation data being optional (#1058) - (5eae0a7) - aaronvg
Fix bug to correctly cast to pydantic types in ambiguous scenarios where BAML knows better (#1059) - (830b0cb) - hellovai
Parser: Prefer case sensitive match over case insensitive (#1063) - (cd6b141) - Antonio Sarosi
Only popup the vscode env var dialog once (#1066) - (1951474) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-3",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.62.0 - 2024-10-21",
    ],
    "content": "
Surface errors in fallbacks containing only erroneous clients (#1061) - (b69ef79) - Greg Hale
Fix parser so that we are able to correctly detect sequences of empty strings. (#1048) - (977e277) - hellovai
Make substring match algorithm case insensitive (#1056) - (fa2c477) - Antonio Sarosi
Fix vertex-ai citation data being optional (#1058) - (5eae0a7) - aaronvg
Fix bug to correctly cast to pydantic types in ambiguous scenarios where BAML knows better (#1059) - (830b0cb) - hellovai
Parser: Prefer case sensitive match over case insensitive (#1063) - (cd6b141) - Antonio Sarosi
Only popup the vscode env var dialog once (#1066) - (1951474) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-3",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0620---2024-10-21",
        "title": "0.62.0 - 2024-10-21",
      },
    ],
    "description": "
Docs for literal types (#1030) - (55e5964) - Antonio Sarosi
Contribution guide (#1055) - (f09d943) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-2",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.62.0 - 2024-10-21",
    ],
    "content": "
Docs for literal types (#1030) - (55e5964) - Antonio Sarosi
Contribution guide (#1055) - (f09d943) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-2",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0620---2024-10-21",
        "title": "0.62.0 - 2024-10-21",
      },
    ],
    "description": "
Fix VSCode metrics (#1044) - (a131336) - hellovai
Add more test cases for unquoted strings in objects (#1054) - (2d1b700) - hellovai

",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#misc",
    "title": "Misc",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.62.0 - 2024-10-21",
    ],
    "content": "
Fix VSCode metrics (#1044) - (a131336) - hellovai
Add more test cases for unquoted strings in objects (#1054) - (2d1b700) - hellovai

",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#misc",
    "title": "Misc",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0611---2024-10-15",
        "title": "0.61.1 - 2024-10-15",
      },
    ],
    "description": "
add musl to the ts release artifacts (#1042) - (e74f3e9) - Samuel Lijin

",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-4",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.61.1 - 2024-10-15",
    ],
    "content": "
add musl to the ts release artifacts (#1042) - (e74f3e9) - Samuel Lijin

",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-4",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0610---2024-10-14",
        "title": "0.61.0 - 2024-10-14",
      },
    ],
    "description": "
Implement literal types (#978) - (9e7431f) - Antonio Sarosi
allow installing the TS library on node-alpine (#1029) - (1c37a0d) - Samuel Lijin
Add WYSIWYG UI (Swagger UI) to baml-cli dev (#1019) - (0c73cab) - Greg Hale
Suppress streaming for Numbers (#1032) - (3f4621b) - Greg Hale
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-3",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.61.0 - 2024-10-14",
    ],
    "content": "
Implement literal types (#978) - (9e7431f) - Antonio Sarosi
allow installing the TS library on node-alpine (#1029) - (1c37a0d) - Samuel Lijin
Add WYSIWYG UI (Swagger UI) to baml-cli dev (#1019) - (0c73cab) - Greg Hale
Suppress streaming for Numbers (#1032) - (3f4621b) - Greg Hale
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-3",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0610---2024-10-14",
        "title": "0.61.0 - 2024-10-14",
      },
    ],
    "description": "
Add limit on connection pool to prevent stalling issues in pyo3 and other ffi boundaries (#1027) - (eb90e62) - hellovai
Update docs (#1025) - (2dd1bb6) - Farookh Zaheer Siddiqui
Fix parsing for streaming of objects more stable (#1031) - (8aa9c00) - hellovai
Fix python BamlValidationError type (#1036) - (59a9510) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-5",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.61.0 - 2024-10-14",
    ],
    "content": "
Add limit on connection pool to prevent stalling issues in pyo3 and other ffi boundaries (#1027) - (eb90e62) - hellovai
Update docs (#1025) - (2dd1bb6) - Farookh Zaheer Siddiqui
Fix parsing for streaming of objects more stable (#1031) - (8aa9c00) - hellovai
Fix python BamlValidationError type (#1036) - (59a9510) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-5",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0610---2024-10-14",
        "title": "0.61.0 - 2024-10-14",
      },
    ],
    "description": "
Popup settings dialog when no env vars set (#1033) - (b9fa52a) - aaronvg
Bump version to 0.61.0 - (ca2242b) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-1",
    "title": "Miscellaneous",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.61.0 - 2024-10-14",
    ],
    "content": "
Popup settings dialog when no env vars set (#1033) - (b9fa52a) - aaronvg
Bump version to 0.61.0 - (ca2242b) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-1",
    "title": "Miscellaneous",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0600---2024-10-09",
        "title": "0.60.0 - 2024-10-09",
      },
    ],
    "description": "
update Dockerfile (#1017) - (51539b7) - Ikko Eltociear Ashimine
Revert "feat: add a WYSIWYG UI (Swagger UI) to baml-cli dev (#1011)" (#1018) - (f235050) - Greg Hale
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores",
    "title": "Miscellaneous Chores",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.60.0 - 2024-10-09",
    ],
    "content": "
update Dockerfile (#1017) - (51539b7) - Ikko Eltociear Ashimine
Revert "feat: add a WYSIWYG UI (Swagger UI) to baml-cli dev (#1011)" (#1018) - (f235050) - Greg Hale
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores",
    "title": "Miscellaneous Chores",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0600---2024-10-09",
        "title": "0.60.0 - 2024-10-09",
      },
    ],
    "description": "
Fix python types for BamlValidationError (#1020) - (520a09c) - aaronvg
coerce floats and ints with commas and other special cases (#1023) - (904492e) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-6",
    "title": "Bug fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.60.0 - 2024-10-09",
    ],
    "content": "
Fix python types for BamlValidationError (#1020) - (520a09c) - aaronvg
coerce floats and ints with commas and other special cases (#1023) - (904492e) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-6",
    "title": "Bug fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0600---2024-10-09",
        "title": "0.60.0 - 2024-10-09",
      },
    ],
    "description": "
Add Docs for Jupyter notebook usage (#1008) - (c51d918) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#docs",
    "title": "Docs",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.60.0 - 2024-10-09",
    ],
    "content": "
Add Docs for Jupyter notebook usage (#1008) - (c51d918) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#docs",
    "title": "Docs",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0590---2024-10-04",
        "title": "0.59.0 - 2024-10-04",
      },
    ],
    "description": "
(vertex) allow specifying creds as JSON object (#1009) - (98868da) - Samuel Lijin
Add prompt, raw_output and error message to BamlValidationError in TS and Python (#1005) - (447dbf4) - aaronvg
Add BamlValidationError to baml-cli serve (#1007) - (3b8cf16) - aaronvg
Include a WYSIWYG UI (Swagger UI) to baml-cli dev (#1011) - (fe9dde4) - imalsogreg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-4",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.59.0 - 2024-10-04",
    ],
    "content": "
(vertex) allow specifying creds as JSON object (#1009) - (98868da) - Samuel Lijin
Add prompt, raw_output and error message to BamlValidationError in TS and Python (#1005) - (447dbf4) - aaronvg
Add BamlValidationError to baml-cli serve (#1007) - (3b8cf16) - aaronvg
Include a WYSIWYG UI (Swagger UI) to baml-cli dev (#1011) - (fe9dde4) - imalsogreg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-4",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0580---2024-10-02",
        "title": "0.58.0 - 2024-10-02",
      },
    ],
    "description": "
Add client registry support for BAML over Rest (OpenAPI) (#1000) - (abe70bf) - Lorenz Ohly
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-5",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.58.0 - 2024-10-02",
    ],
    "content": "
Add client registry support for BAML over Rest (OpenAPI) (#1000) - (abe70bf) - Lorenz Ohly
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-5",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0580---2024-10-02",
        "title": "0.58.0 - 2024-10-02",
      },
    ],
    "description": "
Improve performance of parsing escaped characters in strings during streaming. (#1002) - (b35ae2c) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-7",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.58.0 - 2024-10-02",
    ],
    "content": "
Improve performance of parsing escaped characters in strings during streaming. (#1002) - (b35ae2c) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-7",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0580---2024-10-02",
        "title": "0.58.0 - 2024-10-02",
      },
    ],
    "description": "
Add Docs for Document Extraction API (#996) - (da1a5e8) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-3",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.58.0 - 2024-10-02",
    ],
    "content": "
Add Docs for Document Extraction API (#996) - (da1a5e8) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-3",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0571---2024-09-29",
        "title": "0.57.1 - 2024-09-29",
      },
    ],
    "description": "
[BUGFIX] Parser should require a space between class keyword and class name (#990) - (7528247) - Greg Hale
Remove dynamic string attributes (#991) - (0960ab2) - Greg Hale
ts fixes (#992) - (36af43f) - aaronvg
Bump version to 0.57.1 - (0aa71dd) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-8",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.57.1 - 2024-09-29",
    ],
    "content": "
[BUGFIX] Parser should require a space between class keyword and class name (#990) - (7528247) - Greg Hale
Remove dynamic string attributes (#991) - (0960ab2) - Greg Hale
ts fixes (#992) - (36af43f) - aaronvg
Bump version to 0.57.1 - (0aa71dd) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-8",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0570---2024-09-27",
        "title": "0.57.0 - 2024-09-27",
      },
    ],
    "description": "
Fix Python dynamic types example (#979) - (eade116) - lorenzoh
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-4",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.57.0 - 2024-09-27",
    ],
    "content": "
Fix Python dynamic types example (#979) - (eade116) - lorenzoh
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-4",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0570---2024-09-27",
        "title": "0.57.0 - 2024-09-27",
      },
    ],
    "description": "
teach vscode/fiddle to explain when we drop information (#897) - (93e2b9b) - Samuel Lijin
Add ability for users to reset env vars to their desire. (#984) - (69e6c29) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-6",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.57.0 - 2024-09-27",
    ],
    "content": "
teach vscode/fiddle to explain when we drop information (#897) - (93e2b9b) - Samuel Lijin
Add ability for users to reset env vars to their desire. (#984) - (69e6c29) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-6",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0570---2024-09-27",
        "title": "0.57.0 - 2024-09-27",
      },
    ],
    "description": "
Fixed panic during logging for splitting on UTF-8 strings. (#987) - (c27a64f) - hellovai
Improve SAP for triple quoted strings along with unions (#977) - (44202ab) - hellovai
Add more unit tests for parsing logic inspired by user (#980) - (48dd09f) - hellovai
Improve syntax errors e.g. class / enum parsing and also update pestmodel to handle traling comments (#981) - (adbb6ae) - hellovai
Updating docs for env vars (#985) - (305d6b3) - hellovai
When using openai-generic, use a string as the content type in the api request if theres no media (#988) - (e8fa739) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-9",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.57.0 - 2024-09-27",
    ],
    "content": "
Fixed panic during logging for splitting on UTF-8 strings. (#987) - (c27a64f) - hellovai
Improve SAP for triple quoted strings along with unions (#977) - (44202ab) - hellovai
Add more unit tests for parsing logic inspired by user (#980) - (48dd09f) - hellovai
Improve syntax errors e.g. class / enum parsing and also update pestmodel to handle traling comments (#981) - (adbb6ae) - hellovai
Updating docs for env vars (#985) - (305d6b3) - hellovai
When using openai-generic, use a string as the content type in the api request if theres no media (#988) - (e8fa739) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-9",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0561---2024-09-21",
        "title": "0.56.1 - 2024-09-21",
      },
    ],
    "description": "
Improved parser for unions (#975) - (b390521) - hellovai
[syntax] Allow lists to contain trailing comma (#974) - (9e3dc6c) - Greg Hale
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-10",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.56.1 - 2024-09-21",
    ],
    "content": "
Improved parser for unions (#975) - (b390521) - hellovai
[syntax] Allow lists to contain trailing comma (#974) - (9e3dc6c) - Greg Hale
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-10",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "Shout outs to Nico for fixing some internal Rust dependencies, and to Lorenz for correcting our documentation! We really appreciate it :)",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#0560---2024-09-20",
    "title": "0.56.0 - 2024-09-20",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "Shout outs to Nico for fixing some internal Rust dependencies, and to Lorenz for correcting our documentation! We really appreciate it :)",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#0560---2024-09-20",
    "title": "0.56.0 - 2024-09-20",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0560---2024-09-20",
        "title": "0.56.0 - 2024-09-20",
      },
    ],
    "description": "
use better default for openapi/rust client (#958) - (b74ef15) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-7",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.56.0 - 2024-09-20",
    ],
    "content": "
use better default for openapi/rust client (#958) - (b74ef15) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-7",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0560---2024-09-20",
        "title": "0.56.0 - 2024-09-20",
      },
    ],
    "description": "
push optional-list and optional-map validation to post-parse (#959) - (c0480d5) - Samuel Lijin
improve OpenAPI instructions for windows/java (#962) - (6010efb) - Samuel Lijin
assorted fixes: unquoted strings, openai-generic add api_key for bearer auth, support escape characters in quoted strings (#965) - (847f3a9) - hellovai
serde-serialize can cause a package dependency cycle (#967) - (109ae09) - Nico
make anthropic work in fiddle/vscode (#970) - (32eccae) - Samuel Lijin
make dynamic enums work as outputs in Ruby (#972) - (7530402) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-11",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.56.0 - 2024-09-20",
    ],
    "content": "
push optional-list and optional-map validation to post-parse (#959) - (c0480d5) - Samuel Lijin
improve OpenAPI instructions for windows/java (#962) - (6010efb) - Samuel Lijin
assorted fixes: unquoted strings, openai-generic add api_key for bearer auth, support escape characters in quoted strings (#965) - (847f3a9) - hellovai
serde-serialize can cause a package dependency cycle (#967) - (109ae09) - Nico
make anthropic work in fiddle/vscode (#970) - (32eccae) - Samuel Lijin
make dynamic enums work as outputs in Ruby (#972) - (7530402) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-11",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0560---2024-09-20",
        "title": "0.56.0 - 2024-09-20",
      },
    ],
    "description": "
suggest correct python init command in vscode readme (#954) - (e99c5dd) - Samuel Lijin
add more vscode debugging instructions (#955) - (342b657) - Samuel Lijin
NextJS hook needs to be bound to the correct context (#957) - (ee80451) - aaronvg
update nextjs hooks and docs (#952) - (01cf855) - aaronvg
Fix some documentation typos (#966) - (5193cd7) - Greg Hale
Keywords AI router (#953) - (1c6f975) - aaronvg
Fix post_generate comment (#968) - (919c79f) - lorenzoh
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-5",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.56.0 - 2024-09-20",
    ],
    "content": "
suggest correct python init command in vscode readme (#954) - (e99c5dd) - Samuel Lijin
add more vscode debugging instructions (#955) - (342b657) - Samuel Lijin
NextJS hook needs to be bound to the correct context (#957) - (ee80451) - aaronvg
update nextjs hooks and docs (#952) - (01cf855) - aaronvg
Fix some documentation typos (#966) - (5193cd7) - Greg Hale
Keywords AI router (#953) - (1c6f975) - aaronvg
Fix post_generate comment (#968) - (919c79f) - lorenzoh
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-5",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0560---2024-09-20",
        "title": "0.56.0 - 2024-09-20",
      },
    ],
    "description": "
show actionable errors for string[]? and map<...>? type validation (#946) - (48879c0) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-12",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.56.0 - 2024-09-20",
    ],
    "content": "
show actionable errors for string[]? and map<...>? type validation (#946) - (48879c0) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-12",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0560---2024-09-20",
        "title": "0.56.0 - 2024-09-20",
      },
    ],
    "description": "
add reference docs about env vars (#945) - (dd43bc5) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-6",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.56.0 - 2024-09-20",
    ],
    "content": "
add reference docs about env vars (#945) - (dd43bc5) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-6",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0552---2024-09-11",
        "title": "0.55.2 - 2024-09-11",
      },
    ],
    "description": "
use correct locking strategy inside baml-cli serve (#943) - (fcb694d) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-13",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.55.2 - 2024-09-11",
    ],
    "content": "
use correct locking strategy inside baml-cli serve (#943) - (fcb694d) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-13",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0552---2024-09-11",
        "title": "0.55.2 - 2024-09-11",
      },
    ],
    "description": "
allow using DANGER_ACCEPT_INVALID_CERTS to disable https verification (#901) - (8873fe7) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-8",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.55.2 - 2024-09-11",
    ],
    "content": "
allow using DANGER_ACCEPT_INVALID_CERTS to disable https verification (#901) - (8873fe7) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-8",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0551---2024-09-10",
        "title": "0.55.1 - 2024-09-10",
      },
    ],
    "description": "
in generated TS code, put eslint-disable before ts-nocheck - (16d04c6) - Sam Lijin
baml-cli in python works again - (b57ca0f) - Sam Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-14",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.55.1 - 2024-09-10",
    ],
    "content": "
in generated TS code, put eslint-disable before ts-nocheck - (16d04c6) - Sam Lijin
baml-cli in python works again - (b57ca0f) - Sam Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-14",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0551---2024-09-10",
        "title": "0.55.1 - 2024-09-10",
      },
    ],
    "description": "
update java install instructions (#933) - (b497003) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-7",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.55.1 - 2024-09-10",
    ],
    "content": "
update java install instructions (#933) - (b497003) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-7",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0551---2024-09-10",
        "title": "0.55.1 - 2024-09-10",
      },
    ],
    "description": "
add version headers to the openapi docs (#931) - (21545f2) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-1",
    "title": "Miscellaneous Chores",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.55.1 - 2024-09-10",
    ],
    "content": "
add version headers to the openapi docs (#931) - (21545f2) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-1",
    "title": "Miscellaneous Chores",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "description": "With this release, we're announcing support for BAML in all languages: we now
allow you to call your functions over an HTTP interface, and will generate an
OpenAPI specification for your BAML functions, so you can now generate a client
in any language of your choice, be it Golang, Java, PHP, Ruby, Rust, or any of
the other languages which OpenAPI supports.
Start here to learn more: https://docs.boundaryml.com/docs/get-started/quickstart/openapi",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#0550---2024-09-09",
    "title": "0.55.0 - 2024-09-09",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [],
    "content": "With this release, we're announcing support for BAML in all languages: we now
allow you to call your functions over an HTTP interface, and will generate an
OpenAPI specification for your BAML functions, so you can now generate a client
in any language of your choice, be it Golang, Java, PHP, Ruby, Rust, or any of
the other languages which OpenAPI supports.
Start here to learn more: https://docs.boundaryml.com/docs/get-started/quickstart/openapi",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#0550---2024-09-09",
    "title": "0.55.0 - 2024-09-09",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0550---2024-09-09",
        "title": "0.55.0 - 2024-09-09",
      },
    ],
    "description": "
implement BAML-over-HTTP (#908) - (484fa93) - Samuel Lijin
Add anonymous telemetry about playground actions (#925) - (6f58c9e) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-9",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.55.0 - 2024-09-09",
    ],
    "content": "
implement BAML-over-HTTP (#908) - (484fa93) - Samuel Lijin
Add anonymous telemetry about playground actions (#925) - (6f58c9e) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-9",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0542---2024-09-05",
        "title": "0.54.2 - 2024-09-05",
      },
    ],
    "description": "
Add a setting to disable restarting TS server in VSCode (#920) - (628f236) - aaronvg
Add prompt prefix for map types in ctx.output_format and add more type validation for map params (#919) - (4d304c5) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-10",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.2 - 2024-09-05",
    ],
    "content": "
Add a setting to disable restarting TS server in VSCode (#920) - (628f236) - aaronvg
Add prompt prefix for map types in ctx.output_format and add more type validation for map params (#919) - (4d304c5) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-10",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0542---2024-09-05",
        "title": "0.54.2 - 2024-09-05",
      },
    ],
    "description": "
Fix glibC issues for python linux-x86_64 (#922) - (9161bec) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-15",
    "title": "Bug fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.2 - 2024-09-05",
    ],
    "content": "
Fix glibC issues for python linux-x86_64 (#922) - (9161bec) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-15",
    "title": "Bug fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0542---2024-09-05",
        "title": "0.54.2 - 2024-09-05",
      },
    ],
    "description": "
Add nextjs hooks (#921) - (fe14f5a) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-8",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.2 - 2024-09-05",
    ],
    "content": "
Add nextjs hooks (#921) - (fe14f5a) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-8",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0541---2024-09-03",
        "title": "0.54.1 - 2024-09-03",
      },
    ],
    "description": "
Fix escape characters in quoted strings (#905) - (9ba6eb8) - hellovai

Prior "\n" was interpreted as "\\n" in quoted strings. This has been fixed to interpret "\n" as newline characters and true for other escape characters.",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#breaking-change",
    "title": "BREAKING CHANGE",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.1 - 2024-09-03",
    ],
    "content": "
Fix escape characters in quoted strings (#905) - (9ba6eb8) - hellovai

Prior "\n" was interpreted as "\\n" in quoted strings. This has been fixed to interpret "\n" as newline characters and true for other escape characters.",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#breaking-change",
    "title": "BREAKING CHANGE",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0541---2024-09-03",
        "title": "0.54.1 - 2024-09-03",
      },
    ],
    "description": "
updated dead vs-code-extension link (#914) - (b12f164) - Christian Warmuth
Update docs for setting env vars (#904) - (ec1ca94) - hellovai
Add docs for LMStudio (#906) - (ea4c187) - hellovai
Fix docs for anthropic (#910) - (aba2764) - hellovai
Update discord links on docs (#911) - (927357d) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-9",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.1 - 2024-09-03",
    ],
    "content": "
updated dead vs-code-extension link (#914) - (b12f164) - Christian Warmuth
Update docs for setting env vars (#904) - (ec1ca94) - hellovai
Add docs for LMStudio (#906) - (ea4c187) - hellovai
Fix docs for anthropic (#910) - (aba2764) - hellovai
Update discord links on docs (#911) - (927357d) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-9",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0541---2024-09-03",
        "title": "0.54.1 - 2024-09-03",
      },
    ],
    "description": "
BAML_LOG will truncate messages to 1000 characters (modify using env var BOUNDARY_MAX_LOG_CHUNK_SIZE) (#907) - (d266e5c) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-11",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.1 - 2024-09-03",
    ],
    "content": "
BAML_LOG will truncate messages to 1000 characters (modify using env var BOUNDARY_MAX_LOG_CHUNK_SIZE) (#907) - (d266e5c) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-11",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0541---2024-09-03",
        "title": "0.54.1 - 2024-09-03",
      },
    ],
    "description": "
Improve parsing parsing when there are initial closing ] or } (#903) - (46b0cde) - hellovai
Update build script for ruby to build all platforms (#915) - (df2f51e) - hellovai
Add unit-test for openai-generic provider and ensure it compiles (#916) - (fde7c50) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-16",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.1 - 2024-09-03",
    ],
    "content": "
Improve parsing parsing when there are initial closing ] or } (#903) - (46b0cde) - hellovai
Update build script for ruby to build all platforms (#915) - (df2f51e) - hellovai
Add unit-test for openai-generic provider and ensure it compiles (#916) - (fde7c50) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-16",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0540---2024-08-27",
        "title": "0.54.0 - 2024-08-27",
      },
    ],
    "description": "
Update Default Gemini Base URL to v1beta (#891) - (a5d8c58) - gleed

The default base URL for the Gemini provider has been updated to v1beta. This change is should have no impact on existing users as v1beta is the default version for the Gemini python library, we are mirroring this change in BAML.",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#breaking-change-1",
    "title": "BREAKING CHANGE",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.0 - 2024-08-27",
    ],
    "content": "
Update Default Gemini Base URL to v1beta (#891) - (a5d8c58) - gleed

The default base URL for the Gemini provider has been updated to v1beta. This change is should have no impact on existing users as v1beta is the default version for the Gemini python library, we are mirroring this change in BAML.",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#breaking-change-1",
    "title": "BREAKING CHANGE",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0540---2024-08-27",
        "title": "0.54.0 - 2024-08-27",
      },
    ],
    "description": "
Allow promptfiddle to talk to localhost ollama (#886) - (5f02b2a) - Samuel Lijin
Update Parser for unions so they handle nested objects better (#900) - (c5b9a75) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-17",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.0 - 2024-08-27",
    ],
    "content": "
Allow promptfiddle to talk to localhost ollama (#886) - (5f02b2a) - Samuel Lijin
Update Parser for unions so they handle nested objects better (#900) - (c5b9a75) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-17",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0540---2024-08-27",
        "title": "0.54.0 - 2024-08-27",
      },
    ],
    "description": "
Add ollama to default prompt fiddle example (#888) - (49146c0) - Samuel Lijin
Adding improved docs + unit tests for caching (#895) - (ff7be44) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-10",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.0 - 2024-08-27",
    ],
    "content": "
Add ollama to default prompt fiddle example (#888) - (49146c0) - Samuel Lijin
Adding improved docs + unit tests for caching (#895) - (ff7be44) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-10",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0540---2024-08-27",
        "title": "0.54.0 - 2024-08-27",
      },
    ],
    "description": "
Allow local filepaths to be used in tests in BAML files (image and audio) (#871) - (fa6dc03) - Samuel Lijin
Add support for absolute file paths in the file specifier (#881) - (fcd189e) - hellovai
Implement shorthand clients (You can now use "openai/gpt-4o" as short for creating a complete client.) (#879) - (ddd15c9) - Samuel Lijin
Add support for arbritrary metadata (e.g. cache_policy for anthropic) (#893) - (0d63a70) - hellovai
Expose Exceptions to user code: BamlError, BamlInvalidArgumentError, BamlClientError, BamlClientHttpError, BamlValidationError (#770) - (7da14c4) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-12",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.0 - 2024-08-27",
    ],
    "content": "
Allow local filepaths to be used in tests in BAML files (image and audio) (#871) - (fa6dc03) - Samuel Lijin
Add support for absolute file paths in the file specifier (#881) - (fcd189e) - hellovai
Implement shorthand clients (You can now use "openai/gpt-4o" as short for creating a complete client.) (#879) - (ddd15c9) - Samuel Lijin
Add support for arbritrary metadata (e.g. cache_policy for anthropic) (#893) - (0d63a70) - hellovai
Expose Exceptions to user code: BamlError, BamlInvalidArgumentError, BamlClientError, BamlClientHttpError, BamlValidationError (#770) - (7da14c4) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-12",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0540---2024-08-27",
        "title": "0.54.0 - 2024-08-27",
      },
    ],
    "description": "
AST Restructuring (#857) - (75b51cb) - Anish Palakurthi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#internal",
    "title": "Internal",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.54.0 - 2024-08-27",
    ],
    "content": "
AST Restructuring (#857) - (75b51cb) - Anish Palakurthi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#internal",
    "title": "Internal",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0531---2024-08-11",
        "title": "0.53.1 - 2024-08-11",
      },
    ],
    "description": "

fix github release not passing params to napi script causing issues in x86_64 (#872)


(06b962b) - aaronvg

",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-18",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.53.1 - 2024-08-11",
    ],
    "content": "

fix github release not passing params to napi script causing issues in x86_64 (#872)


(06b962b) - aaronvg

",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-18",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0531---2024-08-11",
        "title": "0.53.1 - 2024-08-11",
      },
    ],
    "description": "
Add Client orchestration graph in playground (#801) - (24b5895) - Anish Palakurthi
increase range of python FFI support (#870) - (ec9b66c) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-13",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.53.1 - 2024-08-11",
    ],
    "content": "
Add Client orchestration graph in playground (#801) - (24b5895) - Anish Palakurthi
increase range of python FFI support (#870) - (ec9b66c) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-13",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0531---2024-08-11",
        "title": "0.53.1 - 2024-08-11",
      },
    ],
    "description": "
Bump version to 0.53.1 - (e4301e3) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#misc-1",
    "title": "Misc",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.53.1 - 2024-08-11",
    ],
    "content": "
Bump version to 0.53.1 - (e4301e3) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#misc-1",
    "title": "Misc",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0530---2024-08-05",
        "title": "0.53.0 - 2024-08-05",
      },
    ],
    "description": "
make image[] render correctly in prompts (#855) - (4a17dce) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-19",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.53.0 - 2024-08-05",
    ],
    "content": "
make image[] render correctly in prompts (#855) - (4a17dce) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-19",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0530---2024-08-05",
        "title": "0.53.0 - 2024-08-05",
      },
    ],
    "description": "
(ruby) implement dynamic types, dynamic clients, images, and audio (#842) - (4a21eed) - Samuel Lijin
Codelenses for test cases (#812) - (7cd8794) - Anish Palakurthi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-14",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.53.0 - 2024-08-05",
    ],
    "content": "
(ruby) implement dynamic types, dynamic clients, images, and audio (#842) - (4a21eed) - Samuel Lijin
Codelenses for test cases (#812) - (7cd8794) - Anish Palakurthi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-14",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0530---2024-08-05",
        "title": "0.53.0 - 2024-08-05",
      },
    ],
    "description": "
removed vertex auth token printing (#846) - (b839316) - Anish Palakurthi
Fix google type deserialization issue - (a55b9a1) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#issue",
    "title": "Issue",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.53.0 - 2024-08-05",
    ],
    "content": "
removed vertex auth token printing (#846) - (b839316) - Anish Palakurthi
Fix google type deserialization issue - (a55b9a1) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#issue",
    "title": "Issue",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0530---2024-08-05",
        "title": "0.53.0 - 2024-08-05",
      },
    ],
    "description": "
clean up release stuff (#836) - (eed41b7) - Samuel Lijin
Add bfcl results to readme, fix links icons (#856) - (5ef7f3d) - aaronvg
Fix prompt fiddle and playground styles, add more logging, and add stop-reason to playground (#858) - (38e3153) - aaronvg
Bump version to 0.53.0 - (fd16839) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-2",
    "title": "Miscellaneous Chores",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.53.0 - 2024-08-05",
    ],
    "content": "
clean up release stuff (#836) - (eed41b7) - Samuel Lijin
Add bfcl results to readme, fix links icons (#856) - (5ef7f3d) - aaronvg
Fix prompt fiddle and playground styles, add more logging, and add stop-reason to playground (#858) - (38e3153) - aaronvg
Bump version to 0.53.0 - (fd16839) - Aaron Villalpando
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-2",
    "title": "Miscellaneous Chores",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0521---2024-07-24",
        "title": "0.52.1 - 2024-07-24",
      },
    ],
    "description": "
build python x86_64-linux with an older glibc (#834) - (db12540) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-20",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.52.1 - 2024-07-24",
    ],
    "content": "
build python x86_64-linux with an older glibc (#834) - (db12540) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-20",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0520---2024-07-24",
        "title": "0.52.0 - 2024-07-24",
      },
    ],
    "description": "
Add official support for ruby (#823) - (e81cc79) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-15",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.52.0 - 2024-07-24",
    ],
    "content": "
Add official support for ruby (#823) - (e81cc79) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-15",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0520---2024-07-24",
        "title": "0.52.0 - 2024-07-24",
      },
    ],
    "description": "
Fix ClientRegistry for Typescript code-gen (#828) - (b69921f) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-21",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.52.0 - 2024-07-24",
    ],
    "content": "
Fix ClientRegistry for Typescript code-gen (#828) - (b69921f) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-21",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0512---2024-07-24",
        "title": "0.51.2 - 2024-07-24",
      },
    ],
    "description": "
Add support for unions / maps / null in TypeBuilder. (#820) - (8d9e92d) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-16",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.51.2 - 2024-07-24",
    ],
    "content": "
Add support for unions / maps / null in TypeBuilder. (#820) - (8d9e92d) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-16",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0512---2024-07-24",
        "title": "0.51.2 - 2024-07-24",
      },
    ],
    "description": "
[Playground] Add a feedback button (#818) - (f749f2b) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-22",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.51.2 - 2024-07-24",
    ],
    "content": "
[Playground] Add a feedback button (#818) - (f749f2b) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-22",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0512---2024-07-24",
        "title": "0.51.2 - 2024-07-24",
      },
    ],
    "description": "
Improvements across docs (#807) - (bc0c176) - Anish Palakurthi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-11",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.51.2 - 2024-07-24",
    ],
    "content": "
Improvements across docs (#807) - (bc0c176) - Anish Palakurthi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-11",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0511---2024-07-21",
        "title": "0.51.1 - 2024-07-21",
      },
    ],
    "description": "
Add a feedback button to VSCode Extension (#811) - (f371912) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-17",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.51.1 - 2024-07-21",
    ],
    "content": "
Add a feedback button to VSCode Extension (#811) - (f371912) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-17",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0511---2024-07-21",
        "title": "0.51.1 - 2024-07-21",
      },
    ],
    "description": "
Allow default_client_mode in the generator #813 (#815) - (6df7fca) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug",
    "title": "Bug",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.51.1 - 2024-07-21",
    ],
    "content": "
Allow default_client_mode in the generator #813 (#815) - (6df7fca) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug",
    "title": "Bug",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0510---2024-07-19",
        "title": "0.51.0 - 2024-07-19",
      },
    ],
    "description": "
Improve BAML Parser for numbers and single-key objects (#785) - (c5af7b0) - hellovai
Add docs for VLLM (#792) - (79e8773) - hellovai
LLVM install and rebuild script (#794) - (9ee66ed) - Anish Palakurthi
Prevent version mismatches when generating baml_client (#791) - (d793603) - aaronvg
fiddle build fix (#800) - (d304203) - aaronvg
Dont drop extra fields in dynamic classes when passing them as inputs to a function (#802) - (4264c9b) - aaronvg
Adding support for a sync client for Python + Typescript (#803) - (62085e7) - hellovai
Fix WASM-related issues introduced in #803 (#804) - (0a950e0) - hellovai
Adding various fixes (#806) - (e8c1a61) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-23",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.51.0 - 2024-07-19",
    ],
    "content": "
Improve BAML Parser for numbers and single-key objects (#785) - (c5af7b0) - hellovai
Add docs for VLLM (#792) - (79e8773) - hellovai
LLVM install and rebuild script (#794) - (9ee66ed) - Anish Palakurthi
Prevent version mismatches when generating baml_client (#791) - (d793603) - aaronvg
fiddle build fix (#800) - (d304203) - aaronvg
Dont drop extra fields in dynamic classes when passing them as inputs to a function (#802) - (4264c9b) - aaronvg
Adding support for a sync client for Python + Typescript (#803) - (62085e7) - hellovai
Fix WASM-related issues introduced in #803 (#804) - (0a950e0) - hellovai
Adding various fixes (#806) - (e8c1a61) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-23",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0510---2024-07-19",
        "title": "0.51.0 - 2024-07-19",
      },
    ],
    "description": "
implement maps in BAML (#797) - (97d7e62) - Samuel Lijin
Support Vertex AI (Google Cloud SDK) (#790) - (d98ee81) - Anish Palakurthi
Add copy buttons to test results in playground (#799) - (b5eee3d) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-18",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.51.0 - 2024-07-19",
    ],
    "content": "
implement maps in BAML (#797) - (97d7e62) - Samuel Lijin
Support Vertex AI (Google Cloud SDK) (#790) - (d98ee81) - Anish Palakurthi
Add copy buttons to test results in playground (#799) - (b5eee3d) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-18",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0510---2024-07-19",
        "title": "0.51.0 - 2024-07-19",
      },
    ],
    "description": "
in fern config, defer to installed version (#789) - (479f1b2) - fern
publish docs on every push to the default branch (#796) - (180824a) - Samuel Lijin
🌿 introducing fern docs (#779) - (46f06a9) - fern
Add test for dynamic list input (#798) - (7528d6a) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-3",
    "title": "Miscellaneous Chores",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.51.0 - 2024-07-19",
    ],
    "content": "
in fern config, defer to installed version (#789) - (479f1b2) - fern
publish docs on every push to the default branch (#796) - (180824a) - Samuel Lijin
🌿 introducing fern docs (#779) - (46f06a9) - fern
Add test for dynamic list input (#798) - (7528d6a) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-3",
    "title": "Miscellaneous Chores",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0500---2024-07-11",
        "title": "0.50.0 - 2024-07-11",
      },
    ],
    "description": "
[Playground] Environment variable button is now visible on all themes (#762) - (adc4da1) - aaronvg
[Playground] Fix to cURL rendering and mime_type overriding (#763) - (67f9c6a) - Anish Palakurthi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-24",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.50.0 - 2024-07-11",
    ],
    "content": "
[Playground] Environment variable button is now visible on all themes (#762) - (adc4da1) - aaronvg
[Playground] Fix to cURL rendering and mime_type overriding (#763) - (67f9c6a) - Anish Palakurthi
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-24",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0500---2024-07-11",
        "title": "0.50.0 - 2024-07-11",
      },
    ],
    "description": "
[Runtime] Add support for clients that change at runtime using ClientRegistry (#683) - (c0fb454) - hellovai
https://docs.boundaryml.com/docs/calling-baml/client-registry
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-19",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.50.0 - 2024-07-11",
    ],
    "content": "
[Runtime] Add support for clients that change at runtime using ClientRegistry (#683) - (c0fb454) - hellovai
https://docs.boundaryml.com/docs/calling-baml/client-registry
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-19",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0500---2024-07-11",
        "title": "0.50.0 - 2024-07-11",
      },
    ],
    "description": "
Add more documentation for TypeBuilder (#767) - (85dc8ab) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-12",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.50.0 - 2024-07-11",
    ],
    "content": "
Add more documentation for TypeBuilder (#767) - (85dc8ab) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-12",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0490---2024-07-08",
        "title": "0.49.0 - 2024-07-08",
      },
    ],
    "description": "
Fixed Azure / Ollama clients. Removing stream_options from azure and ollama clients (#760) - (30bf88f) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-25",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.49.0 - 2024-07-08",
    ],
    "content": "
Fixed Azure / Ollama clients. Removing stream_options from azure and ollama clients (#760) - (30bf88f) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-25",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0490---2024-07-08",
        "title": "0.49.0 - 2024-07-08",
      },
    ],
    "description": "
Add support for arm64-linux (#751) - (adb8ee3) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-20",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.49.0 - 2024-07-08",
    ],
    "content": "
Add support for arm64-linux (#751) - (adb8ee3) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-20",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0480---2024-07-04",
        "title": "0.48.0 - 2024-07-04",
      },
    ],
    "description": "
Fix env variables dialoge on VSCode (#750)
Playground selects correct function after loading (#757) - (09963a0) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-26",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.48.0 - 2024-07-04",
    ],
    "content": "
Fix env variables dialoge on VSCode (#750)
Playground selects correct function after loading (#757) - (09963a0) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-26",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0480---2024-07-04",
        "title": "0.48.0 - 2024-07-04",
      },
    ],
    "description": "
Better error messages on logging failures to Boundary Studio (#754) - (49c768f) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-4",
    "title": "Miscellaneous Chores",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.48.0 - 2024-07-04",
    ],
    "content": "
Better error messages on logging failures to Boundary Studio (#754) - (49c768f) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-4",
    "title": "Miscellaneous Chores",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0470---2024-07-03",
        "title": "0.47.0 - 2024-07-03",
      },
    ],
    "description": "
make settings dialog work in vscode again (#750) (c94e355) - aaronvg
restore releases on arm64-linux (#751) - (adb8ee3) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-27",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.47.0 - 2024-07-03",
    ],
    "content": "
make settings dialog work in vscode again (#750) (c94e355) - aaronvg
restore releases on arm64-linux (#751) - (adb8ee3) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-27",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0460---2024-07-03",
        "title": "0.46.0 - 2024-07-03",
      },
    ],
    "description": "
Fixed tracing issues for Boundary Studio (#740) - (77a4db7) - Samuel Lijin
Fixed flush() to be more reliable (#744) - (9dd5fda) - Samuel Lijin
Remove error when user passes in extra fields in a class (#746) - (2755b43) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-28",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.46.0 - 2024-07-03",
    ],
    "content": "
Fixed tracing issues for Boundary Studio (#740) - (77a4db7) - Samuel Lijin
Fixed flush() to be more reliable (#744) - (9dd5fda) - Samuel Lijin
Remove error when user passes in extra fields in a class (#746) - (2755b43) - aaronvg
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-28",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0460---2024-07-03",
        "title": "0.46.0 - 2024-07-03",
      },
    ],
    "description": "
Add support for base_url for the google-ai provider (#747) - (005b1d9) - hellovai
Playground UX improvements (#742) - (5cb56fd) - hellovai
Prompt Fiddle now auto-switches functions when to change files (#745)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-21",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.46.0 - 2024-07-03",
    ],
    "content": "
Add support for base_url for the google-ai provider (#747) - (005b1d9) - hellovai
Playground UX improvements (#742) - (5cb56fd) - hellovai
Prompt Fiddle now auto-switches functions when to change files (#745)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-21",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0460---2024-07-03",
        "title": "0.46.0 - 2024-07-03",
      },
    ],
    "description": "
Added a large example project on promptfiddle.com (#741) - (f80da1e) - aaronvg
Mark ruby as in beta (#743) - (901109d) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-13",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.46.0 - 2024-07-03",
    ],
    "content": "
Added a large example project on promptfiddle.com (#741) - (f80da1e) - aaronvg
Mark ruby as in beta (#743) - (901109d) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-13",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0450---2024-06-29",
        "title": "0.45.0 - 2024-06-29",
      },
    ],
    "description": "
Fixed streaming in Python Client which didn't show result until later (#726) - (e4f2daa) - Anish Palakurthi
Improve playground stability on first load (#732) - (2ac7b32) - Anish Palakurthi
Add improved static analysis for jinja (#734) - (423faa1) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-29",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.45.0 - 2024-06-29",
    ],
    "content": "
Fixed streaming in Python Client which didn't show result until later (#726) - (e4f2daa) - Anish Palakurthi
Improve playground stability on first load (#732) - (2ac7b32) - Anish Palakurthi
Add improved static analysis for jinja (#734) - (423faa1) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-29",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0450---2024-06-29",
        "title": "0.45.0 - 2024-06-29",
      },
    ],
    "description": "
Docs for Dynamic Types (#722) https://docs.boundaryml.com/docs/calling-baml/dynamic-types
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-14",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.45.0 - 2024-06-29",
    ],
    "content": "
Docs for Dynamic Types (#722) https://docs.boundaryml.com/docs/calling-baml/dynamic-types
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-14",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0450---2024-06-29",
        "title": "0.45.0 - 2024-06-29",
      },
    ],
    "description": "
Show raw cURL request in Playground (#723) - (57928e1) - Anish Palakurthi
Support bedrock as a provider (#725) - (c64c665) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-22",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.45.0 - 2024-06-29",
    ],
    "content": "
Show raw cURL request in Playground (#723) - (57928e1) - Anish Palakurthi
Support bedrock as a provider (#725) - (c64c665) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-22",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0440---2024-06-26",
        "title": "0.44.0 - 2024-06-26",
      },
    ],
    "description": "
Fix typebuilder for random enums (#721)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-30",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.44.0 - 2024-06-26",
    ],
    "content": "
Fix typebuilder for random enums (#721)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-30",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0430---2024-06-26",
        "title": "0.43.0 - 2024-06-26",
      },
    ],
    "description": "
fix pnpm lockfile issue (#720)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-31",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.43.0 - 2024-06-26",
    ],
    "content": "
fix pnpm lockfile issue (#720)
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-31",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0420---2024-06-26",
        "title": "0.42.0 - 2024-06-26",
      },
    ],
    "description": "
correctly propagate LICENSE to baml-py (#695) - (3fda880) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-32",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.42.0 - 2024-06-26",
    ],
    "content": "
correctly propagate LICENSE to baml-py (#695) - (3fda880) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-32",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0420---2024-06-26",
        "title": "0.42.0 - 2024-06-26",
      },
    ],
    "description": "
update jsonish readme (#685) - (b19f04a) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-5",
    "title": "Miscellaneous Chores",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.42.0 - 2024-06-26",
    ],
    "content": "
update jsonish readme (#685) - (b19f04a) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-5",
    "title": "Miscellaneous Chores",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0420---2024-06-26",
        "title": "0.42.0 - 2024-06-26",
      },
    ],
    "description": "
add link to tracing, show token counts (#703) - (64aa18a) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#vscode",
    "title": "Vscode",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "0.42.0 - 2024-06-26",
    ],
    "content": "
add link to tracing, show token counts (#703) - (64aa18a) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#vscode",
    "title": "Vscode",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
rollback git lfs, images broken in docs rn (#534) - (6945506) - Samuel Lijin
search for markdown blocks correctly (#641) - (6b8abf1) - Samuel Lijin
restore one-workspace-per-folder (#656) - (a464bde) - Samuel Lijin
ruby generator should be ruby/sorbet (#661) - (0019f39) - Samuel Lijin
ruby compile error snuck in (#663) - (0cb2583) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-33",
    "title": "Bug Fixes",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
rollback git lfs, images broken in docs rn (#534) - (6945506) - Samuel Lijin
search for markdown blocks correctly (#641) - (6b8abf1) - Samuel Lijin
restore one-workspace-per-folder (#656) - (a464bde) - Samuel Lijin
ruby generator should be ruby/sorbet (#661) - (0019f39) - Samuel Lijin
ruby compile error snuck in (#663) - (0cb2583) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#bug-fixes-33",
    "title": "Bug Fixes",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
add typescript examples (#477) - (532481c) - Samuel Lijin
add titles to code blocks for all CodeGroup elems (#483) - (76c6b68) - Samuel Lijin
add docs for round-robin clients (#500) - (221f902) - Samuel Lijin
add ruby example (#689) - (16e187f) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-15",
    "title": "Documentation",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
add typescript examples (#477) - (532481c) - Samuel Lijin
add titles to code blocks for all CodeGroup elems (#483) - (76c6b68) - Samuel Lijin
add docs for round-robin clients (#500) - (221f902) - Samuel Lijin
add ruby example (#689) - (16e187f) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#documentation-15",
    "title": "Documentation",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
implement baml version --check --output json (#444) - (5f076ac) - Samuel Lijin
show update prompts in vscode (#451) - (b66da3e) - Samuel Lijin
add tests to check that baml version --check works (#454) - (be1499d) - Samuel Lijin
parse typescript versions in version --check (#473) - (b4b2250) - Samuel Lijin
implement round robin client strategies (#494) - (599fcdd) - Samuel Lijin
add integ-tests support to build (#542) - (f59cf2e) - Samuel Lijin
make ruby work again (#650) - (6472bec) - Samuel Lijin
Add RB2B tracking script (#682) - (54547a3) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-23",
    "title": "Features",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
implement baml version --check --output json (#444) - (5f076ac) - Samuel Lijin
show update prompts in vscode (#451) - (b66da3e) - Samuel Lijin
add tests to check that baml version --check works (#454) - (be1499d) - Samuel Lijin
parse typescript versions in version --check (#473) - (b4b2250) - Samuel Lijin
implement round robin client strategies (#494) - (599fcdd) - Samuel Lijin
add integ-tests support to build (#542) - (f59cf2e) - Samuel Lijin
make ruby work again (#650) - (6472bec) - Samuel Lijin
Add RB2B tracking script (#682) - (54547a3) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#features-23",
    "title": "Features",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
add nodemon config to typescript/ (#435) - (231b396) - Samuel Lijin
finish gloo to BoundaryML renames (#452) - (88a7fda) - Samuel Lijin
set up lfs (#511) - (3a43143) - Samuel Lijin
add internal build tooling for sam (#512) - (9ebacca) - Samuel Lijin
delete clients dir, this is now dead code (#652) - (ec2627f) - Samuel Lijin
consolidate vscode workspace, bump a bunch of deps (#654) - (82bf6ab) - Samuel Lijin
Add RB2B tracking script to propmt fiddle (#681) - (4cf806b) - hellovai
Adding better release script (#688) - (5bec282) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-6",
    "title": "Miscellaneous Chores",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
add nodemon config to typescript/ (#435) - (231b396) - Samuel Lijin
finish gloo to BoundaryML renames (#452) - (88a7fda) - Samuel Lijin
set up lfs (#511) - (3a43143) - Samuel Lijin
add internal build tooling for sam (#512) - (9ebacca) - Samuel Lijin
delete clients dir, this is now dead code (#652) - (ec2627f) - Samuel Lijin
consolidate vscode workspace, bump a bunch of deps (#654) - (82bf6ab) - Samuel Lijin
Add RB2B tracking script to propmt fiddle (#681) - (4cf806b) - hellovai
Adding better release script (#688) - (5bec282) - hellovai
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#miscellaneous-chores-6",
    "title": "Miscellaneous Chores",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
patch] Version bump for nightly release [NIGHTLY:cli] [NIGHTLY:vscode_ext] [NIGHTLY:client-python] - (d05a22c) - GitHub Action
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#auto",
    "title": "[AUTO",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
patch] Version bump for nightly release [NIGHTLY:cli] [NIGHTLY:vscode_ext] [NIGHTLY:client-python] - (d05a22c) - GitHub Action
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#auto",
    "title": "[AUTO",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
fix baml-core-ffi script (#521) - (b1b7f4a) - Samuel Lijin
fix engine/ (#522) - (154f646) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#build",
    "title": "Build",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
fix baml-core-ffi script (#521) - (b1b7f4a) - Samuel Lijin
fix engine/ (#522) - (154f646) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#build",
    "title": "Build",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
add ruby test - (c0bc101) - Sam Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#integ-tests",
    "title": "Integ-tests",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
add ruby test - (c0bc101) - Sam Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#integ-tests",
    "title": "Integ-tests",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
add function calling, collapse the table (#505) - (2f9024c) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#readme",
    "title": "Readme",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
add function calling, collapse the table (#505) - (2f9024c) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#readme",
    "title": "Readme",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
bump versions for everything (#662) - (c0254ae) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#release",
    "title": "Release",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
bump versions for everything (#662) - (c0254ae) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#release",
    "title": "Release",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      {
        "slug": "changelog/changelog#0410---2024-06-20",
        "title": "[0.41.0] - 2024-06-20",
      },
    ],
    "description": "
check for updates on the hour (#434) - (c70a3b3) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#vscode-1",
    "title": "Vscode",
    "type": "page-v4",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "[0.41.0] - 2024-06-20",
    ],
    "content": "
check for updates on the hour (#434) - (c70a3b3) - Samuel Lijin
",
    "indexSegmentId": "0",
    "slug": "changelog/changelog#vscode-1",
    "title": "Vscode",
    "type": "page-v3",
    "version": undefined,
  },
  {
    "breadcrumbs": [
      "Boundary Extraction API",
      "API Reference",
    ],
    "content": "Upload one or more files along with a prompt to extract data. The API processes the files based on the prompt and returns the extracted information.

A PDF may generate an array of many extracted JSON blobs, 1 per page for example.
## Path Parameters

- /extract
## Request

### Body

- prompt=string? Instruction for data extraction. Like "focus on the colors of the images in this document" or "only focus on extracting addresses"
## Response

Successful Response

### Body

type_:ExtractResponse: extractions=List<type_:Extraction: unknown > 
usage=type_:Usage: consumed_chars=unknown Number of characters processed.
produced_chars=unknown Number of characters produced.
consumed_megapixels=unknown Number of megapixels processed. Usage statistics for the request. A request goes through the BoundaryML pipeline, where documents can be converted into images. In the process, the number of characters consumed, produced, and the number of megapixels consumed are tracked. 
request_id=string Unique identifier for the request. : Successful Response",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data",
    "title": "Extract",
    "type": "endpoint-v3",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response",
    "title": "Response",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
    ],
    "description": "Unique identifier for the request.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.request_id",
    "title": "request_id",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
    ],
    "description": "Usage statistics for the request. A request goes through the BoundaryML pipeline, where documents can be converted into images. In the process, the number of characters consumed, produced, and the number of megapixels consumed are tracked.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.usage",
    "title": "usage",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.usage",
        "title": "usage",
      },
    ],
    "description": "Number of megapixels processed.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.usage.consumed_megapixels",
    "title": "consumed_megapixels",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.usage",
        "title": "usage",
      },
    ],
    "description": "Number of characters produced.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.usage.produced_chars",
    "title": "produced_chars",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.usage",
        "title": "usage",
      },
    ],
    "description": "Number of characters processed.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.usage.consumed_chars",
    "title": "consumed_chars",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.extractions",
    "title": "extractions",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.extractions",
    "title": "extractions",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.extractions",
        "title": "extractions",
      },
    ],
    "description": "Extracted data from the file, in JSON format.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.extractions.output",
    "title": "output",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.extractions",
        "title": "extractions",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.body.extractions.source",
    "title": "source",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
    ],
    "description": "Invalid Request Parameters",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.BadRequest",
    "title": "Bad Request",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.BadRequest",
    "title": "Bad Request",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.BadRequest",
        "title": "Bad Request",
      },
    ],
    "description": "Error message detailing the issue.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.BadRequest.error",
    "title": "error",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
    ],
    "description": "Unsupported Media Type",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnsupportedMediaType",
    "title": "Unsupported Media Type",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnsupportedMediaType",
    "title": "Unsupported Media Type",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnsupportedMediaType",
        "title": "Unsupported Media Type",
      },
    ],
    "description": "Error message detailing the issue.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnsupportedMediaType.error",
    "title": "error",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
    ],
    "description": "Validation Error",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnprocessableEntity",
    "title": "Unprocessable Entity",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnprocessableEntity",
    "title": "Unprocessable Entity",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnprocessableEntity",
        "title": "Unprocessable Entity",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnprocessableEntity.detail",
    "title": "detail",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnprocessableEntity",
        "title": "Unprocessable Entity",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.UnprocessableEntity.detail",
    "title": "detail",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
    ],
    "description": "Internal Server Error",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.InternalServerError",
    "title": "Internal Server Error",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
    ],
    "description": undefined,
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.InternalServerError",
    "title": "Internal Server Error",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
  {
    "availability": undefined,
    "breadcrumbs": [
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "Boundary Extraction API",
      },
      {
        "slug": "ref/boundary-extraction-api/extract",
        "title": "API Reference",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data",
        "title": "Extract",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response",
        "title": "Response",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error",
        "title": "Errors",
      },
      {
        "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.InternalServerError",
        "title": "Internal Server Error",
      },
    ],
    "description": "Error message detailing the issue.",
    "endpointPath": [
      {
        "type": "literal",
        "value": "/extract",
      },
    ],
    "extends": undefined,
    "indexSegmentId": "0",
    "isResponseStream": false,
    "method": "POST",
    "slug": "ref/boundary-extraction-api/extract/extract-data#response.error.InternalServerError.error",
    "title": "error",
    "type": "endpoint-field-v1",
    "version": undefined,
  },
]