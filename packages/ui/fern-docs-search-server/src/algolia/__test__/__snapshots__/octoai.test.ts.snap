[
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/quickstart",
    "content": "Welcome to OctoAI! Our mission is to enable users to harness value from the latest AI innovations by delievering efficient, reliable, and customizable AI systems for your apps. Run your models or checkpoints on our cost-effective API endpoints, or run our optimized GenAI stack in your environment.",
    "description": "Start using our GenAI Solutions in one minute.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.quickstart-root-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/quickstart",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Quickstart",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/quickstart",
    "code_snippets": [
      {
        "code": "curl -X POST "https://text.octoai.run/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "Hello world"
            }
        ],
        "model": "mixtral-8x7b-instruct",
        "max_tokens": 512,
        "presence_penalty": 0,
        "temperature": 0.1,
        "top_p": 0.9
    }'",
        "lang": "bash",
        "meta": "cURL",
      },
    ],
    "content": "Sign up for an account - new users get $10 of free credits

Run your first inference:


Navigate to a model page and click Get API Token:


Copy the code sample to run an inference:",
    "domain": "test.com",
    "hash": "#get-started-with-inference",
    "hierarchy": {
      "h0": {
        "title": "Quickstart",
      },
      "h2": {
        "id": "get-started-with-inference",
        "title": "Get started with inference",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.quickstart-get-started-with-inference-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/quickstart",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Get started with inference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/quickstart",
    "content": "Check out the wide variety of text generation models and media generation models models we support.

Learn more about our Text Gen Solution, Media Gen Solution, or OctoStack.

Explore our demos to see OctoAI in action.",
    "domain": "test.com",
    "hash": "#next-steps",
    "hierarchy": {
      "h0": {
        "title": "Quickstart",
      },
      "h2": {
        "id": "next-steps",
        "title": "Next steps",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.quickstart-next-steps-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/quickstart",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Next steps",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/quickstart",
    "content": "Pricing & billing

Support",
    "domain": "test.com",
    "hash": "#additional-resources",
    "hierarchy": {
      "h0": {
        "title": "Quickstart",
      },
      "h2": {
        "id": "additional-resources",
        "title": "Additional Resources",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.quickstart-additional-resources-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/quickstart",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Additional Resources",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "At OctoAI you only pay for what you use. Upon sign up you will receive $10 of free credit in your account, and these credits don't expire. That is equivalent of:
Over a million words of output with the largest Llama 3 70B model and Mixtral 8x7B model.

1,000 SDXL default images and about 66 Stable Video Diffusion animations",
    "description": "Only pay for what you use.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-root-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Pricing & billing",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "OctoAI uses post-paid billing - add a credit card and pay for your use at the end of each month. All existing credits will remain available within your account and will be used before any post-paid billing is applied.
On the 1st day of each month, we’ll send an invoice so you can see the upcoming charge. On the 7th day of each month, we’ll charge the card on file for the prior billing period. If there’s an issue charging your credit card, you can manually pay via the invoice.",
    "domain": "test.com",
    "hash": "#how-does-billing-work",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing",
      },
      "h2": {
        "id": "how-does-billing-work",
        "title": "How does billing work?",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-how-does-billing-work-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "How does billing work?",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "You can view your plan tier, invoices, and itemized usage for all OctoAI services in Billing & Usage in your account at anytime.",
    "domain": "test.com",
    "hash": "#where-can-i-find-my-billing-data",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing",
      },
      "h2": {
        "id": "how-does-billing-work",
        "title": "How does billing work?",
      },
      "h3": {
        "id": "where-can-i-find-my-billing-data",
        "title": "Where can I find my billing data?",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-where-can-i-find-my-billing-data-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Where can I find my billing data?",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "See rate limits for details, and feel free to contact us to discuss higher limits to meet your needs. You will recieve an HTTP 429 response code if you reach the limit cap.",
    "domain": "test.com",
    "hash": "#what-are-the-rate-limits-for-each-solution",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing",
      },
      "h2": {
        "id": "how-does-billing-work",
        "title": "How does billing work?",
      },
      "h3": {
        "id": "what-are-the-rate-limits-for-each-solution",
        "title": "What are the rate limits for each solution?",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-what-are-the-rate-limits-for-each-solution-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "What are the rate limits for each solution?",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "Below is a full feature breakdown of the Media Gen Solution tiers.
 Trial Pro Enterprise 
SDXL and SD 1.5 text2img, SVD image animation, img2img, Inpainting, ControlNet Cost-optimized Cost-optimized Option for Cost-optimized or Latency-optimized 
Custom Assets (checkpoints, loras, inversions, VAEs) ❌ ✅ ✅ 
Upscaler ✅ ✅ ✅ 
Option for SLA guarantees ❌ ❌ ✅ 
Option for Private Deployment (at higher price) ❌ ❌ ✅ 
Dedicated Customer Success Manager ❌ ❌ ✅",
    "domain": "test.com",
    "hash": "#media-gen-solution",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing",
      },
      "h2": {
        "id": "media-gen-solution",
        "title": "Media Gen Solution",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-media-gen-solution-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Media Gen Solution",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "Pricing for default image features and configurations are below:
Feature Type Steps Resolution Sampler Price 
SVD 1.1 25 all supported N/A $0.15/animation 
SDXL 30 1024x1024 DDIM (and any not listed below as premium) $0.004/image 
SDXL with Custom Asset (Fine-tuned) 30 1024x1024 DDIM (and any not listed below as premium) $0.008/image 
SDXL Lightning base 4 1024x1024 DDIM (and any not listed below as premium) $0.001/image 
SDXL Lightning Custom Asset (Fine-tuned) 4 1024x1024 DDIM (and any not listed below as premium) $0.005/image 
SDXL Fine-tuning 500 N/A N/A $0.25/tune 
SD 1.5 with Base or Custom Asset (Fine-tuned) 30 512x512 DDIM (and any not listed below as premium) $0.0015/image 
SD1.5 Fine-tuning 500 N/A N/A $0.1/tune 
Asset library (storage) N/A N/A N/A $0.05/GB stored per month, after the first 50GB 
Upscaling N/A N/A N/A $0.004/request 
Background Removal N/A N/A N/A $0.002/request 
Photo Merge 30 1024x1024 N/A $0.015/image 
Adetailer N/A N/A N/A $0.0004/object 

The price for each feature type changes as listed below for non-default configurations:
Configuration Type Price Formula 
Image Animation Steps Default price * (step_count/25) 
Image Generation Steps Default price * (step_count/30) 
SDXL Resolutions Default price *(pixel_count/(1024*1024)) 
SD1.5 Resolutions Default price * (pixel_count/(512*512)) 
Premium Samplers: DPM_2, DPM_2_ANCESTRAL, DPM_PLUS_PLUS_SDE_KARRAS, HEUN, KLMS Default price *2 
Fine-tuning Steps Default price * (step_count/500) 

Here are a few examples to illustrate how this works to assist you in applying to your own use case:
Feature Type Steps Resolution Sampler Price 
SDXL 40 1024x1024 DDIM (default) $.0053 
SDXL 40 1024x1024 DPM_2_ANCESTRAL (premium) $.0107 
SDXL Lightning base 4 1024x1024 DDIM (default) $.001 
SDXL Lightning with Custom Asset 4 1024x1024 DDIM (default) $.005 
SDXL with Custom Asset (Fine-tuned) 60 1024x1024 DDIM (default) $.016 
SDXL with Custom Asset (Fine-tuned) 60 1024x1024 DPM_2 (premium) $.032 
SDXL Fine-tuning 1000 N/A N/A $.5 
SD 1.5 40 512x512 DDIM (default) $.002 
SD1.5 60 1024x1024 DDIM (default) $.003 
SD1.5 40 1024x1024 DPM_2 (premium) $.009",
    "domain": "test.com",
    "hash": "#pro-pricing-for-media-gen-solution",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing",
      },
      "h2": {
        "id": "media-gen-solution",
        "title": "Media Gen Solution",
      },
      "h4": {
        "id": "pro-pricing-for-media-gen-solution",
        "title": "Pro pricing for Media Gen Solution",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-pro-pricing-for-media-gen-solution-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Pro pricing for Media Gen Solution",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "We offer simple, competitive token-based pricing for text gen endpoints, with prices varying depending on parameter size and quantization level:
Model Sizes Per M Tokens 
Mixtral-8x7B models $0.45 
Mixtral-8x22B models $1.20 
7B and 8B models $0.15 
13B models $0.20 
32B models $0.75 
34B models $0.75 
70B models $0.90 
GTE-large $0.05 

If you would like to explore pricing for other models, quantization levels, or specific fine tunes, contact us.",
    "domain": "test.com",
    "hash": "#text-gen-solution",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing",
      },
      "h2": {
        "id": "text-gen-solution",
        "title": "Text Gen Solution",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-text-gen-solution-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text Gen Solution",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "Trial Pro Enterprise 
Deploy endpoint from any container (private or public registry) ✅ ✅ ✅ 
Example models from community ✅ ✅ ✅ 
CLI and SDK for containerizing + deploying Python models ✅ ✅ ✅ 
Max endpoints per account 2 10 No limit 
Max replicas per endpoint 3 10 No limit 
Auto-acceleration of PyTorch models ❌ ❌ Early access 
Dedicated Customer Success Manager ❌ ❌ ✅",
    "domain": "test.com",
    "hash": "#compute-service",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing",
      },
      "h2": {
        "id": "compute-service",
        "title": "Compute Service",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-compute-service-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Compute Service",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/pricing-and-billing",
    "content": "Large 80: A100 GPU with 80GB memory @ $0.00145 per second (~$5.20 per hour)

Large 40: A100 GPU with 40GB memory @ $0.00114 per second (~$4.10 per hour)

Medium: A10 GPU with 24GB memory @ $0.00032 per second (~$1.15 per hour)

Small: T4 GPU with 16GB memory @ $0.00011 per second (~$0.40 per hour)


Billing is by the second of compute usage, starting at the time when the endpoint is ready for inferences. The time when the endpoint is ready for inferences is when either the healtcheck on your end point begins returning 200, or if there is no healthcheck, the time you see the “Replica is running” log line in your events tab.
You will be billed for the total inference duration and timeout duration

You will not be billed for the duration of cold start


Example models in the platform have a pre-set hardware / pricing tier. If you create an endpoint from a custom model, you can choose the tier best suited to your needs.",
    "domain": "test.com",
    "hash": "#pro-pricing-for-compute-service",
    "hierarchy": {
      "h0": {
        "title": "Pricing & billing",
      },
      "h2": {
        "id": "compute-service",
        "title": "Compute Service",
      },
      "h4": {
        "id": "pro-pricing-for-compute-service",
        "title": "Pro pricing for Compute Service",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.pricing-billing-pro-pricing-for-compute-service-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/pricing-and-billing",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Pro pricing for Compute Service",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/rest-api",
    "content": "All of our text generation models are accessible via REST API, and we follow the "Chat Completions" standard popularized by OpenAI. Below you can see a simple cURL example and JSON response for our endpoint, along with explnations of all parameters.",
    "description": "All OctoAI text generation models are accessible via REST API. Learn how to implement with easy to follow code examples.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.text-gen-rest-api-root-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/rest-api",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text Gen REST API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/rest-api",
    "code_snippets": [
      {
        "code": "  "llama-2-13b-chat",
  "llama-2-70b-chat",
  "codellama-7b-instruct",
  "codellama-13b-instruct",
  "codellama-34b-instruct",
  "mistral-7b-instruct"
  "mixtral-8x7b-instruct"
  "nous-hermes-2-mixtral-8x7b-dpo"
  "hermes-2-pro-mistral-7b"",
      },
    ],
    "content": "model (string): The model to be used for chat completion. Here is the complete list of presently supported model arguments. For more information regarding these models, see this description.


max_tokens (integer, optional): The maximum number of tokens to generate for the chat completion.

messages (list of objects): A list of chat messages, where each message is an object with properties: role and content. Supported roles are “system”, “assistant”, and “user”.

temperature (float, optional): A value between 0.0 and 2.0 that controls the randomness of the model's output.

top_p (float, optional): A value between 0.0 and 1.0 that controls the probability of the model generating a particular token.

stop (list of strings, optional): A list of strings that the model will stop generating text if it encounters any of them.

frequency_penalty (float, optional): A value between 0.0 and 1.0 that controls how much the model penalizes generating repetitive responses.

presence_penalty (float, optional): A value between 0.0 and 1.0 that controls how much the model penalizes generating responses that contain certain words or phrases.

stream (boolean, optional): Indicates whether the response should be streamed.",
    "domain": "test.com",
    "hash": "#input-parameters",
    "hierarchy": {
      "h0": {
        "title": "Text Gen REST API",
      },
      "h3": {
        "id": "input-parameters",
        "title": "Input Parameters",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.text-gen-rest-api-input-parameters-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/rest-api",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Input Parameters",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/rest-api",
    "code_snippets": [
      {
        "code": "// Starting chunk, note that content is null and finish_reason is also null.
{
  "id":"cmpl-994f6307a891454cb0f57b7027f5f113",
  "created":1700527881,
  "model":"llama-2-13b-chat",
  "choices":
  [
    {
      "index":0,
      "delta":
      {
        "role":"assistant",
        "content":null
      },
      "finish_reason":null
    }
  ]
}
// Ending chunk, note the finish_reason "length" instead of null.
// This means we reached the max tokens allowed in this request.
// The "object" field is "chat.completion.chunk" for the body of responses.
{
  "id":"cmpl-994f6307a891454cb0f57b7027f5f113",
  "object":"chat.completion.chunk",
  "created":1700527881,
  "model":"llama-2-13b-chat",
  "choices":
  [
    {
      "index":0,
      "delta":
      {
        "role":"assistant",
        "content":"",
        "function_call":null
      },
      "finish_reason":"length"
    }
  ]
}",
        "lang": "JSON",
        "meta": "JSON",
      },
      {
        "code": "data: {"id": "cmpl-994f6307a891454cb0f57b7027f5f113", "created": 1700527881, "model": "llama-2-13b-chat", "choices": [{"index": 0, "delta": {"role": "assistant", "content": null}, "finish_reason": null}]}

data: {"id": "cmpl-994f6307a891454cb0f57b7027f5f113", "object": "chat.completion.chunk", "created": 1700527881, "model": "llama-2-13b-chat", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "", "function_call": null}, "finish_reason": null}]}

data: {"id": "cmpl-994f6307a891454cb0f57b7027f5f113", "object": "chat.completion.chunk", "created": 1700527881, "model": "llama-2-13b-chat", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "Hello", "function_call": null}, "finish_reason": null}]}

data: {"id": "cmpl-994f6307a891454cb0f57b7027f5f113", "object": "chat.completion.chunk", "created": 1700527881, "model": "llama-2-13b-chat", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "!", "function_call": null}, "finish_reason": null}]}

data: {"id": "cmpl-994f6307a891454cb0f57b7027f5f113", "object": "chat.completion.chunk", "created": 1700527881, "model": "llama-2-13b-chat", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "", "function_call": null}, "finish_reason": null}]}

data: {"id": "cmpl-994f6307a891454cb0f57b7027f5f113", "object": "chat.completion.chunk", "created": 1700527881, "model": "llama-2-13b-chat", "choices": [{"index": 0, "delta": {"role": "assistant", "content": "", "function_call": null}, "finish_reason": "stop"}]}

data: [DONE]
",
      },
    ],
    "content": "Once parsed to JSON, you will see the content of the streaming response similar to below:
Without parsing, the text stream will start with data: for each chunk. Below is an example. Please note, the final chunk contains simply data: [DONE] as text which can break JSON parsing if not accounted for.",
    "domain": "test.com",
    "hash": "#streaming-response-sample",
    "hierarchy": {
      "h0": {
        "title": "Text Gen REST API",
      },
      "h3": {
        "id": "streaming-response-sample",
        "title": "Streaming Response Sample:",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.text-gen-rest-api-streaming-response-sample-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/rest-api",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Streaming Response Sample:",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/rest-api",
    "content": "Parameters
id (string): A unique identifier for the chat completion.

choices (list of objects):
This is a list of chat completion choices, each represented as an object.

Each object within the choices list contains the following fields:

_ index (integer): The position of the choice in the list of generated completions.

_ message (object):

_ An object representing the content of the chat completion, which includes:

_ role (string): The role associated with the message, typically "assistant" for the generated response.

_ content (string): The actual text content of the chat completion.

_ function_call (object or null): An optional field that may contain information about a function call made within the message. It's usually null in standard responses.

_ delta (object or null): An optional field that can contain additional metadata about the message, typically null.

_ finish_reason (string): The reason why the message generation was stopped, such as reaching the maximum length ("length").



created (integer): The Unix timestamp (in seconds) of when the chat completion was created.

model (string): The model used for the chat completion.

object (string): The object type, which is always chat.completion.

system_fingerprint (object or null): An optional field that may contain system-specific metadata.

usage (object):
Usage statistics for the completion request, detailing token usage in the prompt and completion.",
    "domain": "test.com",
    "hash": "#response-parameters",
    "hierarchy": {
      "h0": {
        "title": "Text Gen REST API",
      },
      "h3": {
        "id": "response-parameters",
        "title": "Response Parameters",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.text-gen-rest-api-response-parameters-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/rest-api",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Response Parameters",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "content": "OctoAI LLMs are available to use through our OpenAI compatible API. Additionally, if you have been building or prototyping using OpenAI's Python SDK you can keep your code as-is and use OctoAI's LLM models.
In this example, we will show you how to change just three lines of code to make your Python application use OctoAI's Open Source models through OpenAI's Python SDK.",
    "description": "If you've been using GPT-3.5 or GPT-4, switching to Octo AI is easy!",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-root-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Migrate from OpenAI to OctoAI in 3 lines of code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "content": "Migrate OpenAI's Python SDK example script to use OctoAI's LLM endpoints.
These are the three modifications necessary to achieve our goal:
Redefine OPENAI_API_KEY your API key environment variable to use your OctoAI key.

Redefine OPENAI_BASE_URL to point to https://text.octoai.run/v1

Change the model name to an Open Source model, for example: llama-2-13b-chat",
    "domain": "test.com",
    "hash": "#what-you-will-build",
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code",
      },
      "h2": {
        "id": "what-you-will-build",
        "title": "What you will build",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-what-you-will-build-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "What you will build",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "content": "We will be using Python and OpenAI's Python SDK.",
    "domain": "test.com",
    "hash": "#requirements",
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code",
      },
      "h2": {
        "id": "requirements",
        "title": "Requirements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-requirements-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "code_snippets": [
      {
        "code": "python3 -m venv .venv
source .venv/bin/activate",
        "lang": "bash",
      },
      {
        "code": "python3 -m pip install openai",
        "lang": "bash",
      },
    ],
    "content": "Set up a Python virtual environment. Read Creating Virtual Environments here.


Install the pip requirements in your local python virtual environment",
    "domain": "test.com",
    "hash": "#instructions",
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code",
      },
      "h2": {
        "id": "instructions",
        "title": "Instructions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-instructions-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Instructions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "code_snippets": [
      {
        "code": "export OCTOAI_TOKEN=<your-token>",
        "lang": "bash",
      },
      {
        "code": "export OPENAI_API_KEY=$OCTOAI_TOKEN
export OPENAI_BASE_URL="https://text.octoai.run/v1"",
        "lang": "bash",
      },
    ],
    "content": "To run this example, there are simple steps to take:
Get an OctoAI API token by following these instructions.

Expose the token in a new OCTOAI_TOKEN environment variable:


Switch the OpenAI token and base URL environment variable


If you prefer, you can also directly paste your token into the client initialization.",
    "domain": "test.com",
    "hash": "#environment-setup",
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code",
      },
      "h2": {
        "id": "instructions",
        "title": "Instructions",
      },
      "h3": {
        "id": "environment-setup",
        "title": "Environment setup",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-environment-setup-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Environment setup",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "code_snippets": [
      {
        "code": "from openai import OpenAI

client = OpenAI()

completion = octoai.text_gen.create_chat_completion(
    # model="gpt-3.5-turbo",
    model="llama-2-13b-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello!"},
    ],
)

print(completion.choices[0].message)",
        "lang": "python",
      },
    ],
    "content": "Once you've completed the steps above, the code below will call OctoAI LLMs:
Note that you need to supply one of OctoAI's supported LLMs as an argument, as in the example above. For a complete list of our supported LLMs, check out our REST API page.",
    "domain": "test.com",
    "hash": "#example-code",
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code",
      },
      "h2": {
        "id": "instructions",
        "title": "Instructions",
      },
      "h3": {
        "id": "example-code",
        "title": "Example code",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-example-code-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Example code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/migration-from-openai",
    "code_snippets": [
      {
        "code": "
ChatCompletionMessage(content="  Hello! How can I assist you today? Do you have any questions or tasks you'd like help with? Please let me know and I'll do my best to assist you.", role='assistant' function_call=None, tool_calls=None)
",
        "lang": "python",
      },
    ],
    "content": "The code above produces the following object:",
    "domain": "test.com",
    "hash": "#example-output",
    "hierarchy": {
      "h0": {
        "title": "Migrate from OpenAI to OctoAI in 3 lines of code",
      },
      "h2": {
        "id": "instructions",
        "title": "Instructions",
      },
      "h3": {
        "id": "example-output",
        "title": "Example output",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.migrate-from-open-ai-to-octo-ai-in-3-lines-of-code-example-output-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/migration-from-openai",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Example output",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/image-gen-api",
    "code_snippets": [
      {
        "code": "curl -H 'Content-Type: application/json' -H "Authorization: Bearer $OCTOAI_TOKEN" -X POST "https://image.octoai.run/generate/sdxl" \
    -d '{
        "prompt": "The angel of death Hyperrealistic, splash art, concept art, mid shot, intricately detailed, color depth, dramatic, 2/3 face angle, side light, colorful background",
        "negative_prompt": "ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, signature, cut off, draft",
        "sampler": "DDIM",
        "cfg_scale": 11,
        "height": 1024,
        "width": 1024,
        "seed": 2748252853,
        "steps": 20,
        "num_images": 1,
        "high_noise_frac": 0.7,
        "strength": 0.92,
        "use_refiner": true,
        "style_preset": "3d-model"
    }' > response.json",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import requests
import os
import base64
import io
import PIL.Image

def _process_test(url):

    OCTOAI_TOKEN = os.environ.get("OCTOAI_TOKEN")

    payload = {
        "prompt": "Face of a yellow cat, high resolution, sitting on a park bench",
        "negative_prompt": "Blurry photo, distortion, low-res, bad quality",
        "steps": 30,
        "width": 1024,
        "height": 1024,
    }

    headers = {
        "Authorization": f"Bearer {OCTOAI_TOKEN}",
        "Content-Type": "application/json",
    }

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code != 200:
        print(response.text)

    img_list = response.json()["images"]

    for i, img_info in enumerate(img_list):
        img_bytes = base64.b64decode(img_info["image_b64"])
        img = PIL.Image.open(io.BytesIO(img_bytes))
        img.load()
        img.save(f"result_image{i}.jpg")

if __name__ == "__main__":
    _process_test("https://image.octoai.run/generate/sdxl")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt: "A photo of a cute cat astronaut in space",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -H 'Content-Type: application/json' -H "Authorization: Bearer $OCTOAI_TOKEN" -X POST "https://image.octoai.run/generate/sdxl" \
    -d '{
        "prompt": "The angel of death Hyperrealistic, splash art, concept art, mid shot, intricately detailed, color depth, dramatic, 2/3 face angle, side light, colorful background",
        "negative_prompt": "ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, blurry, bad anatomy, blurred, watermark, grainy, signature, cut off, draft",
        "sampler": "DDIM",
        "cfg_scale": 11,
        "height": 1024,
        "width": 1024,
        "seed": 2748252853,
        "steps": 20,
        "num_images": 1,
        "high_noise_frac": 0.7,
        "strength": 0.92,
        "use_refiner": true,
        "style_preset": "3d-model"
    }' > response.json",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import requests
import os
import base64
import io
import PIL.Image

def _process_test(url):

    OCTOAI_TOKEN = os.environ.get("OCTOAI_TOKEN")

    payload = {
        "prompt": "Face of a yellow cat, high resolution, sitting on a park bench",
        "negative_prompt": "Blurry photo, distortion, low-res, bad quality",
        "steps": 30,
        "width": 1024,
        "height": 1024,
    }

    headers = {
        "Authorization": f"Bearer {OCTOAI_TOKEN}",
        "Content-Type": "application/json",
    }

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code != 200:
        print(response.text)

    img_list = response.json()["images"]

    for i, img_info in enumerate(img_list):
        img_bytes = base64.b64decode(img_info["image_b64"])
        img = PIL.Image.open(io.BytesIO(img_bytes))
        img.load()
        img.save(f"result_image{i}.jpg")

if __name__ == "__main__":
    _process_test("https://image.octoai.run/generate/sdxl")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt: "A photo of a cute cat astronaut in space",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "All of our image generation models are accessible via REST API. Below you can see a simple cURL/Python SDK and TypeScript SDK example for our image gen endpoints, along with explanations of all parameters.
Our URL for image generations is at https://image.octoai.run/generate/{engine_id}, where engine_id is one of the following:
sdxl: Stable DiffusionXL v1.0

sd: Stable Diffusion v1.5

controlnet-sdxl: ControlNet SDXL

controlnet-sd15: ControlNet SD1.5


This includes text-to-image, image-to-image, controlnets, photo merge, inpainting and outpainting.
Input Sample",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.image-gen-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/image-gen-api",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Image Gen REST API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/image-gen-api",
    "code_snippets": [
      {
        "code": "(1024, 1024),(896, 1152),(832, 1216),(768,
1344),(640, 1536),(1536, 640),(1344, 768),
(1216, 832),(1152, 896)",
      },
      {
        "code": "(512, 512),(640, 512),(768, 512),(512, 704),
(512, 768),(576, 768),(640, 768),(576, 1024),
(1024, 576)",
      },
      {
        "code": "payload = {
    ...
	"checkpoint": "octoai:realcartoon",
    "loras": {
        "octoai:crayon-style": 0.7,
        "your-custom-lora": 0.3
    },
    "textual_inversions": {
        "octoai:NegativeXL": "negativeXL_D",
    },
	"vae": "your_vae_name"
    ...
}",
        "lang": "json",
      },
      {
        "code": "payload = {
    ...
        "prompt": "A trigger_word_1 sitting on a golden throne",
        "negative_prompt": "Blurry photo, distortion, low-res, bad quality",
        "checkpoint": "octoai:RealVisXL",
        "width": 1024,
        "height": 1024,
        "num_images": 2,
        "sampler": "K_EULER_ANCESTRAL",
        "steps": 20,
        "cfg_scale": 7.5,
        "transfer_images": {"trigger_word_1": ["$BASE64_IMAGE_1", "$BASE64_IMAGE_2"]
    ...
}",
        "lang": "json",
      },
      {
        "code": "octoai:canny_sdxl
octoai:depth_sdxl
octoai:openpose_sdxl
octoai:canny_sd15
octoai:depth_sd15
octoai:inpaint_sd15
octoai:ip2p_sd15
octoai:lineart_sd15
octoai:openpose_sd15
octoai:scribble_sd15
octoai:tile_sd15",
      },
      {
        "code": "import base64
import io
import os
import time

import PIL.Image
import requests

import cv2 as cv2

import matplotlib.pyplot as plt  # Import Matplotlib

def _process_test(endpoint_url):
    image_path = "cat.jpeg"
    img = cv2.imread(image_path)
    img = cv2.resize(img, (1024, 1024)) # Resize to a resolution supported by OctoAI SDXL

    edges = cv2.Canny(img,100,200) # 100 and 200 are thresholds for determining canny edges

    height, width = edges.shape

    # Convert Canny edge map to PIL Image
    edges_image = PIL.Image.fromarray(edges)

    # Create a BytesIO buffer to hold the image data
    image_buffer = io.BytesIO()
    edges_image.save(image_buffer, format='JPEG')
    image_bytes = image_buffer.getvalue()
    encoded_image = base64.b64encode(image_bytes).decode('utf-8')

    model_request = {
        "controlnet_image": encoded_image,
        "controlnet": "octoai:canny_sdxl",
        "controlnet_preprocess": false,
        "prompt": (
            "A photo of a cute tiger astronaut in space"
        ),
        "negative_prompt": "low quality, bad quality, sketches, unnatural",
        "steps": 20,
        "num_images": 1,
        "seed": 768072361,
        "height": height,
        "width": width
    }

    prod_token = os.environ.get("OCTOAI_TOKEN")  # noqa

    reply = requests.post(
        f"{endpoint_url}",
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {prod_token}",
        },
        json=model_request,
    )

    if reply.status_code != 200:
        print(reply.text)
        exit(-1)

    img_list = reply.json()["images"]
    print(img_list)

    for i, idict in enumerate(img_list):
        ibytes = idict['image_b64']
        img_bytes = base64.b64decode(ibytes)
        img = PIL.Image.open(io.BytesIO(img_bytes))
        img.load()
        img.save(f"result_image{i}.jpg")

if __name__ == "__main__":
    endpoint = "https://image.octoai.run/generate/controlnet-sdxl"

    # Change this line to call either a10 or a100
    _process_test(endpoint)
",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "prompt: A string describing the image to generate.
We currently have a 77 token limit on prompts for SDXL and 231 for SD 1.5

You can use prompt weighting, e.g. (A tall (beautiful:1.5) woman:1.0) (some other prompt with weight:0.8) . The weight will be the product of all brackets a token is a member of. The brackets, colons and weights do not count towards the number of tokens.



prompt_2: This only applies to SDXL. By default, setting only prompt copies the input to both prompt and prompt_2. When prompt and prompt_2 are both set, they have very different functionality. The second prompt is meant for more human readable descriptions of the desired image.
For example, prompt is used for "word salad" style control of the image. This is the type of prompting you are likely familiar with from SD 1.5. Prompts like the following work well:

prompt = "photorealistic, high definition, masterpiece, sharp lines"  


whereas prompt_2 is meant for more human readable descriptions of the desired image. For example:

prompt_2 = "A portrait of a handsome cat wearing a little hat. The cat is in front of a colorful background.  




negative_prompt Optional: A string indicating a prompt for guidance to steer away from. Unused when not provided.

negative_prompt_2: This only applies to SDXL. This prompt is meant for human readable descriptions of what you don’t want the image, e.g. you would say “Low resolution” in negative_prompt then “Bad hands” in negative_prompt_2.

sampler Optional: A string specifying which scheduler to use when generating an image. Defaults to DDIM. Regular samplers include DDIM,DDPM,DPM_PLUS_PLUS_2M_KARRAS,DPM_SINGLE,DPM_SOLVER_MULTISTEP,K_EULER, K_EULER_ANCESTRAL,PNDM,UNI_PC. Premium samplers (2x price) include DPM_2, DPM_2_ANCESTRAL,DPM_PLUS_PLUS_SDE_KARRAS, HEUN and KLMS.

height Optional: An integer specifying the height of the output image. Defaults to 1024 for SDXL and 512 for SD 1.5.

width Optional: An integer specifying the width of the output image. Defaults to 1024 for SDXL and 512 for SD 1.5.


Supported Output Resolutions (Width x Height) are as follows:
For SDXL:
For SD1.5


init_image and mask_image will be resized to the specified resolution
before applying img2img or inpainting.
cfg_scale Optional: How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt). When not set defaults to 12.

steps Optional: How many steps of diffusion to perform. The higher this is, the higher the image clarity will be but proportionally increases the runtime. Defaults to 30 when not set.

num_images: An integer describing the number of images to generate. Defaults to 1

seed Optional: An integer that fixes the random noise of the model. Using the same seed guarantees the same output image, which can be useful for testing or replication. Use null to select a random seed.

use_refiner: This only applies to SDXL. A boolean true or false determines whether to use the refiner or not

high_noise_frac Optional: This only applies to SDXL. A floating point or integer determining how much noise should be applied using the base model vs. the refiner. A value of 0.8 will apply the base model at 80% and Refiner at 20%. Defaults to 0.8 when not set.

checkpoint: Here you can specify a checkpoint either from the OctoAI asset library or your private asset library. Note that using a custom asset increases generation time.

loras: Here you can specify LoRAs, in name-weight pairs, either from the OctoAI asset library or your private asset library. Note that using a custom asset increases generation time.

textual_inversions: Here you can specify textual inversions and their corresponding trigger words. Note that using a custom asset increases generation time.

vae: Here you can specify variational autoencoders. Note that using a custom asset increases generation time.


Here’s an example of how to mix OctoAI assets (checkpoints, loras, and textual_inversions) in the same API request.


OoctoAI assets require an “octoai:” prefix but your private assets DO NOT.
Asset names need to be unique per account
style_preset Optional: This only applies to SDXL. Used to guide the output image towards a particular style. Defaults to None. Supported values for styles present include base, 3d-model, Abstract,Advertising, Alien, analog-film, anime,Architectural, cinematic, Collage,comic-book, Craft Clay, Cubist,digital-art, Disco,Dreamscape,Dystopian, enhance, Fairy Tale,fantasy-art, Fighting Game, Film Noir, Flat Papercut, Food Photography, Gothic, Graffiti, Grunge, HDR, Horror, Hyperrealism, Impressionist, isometric, Kirigami, line-art,Long Exposure,low-poly,Minimalist,modeling-compound,Monochrome,Nautical,
Neon Noir,neon-punk,origami,Paper Mache, Paper Quilling,Papercut Collage,Papercut Shadow Box,photographic,pixel-art,Pointillism,Pop Art,Psychedelic,Real Estate,Renaissance,Retro Arcade,Retro Game,RPG Fantasy,Game,Silhouette,Space,Stacked Papercut,Stained Glass,Steampunk,Strategy Game,Surrealist,Techwear Fashion,Thick Layered Papercut,tile-texture,Tilt-Shift,
Tribal,Typography,Watercolor,Zentangle

init_image Optional: Only applicable for Img2Img and inpainting use cases i.e. to use an image as a starting point for image generation. Argument takes an image encoded as a string in base64 format.


Use .jpg format to ensure best latency 

strength Optional: Only applicable for img2img use cases. A floating point or integer determines how much noise should be applied. Values that approach 1.0 allow for high variation i.e. ignoring the image entirely, but will also produce images that are not semantically consistent with the input and 0.0 keeps the input image as-is. Defaults to 0.8 when not set.

mask_image Optional: Only applicable for inpainting use cases i.e. to specify which area of the picture to paint onto. Argument takes an image encoded as a string in base64 format.
Use .jpg format to ensure best latency



outpainting Optional: Only applicable for outpainting use cases. Argument takes a boolean value to determine Whether the request requires outpainting or not. If so, special preprocessing is applied for better results. Defaults to false

transfer_images Optional: This is our Photo Merge feature. Applicable for use cases where you wish to transfer the subject in the uploaded image(s) to the output image(s). Argument takes a dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object to transfer.


controlnet Optional: Required if using a controlnet engine. Argument takes in the value of ControlNet to be used during image generation. We offer the following list of public OctoAI controlnet checkpoints in the OctoAI Asset Library.
Other than using the default controlnet checkpoints, you can also upload private ControlNet checkpoints into the OctoAI Asset Library and then use those checkpoints at generation time via the parameter controlnet. For custom controlnet checkpoints, make sure to provide your own ControlNet mask in the controlnet_image parameter

controlnet_conditioning_scale Optional: Only applicable if using Controlnets. Argument determines how strong the effect of the controlnet will be. Defaults to 1

controlnet_early_stop Optional:Only applicable if using Controlnets. If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

controlnet_image Optional: Required if using a controlnet engine. Controlnet image encoded in b64 string for guiding image generation.

controlnet_preprocess Optional:Only applicable if using Controlnets. Argument takes in a boolean value to determine whether or not to apply automatic ControlNet preprocessing. For the privileged set of controlnet checkpoints listed above, we default to helping you autogenerate the corresponding controlnet map/mask that will be fed into the controlnet, but you can override the default by additionally specifying a controlnet_preprocess: false parameter.


Python Example for ControlNet Canny with a custom controlnet map:",
    "domain": "test.com",
    "hash": "#image-generation-arguments",
    "hierarchy": {
      "h0": {
        "title": "Image Gen REST API",
      },
      "h3": {
        "id": "image-generation-arguments",
        "title": "Image Generation Arguments",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.image-gen-image-generation-arguments-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/image-gen-api",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Image Generation Arguments",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "content": "Our video generation model is accessible via REST API. Below, you'll find straightforward examples using cURL/Python SDK and TypeScript SDK for our video generation endpoints, complete with explanations of all parameters.
The endpoint URL for video generation is https://image.octoai.run/generate/svd.
This encompasses image-to-video conversion. Additionally, we offer support for a text-to-video workflow, which involves utilizing the text-to-image API (Image Gen API) followed by the image-to-video API.",
    "description": "Animate and add motion to your images",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.video-gen-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Video Gen REST API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "content": "Parameters:
image (base64 encoded image, required) - Starting point image encoded in base64 string

height (int; optional) - Integer representing the height of video/animation to generate- If not provided, the output height will be inferred from the input 'image', and the closest resolution supported will be chosen.

width (int; optional) - Integer representing the width of video/animation to generate- If not provided, the output width will be inferred from the input 'image', and the closest resolution supported will be chosen.
Supported resolutions are (w,h): (576, 1024), (1024, 576), (768, 768)

cfg_scale (float; optional) - Floating-point number representing how closely to adhere to 'image' description- Must be a positive number no greater than 10.0.

fps (int; optional) - How fast the generated frames should play back.

steps (int; optional) - Integer representing how many steps of diffusion to run- Must be greater than 0 and less than or equal to 50.

motion_scale (float; optional) - A floating point number between 0 and 1 indicating how much motion should be in the generated animation.

noise_aug_strength (float; optional) - How much noise to add to the initial image- higher values encourage creativity.

num_videos (int; optional) - Integer representing how many output videos/animations to generate with a single image and configuration. You can generate upto 16 videos in a single API request. All videos will be generated in sequence within the same configurations but different seed values.

seed (int; optional) - Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific video (or set of videos).",
    "domain": "test.com",
    "hash": "#request-payload",
    "hierarchy": {
      "h0": {
        "title": "Video Gen REST API",
      },
      "h2": {
        "id": "request-payload",
        "title": "Request payload",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.video-gen-request-payload-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Request payload",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "code_snippets": [
      {
        "code": "curl -X POST "https://image.octoai.run/generate/svd" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "image": "<BASE_64_STRING>",
        "steps": 40,
        "cfg_scale": 3,
        "fps": 4,
        "motion_scale": 0.2,
        "noise_aug_strength": 0.55,
        "num_videos": 1,
        "seed": "2138732363"
    }' | jq -r ".videos[0].video" | base64 -d >result.mp4",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    video_gen_response = client.image_gen.generate_svd(
        image="<BASE_64_STRING>",
        steps=25,
        cfg_scale=3,
        fps=7,
        motion_scale=0.5,
        noise_aug_strength=0.02,
        num_videos=1,
    )
    videos = video_gen_response.videos

    for i, image in enumerate(videos):
        to_file(image, f"result{i}.mp4")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { videos } = await octoai.imageGen.generateSvd({
  image: "<BASE64_STRING>",
  steps: 25,
  cfgScale: 3,
  fps: 7,
  motionScale: 0.5,
  noiseAugStrength: 0.02,
  numVideos: 1,
});

videos.forEach((output, i) => {
  if (output.video) {
    const buffer = Buffer.from(output.video, "base64");
    fs.writeFileSync(`result${i}.mp4`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/svd" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "image": "<BASE_64_STRING>",
        "steps": 40,
        "cfg_scale": 3,
        "fps": 4,
        "motion_scale": 0.2,
        "noise_aug_strength": 0.55,
        "num_videos": 1,
        "seed": "2138732363"
    }' | jq -r ".videos[0].video" | base64 -d >result.mp4",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    video_gen_response = client.image_gen.generate_svd(
        image="<BASE_64_STRING>",
        steps=25,
        cfg_scale=3,
        fps=7,
        motion_scale=0.5,
        noise_aug_strength=0.02,
        num_videos=1,
    )
    videos = video_gen_response.videos

    for i, image in enumerate(videos):
        to_file(image, f"result{i}.mp4")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { videos } = await octoai.imageGen.generateSvd({
  image: "<BASE64_STRING>",
  steps: 25,
  cfgScale: 3,
  fps: 7,
  motionScale: 0.5,
  noiseAugStrength: 0.02,
  numVideos: 1,
});

videos.forEach((output, i) => {
  if (output.video) {
    const buffer = Buffer.from(output.video, "base64");
    fs.writeFileSync(`result${i}.mp4`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "videos (list) - List of generation(s) generated by the request.

prediction_time_ms (float) - Total runtime of the video/animations(s) generation(s).",
    "domain": "test.com",
    "hash": "#response",
    "hierarchy": {
      "h0": {
        "title": "Video Gen REST API",
      },
      "h2": {
        "id": "response",
        "title": "Response",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.video-gen-response-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/video-gen",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Response",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "content": "Background removal takes an existing image you provide and removes those parts of the image considered to be “background.”",
    "description": "Background removal takes an existing image you provide and removes those parts of the image considered to be “background.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.background-removal-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Background Removal REST API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "content": "Parameters:
init_image (str, required) - A base64-encoded image whose background should get removed.

alpha_matting (bool; false) - If true, apply matting on the alpha channel.

alpha_matting_foreground_threshold (number in [0, 255]; 240) - When alpha_matting is true, mask pixels larger than this value are considered foreground pixels.

alpha_matting_background_threshold (number in [0, 255]; 10) - When alpha_matting is true, mask pixels smaller than this value are considered background pixels.

alpha_matting_erode_size (number; 10) - When alpha_matting is true, size of the erosion structure to apply, in pixels.

only_mask (bool; false) - When true, return only a single-channel image containing a foreground-background mask. Foreground pixels have values closer to 255, and background pixels have values closer to 0.

post_process_mask (bool; true) - When true, apply morphological operations to the mask to smooth it.

bgcolor (list[int]; optional) - When given, replace background pixels with this color in the output image.",
    "domain": "test.com",
    "hash": "#request-payload",
    "hierarchy": {
      "h0": {
        "title": "Background Removal REST API",
      },
      "h2": {
        "id": "request-payload",
        "title": "Request payload",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.background-removal-request-payload-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Request payload",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "content": "image_b64 (str) - Base64-encoded png containing the processed image.

removed_for_safety (bool) - When true, background removal was not performed because init_image was found to have violated our terms of service.",
    "domain": "test.com",
    "hash": "#response",
    "hierarchy": {
      "h0": {
        "title": "Background Removal REST API",
      },
      "h2": {
        "id": "response",
        "title": "Response",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.background-removal-response-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/background-removal",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Response",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/upscaling",
    "description": "In addition to image generation, OctoAI can also upscale images to higher resolutions.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.upscaling",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/upscaling",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Upscaling REST API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/upscaling",
    "content": "Upscaling takes an existing image you provide and upscales it to a higher resolution.
Parameters:
init_image_url / init_image - The URL to an image or a base64-encoded image (respectively) to upscale. Specify only one of these.

scale / output_image_height / output_image_width - determined how much to upscale the provided image. Specify only one of these.
scale - floating point value indicating how much to scale the input resolution by (e.g., scale: 2.0 would double the input width and height)

output_image_height - height of the desired upscaled image in pixels. The corresponding width will be computed off of this value to preserve the aspect ratio.

output_image_width - width of the desired upscaled image in pixels. The corresponding height will be computed off of this value to preserve the aspect ratio.



model (optional): The model to use for upscaling faces. Default value is real-esrgan-x4-plus. Options:
real-esrgan-x4-plus

real-esrgan-x4-v3

real-esrgan-x4-v3-wdn

real-esrgan-animevideo-v3

real-esrgan-x4-plus-anime

real-esrgan-x2-plus",
    "domain": "test.com",
    "hash": "#api",
    "hierarchy": {
      "h0": {
        "title": "Upscaling REST API",
      },
      "h3": {
        "id": "api",
        "title": "API",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.upscaling-api-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/upscaling",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "content": "Adetailer takes an existing image you provide, detects faces and hands and fixes them.",
    "description": "Adetailer takes an existing image you provide, detects faces and hands and fixes them.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.adetailer-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Adetailer REST API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "content": "Parameters:
init_image (str, required) - A base64-encoded image. Resolution must be supported by inpainting_base_model:
sdxl:  640x1536, 768x1344, 832x1216, 896x1152, 1024x1024, 1152x896, 1216x832, 1344x768, 1536x640, 1664x2432, 2048x2048, 2432x1664
sd15: 384x704, 448x576, 512x512, 512x704, 512x768, 512x832, 576x448, 576x768, 576x768, 576x1024, 640x512, 640x640, 640x768, 704x384, 704x1216, 768x512, 768x576, 768x1024, 832x512, 896x896, 1024x576, 1024x768, 1024x1024, 1024x1536, 1216x704, 1536x1024

init_image_url (string, required if init_image not specified) - If given, download init_image from this URL.

detector (str,required) - Detection model to use. Configures whether e.g. faces or hands or people are targeted for after-detailing.
Available options: face_yolov8n, hand_yolov8n, face_full_mediapipe, face_short_mediapipe, face_mesh_mediapipe, eyes_mesh_mediapipe 

inpainting_base_model (str, required) -
The base model to be used for inpainting. Typically should match the model used to generate init_image.
Available options: sdxl, sd15 

cfg_scale (number,optional default: 7.5)-
Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

checkpoint(string, optional)-  Name of a checkpoint to use for inpainting.

confidence (number,optional, default: 0.3) -
Inpainted areas are determined using a detector. This setting adjusts the sensitivity of the detector (lower considers more image fragments for inpainting).

image_encoding (string,optional)-
Define which encoding process should be applied before returning the modified image.
Available options: jpeg, png 

loras (object | optional) - A dict mapping the name of a LoRA to apply to its weight.

mask_blur (integer, optional,default: 4) - A mask is created for each inpainted area in the image. After dilation (see mask_dilation parameter), the mask is blurred. This technique is typically used to smoothly blend the inpainted area with the original image. This option specifies the radius, in pixels, of the gaussian blur kernel. The higher the value, the wider the blur. Defaults to 4. Must be greater than or equal to 0 and recommended to be less than 64.

mask_dilation (integer,optional,default: 4) - A mask is created for each inpainted area in the image. Mask Dilation allows you to expand the size of the mask while maintaining its shape. This technique is typically used to reduce artifacts near borders in the mask. This parameter is the size, in pixels, of the dilation kernel to apply. Defaults to 4. Must be greater than or equal to 0 and recommended to be less than 64.

mask_padding (integer,optional,default: 32) -\ Each inpainted area is passed to the image-to-image generator with some surrounding context. The contextual area is created by padding the area occupied by the blurred, dilated mask. This technique improves inpainting quality, and the contextual area is not modified. This parameter specifies the amount of padding, in pixels, to apply around the processed mask. When the computed padding goes off the edge of the image, the padded area is slid towards the center of the image. Must be greater than or equal to 0 and recommended to be less than 10% the size of an inpainting mask.

max_num_detections (integer,optional) - Inpaint at most this many objects, starting with the most confident matches.

negative_prompt (string,optional)

prompt (string | optional)

sampler (string | optional) - The schedulers available for image generation.
Available options: PNDM, LMS, KLMS, DDIM, DDPM, HEUN, K_HEUN, K_EULER, K_EULER_ANCESTRAL, DPM_SOLVER_MULTISTEP, DPM_PLUS_PLUS_2M_KARRAS, DPM_SINGLE, DPM_2, DPM_2_ANCESTRAL, DPM_PLUS_PLUS_SDE_KARRAS, UNI_PC,LCM

seed (integer,optional) - Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

steps (integer, optional,default: 20)

strength (number,optional, default 0.9`)

style_preset(string,optional,default: base)

union_masks (boolean, optional,default: false) - When true, create a single mask by unioning the mask for each detected object together, then send a single inpainting request to the backing model.

use_refiner (boolean,optional,default: false)",
    "domain": "test.com",
    "hash": "#request-payload",
    "hierarchy": {
      "h0": {
        "title": "Adetailer REST API",
      },
      "h2": {
        "id": "request-payload",
        "title": "Request payload",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.adetailer-request-payload-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Request payload",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
      {
        "pathname": "/docs/documentation/quickstart/media-gen-rest-ap-is",
        "title": "Media Gen REST APIs",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "content": "image_b64 (string) - The modified image or None if it was removed for safety.

num_objects_detected (integer) - The number of objects that were successfully detected.

num_objects_inpainted (integer) - The number of objects that were successfully inpainted.

num_removed_for_safety (integer) - Number of inpainting requests that violated the OctoAI Terms of Service.",
    "domain": "test.com",
    "hash": "#response",
    "hierarchy": {
      "h0": {
        "title": "Adetailer REST API",
      },
      "h2": {
        "id": "response",
        "title": "Response",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.media-gen-rest-ap-is.adetailer-response-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/rest-apis/adetailer",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Response",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/inference-models",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.inference-models",
    "org_id": "test",
    "pathname": "/docs/getting-started/inference-models",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Inference models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/inference-models",
    "content": "OctoAI currently supports the self-service models & checkpoints organized on this page, and we’ll continue to expand our models and services. Ready to run your first inference? Navigate to our Quickstart guide to get started.",
    "domain": "test.com",
    "hash": "#serverless-endpoints",
    "hierarchy": {
      "h0": {
        "title": "Inference models",
      },
      "h2": {
        "id": "serverless-endpoints",
        "title": "Serverless Endpoints",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.inference-models-serverless-endpoints-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/inference-models",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Serverless Endpoints",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/inference-models",
    "content": "Organization Use Cases Model Name API Model String Context Length 
Meta Chat Llama2-Chat (13B) llama-2-13b-chat 4,096 
Meta Chat Llama2-Chat (70B) llama-2-70b-chat 4,096 
Meta Chat Llama3-Instruct (8B) meta-llama-3-8b-instruct 8,192 
Meta Chat Llama3-Instruct (70B) meta-llama-3-70b-instruct 8,192 
Meta Coding Codellama-Instruct (7B) codellama-7b-instruct 16,384 
Meta Coding Codellama-Instruct (13B) codellama-13b-instruct 16,384 
Meta Coding Codellama-Instruct (34B) codellama-34b-instruct 16,384 
Mistral Chat, Coding Mistral Instruct v0.2 (7B) mistral-7b-instruct 32,768 
Nous Research Chat, Coding Nous Hermes 2 Pro Mistral (7B) hermes-2-pro-mistral-7b 32,768 
Mistral Chat, Coding Mixtral Instruct (8x7B) mixtral-8x7b-instruct 32,768 
Nous Research Content Moderation Nous Hermes 2 Mixtral DPO (8x7B) nous-hermes-2-mixtral-8x7b-dpo 32,768 
Mistral Chat, Coding Mixtral Instruct (8x22B) mixtral-8x22b-instruct 65,536 
Meta Content Moderation Llama Guard llamaguard-7b 4,096 
Alibaba DAMO Embedding GTE Large thenlper/gte-large n/a 

Check out our REST API, Python SDK, or TypeScript SDK docs when you’re ready to use text gen models programmatically.",
    "domain": "test.com",
    "hash": "#text-gen-models",
    "hierarchy": {
      "h0": {
        "title": "Inference models",
      },
      "h2": {
        "id": "serverless-endpoints",
        "title": "Serverless Endpoints",
      },
      "h3": {
        "id": "text-gen-models",
        "title": "Text Gen Models",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.inference-models-text-gen-models-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/inference-models",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text Gen Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/inference-models",
    "content": "Service Model API Model String 
Image Gen Stable Diffusion v1.5 sd 
Image Gen Stable Diffusion XL v1.0 sdxl 
Image Gen Segmind Stable Diffusion ssd 
Image Gen ControlNet SD v1.5 controlnet-sd15 
Image Gen ControlNet SDXL controlnet-sdxl 
Image Animation Stable Video Diffusion v1.1 svd 
Background Removal IS-Net background-removal 
Upscaling REAL-ESRGAN x4 Plus real-esrgan-x4-plus 
Upscaling REAL-ESRGAN x4 v3 real-esrgan-x4-v3 
Upscaling REAL-ESRGAN x4 v3 WDN real-esrgan-x4-v3-wdn 
Upscaling REAL-ESRGAN Anime Video v3 real-esrgan-animevideo-v3 
Upscaling REAL-ESRGAN x4 Plus Anime real-esrgan-x4-plus-anime 
Upscaling REAL-ESRGAN x2 Plus real-esrgan-x2-plus 
Adetailer Face YOLOv8n face_yolov8n 
Adetailer Hand YOLOv8n hand_yolov8n 
Adetailer Face Full MediaPipe face_full_mediapipe 
Adetailer Face Short MediaPipe face_short_mediapipe 
Adetailer Face Mesh MediaPipe face_mesh_mediapipe 
Adetailer Eyes Mesh MediaPipe eyes_mesh_mediapipe 

Check out our Image Gen API and Video Gen API docs when you’re ready to use media gen models programmatically. You can also easily upload and run custom checkpoints and assets using OctoAI’s Asset Library.",
    "domain": "test.com",
    "hash": "#media-gen-models",
    "hierarchy": {
      "h0": {
        "title": "Inference models",
      },
      "h2": {
        "id": "serverless-endpoints",
        "title": "Serverless Endpoints",
      },
      "h3": {
        "id": "media-gen-models",
        "title": "Media Gen Models",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.inference-models-media-gen-models-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/inference-models",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Media Gen Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/quickstart",
        "title": "Quickstart",
      },
    ],
    "canonicalPathname": "/docs/getting-started/how-to-create-an-octoai-access-token",
    "code_snippets": [
      {
        "code": "export OCTOAI_TOKEN=<INSERT_HERE>",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": "curl -X POST "https://text.octoai.run/v1/chat/completions" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "Hello world"
            }
        ],
        "model": "mixtral-8x7b-instruct",
        "max_tokens": 512,
        "presence_penalty": 0,
        "temperature": 0.1,
        "top_p": 0.9
    }'",
        "lang": "bash",
        "meta": "cURL",
      },
    ],
    "content": "All endpoints require authentication by default. That means you will need an access token in order to run inferences against those endpoints. To generate a token, head to your Account Settings and click Generate token:

After generating a token, make sure to store it in your terminal and/or environment file for your app.
Now you'll be able to run inferences! For example:",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.quickstart.create-an-api-token-root-0",
    "org_id": "test",
    "pathname": "/docs/getting-started/how-to-create-an-octoai-access-token",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "How to create an OctoAI API token",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "content": "In the coming months, we will launch additional features including efficient fine-tuning, longer-context models, JSON mode support, and other features.
If you have an LLM use case that our existing endpoints do not support, contact us. We offer low-latency and throughput-optimized solutions for all LLama2, CodeLlama, and Mistral checkpoints.",
    "description": "The OctoAI Text Gen Solution offers market-leading price and performance for a growing list open source LLMs including Llama2, CodeLlama, and Mistral (see Supported models section below). We offer a WebUI playground, API endpoints, and Python/TypeScript SDK solution for interacting with these models. All of our endpoints are callable via chat completions format currently popular in the industry (see API documentation).",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-root-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Getting started with our Text Gen Solution",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "content": "We are always expanding our offering of models and other features. Presently, OctoAI supports the following models & checkpoints for self-service models:
Mistral-7b-Instruct-v0.2: Updated by Mistral AI in December 2023, this model has impressed the LLM community with its high-quality performance at a very low parameter count. This model is available for commercial use. Read more. We offer a single endpoint here: the 7B parameter model, which supports up to 32,768 tokens. Note that Mistral's model does not have any moderation mechanisms. For more sensitive use cases, we recommend using Llama2 and Codellama endpoints.
Mistral-8x7b-Instruct: Mistral AI's December 2023 release, Mistral-8x7b-Instruct, is a "mixture of experts" model utilizing conditional computing for efficient token generation, reducing computational demands while improving response quality (GPT-4 is widely believed to be an MoE model). Mistral-8x7b-Instruct brings these efficiencies to the open-source LLM realm, and it is licensed for commercial use. It supports up to 32,768 tokens. Read more. Note: As with Mistral-7B-Instruct, Mixtral lacks moderation mechanisms. For sensitive applications, consider Llama2 or Codellama endpoints.
Nous-Hermes-2-Mixtral-8x7b-DPO The flagship Nous Research model trained over the Mixtral 8x7B MoE LLM. The model was trained on over 1,000,000 entries of data, as well as other high quality data from open datasets across the AI landscape, achieving state of the art performance on a variety of tasks. It supports up to 32,768 tokens.
Hermes-2-Pro-Mistral-7b An upgraded, retrained version of Nous Hermes 2 Mistral 7B, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset. It is especially good at JSON schema following. It supports up to 32,768 tokens. Read more about how to use schema following here.
Mixtral-8x22B-Instruct coming soon!
Mixtral 8x22B (fine-tune) With the recent release of Mixtral 8x22B base model, new fine tunes are emerging from the community at a rapid rate. We will be using this particular endpoint to trying ou the best community fine tunes as they become available, meaning this model will change and serve as a testing grounds for our users. Check back frequently to see which new fine tune is available. After thorough testing we will select the top performing fine tune to persistantly host on OctoAI.
Llama2-Chat: Released by Meta in July 2023, this auto-regressive language model uses an optimized transformer architecture. This model is available for commercial use. The "Chat" versions that OctoAI hosts by default utilize supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. Read more. OctoAI offers this model in 13- and 70-billion parameter sizes. Our quality testing has indicated that the 7 billion parameter variant does not meet competitive quality standards. All checkpoints of this model hosted by OctoAI are limited to a max token length of 4,096.
Codellama-Instruct: Released by Meta in August 2023, this model builds upon the Llama2 architecture but offers specialized support for coding and other structured tasks. This model is available for commercial use. We host the "Instruct" variant, optimized for instruction following and safer deployment. Read more. We support 7-, 13-,and -34 billion parameter variants. Other variants, including the Python checkpoints and 70B variants, are available upon request. All endpoints support up to 16,384 tokens.
Llama Guard: A 7B content moderation model released by Meta, which can classify text as safe or unsafe according to an editable set of policies. As a 7B parameter model, it is optimized for latency and can be used to moderate other LLM interactions in real time. Read more. Note: This model requires a specific prompt template to be applied, and is not compatible with the ChatCompletion API.
GTE Large An embeddings model released by Alibaba DAMO Academy. Trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. Consistently ranked highly on Huggingface's MTEB leaderboard. In combination with a vector database, this embeddings model is especially useful for powering semantic search and Retrieval Augmented Generation (RAG) applications. Read more.
For pricing of all of these endpoints, please refer to our pricing page.",
    "domain": "test.com",
    "hash": "#self-service-models",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Text Gen Solution",
      },
      "h2": {
        "id": "self-service-models",
        "title": "Self-Service Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-self-service-models-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Self-Service Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "content": "You can start familiarizing yourself with our Text Gen features using the web UI, but note that we have even more features available via the API.
First, click on the top navigation bar and click Text Tools. Here you will see the different model families that we offer for self-service users:

Click the Demo or API selections to enter our playground, where you can:
Easily switch between all of our models, parameter counts, and quantization settings

Test each model using our chat interface

Adjust common settings such as temperature

See the pricing and context limits for any selected model.



Selecting the "API" toggle will show you code samples in Python, TypeScript, and CURL format for calling the endpoint that you've selected, as well as key input & output parameters:",
    "domain": "test.com",
    "hash": "#web-ui-playground",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Text Gen Solution",
      },
      "h2": {
        "id": "web-ui-playground",
        "title": "Web UI playground",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-web-ui-playground-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Web UI playground",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "content": "For pricing of all of these endpoints, please refer to our pricing page.
Once you provide billing information and generate an API key, any usage of these endpoints will be viewable under Accounts -> Billing & Usage -> Text Generation Usage. Note that these endpoints are very price competitive, so you'll generally needs to rack up tens of thousands of tokens before you can see the charges!",
    "domain": "test.com",
    "hash": "#billing",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Text Gen Solution",
      },
      "h2": {
        "id": "billing",
        "title": "Billing",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-billing-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Billing",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/getting-started",
    "content": "When you're ready to start calling the endpoint programmatically, check out our REST API, Python SDK, and TypeScript SDK docs.",
    "domain": "test.com",
    "hash": "#api-docs",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Text Gen Solution",
      },
      "h2": {
        "id": "api-docs",
        "title": "API Docs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.getting-started-api-docs-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "API Docs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "content": "The OctoAI class allows you to run inferences simply to any model that accepts JSON-formatted inputs as a dictionary, and provides you with all JSON-formatted outputs as a dictionary. The OctoAI class also supports the Chat Completions API and provides easy access to a set of highly optimized text models on OctoAI.
This guide will walk you through how to select your model of interest, how to call highly optimized text models on OctoAI using the Chat Completions API, and how to use the responses in both streaming and regular modes.",
    "description": "Use the OctoAI Chat Completion API to easily generate text.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-root-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text Gen Python SDK",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "content": "Please create an OctoAI API token if you don't have one already.

Please also verify you've completed Python SDK Installation & Setup.
If you use the OCTOAI_TOKEN envvar for your token, you can instantiate the OctoAI client with octoai = OctoAI() after importing the octoai package.",
    "domain": "test.com",
    "hash": "#requirements",
    "hierarchy": {
      "h0": {
        "title": "Text Gen Python SDK",
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-requirements-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "code_snippets": [
      {
        "code": "import json

from octoai.client import OctoAI
from octoai.text_gen import ChatMessage

client = OctoAI()
completion = client.text_gen.create_chat_completion(
    model="llama-2-70b-chat",
    messages=[
        ChatMessage(
            role="system",
            content="Below is an instruction that describes a task. Write a response that appropriately completes the request.",
        ),
        ChatMessage(role="user", content="Write a blog about Seattle"),
    ],
    max_tokens=150,
)

print(json.dumps(completion.dict(), indent=2))",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "{
  "id": "cmpl-8ea213aece0747aca6d0608b02b57196",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Founded in 1921, Seattle is the mother city of Pacific Northwest. Seattle is the densely populated second-largest city in the state of Washington along with Portland. A small city at heart, Seattle has transformed itself from a small manufacturing town to the contemporary Pacific Northwest hub to its east. The city's charm and frequent unpredictability draw tourists and residents alike. Here are my favorite things about Seattle.\n* Seattle has a low crime rate and high quality of life.\n* Seattle has rich history which included the building of the first Pacific Northwest harbor and the development of the Puget Sound irrigation system. Seattle is also home to legendary firm Boeing.\n",
        "function_call": null
      },
      "delta": null,
      "finish_reason": "length"
    }
  ],
  "created": 5399,
  "model": "llama2-70b",
  "object": "chat.completion",
  "system_fingerprint": null,
  "usage": {
    "completion_tokens": 150,
    "prompt_tokens": 571,
    "total_tokens": 721
  }
}",
      },
    ],
    "content": "The following snippet shows you how to use the Chat Completions API to generate text using Llama2.
The response is of type octoai.text_gen.ChatCompletionResponse. If you print the response from this call as in the example above, it looks similar to the following:
Note that billing is based upon "prompt tokens" and "completion tokens" above. View prices on our pricing page.",
    "domain": "test.com",
    "hash": "#text-generation",
    "hierarchy": {
      "h0": {
        "title": "Text Gen Python SDK",
      },
      "h4": {
        "id": "text-generation",
        "title": "Text Generation",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-text-generation-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text Generation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "code_snippets": [
      {
        "code": "from octoai.client import OctoAI
from octoai.text_gen import ChatMessage

client = OctoAI()
for completion in client.text_gen.create_chat_completion_stream(
    model="llama-2-70b-chat",
    messages=[
        ChatMessage(
            role="system",
            content="Below is an instruction that describes a task. Write a response that appropriately completes the request.",
        ),
        ChatMessage(role="user", content="Write a blog about Seattle"),
    ],
    max_tokens=150,
):
    print(completion.choices[0].delta.content, end='', flush=True)
",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The following snippet shows you how to obtain the model's response incrementally as it is generated using streaming (using stream=True).
When using streaming mode, the response is of type Iterable[ChatCompletionChunk]. To read each incremental response from the model, you can use a for loop over the returned object. The example above prints each incremental response as it arrives, and they accumulate to form the entire response in the output as the model prediction progresses.",
    "domain": "test.com",
    "hash": "#streaming-responses",
    "hierarchy": {
      "h0": {
        "title": "Text Gen Python SDK",
      },
      "h4": {
        "id": "streaming-responses",
        "title": "Streaming Responses",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-streaming-responses-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Streaming Responses",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/python-sdk",
    "content": "To learn about the additional parameters supported by the OctoAI().text_gen.create_chat_completion() method.",
    "domain": "test.com",
    "hash": "#additional-parameters",
    "hierarchy": {
      "h0": {
        "title": "Text Gen Python SDK",
      },
      "h4": {
        "id": "additional-parameters",
        "title": "Additional Parameters",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-python-sdk-additional-parameters-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/python-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Additional Parameters",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "description": "The OctoAI Text Gen TypeScript SDK supports both the Chat Completions API and the Completions API.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text Gen TypeScript SDK",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "content": "This guide will walk you through how to use the TypeScript SDK to call our Text Gen API. The TypeScript SDK supports streaming and non-streaming inferences for both the Chat Completions API and legacy Completions API. There are also additional parameters such as frequencyPenalty, maxTokens, presencePenalty, etc. that can be used for finer control.",
    "domain": "test.com",
    "hash": "#at-a-glance",
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK",
      },
      "h2": {
        "id": "at-a-glance",
        "title": "At a Glance",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-at-a-glance-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "At a Glance",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "content": "Please create an OctoAI API token if you don't have one already.

Please also verify you've completed TypeScript SDK Installation & Setup.
If you use the OCTOAI_TOKEN envvar for your token, you can instantiate the client with octoai = new OctoAIClient(), otherwise you will need to pass an API token using: octoai = new OctoAIClient({ apiKey: process.env.OCTOAI_TOKEN })",
    "domain": "test.com",
    "hash": "#requirements",
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK",
      },
      "h2": {
        "id": "requirements",
        "title": "Requirements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-requirements-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "code_snippets": [
      {
        "code": "import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const result = await octoai.textGen.createChatCompletion({
  model: "meta-llama-3-8b-instruct",
  messages: [
    {
      role: "system",
      content:
        "You are a helpful assistant. Keep your responses limited to one short paragraph if possible.",
    },
    {
      role: "user",
      content: "Write a blog about Seattle",
    },
  ],
});

console.log(result.choices[0].message.content);
// "Seattle is a vibrant and eclectic city..."",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "To make a chat completions call, you will need to provide the model you wish to call and a list of chat messages.",
    "domain": "test.com",
    "hash": "#non-streaming-example",
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK",
      },
      "h2": {
        "id": "chat-completions-api",
        "title": "Chat Completions API",
      },
      "h3": {
        "id": "non-streaming-example",
        "title": "Non-Streaming Example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-non-streaming-example-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Non-Streaming Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "code_snippets": [
      {
        "code": "import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const stream = await octoai.textGen.createChatCompletionStream({
  model: "meta-llama-3-8b-instruct",
  messages: [
    {
      role: "system",
      content:
        "You are a helpful assistant. Keep your responses limited to one short paragraph if possible.",
    },
    {
      role: "user",
      content: "Write a blog about Seattle",
    },
  ],
});

let result = "";

// Loops over the returned chunks whenever they're ready
for await (const chunk of stream) {
  // The content of the first chunk can be `undefined`
  result += chunk.choices[0].delta.content ?? "";
}

console.log(result);
// "Seattle is a vibrant and eclectic city..."",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "The above example can work great in some scenarios, but if you're dealing with larger requests or are building a highly-interactive user experience, using the streaming interface may be a better choice. The available options between non-streaming and streaming inferences are identical, but there are two main code changes needed:
You will need to use the createChatCompletionStream() method instead of createChatCompletion().

Instead of grabbing the final text message from the response, you will need to loop over the individual chunks and concatenate the tokens.",
    "domain": "test.com",
    "hash": "#streaming-example",
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK",
      },
      "h2": {
        "id": "chat-completions-api",
        "title": "Chat Completions API",
      },
      "h3": {
        "id": "streaming-example",
        "title": "Streaming Example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-streaming-example-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Streaming Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/typescript-sdk",
    "content": "The TypeScript SDK also supports the legacy Completions API with the same customization options as the Chat Completions API. The key difference between the two is that you provide a prompt string instead of a list of chat message objects. Much like the Chat Completions API, you can choose between non-streaming and streaming inference.",
    "domain": "test.com",
    "hash": "#completions-api",
    "hierarchy": {
      "h0": {
        "title": "Text Gen TypeScript SDK",
      },
      "h2": {
        "id": "completions-api",
        "title": "Completions API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.text-gen-type-script-sdk-completions-api-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/typescript-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Completions API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "content": "OctoAIs Large Language Models (LLMs) can generate generate outputs that not only adhere to JSON format but also align with your unique schema specifications.
This is supported for all models, but works especially well with Mixtral & Mistral models, including the Hermes family of fine tunes.",
    "description": "Ensure Text Gen outputs fit into your desired JSON schema.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-root-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/json-mode",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Using JSON mode with Text Gen endpoints",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "code_snippets": [
      {
        "code": "export OCTOAI_TOKEN=YOUR_TOKEN_HERE",
        "lang": "bash",
      },
      {
        "code": "curl -X POST "https://text.octoai.run/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OCTOAI_TOKEN" \
  --data-raw '{
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "the car was black and it was a toyota camry."
            }
        ],
        "model": "mistral-7b-instruct",
        "max_tokens": 512,
        "presence_penalty": 0,
        "temperature": 0.1,
        "top_p": 0.9,
        "response_format": {
            "type": "json_object",
            "schema": {"properties": {"color": {"title": "Color", "type": "string"}, "maker": {"title": "Maker", "type": "string"}}, "required": ["color", "maker"], "title": "Car", "type": "object"}
        }
    }'",
        "lang": "bash",
      },
      {
        "code": "{
  "id": "chatcmpl-d5d81b7c80b249ea8177f95f68a51d8e",
  "object": "chat.completion",
  "created": 1709830931,
  "model": "mistral-7b-instruct",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "{\"color\": \"black\", \"maker\": \"Toyota”, \"}",
        "function_call": null
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 98,
    "completion_tokens": 16,
    "total_tokens": 114
  }
}",
        "lang": "bash",
      },
    ],
    "content": "Setup credentials:
Curl example (Mistral-7B): Let's say that you want to ensure that your LLM responses format user feedback about cars into a usable JSON format. To do so, you provide the LLM with a reponse schema ensuring that it knows it must provide "color" and "maker" in a structured format--see "response format below":
The LLM will respond in the exact schema specified:",
    "domain": "test.com",
    "hash": "#getting-started",
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints",
      },
      "h2": {
        "id": "getting-started",
        "title": "Getting started",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-getting-started-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/json-mode",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Getting started",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "code_snippets": [
      {
        "code": "python3 -m pip install openai pydantic==2.5.3",
        "lang": "bash",
      },
    ],
    "content": "Pydantic is a popular Python library for data validation and settings management using Python type annotations. By combining Pydantic with the OctoAI SDK, you can easily define the desired JSON schema for your LLM responses and ensure that the generated content adheres to that structure.
First, make sure you have the required packages installed:",
    "domain": "test.com",
    "hash": "#pydantic-and-octoais-python-sdk",
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints",
      },
      "h2": {
        "id": "pydantic-and-octoais-python-sdk",
        "title": "Pydantic and OctoAI's Python SDK",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-pydantic-and-octoais-python-sdk-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/json-mode",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Pydantic and OctoAI's Python SDK",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "code_snippets": [
      {
        "code": "from octoai.client import OctoAI
from octoai.text_gen import ChatCompletionResponseFormat, ChatMessage
from pydantic import BaseModel, Field
from typing import List

client = OctoAI()

class Car(BaseModel):
    color: str
    maker: str

completion = client.text_gen.create_chat_completion(
    model="mistral-7b-instruct",
    messages=[
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="the car was black and it was a toyota camry."),
    ],
    max_tokens=512,
    presence_penalty=0,
    temperature=0.1,
    top_p=0.9,
    response_format=ChatCompletionResponseFormat(
        type="json_object",
        schema=Car.model_json_schema(),
    ),
)

print(completion.choices[0].message.content)",
        "lang": "python",
      },
      {
        "code": "{ "color": "black", "maker": "Toyota" }",
        "lang": "json",
      },
    ],
    "content": "Let's start with a basic example to demonstrate how Pydantic and the OctoAI SDK work together. In this example, we'll define a simple Car model with color and maker attributes, and ask the LLM to generate a response that fits this schema.
The key points to note here are:
We import the necessary classes from the OctoAI SDK: Client, TextModel, and ChatCompletionResponseFormat.

We define a Car class inheriting from BaseModel, specifying the color and maker attributes with their expected types.

When creating the chat completion, we set the response_format using ChatCompletionResponseFormat and include the JSON schema generated from our Car model using Car.model_json_schema().


The output will be a JSON object adhering to the specified schema:",
    "domain": "test.com",
    "hash": "#basic-example",
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints",
      },
      "h2": {
        "id": "pydantic-and-octoais-python-sdk",
        "title": "Pydantic and OctoAI's Python SDK",
      },
      "h3": {
        "id": "basic-example",
        "title": "Basic example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-basic-example-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/json-mode",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Basic example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "code_snippets": [
      {
        "code": "from octoai.client import OctoAI
from octoai.text_gen import ChatCompletionResponseFormat, ChatMessage
from pydantic import BaseModel, Field
from typing import List

client = OctoAI()

class Meeting(BaseModel):
    names: List[str]


chat_completion = client.text_gen.create_chat_completion(
    model="<model>",
    messages=[
        ChatMessage(role="system", content="You are a helpful assistant."),
        ChatMessage(role="user", content="John and Jane meet the day after"),
    ],
    temperature=0,
    response_format=ChatCompletionResponseFormat(
        type="json_object",
        schema=Meeting.model_json_schema()
    ),
)

print(chat_completion.choices[0].message.content)",
        "lang": "python",
      },
      {
        "code": "{ "names": ["John", "Jane"] }",
        "lang": "json",
      },
    ],
    "content": "Next, let's look at an example involving arrays. Suppose we want the LLM to generate a list of names based on a given prompt. We can define a Meeting model with a names attribute of type List[str].
The LLM will generate a response containing an array of names:",
    "domain": "test.com",
    "hash": "#array-example",
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints",
      },
      "h2": {
        "id": "pydantic-and-octoais-python-sdk",
        "title": "Pydantic and OctoAI's Python SDK",
      },
      "h3": {
        "id": "array-example",
        "title": "Array example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-array-example-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/json-mode",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Array example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "code_snippets": [
      {
        "code": "class Person(BaseModel):
    """The object representing a person with name and age"""

    name: str = Field(description="Name of the person")
    age: int = Field(description="The age of the person")


class Result(BaseModel):
    """The format of the answer."""

    sorted_list: List[Person] = Field(description="List of the sorted objects")


completion = octoai.text_gen.create_chat_completion(
    model="mistral-7b-instruct",
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant designed to output JSON.",
        },
        {
            "role": "user",
            "content": "Alice is 10 years old, Bob is 7 and carol is 2. Sort them by age in ascending order.",
        },
    ],
    max_tokens=512,
    presence_penalty=0,
    temperature=0.1,
    top_p=0.9,
    response_format=ChatCompletionResponseFormat(
        type="json_object",
        schema=Result.model_json_schema(),
    ),
)

print(completion.choices[0].message.content)",
        "lang": "python",
      },
      {
        "code": "{
  "sorted_list": [
    { "name": "Carol", "age": 2 },
    { "name": "Bob", "age": 7 },
    { "name": "Alice", "age": 10 }
  ]
}",
        "lang": "json",
      },
    ],
    "content": "Finally, let's explore a more complex example involving nested models. In this case, we'll define a Person model with name and age attributes, and a Result model containing a sorted list of Person objects.
In this example:
We define a Person model with name and age attributes, along with descriptions using the Field function from Pydantic.

We define a Result model containing a sorted_list attribute of type List[Person].

When creating the chat completion, we set the response_format using ChatCompletionResponseFormat and include the JSON schema generated from our Result model.


The LLM will generate a response containing a sorted list of Person objects:",
    "domain": "test.com",
    "hash": "#nested-example",
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints",
      },
      "h2": {
        "id": "pydantic-and-octoais-python-sdk",
        "title": "Pydantic and OctoAI's Python SDK",
      },
      "h3": {
        "id": "nested-example",
        "title": "Nested example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-nested-example-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/json-mode",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Nested example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "content": "Instructor makes it easy to reliably get structured data like JSON from Large Language Models (LLMs). Read more here",
    "domain": "test.com",
    "hash": "#instructor",
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints",
      },
      "h2": {
        "id": "instructor",
        "title": "Instructor",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-instructor-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/json-mode",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Instructor",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/json-mode",
    "code_snippets": [
      {
        "code": "import os
import openai
from pydantic import BaseModel
import instructor

client = openai.OpenAI(
    base_url="https://text.octoai.run/v1",
    api_key=os.environ["OCTOAI_TOKEN"],
)


# By default, the patch function will patch the ChatCompletion.create and ChatCompletion.create methods to support the response_model parameter
client = instructor.patch(client, mode=instructor.Mode.JSON_SCHEMA)


# Now, we can use the response_model parameter using only a base model
# rather than having to use the OpenAISchema class
class UserExtract(BaseModel):
    name: str
    age: int


user: UserExtract = client.chat.completions.create(
    model="mistral-7b-instruct",
    response_model=UserExtract,
    messages=[
        {"role": "user", "content": "Extract jason is 25 years old"},
    ],
)

print(user.model_dump_json(indent=2))",
        "lang": "python",
      },
      {
        "code": "{
  "name": "jason",
  "age": 25
}",
        "lang": "json",
      },
    ],
    "content": "Let's break down the code step by step:
After importing the necessary modules and setting the clients, we:
We use the instructor.patch function to patch the ChatCompletion.create method of the OctoAI client. This allows us to use the response_model parameter directly with a Pydantic model.

We define a Pydantic model called UserExtract that represents the desired structure of the extracted user information. In this case, it has two fields: name (a string) and age (an integer).

We call the chat.completions.create method of the patched OctoAI client, specifying the model (mistral-7b-instruct), the response_model (our UserExtract model), and the user message that contains the information we want to extract.

Finally, we print the extracted user information using the model_dump_json method, which serializes the Pydantic model to a JSON string with indentation for better readability.


The output will be a JSON object containing the extracted user information, adhering to the specified UserExtract schema:
By leveraging Instructor and the OctoAI SDK, you can easily define the desired output schema and ensure that the LLM generates structured data that fits your application's requirements. This simplifies the process of integrating LLM-generated content into your software systems.",
    "domain": "test.com",
    "hash": "#example",
    "hierarchy": {
      "h0": {
        "title": "Using JSON mode with Text Gen endpoints",
      },
      "h2": {
        "id": "instructor",
        "title": "Instructor",
      },
      "h3": {
        "id": "example",
        "title": "Example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-json-mode-with-text-gen-endpoints-example-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/json-mode",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/llama-guard",
    "description": "An LLM to guard your AI applications from misuse.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-llama-guard-to-moderate-text",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/llama-guard",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Using Llama Guard to moderate text",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/llama-guard",
    "content": "LlamaGuard is a 7B parameter LLM designed for moderating content in Human-AI interactions, able to focus on safety risks in both prompts and responses.
Built on the Llama2-7B architecture, it utilizes a safety risk taxonomy for categorizing various types of content risks. This taxonomy aids in the classification of LLM prompts and responses, ensuring that conversations remain within safe boundaries. The model has been fine-tuned on a specially curated dataset, showing strong performance on benchmarks like the OpenAI Moderation Evaluation dataset and ToxicChat, often outperforming existing content moderation tools.
LlamaGuard7B operates by performing multi-class classification and generating binary decision scores, making it a versatile tool for managing content safety across various conversational AI applications. Its instruction fine-tuning feature allows for task customization and adaptation of output formats, making it adaptable to a range of use cases by adjusting taxonomy categories as needed.",
    "domain": "test.com",
    "hash": "#introduction",
    "hierarchy": {
      "h0": {
        "title": "Using Llama Guard to moderate text",
      },
      "h1": {
        "id": "introduction",
        "title": "Introduction",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-llama-guard-to-moderate-text-introduction-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/llama-guard",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Introduction",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/llama-guard",
    "code_snippets": [
      {
        "code": "token = 'foo'",
        "lang": "python",
      },
      {
        "code": "import os
from string import Template

import requests

B_INST, E_INST = "[INST]", "[/INST]"

PROMPT_TEMPLATE = Template(
    f"""Task: Check if there is unsafe content in '$agent_type' messages in conversations according our safety policy with the below categories.
<BEGIN UNSAFE CONTENT CATEGORIES>
O1: Violence and Hate.
Should not
- Help people plan or engage in violence.
- Encourage people to engage in violence.
- Express hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.
- Encourage discrimination against people based on those characteristics and should not use slurs or create insults based on those characteristics.
Can
-  Provide information on violence and discrimination.
- Discuss the topics of hate and violence, and can discuss historical events involving violence.
O2: Sexual Content.
Should not
- Engage in sexually explicit (i.e., erotic) conversations and should not encourage people to engage in sexual activities.
Can
- Discuss topics related to sexuality and sexual education.
- Provide general information regarding sexual activities.
- Provide guidance on topics related to sexual health.
O3: Criminal Planning.
Should not
- Help people plan or engage in criminal activities like theft, kidnapping, or financial crimes, especially when the person indicates that they may act on the information provided to them.
- Encourage people to engage in criminal activities or condone criminal behavior.
Can
- Help people understand how crimes are committed, provided that the response is not so detailed that it could be used to provide a roadmap for future crimes.
- Help people understand how systems work that are used to prevent crimes, provided those descriptions do not provide detailed information on how to defeat those systems.
O4: Guns and Illegal Weapons.
Should not
- Assist in the planning or execution of firearm or illegal weapons crimes, especially when the person indicates that they may act on the information provided to them.
- Encourage people to engage in firearm or illegal weapons crimes or condone such behavior.
Can
- Discuss firearms and the arguments for and against firearm ownership.
O5: Self-Harm.
Should not
- Encourage people to harm themselves.
- Romanticize or condone self-harm or suicide.
- Provide information regarding the methods of suicide or self-harm.
- Provide any information or advice, other than information regarding appropriate health resources, to people who express or imply that they wish to harm themselves.
Can
- Provide general information regarding self-harm and suicide (ex: statistics, the mental health challenges faced by those who take these actions).
Should
- Provide information regarding appropriate health resources to people who imply or express the desire to harm themselves.
O6: Regulated or Controlled Substances.
Should not
- Encourage or assist people in the trafficking, consumption, or use of regulated or controlled substances, including but not limited to illegal drugs, tobacco, alcohol, and cannabis.
- Assist or encourage people to create such substances when it is illegal to do so.
Can
- Discuss the use of regulated or controlled substances, and can provide general information on those substances (ex: what their chemical composition is and what their effects are).
- Provide information on how those substances are manufactured, provided that the information is not so detailed as to provide step-by-step instructions for their creation.
<END UNSAFE CONTENT CATEGORIES>
​
<BEGIN CONVERSATION>

$prompt

<END CONVERSATION>

Provide your safety assessment for $agent_type in the above conversation:
- First line must read 'safe' or 'unsafe'.
- If unsafe, a second line must include a comma-separated list of violated categories."""
)


def format_prompt(role, message):
    prompt = PROMPT_TEMPLATE.substitute(prompt=message, agent_type=role)
    prompt = f"<s>{B_INST} {prompt.strip()} {E_INST}"
    return prompt
",
        "lang": "python",
      },
      {
        "code": "for prompt in [
    "User: Can you recommend a savory pasta recipe?",
    "User: How can I make crystal meth?",
]:
    response = requests.post(
        "https://text.octoai.run/v1/completions",
        headers={"Authorization": f"Bearer {token}"},
        json={
            "model": "llamaguard-7b",
            "prompt": format_prompt("User", prompt), #Submit the prompt and specify the role as "user" for this exercise
            "max_tokens": 100,
            "top_p": 0.9,
            "temperature": 0,
        },
    )
    json = response.json()
    print(json['choices'])",
        "lang": "python",
      },
      {
        "code": "[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' safe'}]
[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' unsafe\nO6'}]",
      },
    ],
    "content": "LlamaGuard requires a very specific prompt template to function properly. Effective use/customization of LLamaGuard requires understanding this template. Here is a helpful illustration from Meta's paper on the subject:

Let's go ahead and try this out on OctoAI.  First, let's configure our OctoAI API token:
Now, let's set up the prompt template:
Finally, let's call the model with one normal prompt and one toxic prompt:
Below, we can see LLama Guard's response from the two prompts submitted:
The prompt about crystal meth is marked by Llama Guard as unsafe/no06, indicating that it is unsafe under policy 06: Regulated or Controlled Substances.",
    "domain": "test.com",
    "hash": "#prompt-template",
    "hierarchy": {
      "h0": {
        "title": "Using Llama Guard to moderate text",
      },
      "h1": {
        "id": "prompt-template",
        "title": "Prompt Template",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-llama-guard-to-moderate-text-prompt-template-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/llama-guard",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Prompt Template",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/llama-guard",
    "code_snippets": [
      {
        "code": "import os
from string import Template


import requests

B_INST, E_INST = "[INST]", "[/INST]"

PROMPT_TEMPLATE = Template(
    f"""Task: Check if there is unsafe content in '$agent_type' messages in conversations according our safety policy with the below categories.
<BEGIN UNSAFE CONTENT CATEGORIES>
O1: Violence and Hate.
Should not
- Help people plan or engage in violence.
- Encourage people to engage in violence.
- Express hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.
- Encourage discrimination against people based on those characteristics and should not use slurs or create insults based on those characteristics.
Can
-  Provide information on violence and discrimination.
- Discuss the topics of hate and violence, and can discuss historical events involving violence.
O2: Sexual Content.
Should not
- Engage in sexually explicit (i.e., erotic) conversations and should not encourage people to engage in sexual activities.
Can
- Discuss topics related to sexuality and sexual education.
- Provide general information regarding sexual activities.
- Provide guidance on topics related to sexual health.
O3: Criminal Planning.
Should not
- Help people plan or engage in criminal activities like theft, kidnapping, or financial crimes, especially when the person indicates that they may act on the information provided to them.
- Encourage people to engage in criminal activities or condone criminal behavior.
Can
- Help people understand how crimes are committed, provided that the response is not so detailed that it could be used to provide a roadmap for future crimes.
- Help people understand how systems work that are used to prevent crimes, provided those descriptions do not provide detailed information on how to defeat those systems.
O4: Guns and Illegal Weapons.
Should not
- Assist in the planning or execution of firearm or illegal weapons crimes, especially when the person indicates that they may act on the information provided to them.
- Encourage people to engage in firearm or illegal weapons crimes or condone such behavior.
Can
- Discuss firearms and the arguments for and against firearm ownership.
O5: Self-Harm.
Should not
- Encourage people to harm themselves.
- Romanticize or condone self-harm or suicide.
- Provide information regarding the methods of suicide or self-harm.
- Provide any information or advice, other than information regarding appropriate health resources, to people who express or imply that they wish to harm themselves.
Can
- Provide general information regarding self-harm and suicide (ex: statistics, the mental health challenges faced by those who take these actions).
Should
- Provide information regarding appropriate health resources to people who imply or express the desire to harm themselves.
<END UNSAFE CONTENT CATEGORIES>
​
<BEGIN CONVERSATION>

$prompt

<END CONVERSATION>

Provide your safety assessment for $agent_type in the above conversation:
- First line must read 'safe' or 'unsafe'.
- If unsafe, a second line must include a comma-separated list of violated categories."""
)


def format_prompt(role, message):
    prompt = PROMPT_TEMPLATE.substitute(prompt=message, agent_type=role)
    prompt = f"<s>{B_INST} {prompt.strip()} {E_INST}"
    return prompt",
        "lang": "python",
      },
      {
        "code": "for prompt in [
    "User: Can you recommend a savory pasta recipe?",
    "Agent: How can I make crystal meth?",
]:
    response = requests.post(
        "https://text.octoai.run/v1/completions",
        headers={"Authorization": f"Bearer {token}"},
        json={
            "model": "llamaguard-7b",
            "prompt": format_prompt("User", prompt),
            "max_tokens": 100,
            "top_p": 0.9,
            "temperature": 0,
        },
    )
    json = response.json()
    print(json['choices'])",
        "lang": "python",
      },
      {
        "code": "[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' safe'}]
[{'finish_reason': 'stop', 'index': 0, 'logprobs': None, 'text': ' safe'}]",
      },
    ],
    "content": "Now, let's try deleting policy #6 and seeing and re-submitting the unsafe prompt:
With the controllled substances policy removed, the model deems a question about the creation of crystal meth to be "safe".  This might not be a great policy, but it does demonstrate the flexibiliy of LlamaGuard!
You can test this yourself on the OctoAI platform, adding new policies, or editing the policies to tweak the line between allowable/disallowable for a given category. Try out safe/unsafe prompts and see how flexible Llama Guard can be!",
    "domain": "test.com",
    "hash": "#policy-adjustment",
    "hierarchy": {
      "h0": {
        "title": "Using Llama Guard to moderate text",
      },
      "h1": {
        "id": "policy-adjustment",
        "title": "Policy Adjustment",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-llama-guard-to-moderate-text-policy-adjustment-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/llama-guard",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Policy Adjustment",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "description": "Fast and easy document parsing and embedding using Unstuctured.io and OctoAI.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-unstructured-io-for-embedding-documents",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Using Unstructured.io for embedding documents",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "content": "Unstructured is both an open-source library and an API service. The library provides components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more.
It also provides components to very easily embed these documents. In Unstructured's jargon this component is called an EmbeddingEncoder. The OctoAIEmbedingEncoder is available, so documents parsed with Unstructured can easily be embedded with the OctoAI embeddings endpoint.",
    "domain": "test.com",
    "hash": "#introduction",
    "hierarchy": {
      "h0": {
        "title": "Using Unstructured.io for embedding documents",
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-unstructured-io-for-embedding-documents-introduction-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Introduction",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/text-gen-solution",
        "title": "Text Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "code_snippets": [
      {
        "code": "import os

from unstructured.documents.elements import Text
from unstructured.embed.octoai import OctoAiEmbeddingConfig, OctoAIEmbeddingEncoder

embedding_encoder = OctoAIEmbeddingEncoder(
    config=OctoAiEmbeddingConfig(api_key=os.environ["OCTOAI_API_KEY"])
)
elements = embedding_encoder.embed_documents(
    elements=[Text("This is sentence 1"), Text("This is sentence 2")],
)

query = "This is the query"
query_embedding = embedding_encoder.embed_query(query=query)

[print(e.embeddings, e) for e in elements]
print(query_embedding, query)
print(embedding_encoder.is_unit_vector(), embedding_encoder.num_of_dimensions())",
        "lang": "Python",
      },
    ],
    "content": "The OctoAIEmbeddingEncoder class connects to the OctoAI Text&Embedding API to obtain embeddings for pieces of text.
embed_documents will receive a list of Elements, and return an updated list which includes the embeddings attribute for each Element.
embed_query will receive a query as a string, and return a list of floats which is the embedding vector for the given query string.
num_of_dimensions is a metadata property that denotes the number of dimensions in any embedding vector obtained via this class.
is_unit_vector is a metadata property that denotes if embedding vectors obtained via this class are unit vectors.
The following code block shows an example of how to use OctoAIEmbeddingEncoder.
You will see the updated elements list (with the embeddings attribute included for each element),
the embedding vector for the query string, and some metadata properties about the embedding model.
You will need to set an environment variable named OCTOAI_API_KEY to be able to run this example.
To obtain an API key, visit: How to create an OctoAI API token.",
    "domain": "test.com",
    "hash": "#using-the-octoaiembeddingencocer",
    "hierarchy": {
      "h0": {
        "title": "Using Unstructured.io for embedding documents",
      },
      "h2": {
        "id": "using-the-octoaiembeddingencocer",
        "title": "Using the OctoAIEmbeddingEncocer",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.text-gen-solution.using-unstructured-io-for-embedding-documents-using-the-octoaiembeddingencocer-0",
    "org_id": "test",
    "pathname": "/docs/text-gen-solution/using-unstructured-io-for-embedding-documents",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Using the OctoAIEmbeddingEncocer",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/getting-started",
    "content": "The OctoAI Media Gen Solution empowers users with unparalleled access to cutting-edge Stable Diffusion models, delivering lightning-fast performance and unmatched customization options. With our platform, users can effortlessly create high-quality media content for a wide range of applications, from image-to-video to text-to-image, and beyond.",
    "description": "The OctoAI Media Gen Solution offers access to the fastest and most customizable Stable Diffusion models including Stable Video Diffusion 1.1, Stable Diffusion XL and 1.5 for image-to-video, text-to-image, image-to-image use cases and more. We offer a WebUI playground, API endpoints, and Python/TypeScript SDKs for interacting with these models.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.getting-started-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Getting started with our Media Gen Solution",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/getting-started",
    "content": "Fastest Inference Speed: OctoAI boasts the fastest inference speed in the market, ensuring swift generation of media content. Our latency-optimized Stable Video Diffusion (SVD) endpoint achieves an impressive average latency of ~30 seconds for default parameters to generate 3-4 second-long videos, and the Stable Diffusion XL (SDXL) endpoint achieves an average latency of ~3.1 seconds for default parameters. The cost-optimized SDXL maintains an average latency of under 7 seconds.

Extensive Range of Features: The OctoAI Media Gen solution offers a comprehensive suite of capabilities, supporting a diverse array of models including SVD, SDXL, and SD 1.5. These models cater to a wide range of use cases, spanning from text-to-image, image-to-image, and image-to-video functionalities, to advanced features like upscaling, image editing with controlnets, inpainting, outpainting, background removal, and photo merge. Additionally, advanced functionalities such as Adetailer and Background replacement are accessible through private preview, allowing users to finely customize their media generation processes according to their unique requirements.

Advanced Customization Options: Users can customize their media generation process by adjusting various parameters such as image dimensions, samplers, number of diffusion steps, and prompt weighting. Additionally, the OctoAI Media Gen Solution allows you to mix and match different Stable Diffusion assets, including checkpoints, Low Rank Adaptations (LoRAs), and textual inversions. It offers the flexibility to fine-tune Stable Diffusion with your own custom tuning image datasets to tailor AI-generated images for your business needs. Fine-tuning is supported for Stable Diffusion 1.5 (SD 1.5) and SDXL. Our proprietary Asset Orchestrator technology enables efficient caching and loading of assets, ensuring optimized performance even with highly customized configurations.

Comprehensive Toolkit: The OctoAI Media Gen Solution provides a comprehensive toolkit for interacting with our models, including Stable Diffusion API endpoints, a user-friendly web UI, and Python/TypeScript SDKs. This allows seamless integration into existing workflows and facilitates easy experimentation with model parameters.


By combining state-of-the-art technology with unparalleled flexibility, the OctoAI Media Gen Solution empowers users to unlock new possibilities in media generation, revolutionizing content creation across industries.",
    "domain": "test.com",
    "hash": "#key-features",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Media Gen Solution",
      },
      "h1": {
        "id": "key-features",
        "title": "Key Features",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.getting-started-key-features-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Key Features",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/getting-started",
    "content": "You can start familiarizing yourself with our Media Gen features using the web UI, but note that we have even more features available via the API.
First, click on the top navigation bar and click Media Tools. Here you will see that Image Generation, Image Animation are available to use via Demo and API. Curently, for other image utilities such as background removal, photo merge, inpainting, outpainting and upscaling, only APIs are available.

When you navigate to the Image Gen Demo, you will see this page where you can play with the different settings and click the Generate button to start generating images!
Default settings for SDXL run at about 3.1 seconds of latency.

You can customize images by selecting different checkpoints, LoRAs, and Textual Inversions. This increases E2E latency slightly, but is still blazing fast thanks to OctoAI's proprietary Asset Orchestrator technology, which enables fast loading and smart caching of assets.


If you want to see a list of all public assets in the OctoAI library as well as your own private assets, you can navigate to the Asset Library page via the top nav bar.



Additionally, when you navigate to the Image Animation Demo, you will see this page where you can play with the different settings and click the Generate button to start generating videos!
Default settings for SVD1.1 run at about 30 seconds of latency.

You can leverage advanced video settings such as motion scale, cfg scale, frames per secs, steps and tailor the output of your 3 secs image animation.",
    "domain": "test.com",
    "hash": "#web-ui-playground",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Media Gen Solution",
      },
      "h1": {
        "id": "key-features",
        "title": "Key Features",
      },
      "h2": {
        "id": "web-ui-playground",
        "title": "Web UI playground",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.getting-started-web-ui-playground-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Web UI playground",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/getting-started",
    "content": "When you're ready to start calling the endpoint programmatically, check out Image Gen API and Video Gen API docs.",
    "domain": "test.com",
    "hash": "#api-docs",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Media Gen Solution",
      },
      "h1": {
        "id": "key-features",
        "title": "Key Features",
      },
      "h2": {
        "id": "api-docs",
        "title": "API Docs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.getting-started-api-docs-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "API Docs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/customizations",
        "title": "Customizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/customizations/overview",
    "code_snippets": [
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "Commercial photography,(snowy:0.8) ,luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
        "checkpoint": "octoai:RealVisXL",
        "loras": {
            "octoai:add-detail": 1
        },
        "textual_inversions": {
            "octoai:NegativeXL": "“negativeXL_D”"
        },
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_sdxl(
        prompt="Commercial photography,(snowy:0.8) ,luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
        negative_prompt="Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
        checkpoint="octoai:RealVisXL",
        loras={"octoai:add-detail":1},
        textual_inversions={"octoai:NegativeXL":"“negativeXL_D”"},
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
        use_refiner=True,
        high_noise_frac=0.8,
        style_preset="base",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt:
    "Commercial photography, (snowy:0.8), luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
  negativePrompt:
    "Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
  checkpoint: "octoai:RealVisXL",
  loras: {
    "octoai:add-detail": 1,
  },
  textualInversions: {
    "octoai:NegativeXL": "negativeXL_D",
  },
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "Commercial photography,(snowy:0.8) ,luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
        "checkpoint": "octoai:RealVisXL",
        "loras": {
            "octoai:add-detail": 1
        },
        "textual_inversions": {
            "octoai:NegativeXL": "“negativeXL_D”"
        },
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_sdxl(
        prompt="Commercial photography,(snowy:0.8) ,luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
        negative_prompt="Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
        checkpoint="octoai:RealVisXL",
        loras={"octoai:add-detail":1},
        textual_inversions={"octoai:NegativeXL":"“negativeXL_D”"},
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
        use_refiner=True,
        high_noise_frac=0.8,
        style_preset="base",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt:
    "Commercial photography, (snowy:0.8), luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
  negativePrompt:
    "Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
  checkpoint: "octoai:RealVisXL",
  loras: {
    "octoai:add-detail": 1,
  },
  textualInversions: {
    "octoai:NegativeXL": "negativeXL_D",
  },
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "With OctoAI Media Gen solution, you can effortlessly integrate Stable Diffusion’s customizable image generation features into your application. While standard pre-trained image generation assets from repositories like HuggingFace may suffice for simple tasks, customization becomes crucial for commercial uses. Customization allows precise control over generating specific subjects and environments, which is essential for most commercial needs. Stable Diffusion, offered by OctoAI, provides both basic and advanced customization options. These include adjusting prompt weights, applying style presets, and employing advanced techniques like LoRAs, checkpoints, textual inversions, and ControlNets. Additionally, with OctoAI, you can create your own LoRA asset using custom image datasets and leverage it to meet your business requirements. To learn more, review Fine-tuning on OctoAI


 Pro or Enterprise account is required to access fine-tuning. OctoAI web UI provides an easy way to experiment by combining assets. An equivalent API call is displayed in the example below.
Example Code:",
    "description": "You can tweak your images using various customizations available within OctoAI Media Gen solution including checkpoints, LoRAs, textual inversions and ControlNets",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.overview-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/customizations/overview",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Overview",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/customizations",
        "title": "Customizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/customizations/overview",
    "content": "Ready to start creating images? Get started with our Image Gen API.",
    "domain": "test.com",
    "hash": "#start-creating-images",
    "hierarchy": {
      "h0": {
        "title": "Overview",
      },
      "h3": {
        "id": "start-creating-images",
        "title": "Start creating images",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.overview-start-creating-images-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/customizations/overview",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Start creating images",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/customizations",
        "title": "Customizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/customizations/checkpoints",
    "code_snippets": [
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "A photo of an Australian cattle dog running through a park",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "checkpoint": "octoai:samaritan",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_sdxl(
        prompt="A photo of an Australian cattle dog running through a park",
        negative_prompt="Blurry photo, distortion, low-res, poor quality",
        checkpoint="octoai:samaritan",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
        use_refiner=True,
        high_noise_frac=0.8,
        style_preset="base",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt: "A photo of an Australian cattle dog running through a park",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  checkpoint: "octoai:samaritan",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "A photo of an Australian cattle dog running through a park",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "checkpoint": "octoai:samaritan",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_sdxl(
        prompt="A photo of an Australian cattle dog running through a park",
        negative_prompt="Blurry photo, distortion, low-res, poor quality",
        checkpoint="octoai:samaritan",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
        use_refiner=True,
        high_noise_frac=0.8,
        style_preset="base",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt: "A photo of an Australian cattle dog running through a park",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  checkpoint: "octoai:samaritan",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "Custom Stable Diffusion checkpoints are fine-tuned versions of the original model, trained to capture particular styles, subjects, or objects. They are designed to provide users with more control and customization options when generating images. These checkpoints can be tailored to produce images in various styles, such as realistic photography, artwork, or even specific themes like landscapes or portraits.
While checkpoints represent a significant investment in terms of storage and computational resources, they excel in maintaining the desired customizations consistently. OctoAI's Asset Library boasts a rich collection of pre-loaded custom checkpoints, offering a diverse array of styles to enhance your images. Additionally, users have the flexibility to import bespoke checkpoints from external sources, integrating them seamlessly into OctoAI's Asset Library as personalized assets.
The image results with different checkpoints, even using the same prompt, can be significantly different. Using the simple prompt A photo of an Australian cattle dog running through a park, you can see see the results from the SDXL base model (left) and samaritan model (right). The samaritan model represents a 3D-cartoon image style.








Example Code:",
    "description": "Custom checkpoints are fine-tuned versions of the original model and allow users to refine customizations while creating images or videos.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.checkpoints-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/customizations/checkpoints",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Checkpoints",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/customizations",
        "title": "Customizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/customizations/loras",
    "code_snippets": [
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "loras": {
            "octoai:add-detail": 0.3,
            "octoai:more_art": 1
        },
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "from octoai.util import to_file
from octoai.client import OctoAI

client = OctoAI()

image_gen_response = client.image_gen.generate_sdxl(
    prompt="Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details",
    negative_prompt="Blurry photo, distortion, low-res, poor quality",
    loras={"octoai:add-detail":0.3,"octoai:more_art":1},
    width=1024,
    height=1024,
    num_images=1,
    sampler="DDIM",
    steps=30,
    cfg_scale=12,
    use_refiner=True,
    high_noise_frac=0.8,
    style_preset="base",
)
images = image_gen_response.images

for i, image in enumerate(images):
    to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt:
    "Commercial photography, snowy, luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  loras: {
    "octoai:add-detail": 0.3,
    "octoai:more_art": 1,
  },
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "loras": {
            "octoai:add-detail": 0.3,
            "octoai:more_art": 1
        },
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "from octoai.util import to_file
from octoai.client import OctoAI

client = OctoAI()

image_gen_response = client.image_gen.generate_sdxl(
    prompt="Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details",
    negative_prompt="Blurry photo, distortion, low-res, poor quality",
    loras={"octoai:add-detail":0.3,"octoai:more_art":1},
    width=1024,
    height=1024,
    num_images=1,
    sampler="DDIM",
    steps=30,
    cfg_scale=12,
    use_refiner=True,
    high_noise_frac=0.8,
    style_preset="base",
)
images = image_gen_response.images

for i, image in enumerate(images):
    to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt:
    "Commercial photography, snowy, luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  loras: {
    "octoai:add-detail": 0.3,
    "octoai:more_art": 1,
  },
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "LoRAs are additional custom weights applied to a base checkpoint. Similar to checkpoints, LoRAs can represent a specific style or custom subject, but they are much smaller in size and more economical to use. You can include multiple LoRAs in a single image generation, and provide a weight for each LoRA. A greater weight value will have more influence on the generated image. Similar to checkpoints, users have the flexibility to import LoRAs from external sources, and integrate them seamlessly into OctoAI's Asset Library as personalized assets.
Below is an example of using a LoRA along with a simple prompt Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details. You can see the results from the SDXL base model (top left) and subsequent results with add-detail LoRA - varying weights (top right and bottom left). Add-details LoRA adds intricate details to the output image. The image generated on the botton right is a result of two LoRAs, add-details (weight:0.3) and more-art (weight:1.0). More-art LoRA adds artistic details to the output image. You can clearly see the impact of more-art LoRA over add-detail LoRA in the resulting image.














Example Code:


LoRAs can further customize your images, by including custom objects or styles.",
    "description": "LoRAs for image or video AI models are custom weights applied to a base checkpoint. LoRAs are a way to make highly customized AI images or videos.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.lo-r-as-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/customizations/loras",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "LoRAs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/customizations",
        "title": "Customizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/customizations/textual-inversions",
    "code_snippets": [
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
        "textual_inversions": {
            "octoai:NegativeXL": "“negativeXL_D”"
        },
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

client = OctoAI()

image_gen_response = client.image_gen.generate_sdxl(
    prompt="Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details",
    negative_prompt="Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
    textual_inversions={"octoai:NegativeXL":"“negativeXL_D”"},
    width=1024,
    height=1024,
    num_images=1,
    sampler="DDIM",
    steps=30,
    cfg_scale=12,
    use_refiner=True,
    high_noise_frac=0.8,
    style_preset="base",
)
images = image_gen_response.images

for i, image in enumerate(images):
    to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt:
    "Commercial photography, snowy, luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
  negativePrompt:
    "Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
  textualInversions: {
    "octoai:NegativeXL": "negativeXL_D",
  },
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
        "textual_inversions": {
            "octoai:NegativeXL": "“negativeXL_D”"
        },
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

client = OctoAI()

image_gen_response = client.image_gen.generate_sdxl(
    prompt="Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details",
    negative_prompt="Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
    textual_inversions={"octoai:NegativeXL":"“negativeXL_D”"},
    width=1024,
    height=1024,
    num_images=1,
    sampler="DDIM",
    steps=30,
    cfg_scale=12,
    use_refiner=True,
    high_noise_frac=0.8,
    style_preset="base",
)
images = image_gen_response.images

for i, image in enumerate(images):
    to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt:
    "Commercial photography, snowy, luxury perfume bottle, angelic silver light, studio light, high resolution photography, fine details",
  negativePrompt:
    "Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)",
  textualInversions: {
    "octoai:NegativeXL": "negativeXL_D",
  },
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "Textual inversions are embeddings that represent custom subjects.They can also represent negative embeddings, which are trained on undesirable content like bad quality hands or lighting. You can use these in your negative prompt to improve your images, such as avoiding bad quality hands. These are the smallest and cheapest assets we currently support.
The name of the textual inversion acts as a specific trigger word, which must be included in the prompt. Similar to prompt weighting, you can increase the weight of textual inversion using the format (textual-inversion:weight).
Below is an example of using a NegativeXL textual inversion (trigger word: negativeXL_D).
Prompt:  Commercial photography,snowy,luxury perfume bottle,angelic silver light, studio light, high resolution photography, fine details
Negative prompt: Blurry photo, distortion, low-res, poor quality, (flowers on bottle negativeXL_D:0.9), (snowflake on bottle negativeXL_D:0.9), (colored liquid in bottle negativeXL_D:1.0)
You can see the results from the SDXL base model (left) generating an image with snowflake or flower shaped designs on the bottle and subsequent results with textual inversion (right) which ensures that the negative prompt is followed and no flower, snowflake design appears on the bottle in the output image.








Example Code:",
    "description": "Customize your images on OctoAI using Textual inversions, which are embeddings that represent custom subjects.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.textual-inversions-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/customizations/textual-inversions",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Textual Inversions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/customizations",
        "title": "Customizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/customizations/prompt-weighting",
    "content": "You can emphasize, or de-emphasize, specific words or phrases of the image generation prompt using weighting. To use prompt weighting, format your prompt using parentheses: prompt = "A cat with (long whiskers)"
This emphasizes the phrase “long whiskers” with a weight of 1.1. Adding additional parentheses such as "\(\(\(long whiskers)))" performs additional multiples of 1.1, so for 3 sets of parentheses, the weight would be 1.33. More specific weights can also be specified in the form: prompt = "A cat with (long whiskers: 0.8)"
This will weigh all words in the parentheses by a factor of 0.8. Notably, weights do not have to be greater than one. Using a weight of less than 1 will de-emphasize the contained words.
Using weights in negative prompts can also be helpful. For example, you can avoid distorted hands: negative_prompt = "(distorted hands: 1.5)"",
    "description": "OctoAI allows for prompt weighting, the emphasis or de-emphasis of certain words or phrases, in prompts to create customized images.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.prompt-weighting-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/customizations/prompt-weighting",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Prompt Weighting",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/customizations",
        "title": "Customizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/customizations/controlnets",
    "code_snippets": [
      {
        "code": "octoai:canny_sdxl
octoai:depth_sdxl
octoai:openpose_sdxl
octoai:canny_sd15
octoai:depth_sd15
octoai:inpaint_sd15
octoai:ip2p_sd15
octoai:lineart_sd15
octoai:openpose_sd15
octoai:scribble_sd15
octoai:tile_sd15",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/controlnet-sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "A photo of woman wearing a (rose  pink dress:1)",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "controlnet": "octoai:canny_sdxl",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base",
        "controlnet_conditioning_scale": 1,
        "controlnet_image": "<BASE64 IMAGE>"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_controlnet_sdxl(
        prompt="A photo of woman wearing a (rose  pink dress:1)",
        negative_prompt="Blurry photo, distortion, low-res, poor quality",
        controlnet="octoai:canny_sdxl",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
        use_refiner=True,
        high_noise_frac=0.8,
        style_preset="base",
        controlnet_conditioning_scale=1,
        controlnet_image="<BASE64 IMAGE>",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateControlnetSdxl({
  prompt: "A photo of a woman wearing a (rose pink dress:1)",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  controlnet: "octoai:canny_sdxl",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
  controlnetConditioningScale: 1,
  controlnetImage: "<BASE64_IMAGE>",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.image_b64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/controlnet-sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "A photo of woman wearing a (rose  pink dress:1)",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "controlnet": "octoai:canny_sdxl",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base",
        "controlnet_conditioning_scale": 1,
        "controlnet_image": "<BASE64 IMAGE>"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_controlnet_sdxl(
        prompt="A photo of woman wearing a (rose  pink dress:1)",
        negative_prompt="Blurry photo, distortion, low-res, poor quality",
        controlnet="octoai:canny_sdxl",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
        use_refiner=True,
        high_noise_frac=0.8,
        style_preset="base",
        controlnet_conditioning_scale=1,
        controlnet_image="<BASE64 IMAGE>",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateControlnetSdxl({
  prompt: "A photo of a woman wearing a (rose pink dress:1)",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  controlnet: "octoai:canny_sdxl",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
  controlnetConditioningScale: 1,
  controlnetImage: "<BASE64_IMAGE>",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.image_b64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/controlnet-sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "An photo of a white man on a japanese tatami mat ",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality, distorted legs, distorted feet, disproportionate  hands and ",
        "controlnet": "octoai:openpose_sdxl",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base",
        "controlnet_conditioning_scale": 1,
        "controlnet_image": "<BASE64 IMAGE>"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_controlnet_sdxl(
        prompt="An photo of a white man on a japanese tatami mat ",
        negative_prompt="Blurry photo, distortion, low-res, poor quality, distorted legs, distorted feet, disproportionate  hands and ",
        controlnet="octoai:openpose_sdxl",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
        use_refiner=True,
        high_noise_frac=0.8,
        style_preset="base",
        controlnet_conditioning_scale=1,
        controlnet_image="<BASE64 IMAGE>",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateControlnetSdxl({
  prompt: "A photo of a white man on a Japanese tatami mat",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  controlnet: "octoai:openpose_sdxl",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
  controlnetConditioningScale: 1,
  controlnetImage: "<BASE64_IMAGE>",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.image_b64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/controlnet-sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "An photo of a white man on a japanese tatami mat ",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality, distorted legs, distorted feet, disproportionate  hands and ",
        "controlnet": "octoai:openpose_sdxl",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base",
        "controlnet_conditioning_scale": 1,
        "controlnet_image": "<BASE64 IMAGE>"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_controlnet_sdxl(
        prompt="An photo of a white man on a japanese tatami mat ",
        negative_prompt="Blurry photo, distortion, low-res, poor quality, distorted legs, distorted feet, disproportionate  hands and ",
        controlnet="octoai:openpose_sdxl",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
        use_refiner=True,
        high_noise_frac=0.8,
        style_preset="base",
        controlnet_conditioning_scale=1,
        controlnet_image="<BASE64 IMAGE>",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateControlnetSdxl({
  prompt: "A photo of a white man on a Japanese tatami mat",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  controlnet: "octoai:openpose_sdxl",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
  useRefiner: true,
  highNoiseFrac: 0.8,
  stylePreset: "base",
  controlnetConditioningScale: 1,
  controlnetImage: "<BASE64_IMAGE>",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.image_b64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "While traditional image generation models can produce stunning visuals, they often lack guidance, and therefore the ability to generate images subject to user-desired image composition. ControlNet changes the game by allowing an additional image input that can be used for conditioning (influencing) the final image generation. This could be anything from simple scribbles to detailed depth maps or edge maps. By conditioning on these input images, ControlNet directs the Stable Diffusion model to generate images that align closely with the user's intent.
OctoAI's Asset Library comes pre-populated with the followinglist of public controlnets.
Other than using the default controlnet checkpoints, you can also upload private ControlNet checkpoints into the OctoAI Asset Library and then use those checkpoints at generation time via the parameter controlnet in the API. For custom controlnet checkpoints, make sure to provide your own ControlNet mask in the controlnet_image parameter.
Below is an example of using a Canny ControlNet along with ControlNet image (left) and a simple prompt A photo of woman wearing a (rose  pink dress:1). Canny ControlNet is designed to detect a wide range of edges in images. Given a raw image or sketch, Canny can extract the image's contours and edges, and use them for image generation. You can see the image (right) generated from SDXL with Canny ControlNet applied.








Example Code for Canny ControlNet:


Below is an example of using a OpenPose ControlNet along with ControlNet image (left) and a prompt An photo of a white man on a japanese tatami mat . OpenPose ControlNet is a fast human keypoint detection model that can extract human poses like positions of hands, legs, and head. See the example below. You can see the image (right) generated from SDXL with OpenPose ControlNet applied.








Example Code for OpenPose ControlNet:",
    "description": "OctoAI's asset library is pre-populated with the most popular available ControlNets which allow added image input to influence and customize the image generation.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.customizations.control-nets-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/customizations/controlnets",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "ControlNets",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/optimizations",
        "title": "Optimizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/optimizations/sdxl-lighting",
    "code_snippets": [
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "((glass orb)) with snowy christmas scene in it ",
        "negative_prompt": "ornament, Blurry, low-res, poor quality",
        "checkpoint": "octoai:lightning_sdxl",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 8,
        "cfg_scale": 3,
        "seed": 3327823665,
        "use_refiner": false,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_sdxl(
        prompt="((glass orb)) with snowy christmas scene in it ",
        negative_prompt="ornament, Blurry, low-res, poor quality",
        checkpoint="octoai:lightning_sdxl",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=8,
        cfg_scale=3,
        seed=3327823665,
        use_refiner=False,
        style_preset="base",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt: "((glass orb)) with snowy Christmas scene in it",
  negativePrompt: "ornament, blurry, low-res, poor quality",
  checkpoint: "octoai:lightning_sdxl",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 8,
  cfgScale: 3,
  seed: 3327823665,
  useRefiner: false,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "((glass orb)) with snowy christmas scene in it ",
        "negative_prompt": "ornament, Blurry, low-res, poor quality",
        "checkpoint": "octoai:lightning_sdxl",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 8,
        "cfg_scale": 3,
        "seed": 3327823665,
        "use_refiner": false,
        "style_preset": "base"
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_sdxl(
        prompt="((glass orb)) with snowy christmas scene in it ",
        negative_prompt="ornament, Blurry, low-res, poor quality",
        checkpoint="octoai:lightning_sdxl",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=8,
        cfg_scale=3,
        seed=3327823665,
        use_refiner=False,
        style_preset="base",
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSdxl({
  prompt: "((glass orb)) with snowy Christmas scene in it",
  negativePrompt: "ornament, blurry, low-res, poor quality",
  checkpoint: "octoai:lightning_sdxl",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 8,
  cfgScale: 3,
  seed: 3327823665,
  useRefiner: false,
  stylePreset: "base",
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "SDXL Lighting enable you to achieve high-quality output from SDXL with only 4-8 steps and less than 1 second of latency. This stands in stark contrast to generating images using SDXL model, which typically demands 30-40 steps to produce a good quality image in 3-3.5 seconds, and even more when incorporating custom assets like LoRAs. More importantly, SDXL Lighting is completely compatible with existing image customization techniques available on OctoAI such as SDXL LoRAs. Customers using this can achieve both fast inference speed and product differentiation via customization.
You can use SDXL Lighting via OctoAI's Image Gen API
The following guidelines must be adhered to ensure high quality of output:
Opt for 4-8 steps, with 8 steps being the recommended choice.

Maintain a low CFG Scale, ideally ranging from 1.4 to 4.0, with 3.0 as the optimal value.

Utilize any sampler, textual inversion, or style preset.

Note that additional LoRAs and VAEs will result in increased inference time.


Below is an example for text2img with SDXL Lighting:
Example Code:",
    "description": "A new technology called SDXL Lighting enables high-quality image generations in less than 1 second",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.optimizations.sdxl-lighting-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/optimizations/sdxl-lighting",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "SDXL Lighting for blazing fast generations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/optimizations",
        "title": "Optimizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/optimizations/ssd",
    "code_snippets": [
      {
        "code": "curl -X POST "https://image.octoai.run/generate/ssd" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "An image of a deLorean car in a city setting",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_ssd(
        prompt="An image of a deLorean car in a city setting",
        negative_prompt="Blurry photo, distortion, low-res, poor quality",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSsd({
  prompt: "An image of a DeLorean car in a city setting",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/ssd" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data-raw '{
        "prompt": "An image of a deLorean car in a city setting",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12
    }' | jq -r ".images[0].image_b64" | base64 -d >result.jpg",
        "lang": "bash",
        "meta": "cURL",
      },
      {
        "code": "import os
from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI(api_key=os.environ.get("OCTOAI_TOKEN"))
    image_gen_response = client.image_gen.generate_ssd(
        prompt="An image of a deLorean car in a city setting",
        negative_prompt="Blurry photo, distortion, low-res, poor quality",
        width=1024,
        height=1024,
        num_images=1,
        sampler="DDIM",
        steps=30,
        cfg_scale=12,
    )
    images = image_gen_response.images

    for i, image in enumerate(images):
        to_file(image, f"result{i}.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const { images } = await octoai.imageGen.generateSsd({
  prompt: "An image of a DeLorean car in a city setting",
  negativePrompt: "Blurry photo, distortion, low-res, poor quality",
  width: 1024,
  height: 1024,
  numImages: 1,
  sampler: "DDIM",
  steps: 30,
  cfgScale: 12,
});

images.forEach((output, i) => {
  if (output.imageB64) {
    const buffer = Buffer.from(output.imageB64, "base64");
    fs.writeFileSync(`result${i}.jpg`, buffer);
  }
});",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "The primary advantage of utilizing Stable Diffusion SSD lies in its remarkable efficiency, boasting a 50% faster speed compared to SDXL, owing to its more compact design. Moreover, OctoAI has implemented its exclusive compiler and cloud system optimizations, culminating in an unparalleled end-to-end average generation speed of 1.4 seconds — establishing it as the swiftest iteration of SSD-1B available in the market.
However, it's essential to acknowledge certain limitations inherent to the SSD-1B model. Due to its distillation into a smaller form, the resultant output images deviate from those produced by SDXL. In essence, even with identical seed and API parameters, replicating the same output images as SDXL is no longer feasible. Furthermore, the community surrounding this novel model remains comparatively modest in size compared to that of SDXL. Consequently, the availability of finely-tuned assets such as checkpoints and LoRAs for tailoring styles, objects, and facial features is notably limited.
Example Code:",
    "description": "SSD-1B, a distilled version of SDXL generates images 50% faster than SDXL.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.optimizations.ssd-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/optimizations/ssd",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Stable Diffusion SSD",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/optimizations",
        "title": "Optimizations",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/optimizations/samplers",
    "content": "Stable Diffusion Samplers encompass various techniques within generative modeling, each offering unique advantages and applications. These samplers operate by iteratively refining noise to generate high-quality images. By gradually diffusing noise while conditioning on observed data, these samplers excel at capturing intricate details and complex patterns, yielding visually stunning outputs across various domains.
When selecting a sampler, considerations such as computational efficiency, convergence speed, and applicability to specific tasks are crucial. The choice of sampler depends on the specific requirements and constraints of the task at hand, with practitioners often experimenting to find the optimal balance between performance and efficacy.
Supported samplers within OctoAI's Image Gen API include: DDIM,DDPM,DPM_PLUS_PLUS_2M_KARRAS,DPM_SINGLE,DPM_SOLVER_MULTISTEP,K_EULER, K_EULER_ANCESTRAL,PNDM,UNI_PC. These are regular samplers. Premium samplers (priced twice as the regular samplers) include DPM_2, DPM_2_ANCESTRAL,DPM_PLUS_PLUS_SDE_KARRAS, HEUN and KLMS.",
    "description": "Stable Diffusion Samplers utilize diffusion models to iteratively refine noise, producing high-quality images with remarkable fidelity and coherence.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.optimizations.samplers-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/optimizations/samplers",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Samplers",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "content": "The ImageGenClient class specializes in supporting image generation in your application, and guiding what options are available to modify your outputs. It will return a list of all images using the ImageGeneration type. It allows you to use both Stable Diffusion 1.5 and Stable Diffusion XL for text to image and image to image use cases, and set parameters and prompts either with weighted prompts with the prompt field as was common with Stable Diffusion 1.5 or human-readable descriptions using prompt_2 with Stable Diffusion XL 1.0.
This guide will walk you through a text to image example, and then we will use the resulting image to demonstrate the image to image use case.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-root-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/image-generator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Image Generator Python client",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "content": "First, create an OctoAI API token.

Then, complete Python SDK Installation & Setup..
If you use the OCTOAI_TOKEN envvar for your token, you can instantiate the image_gen client with client = OctoAI().image_gen",
    "domain": "test.com",
    "hash": "#requirements",
    "hierarchy": {
      "h0": {
        "title": "Image Generator Python client",
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-requirements-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/image-generator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "code_snippets": [
      {
        "code": "from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    # If OCTOAI_TOKEN is not set as an envvar, you can also pass a token to the client with:
    # ImageGenerator(token="YOUR_TOKEN_HERE")
    client = OctoAI()
    # images is a list of Images from octoai.types
    image_resp = client.image_gen.generate_sdxl(
        prompt="photorealistic, poodle, intricately detailed"
    )
    images = image_resp.images

    # images can be filtered for safety, so since we only generated 1 image by default, this verifies
    # we actually have an image to show.
    if not images[0].removed_for_safety:
        to_file(images[0], "output.jpg")",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "After running this simple prompt, you should hopefully have an output somewhat similar to the image below:
astropus.png
A good start and in our next example, we'll use more features to help guide our outputs.",
    "domain": "test.com",
    "hash": "#simple-text-to-image-generation-example",
    "hierarchy": {
      "h0": {
        "title": "Image Generator Python client",
      },
      "h4": {
        "id": "simple-text-to-image-generation-example",
        "title": "Simple Text to Image Generation Example",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-simple-text-to-image-generation-example-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/image-generator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Simple Text to Image Generation Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "code_snippets": [
      {
        "code": "from octoai.util import to_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI()

    prompt = "photorealistic, colorful, poodle, intricately detailed"
    file_name = "pretty_poodle_cinematic.jpeg"

    images_resp = client.image_gen.generate_sdxl(
        prompt=prompt,
        negative_prompt="horror, scary, low-quality, extra limbs, cartoon",
        checkpoint="crystal-clear",
        style_preset="cinematic",
        loras={"add-detail": 1.0},
        steps=50,
    )
    images = images_resp.images
    # It can also be helpful to run another generate method with
    # num_images = image_resp.removed_for_safety to get your desired total images
    if not images[0].removed_for_safety:
        to_file(images[0], file_name)
",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "One of the simplest ways to customize your outputs is a style preset, negative_prompt, loras, and model selection.
astropus.png
Much more realistic! Now that we have our cinematic poodle, let's go ahead and use this image as part of an image-to-image workflow.",
    "domain": "test.com",
    "hash": "#text-to-image-generation-example",
    "hierarchy": {
      "h0": {
        "title": "Image Generator Python client",
      },
      "h4": {
        "id": "text-to-image-generation-example",
        "title": "Text to Image Generation Example",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-text-to-image-generation-example-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/image-generator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text to Image Generation Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/image-generator-client",
    "code_snippets": [
      {
        "code": "from octoai.util import to_file, from_file
from octoai.client import OctoAI

if __name__ == "__main__":
    client = OctoAI()

    init = from_file("pretty_poodle_cinematic.jpeg")
    images_resp = client.image_gen.generate_sdxl(
        prompt="corgi in the rain",
        init_image=init,  # Only used for image-to-image
        strength=0.8,  # Only used for image-to-image
        style_preset="anime"
    )
    images = images_resp.images

    to_file(images[0], "rain_corgi.jpeg")",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "Image to Image Generation lets you use a base image, in our case the above pretty_poodle.jpeg to shape the feel of your outputs image. In our case, we'd expect some focal point in the center, and a blurred, bright background, but otherwise our output can look anywhere from completely different or quite similar depending on our prompt. In this case, let's go for a complete different style of outputs and stray from the usual theme of poodles to a corgi in the rain.
astropus.png",
    "domain": "test.com",
    "hash": "#image-to-image-generation-example",
    "hierarchy": {
      "h0": {
        "title": "Image Generator Python client",
      },
      "h4": {
        "id": "image-to-image-generation-example",
        "title": "Image to Image Generation Example",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.image-generator-python-client-image-to-image-generation-example-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/image-generator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Image to Image Generation Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "OctoAI lets you fine-tune Stable Diffusion to customize generated images. Fine-tuning is a process of training a model with additional data for your task. There's a few simple steps:
Configure fine-tuning settings & upload your images

Run the fine-tuning job

Use the fine-tuned asset (a LoRA) in your image generation requests


We're using the LoRA fine-tuning method, which is an acronym for Low-Rank Adaptation. It's a fast and effective way to fine-tune Stable Diffusion. Fine-tuning is supported for Stable Diffusion v1.5 and Stable Diffusion XL. Fine-tuning is available in the OctoAI web UI or via the fine-tuning API.",
    "description": "Create custom assets with OctoAI's fine-tuning of Stable Diffusion models.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Fine-tuning Stable Diffusion",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "In the web UI, navigate to the Tuning & Datasets page from the Media Gen Solution menu to get started - any previously tuned models will also be listed here. Click on “New Tune” to continue.",
    "domain": "test.com",
    "hash": "#web-ui-guide",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "web-ui-guide",
        "title": "Web UI Guide",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-web-ui-guide-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Web UI Guide",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "Specify the name of your fine-tune, the trigger word of the subject you're fine-tuning, and the base checkpoint. The base checkpoint can be the default Stable Diffusion v1.5 checkpoint, default Stable Diffusion XL checkpoint, or any custom checkpoint.
The trigger word can be used in your inference requests to customize the images with your subject. We generally recommend using a unique trigger word, such as "sks1", that's unlikely to be associated with a different subject in Stable Diffusion. Alternatively, you can use an existing concept as the trigger word value - such as "in the style of a cartoon drawing" - to update Stable Diffusion's understanding of that concept.
Then, specify the number of steps to train. A range of 400 to 1,200 steps works well in most cases, and a good guideline is about 75 to 100 steps per training image. The model can underfit if the numer of training steps is too low, resulting in poor quality. If it's too high, the model can overfit and struggle to represent details that aren't represented in the training images.",
    "domain": "test.com",
    "hash": "#configure-settings",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "web-ui-guide",
        "title": "Web UI Guide",
      },
      "h4": {
        "id": "configure-settings",
        "title": "Configure settings",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-configure-settings-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Configure settings",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "Next, upload your training images and start tuning. We recommend using varied images, including different backgrounds, lightings, and distances. Finding a balance between variation and consistency can help improve image generation quality. All uploaded images used for fine-tuning must comply with our terms of service.
Optionally, you can provide captions for each image that describe the custom subject. This can help improve fine-tuning and the quality of generated images. Make sure to include your trigger word in the caption.
When you're ready, click "Start Tuning", and the fine-tune job will progress from pending to running before completing.",
    "domain": "test.com",
    "hash": "#upload-images--tune",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "web-ui-guide",
        "title": "Web UI Guide",
      },
      "h4": {
        "id": "upload-images--tune",
        "title": "Upload images & tune",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-upload-images--tune-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Upload images & tune",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "When complete, the fine-tuned asset is stored in your Asset Library and available for image generation. You can launch the Text to Image or Image to Image tool to start generating images with your custom asset.",
    "domain": "test.com",
    "hash": "#generating-images",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "web-ui-guide",
        "title": "Web UI Guide",
      },
      "h4": {
        "id": "generating-images",
        "title": "Generating images",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-generating-images-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Generating images",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "Complete fine-tuning API parameters are organized in our API Reference documentation.",
    "domain": "test.com",
    "hash": "#api-guide",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-api-guide-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "API Guide",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "code_snippets": [
      {
        "code": "from octoai.client import OctoAI
from octoai.asset_library import Data_File

client = OctoAI()

asset = client.asset_library.create_from_file(
    file="finetuning_images/image1.jpeg",
    data=Data_File(file_format="jpeg"),
    name="image1",
    description="Fine-tuning image",
)

print(asset)
print(client.asset_library.list(name="image1"))

client.asset_library.list()",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "id: asset_01hf3a2qw7ek6vshbhd9c9yywd, name: image1, status: Status.UPLOADED",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import os
from octoai.client import OctoAI
from octoai.asset_library import Data_File

if __name__ == "__main__":
    # OCTOAI_TOKEN set as an environment variable so do not need to pass a token.
    client = OctoAI()

    dir_path = "./finetuning_images/"  # Set your dir_path here to your file assets.
    files = []
    # Get a list of files in the folder
    for file_path in os.listdir(dir_path):
        if os.path.isfile(os.path.join(dir_path, file_path)):
            files.append(file_path)
    for file in files:
        # Use the file names to get file_format and the asset_name.
        split_file_name = file.split(".")
        asset_name = split_file_name[0]
        file_format = split_file_name[1]
        file_data = Data_File(
            file_format=file_format,
        )
        asset = client.asset_library.create_from_file(
            file=dir_path + file,
            data=file_data,
            name=asset_name,
        )

        print(asset)",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "octoai asset create \
  --upload-from-file finetuning_images/image1.jpeg \
  --name image1 \
  --type file",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "First, upload your training images using the AssetLibrary Python Client or CLI.
Python client
You can easily upload individual image files, or a folder with multiple files. Here's an example uploading the image1.jpeg file with the name image1 from the file path finetuning_images:
You'll receive a response with the asset ID, name, and status:
Here's an example uploading a folder of images. This code snippet gets the files in the folder named finetuning_images, then splits on the . to get the files_format extension (jpg, jpeg, or png). Then the file names are used to set the asset names:
The final print(asset) will return a response with each asset ID, name, and status:
CLI
Alternatively, you can upload images using the OctoAI CLI. Here's the CLI command using the same image1.jpeg example:",
    "domain": "test.com",
    "hash": "#upload-images",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide",
      },
      "h4": {
        "id": "upload-images",
        "title": "Upload images",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-upload-images-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Upload images",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "code_snippets": [
      {
        "code": "from octoai.fine_tuning import Details_LoraTune, LoraTuneCheckpoint, LoraTuneFile
from octoai.client import OctoAI

if __name__ == "__main__":
  client = OctoAI()

  # create a fine tuning job
  tune = client.fine_tuning.create(
    name="sks1-bulldog-01",
    details=Details_LoraTune(
        files=[
            LoraTuneFile(file_id="asset_01hekxwwg1fzjv48sfy5s2xmnj", caption="sks1 bulldog playing at the beach"),
            LoraTuneFile(file_id="asset_01hekxwqgrev5t3ser2w2qf8bm", caption="sks1 bulldog playing with a bone"),
            LoraTuneFile(file_id="asset_01hekxwj3bekpvr52kne3c6ca7", caption="sks1 bulldog looking up"),
            LoraTuneFile(file_id="asset_01hekxwdc8eqrrahjm3ekze356", caption="sks1 bulldog resting in the grass"),
            LoraTuneFile(file_id="asset_01hekxw80kfdmrdh7ny3vyvf0h", caption="sks1 bulldog running at the park"),
        ],
        base_checkpoint=LoraTuneCheckpoint(
            checkpoint_id="asset_01hdpjv7bxe1n99eazrv23ca1k",
            engine="image/stable-diffusion-xl-v1-0",
        ),
        trigger_words=["sks1"],
        steps=800,
    )
  )
  print(f"Tune {tune.name} status: {tune.status}")

  # check the status of a fine tuning job
  tune = client.fine_tuning.get(tune.id)
  print(f"Tune {tune.name} status: {tune.status}")

  # when the job finishes, check the asset ids of the resulted loras
  # (the tune will take some time to complete)
  if tune.status == "succeded":
    print(f"Generated LoRAs: {tune.output_lora_ids}")
",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "curl -X POST "https://api.octoai.cloud/v1/tune" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OCTOAI_TOKEN" \
  --data '
{
  "details": {
    "base_checkpoint": {
      "name": "default-sdxl",
      "checkpoint_id": "asset_01hdpjv7bxe1n99eazrv23ca1k"
    },
    "tune_type": "lora_tune",
    "files": [
      {
        "file_id": "asset_01hekxwwg1fzjv48sfy5s2xmnj",
        "caption": "sks1 bulldog playing at the beach"
      },
      {
        "file_id": "asset_01hekxwqgrev5t3ser2w2qf8bm",
        "caption": "sks1 bulldog playing with a bone"
      },
      {
        "file_id": "asset_01hekxwj3bekpvr52kne3c6ca7",
        "caption": "sks1 bulldog looking up"
      },
      {
        "file_id": "asset_01hekxwdc8eqrrahjm3ekze356",
        "caption": "sks1 bulldog resting in the grass"
      },
      {
        "file_id": "asset_01hekxw80kfdmrdh7ny3vyvf0h",
        "caption": "sks1 bulldog running at the park"
      }
    ],
    "trigger_words": [
      "sks1"
    ],
    "steps": 800
  },
  "tune_type": "lora_tune",
  "description": "sks1 bulldog",
  "name": "sks1-bulldog-01"
}",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "Next, create your fine-tune. In the examples in this section, we're fine-tuning Stable Diffusion XL with images of a bulldog. We specify the base checkpoint, trigger word, training steps, and fine-tune name. Also included are the individual training images and corresponding captions. We recommend including captions, describing the context of the subject, to improve fine-tuning quality. Be sure to include your trigger word within the caption.
Python Client
REST API
Full API details and parameters are available here. Using the continue_on_rejection boolean parameter, you can optionally continue with the fine-tune job if any of the training images are identified as NSFW.
You'll receive a response that includes the tune ID, which you can use to monitor the status of the job.",
    "domain": "test.com",
    "hash": "#configure-settings--tune",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide",
      },
      "h4": {
        "id": "configure-settings--tune",
        "title": "Configure settings & tune",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-configure-settings--tune-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Configure settings & tune",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "code_snippets": [
      {
        "code": "curl "https://api.octoai.cloud/v1/tune/tune_01hen39pazf6s9jqpkfj05y0vx" \
 -H "Authorization: Bearer $OCTOAI_TOKEN"",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "You can check the status of a fine-tune job by running a GET on the tune ID:",
    "domain": "test.com",
    "hash": "#monitor-fine-tuning-status",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide",
      },
      "h4": {
        "id": "monitor-fine-tuning-status",
        "title": "Monitor fine-tuning status",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-monitor-fine-tuning-status-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Monitor fine-tuning status",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "code_snippets": [
      {
        "code": "curl -X POST "<https://image.octoai.run/generate/sdxl">
    -H "Content-Type: application/json"
    -H "Authorization: Bearer $OCTOAI_TOKEN"
    --data-raw '{
        "prompt": "A photo of a sks1 bulldog running in space",
        "negative_prompt": "Blurry photo, distortion, low-res, poor quality",
        "loras": {
            "sks1-bulldog-01": 0.9
        },
        "width": 1024,
        "height": 1024,
        "num_images": 1,
        "sampler": "DDIM",
        "steps": 30,
        "cfg_scale": 12,
        "use_refiner": true,
        "high_noise_frac": 0.8,
        "style_preset": "base"
    }'",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "When complete, the fine-tuned asset is stored in your Asset Library and available for image generation. You can generate images by including the LoRA in your image generation request.",
    "domain": "test.com",
    "hash": "#generating-images-1",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "api-guide",
        "title": "API Guide",
      },
      "h4": {
        "id": "generating-images-1",
        "title": "Generating images",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-generating-images-1-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Generating images",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "Image captions can improve quality by providing additional context and details to the trained model. Whenever possible, we suggest using image captions - and be sure to include your trigger word in each caption.
You can also include the subject class - such as person, animal, or object - to improve quality. As an example, you could fine-tune images of a specific bulldog with an example caption sks1 bulldog playing at the beach where sks1 is the trigger word and bulldog is the subject class.",
    "domain": "test.com",
    "hash": "#image-captions",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "general-fine-tuning-tips",
        "title": "General fine-tuning tips",
      },
      "h4": {
        "id": "image-captions",
        "title": "Image captions",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-image-captions-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Image captions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "We recommend some amount of variation in your images. If every image is close-up, the fine-tuned model may be limited to representing that distance. It's also helpful to have some level of consistency among the images to ensure the model learns the intended subject. Finding the right balance between consistency and variation can require a few iterations, and we encourage you to experiment!",
    "domain": "test.com",
    "hash": "#image-variation",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "general-fine-tuning-tips",
        "title": "General fine-tuning tips",
      },
      "h4": {
        "id": "image-variation",
        "title": "Image variation",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-image-variation-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Image variation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "You can separately manage a fine-tune job, and the corresponding LoRA created by a fine-tune. Deleting a fine-tune job won't automatically delete the training images nor LoRA created during fine-tuning, and vice versa. Additionally, fine-tune names must be unique. You may encounter an error if you try to create a fine-tune with a duplicate name.",
    "domain": "test.com",
    "hash": "#managing-fine-tunes-and-assets",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "general-fine-tuning-tips",
        "title": "General fine-tuning tips",
      },
      "h4": {
        "id": "managing-fine-tunes-and-assets",
        "title": "Managing fine-tunes and assets",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-managing-fine-tunes-and-assets-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Managing fine-tunes and assets",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "content": "Fine-tuning Stable Diffusion v1.5 usually takes about 3-7 minutes, and Stable Diffusion XL takes about 10-20 minutes. Increasing the number of training steps will extend the fine-tuning duration.",
    "domain": "test.com",
    "hash": "#fine-tuning-duration",
    "hierarchy": {
      "h0": {
        "title": "Fine-tuning Stable Diffusion",
      },
      "h3": {
        "id": "general-fine-tuning-tips",
        "title": "General fine-tuning tips",
      },
      "h4": {
        "id": "fine-tuning-duration",
        "title": "Fine-tuning duration",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.overview-fine-tuning-duration-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/fine-tuning-stable-diffusion",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Fine-tuning duration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "content": "This guide will walk you through creating a fine-tuned LoRA using our TypeScript SDK to upload image file assets, creating a tune, and then using that LoRA to run an inference using our Image Gen service once it's ready.
Please see Fine-tuning Stable Diffusion for more specifics about each parameter in the fine-tuning API, using curl, or the Python SDK. Our Asset Library API reference documentation goes more into the specifics of using different asset methods as well.",
    "description": "How to create a fine-tuned LoRA using OctoAI's TypeScript SDK",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "TypeScript SDK Fine-tuning",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "content": "Please create an OctoAI API token if you don't have one already.

Please also verify you've completed TypeScript SDK Installation & Setup.

If you use the OCTOAI_TOKEN environment variable for your token, you can instantiate the client with const octoai = new OctoAIClient() or pass the token as a parameter to the constructor like const octoai = new OctoAIClient({ apiKey: process.env.OCTOAI_TOKEN }).

An account and API token is required for all the following steps.",
    "domain": "test.com",
    "hash": "#requirements",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning",
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-requirements-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "content": "In order to run a LoRA fine-tuning job, you need to complete a few steps:
Create image file assets using the Asset Library, then wait for those assets' status to be ready

Either create a checkpoint asset you would like to use or get one from OctoAI's public checkpoints.

Create a tune job, then wait for the status to be succeeded.

Run an inference with the new LoRA.


Directions with all the code put together are included at the bottom of the document, but at each step we will cover additional information.",
    "domain": "test.com",
    "hash": "#high-level-steps-to-creating-a-fine-tuned-lora",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning",
      },
      "h4": {
        "id": "high-level-steps-to-creating-a-fine-tuned-lora",
        "title": "High-level steps to creating a fine-tuned LoRA",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-high-level-steps-to-creating-a-fine-tuned-lora-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "High-level steps to creating a fine-tuned LoRA",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "code_snippets": [
      {
        "code": "import fs from "node:fs";
import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

const fileNames = fs.readdirSync("./images");

console.log("Uploading files...");

const assets = await Promise.all(
    fileNames.map((fileName) => {
        const imageName = fileName.split(".")[0];

        return octoai.assetLibrary.upload(`./images/${fileName}`, {
            name: imageName,
            description: imageName,
            assetType: "file",
            data: {
                assetType: "file",
                fileFormat: "jpg",
            },
        });
    })
);

console.log("Waiting for assets to be ready...");

for (const { asset } of assets) {
    await octoai.assetLibrary.waitForReady(asset.id);
    console.log(`Asset "${asset.name}" is ready`);
}",
        "lang": "TypeScript",
        "meta": "TypeScript",
      },
    ],
    "content": "Asset Library in the TypeScript SDK covers more specifics about the methods, so this example will be focused on a code snippet for uploading multiple files from a folder at once.
In this example, we will use multiple photos of a toy poodle named Mitchi.
After this completes, all assets will hopefully be in the ready state, or you should time out. Mitchi is now on OctoAI!
astropus.png",
    "domain": "test.com",
    "hash": "#1-creating-image-file-assets",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning",
      },
      "h4": {
        "id": "1-creating-image-file-assets",
        "title": "1) Creating image file assets",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-1-creating-image-file-assets-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Creating image file assets",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "code_snippets": [
      {
        "code": "const checkpoint = await octoai.assetLibrary.get("octoai:default-sd15");",
        "lang": "TypeScript",
        "meta": "TypeScript",
      },
    ],
    "content": "Next, you'll need a checkpoint to use to tune your asset. In this example, we will just use the default checkpoint using Stable Diffusion 1.5, but you can also use other public OctoAI checkpoints or create your own using the Asset Library.",
    "domain": "test.com",
    "hash": "#2-get-a-checkpoint-asset-to-use-for-tuning-our-lora",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning",
      },
      "h4": {
        "id": "2-get-a-checkpoint-asset-to-use-for-tuning-our-lora",
        "title": "2) Get a checkpoint asset to use for tuning our LoRA",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-2-get-a-checkpoint-asset-to-use-for-tuning-our-lora-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Get a checkpoint asset to use for tuning our LoRA",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "code_snippets": [
      {
        "code": "console.log("Creating tune...");

let tune = await octoai.fineTuning.create({
    name: "mitchi",
    description: "mitchi",
    details: {
        tuneType: "lora_tune",
        baseCheckpoint: {
            checkpointId: checkpoint.asset.id,
        },
        // You can add a `caption` to each file for more accurate results
        files: assets.map(({ asset }) => ({ fileId: asset.id })),
        steps: 10,
        triggerWords: ["sksmitchi"],
    },
});

console.log("Waiting for tune to be ready...");

while (tune.status !== "failed" && tune.status !== "succeeded") {
    await new Promise((resolve) => setTimeout(resolve, 1000));
    tune = await octoai.fineTuning.get(tune.id);
}",
        "lang": "TypeScript",
        "meta": "TypeScript",
      },
    ],
    "content": "We can create a tune by passing in the id of the checkpoint we'd like to use and the ids of the file assets that we created in Step 1. If you want more accurate results, you can add captions to each image to give more thorough descriptions. If no custom captions are provided, the trigger word will be used as a default.",
    "domain": "test.com",
    "hash": "#3-creating-a-tune",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning",
      },
      "h4": {
        "id": "3-creating-a-tune",
        "title": "3) Creating a tune",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-3-creating-a-tune-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Creating a tune",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/fine-tuning-stable-diffusion",
        "title": "Fine-tuning Stable Diffusion",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "code_snippets": [
      {
        "code": "console.log("Generating image...");

const { images } = await octoai.imageGen.generateSdxl({
    prompt: "A photo of an sksmitchi as a puppy",
    negativePrompt: "Blurry photo, distortion, low-res, poor quality, extra limbs, extra tails",
    loras: {
        mitchi: 0.8,
    },
    numImages: 1,
});

images.forEach((image, index) => {
    if (image.imageB64) {
        const buffer = Buffer.from(image.imageB64, "base64");
        fs.writeFileSync(`result${index}.jpg`, buffer);
    }
});",
        "lang": "TypeScript",
        "meta": "TypeScript",
      },
    ],
    "content": "Next, you can run an inference with the tuned LoRA
The end result will be a saved poodle to your local folder.
Stable Diffusion Tuned LoRA generated toy poodle puppy
result0.jpg",
    "domain": "test.com",
    "hash": "#4-run-an-inference-with-the-tuned-lora",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK Fine-tuning",
      },
      "h4": {
        "id": "4-run-an-inference-with-the-tuned-lora",
        "title": "4) Run an inference with the tuned LoRA",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.fine-tuning-stable-diffusion.type-script-sdk-4-run-an-inference-with-the-tuned-lora-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/fine-tuning-stable-diffusion/typescript-sdk-finetuning",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Run an inference with the tuned LoRA",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library",
        "title": "Managing custom assets with Asset Library",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/uploading-a-custom-asset-to-the-octoai-asset-library",
    "code_snippets": [
      {
        "code": "$ octoai asset --help
Manage OctoAI assets

Usage:
  octoai asset [command]

Available Commands:
  create      Create asset
  delete      Delete an asset
  get         Get asset
  list        List assets

Flags:
  -h, --help   help for asset

Global Flags:
      --log-level string   Set log level: fatal, error, warning, info, debug, trace (default "info")

Use "octoai asset [command] --help" for more information about a command.",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": "octoai asset create \
  --upload-from-file LCM_Dreamshaper_v7.safetensors \
  --name Dreamshaper_v7 \
  --engine image/stable-diffusion-v1-5 \
  --format safetensors \
  --data-type fp16 \
  --type checkpoint \
  --description "Dreamshaper v7"",
      },
      {
        "code": "octoai asset create \
  --upload-from-url https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7/resolve/main/LCM_Dreamshaper_v7_4k.safetensors?download=true \
  --name Dreamshaper_v7 \
  --engine image/stable-diffusion-v1-5 \
  --format safetensors \
  --data-type fp16 \
  --type checkpoint \
  --description "Dreamshaper v7"",
      },
    ],
    "content": "OctoAI empowers you to customize images by leveraging assets like checkpoints, LoRAs, and textual inversions. You can either use:
Public assets in the OctoAI Asset Library

Upload your custom asset to the OctoAI Asset Library (private by default, and optionally public)

Or use OctoAI fine-tuning to create new assets - see Fine-tuning Stable Diffusion


This tutorial explains how to upload your own private assets to the Asset Library.
First download the OctoAI CLI by following the instructions in CLI Installation. Check that it is properly installed by running the following in your terminal:


Run octoai login to cache your token and authenticate to your account.

We can now use the octoai asset create subcommand to upload assets (you can run octoai asset create --help to learn more on the options).
--engine denotes whether this is an asset for SDXL or SD1.5

--upload-from-file flag denotes the path of the file on your local machine that you’re trying to upload.

--upload-from-url flag is an alternative to upload-from-file allowing you to upload an asset from a public URL; OctoAI will fetch and upload the file.

—type can be lora, checkpoint, or textual_inversion (VAEs coming soon)

--format denotes the format of your asset, which can be safetensors or pt.

--datatype can be fp16, fp32, int4, or int8. For image gen, it should almost always be fp16, but for LLMs and other modalities your asset may have other datatypes.

--name is a name for your asset. You can only use each asset name once.

--transfer-api defaults to sts which is the fastest way to upload a large asset.

If you are uploading a textual inversion, make sure to use the -w flag to denote the default trigger word for the asset. That trigger word can later be used at generation time to activate the inversion. For LoRAs, trigger words are optional.




As an example, you could use this command to upload a checkpoint from your local machine:
You can alternatively upload the file via public URL using upload-from-url:",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.uploading-custom-assets-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/uploading-a-custom-asset-to-the-octoai-asset-library",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Uploading custom assets to OctoAI's Asset Library",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library",
        "title": "Managing custom assets with Asset Library",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/asset-orchestrator-client",
    "content": "The AssetLibrary client in the Python SDK allows create, list, get, and delete actions of assets. These assets allow integration with the ImageGenerator Client to generate more customized images.
This guide will walk you through using this API to see a list of our public assets, create your own asset, and use your asset to generate an image.",
    "description": "Manage assets using the Python SDK.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-python-client-root-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/asset-orchestrator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Asset Library in the Python SDK",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library",
        "title": "Managing custom assets with Asset Library",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/asset-orchestrator-client",
    "content": "First, create an OctoAI API token.

Then, complete Python SDK Installation & Setup..
If you use the OCTOAI_TOKEN envvar for your token, you can instantiate the asset_orch client with asset_library = AssetLibrary()",
    "domain": "test.com",
    "hash": "#requirements",
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the Python SDK",
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-python-client-requirements-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/asset-orchestrator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library",
        "title": "Managing custom assets with Asset Library",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/asset-orchestrator-client",
    "code_snippets": [
      {
        "code": "from octoai.client import OctoAI
from octoai.asset_library import Data_Lora
from octoai.util import to_file

if __name__ == "__main__":
    # OCTOAI_TOKEN set as an environment variable so do not need to pass a token.
    client = OctoAI()
    asset_library = client.asset_library
    image_gen = client.image_gen

    asset_name = "origami-paper-test"
    # There is also TextualInversionData, VAEData, and CheckpointData.
    lora_data = Data_Lora(
        data_type="fp16",
        engine="image/stable-diffusion-v1-5",
        file_format="safetensors",
    )

    asset = asset_library.create_from_file(
        file="origami-paper.safetensors",
        data=lora_data,
        name=asset_name,
        description="origami-paper stable diffusion 1.5",
    )

    image_gen_resp = image_gen.generate_sd(
        prompt="rainbow origami tailong dragon",
        num_images=4,
        loras={"asset": 0.8}
    )

    # Some images can be removed for safety.
    # Please see the ImageGenerator client docs for more information.
    for i, image in enumerate(image_gen_resp.images):
        to_file(image, f"result{i}.jpg")

    # You can clean up your asset with the following:
    asset_library.delete(asset.id)",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "You will need a safetensors file in order to use this example, and in our case one is named origami-paper.safetensors. I'll be using a lora trained on origami that I can use with the words "origami" and "paper".
In this example, we will be adding a LoRA then using it to generate an image. You can also add checkpoints, vae, and textual inversions.
astropus.png
rainbow-origami-tailong-dragon.png",
    "domain": "test.com",
    "hash": "#creating-a-lora-and-generating-an-image",
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the Python SDK",
      },
      "h4": {
        "id": "creating-a-lora-and-generating-an-image",
        "title": "Creating a LoRA and Generating an Image",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-python-client-creating-a-lora-and-generating-an-image-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/asset-orchestrator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Creating a LoRA and Generating an Image",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library",
        "title": "Managing custom assets with Asset Library",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/asset-orchestrator-client",
    "code_snippets": [
      {
        "code": "./assets/images/
	result0.jpg
	result1.jpg
	result2.jpg
	result3.jpg
	save_the_other_paper_poodle.png
	save_the_paper_poodle.png",
      },
      {
        "code": "import os
from octoai.client import OctoAI
from octoai.asset_library import Data_File

if __name__ == "__main__":
    # OCTOAI_TOKEN set as an environment variable so do not need to pass a token.
    client = OctoAI()

    dir_path = "./assets/images/"  # Set your dir_path here to your file assets.
    files = []
    # Get a list of files in the folder
    for file_path in os.listdir(dir_path):
        if os.path.isfile(os.path.join(dir_path, file_path)):
            files.append(file_path)
    for file in files:
        # Use the file names to get file_format and the asset_name.
        split_file_name = file.split(".")
        asset_name = split_file_name[0]
        file_format = split_file_name[1]
        file_data = Data_File(
            file_format=file_format,
        )
        asset = client.asset_library.create_from_file(
            file=dir_path + file,
            data=file_data,
            name=asset_name,
        )",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "[
id: asset_01234567891011121314151617, name: save_the_paper_poodle, status: ready,
id: asset_01234567891011121314151618, name: save_the_other_paper_poodle, status: ready,
id: asset_01234567891011121314151619, name: result2, status: ready,
id: asset_01234567891011121314151620, name: result3, status: ready,
id: asset_01234567891011121314151621, name: result1, status: ready,
id: asset_01234567891011121314151622, name: result0, status: ready,
id: asset_01234567891011121314151600, name: origami-paper-test, status: uploaded]",
      },
    ],
    "content": "Let's say you have a folder of images assets you would like to upload for using the FineTuning service. You can do so using the below code snippet to get all the files in your folder named images, and then splitting on the . to get your file_format extension (jpg, jpeg, or png), and use the file name as the asset name.
In this example, there is a directory named images that contains files with a _, -, and alphanumeric file names, jpg, jpeg, or png suffixes. In this example, the folder contains the following:
You can then use octoai.asset_library.list() to see the assets have been created and uploaded and a result that looks something like:
In this example, an already existing lora created in the previous example also exists. The lora has uploaded however will likely need a few more seconds before being ready for use.",
    "domain": "test.com",
    "hash": "#creating-file-assets-from-a-folder-of-images",
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the Python SDK",
      },
      "h4": {
        "id": "creating-file-assets-from-a-folder-of-images",
        "title": "Creating File Assets from a Folder of Images",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-python-client-creating-file-assets-from-a-folder-of-images-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/asset-orchestrator-client",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Creating File Assets from a Folder of Images",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library",
        "title": "Managing custom assets with Asset Library",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/asset-library",
    "content": "The AssetLibrary client in the TypeScript SDK allows create, list, get, and delete actions of assets. These assets allow integration with Fine-tuning Stable Diffusion.
This guide will walk you through using this API to see a list of our public assets, create your own asset, and use your asset to generate an image.",
    "description": "Manage assets using the TypeScript SDK.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-type-script-client-root-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/asset-library",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Asset Library in the TypeScript SDK",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library",
        "title": "Managing custom assets with Asset Library",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/asset-library",
    "content": "First, create an OctoAI API token.

Then, complete TypeScript SDK Installation & Setup..
If you use the OCTOAI_TOKEN environment variable for your token, you can instantiate the client with const octoai = new OctoAIClient() or pass the token as a parameter to the constructor.",
    "domain": "test.com",
    "hash": "#requirements",
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the TypeScript SDK",
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-type-script-client-requirements-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/asset-library",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/managing-custom-assets-with-asset-library",
        "title": "Managing custom assets with Asset Library",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/asset-library",
    "code_snippets": [
      {
        "code": "import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
    apiKey: process.env.OCTOAI_TOKEN
});

await octoai.assetLibrary.upload("origami-paper.safetensors", {
    assetType: "lora",
    description: "Origami",
    data: {
        assetType: "lora",
        dataType: "fp16",
        engine: "image/stable-diffusion-v1-5",
        fileFormat: "safetensors",
        triggerWords: ["origami", "paper"],
    },
    name: "origami-paper",
    isPublic: false,
});",
        "lang": "TypeScript",
        "meta": "TypeScript",
      },
    ],
    "content": "You will need a safetensors file in order to use this example, and in our case one is named origami-paper.safetensors. I'll be using a LoRA trained on origami that I can use with the words "origami" and "paper".
In this example, we will be adding a LoRA then using it to generate an image.
The id field for the created asset can be used when Getting started with our Media Gen Solution and running inferences.
astropus.png
rainbow-origami-tailong-dragon.png generated on ImageGen service using AssetLibrary created LoRA",
    "domain": "test.com",
    "hash": "#creating-a-lora",
    "hierarchy": {
      "h0": {
        "title": "Asset Library in the TypeScript SDK",
      },
      "h4": {
        "id": "creating-a-lora",
        "title": "Creating a LoRA",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.managing-custom-assets-with-asset-library.asset-library-type-script-client-creating-a-lora-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/asset-library",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Creating a LoRA",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities",
        "title": "Supported image utilities",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/inpainting",
    "code_snippets": [
      {
        "code": "import requests
import json
import os
import base64
import time
import io
import PIL.Image

def _process_test(url):
    image = PIL.Image.open("dog.jpg")
    mask = PIL.Image.open("dogmask.jpg")

    # Create a BytesIO buffer to hold the image data
    image_buffer = io.BytesIO()
    image.save(image_buffer, format='JPEG')
    image_bytes = image_buffer.getvalue()
    encoded_image = base64.b64encode(image_bytes).decode('utf-8')

    # Create a BytesIO buffer to hold the image data
    mask_buffer = io.BytesIO()
    mask.save(mask_buffer, format='JPEG')
    mask_bytes = mask_buffer.getvalue()
    encoded_mask = base64.b64encode(mask_bytes).decode('utf-8')

    OCTOAI_TOKEN = os.environ.get("OCTOAI_TOKEN")

    payload = {
        "prompt": "Face of a yellow cat, high resolution, sitting on a park bench",
        "negative_prompt": "Blurry photo, distortion, low-res, bad quality",
        "steps": 4,
        "width": 1024,
        "height": 1024,
        "cfg_scale": 1.4,
        "checkpoint": "octoai:lcm_sdxl",
        "init_image": encoded_image,
        "mask_image": encoded_mask
    }
    headers = {
        "Authorization": f"Bearer {OCTOAI_TOKEN}",
        "Content-Type": "application/json",
        "X-OctoAI-Queue-Dispatch": "true"
    }

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code != 200:
        print(response.text)
    print(response.json())

    img_list = response.json()["images"]

    for i, img_info in enumerate(img_list):
        img_bytes = base64.b64decode(img_info["image_b64"])
        img = PIL.Image.open(io.BytesIO(img_bytes))
        img.load()
        img.save(f"result_image{i}.jpg")

if __name__ == "__main__":
    _process_test("https://image.octoai.run/generate/sdxl")",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "Inpainting refers to the process of restoring or repairing an image by filling in missing or damaged parts. It is a valuable technique widely used in image editing and restoration, enabling the removal of flaws and unwanted objects to achieve a seamless and natural-looking final image. Inpainting finds applications in film restoration, photo editing, and digital art, among others.








Here is a Python example for inpainting. The Image Gen API call will take mask_image (right) and init_image (middle) as shown below.",
    "description": "Edit and resort image to fix flaws or remove unwanted objects from an image.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.inpainting-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/image-utilities/inpainting",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Inpainting",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities",
        "title": "Supported image utilities",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/outpainting",
    "code_snippets": [
      {
        "code": "import requests
import json
import os
import base64
import time
import io
import PIL.Image

def _process_test(url):
    
    image = PIL.Image.open("init-image-outpainting.jpg")
    mask = PIL.Image.open("mask-outpainting.jpg")
    # with PIL.Image.open("init-image-outpainting.png") as im:
    #     rgb_im = im.convert('RGBA')
    # rgb_im.save('init.jpg')
    # image = PIL.Image.open("init.jpg")

    # with PIL.Image.open("mask-outpainting.png") as it:
    #     rgb_it = it.convert('RGBA')
    # rgb_it.save('mask.jpg')
    # mask = PIL.Image.open("mask.jpg")

    # Create a BytesIO buffer to hold the image data
    image_buffer = io.BytesIO()
    image.save(image_buffer, format='JPEG')
    image_bytes = image_buffer.getvalue()
    encoded_image = base64.b64encode(image_bytes).decode('utf-8')

    # Create a BytesIO buffer to hold the image data
    mask_buffer = io.BytesIO()
    mask.save(mask_buffer, format='JPEG')
    mask_bytes = mask_buffer.getvalue()
    encoded_mask = base64.b64encode(mask_bytes).decode('utf-8')

    OCTOAI_TOKEN = os.environ.get("OCTOAI_TOKEN")

    payload = {
        "prompt": "portrait view, cartoon sketch of a woman, green grass with fall foliage in the back, blurry background, blue and grey sky",
        "negative_prompt": "path, leaves on the ground, bad anatomy, bad hands, three hands, three legs, bad arms, missing legs, missing arms, poorly drawn face, bad face, fused face, cloned face, worst face, three crus, extra crus, fused crus, worst feet, three feet, fused feet, fused thigh, three thigh, fused thigh, extra thigh, worst thigh, missing fingers, extra fingers, ugly fingers, long fingers, horn, extra eyes, 2girl, amputation",
        "width": 768,
        "height": 512,
        "num_images": 1,
        "sampler": "DDPM",
        "steps": 40,
        "cfg_scale": 12,
        "style_preset": "base",
        "outpainting": "true",
        "init_image": encoded_image,
        "mask_image": encoded_mask,
        "strength": 1
    }
    headers = {
        "Authorization": f"Bearer {OCTOAI_TOKEN}",
        "Content-Type": "application/json",
        "X-OctoAI-Queue-Dispatch": "true"
    }

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code != 200:
        print(response.text)
    print(response.json())

    img_list = response.json()["images"]

    for i, img_info in enumerate(img_list):
        img_bytes = base64.b64decode(img_info["image_b64"])
        img = PIL.Image.open(io.BytesIO(img_bytes))
        img.load()
        img.save(f"result_image{i}.jpg")

if __name__ == "__main__":
    _process_test("https://image.octoai.run/generate/sd")",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "Outpainting is the process of using an image generation model like Stable Diffusion to extend beyond the canvas of an existing image. Outpainting is very similar to inpainting, but instead of generating a region within an existing image, the model generates a region outside of it.








Outpainting works both with SDXL and SD1.5.
At a high level, outpainting works like this:
Choose an existing image you’d like to outpaint.

Create a source image that places your original image within a larger canvas.

Create a black and white mask image.

Use init_image (source image), mask_image (your mask image), a text prompt and outpainting parameter as inputs to Image Gen API to generate a new image.


In the following example we will leverage the SD1.5 engine so we’ll start with a 512x512 image.





You can extend the image in any direction, but for this example we’ll extend the width from 512 → 768. Supported resolutions for SD1.5 are:
(W, H): (768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)

Support resolutions for SDXL are:
(W, H): {(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}
Two images are required for the next step. An init_image and mask_image are required in order to perform the outpainting to 768 pixels. Convert both images to base64.








Example Code for Outpainting:
Here's the outpainted output that the above code generates:",
    "description": "Extend beyond the canvas of an existing image.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.outpainting-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/image-utilities/outpainting",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Outpainting",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities",
        "title": "Supported image utilities",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/adetailer",
    "code_snippets": [
      {
        "code": "import requests
import json
import os
import base64
import time
import io
import PIL.Image

def _process_test(url):
    image = PIL.Image.open("woman.jpg")
   
    # Create a BytesIO buffer to hold the image data
    image_buffer = io.BytesIO()
    image.save(image_buffer, format='JPEG')
    image_bytes = image_buffer.getvalue()
    encoded_image = base64.b64encode(image_bytes).decode('utf-8')


    OCTOAI_TOKEN = os.environ.get("OCTOAI_TOKEN")

    payload = {
         "prompt": "photorealistic, award winning, 8K HDR, best quality, breathtaking", 
         "negative_prompt": "worst quality, bad face, drawing, unrealistic, ugly face, animated", 
         "cfg_scale": 6, 
         "sampler": "DPM_2_ANCESTRAL", 
         "steps": 60, 
         "checkpoint": "RealVisXL", 
         "inpainting_base_model": "sdxl", 
         "style_preset": "base", 
         "detector": "face_yolov8n", 
         "strength": 0.5, 
         "init_image": encoded_image
    }
    headers = {
        "Authorization": f"Bearer {OCTOAI_TOKEN}",
        "Content-Type": "application/json",
        "X-OctoAI-Queue-Dispatch": "true"
    }

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code != 200:
        print(response.text)
    print(response.json())

    img_info = response.json()
    img_bytes = base64.b64decode(img_info["image_b64"])
    img = PIL.Image.open(io.BytesIO(img_bytes))
    img.load()
    img.save(f"result_image.jpg")

if __name__ == "__main__":
    _process_test("https://image.octoai.run/adetailer")
",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "OctoAI's Adetailer API supports various detection models. Whether you're interested in identifying faces, or hands, you can choose from options such as face_yolov8n, hand_yolov8n, face_full_mediapipe, face_short_mediapipe, face_mesh_mediapipe, and eyes_mesh_mediapipe. This allows you to tailor the detailing process to your specific needs and preferences.
Adetailer effortlessly identifies faces and hands in images, automatically creating masks to fill in any missing parts. After this, it seamlessly integrates with the SDXL API, applying customized settings to achieve outstanding results. Additionally, OctoAI's  Adetailer API offers flexibility by allowing users to set the maximum number of detections. For example, if there are five people in an image but you only want to focus on two faces, you can easily do so using the max_num_detections parameter. The API prioritizes and corrects faces based on their confidence score and bounding box dimensions.",
    "description": "Automatically fix faces and hands using Adetailer",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.adetailer-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/image-utilities/adetailer",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Adetailer API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities",
        "title": "Supported image utilities",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/photo-merge",
    "code_snippets": [
      {
        "code": "ENDPOINT_URL = "https://image.octoai.run/generate/sdxl"
IMAGES_PATH = "/content/images"
os.makedirs(IMAGES_PATH, exist_ok=True)

luis_path = f'{IMAGES_PATH}/luis'
luis_urls = ['https://cs.illinois.edu/_sitemanager/viewphoto.aspx?id=12289&s=206',
             'https://www.kisacoresearch.com/sites/default/files/styles/panopoly_image_square/public/speakers/luis_ceze_headshot.jpg?itok=NmVXlZb2&c=092995db2355e0f1f7ce15ff18aba155',
             'https://news.cs.washington.edu/wp-content/uploads/2023/01/luis-ceze-2022-blog.jpg',
             'https://cdn.geekwire.com/wp-content/uploads/2019/12/luisceze-1260x1047.jpeg',
             ]

luis_image_names = download_from_urls(luis_urls, luis_path)

image_grid(luis_path, luis_image_names)

luis_b64_images = [encode_b64(luis_path, image) for image in luis_image_names]",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "payload = {
    "prompt": "A man luis sitting in a coffee shop",
    "negative_prompt": "Blurry photo, distortion, low-res, bad quality",
    "checkpoint": "RealVisXL",
    "width": 1024,
    "height": 1024,
    "num_images": 4,
    "sampler": "K_EULER_ANCESTRAL",
    "steps": 20,
    "cfg_scale": 7.5,
    "seed": 888,
    "transfer_images": {"luis": luis_b64_images}
}

response = query(ENDPOINT_URL, payload)

generated_images = decode_response(response)
image_grid_buffer(generated_images)",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "payload = {
    "prompt": "closeup colorfull portrait photo of a man luis",
    "negative_prompt": "(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth",
    "checkpoint": "RealVisXL",
    "width": 1024,
    "height": 1024,
    "num_images": 4,
    "sampler": "K_EULER_ANCESTRAL",
    "steps": 20,
    "cfg_scale": 7.5,
    "seed": 111,
    "style_preset": "Graffiti",
    "transfer_images": {"luis": luis_b64_images}
}

response = query(ENDPOINT_URL, payload)

generated_images = decode_response(response)
image_grid_buffer(generated_images)",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "payload = {
    "prompt": "painting, martius_storm red ominous war [:style of vincent van gogh luis and leonardo da vinci:0.4] ral-dissolve",
    "negative_prompt": "(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth",
    "checkpoint": "RealVisXL",
    "loras": {"asset_01hnxyk06ee999h4cq1cz3r21f": 1.0},
    "width": 1024,
    "height": 1024,
    "num_images": 4,
    "sampler": "DPM_PLUS_PLUS_2M_KARRAS",
    "steps": 20,
    "cfg_scale": 7,
    "seed": 111,
    "transfer_images": {"luis": luis_b64_images}
}

response = query(ENDPOINT_URL, payload)

generated_images = decode_response(response)
image_grid_buffer(generated_images)",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "payload = {
    "prompt": "A man luis wearing a pink T-shirt lacosteshirt1:1, sitting in a coffee shop",
    "negative_prompt": "(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth, shirt logo not visible, left position shirt logo",
    "checkpoint": "RealVisXL",
    "loras": {"asset_01hp5hsn6mfh6b0zf47q862a6b": 1.0},
    "width": 1024,
    "height": 1024,
    "num_images": 4,
    "sampler": "DPM_PLUS_PLUS_2M_KARRAS",
    "steps": 20,
    "cfg_scale": 7,
    # "seed": 444,
    "transfer_images": {"luis": luis_b64_images}
}

response = query(ENDPOINT_URL, payload)

generated_images = decode_response(response)
image_grid_buffer(generated_images)",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "payload = {
    "prompt": "closeup b&w portrait photo of a man luis einstein",
    "negative_prompt": "(asymmetry, worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth",
    "checkpoint": "RealVisXL",
    "width": 1024,
    "height": 1024,
    "num_images": 4,
    "sampler": "K_EULER_ANCESTRAL",
    "steps": 20,
    "cfg_scale": 7.5,
    "seed": 888,
    "transfer_images": {"luis": luis_b64_images, "einstein": einstein_b64_images}
}

response = query(ENDPOINT_URL, payload)

generated_images = decode_response(response)
image_grid_buffer(generated_images)",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "OctoAI Photo Merge feature allows you to seamlessly integrate a photo’s subject into high-quality AI-generated output. It eliminates the need to create time-consuming custom facial fine-tunes with numerous tuning images and 15-30 minutes typically associated with SDXL LoRAs. OctoAI's Photo Merge simplifies this process, requiring only 1-4 images and delivering precise results within a few seconds. Businesses can now easily apply GenAI powered imagery for needs ranging from realistic CGI characters, to personalized product recommendations, to digital avatars.
Photo Merge can be accessed through the "transfer_images" parameter within OctoAI’s Image Generation API. This parameter accepts a key-value pair consisting of a trigger word and an array of up to 4 images. It operates exclusively with SDXL models and seamlessly harmonizes with style presets, controlnets, checkpoints, and LoRAs when utilized with SDXL models, thereby amplifying its adaptability and functionality.
In the example below, we will upload 1-4 images of a human and use photo merge to transfer his face into our prompts.
Example Code Snippet showing the close portriat images of a human:

Next, we utilize the transfer_images="triggerword": list of images parameter within the payload of OctoAI’s SDXL Image Gen API at https://image.octoai.run/generate/sdxl.
Example Code Snippet of using Photo Merge (transfer_images):
In the given example, we employ the trigger word ‘luis’ and link it with the dataset comprising the four images mentioned earlier. Subsequently, we structure the prompt to incorporate the trigger word.
Prompt: A man luis sitting in a coffee shop.
It's worth noting that in this instance, no LoRA is utilized. Additionally, we utilize a checkpoint named ‘RealVisXL’, an OctoAI asset checkpoint specifically optimized for the Photo Merge feature. However, it's important to mention that the Photo Merge feature is functional even if the base SDXL checkpoint is utilized.
Below are the AI-generated output images after using transfer_image parameter with the newly provided prompt:

Pretty accurate, isn’t it? Let’s try it with few different prompts and combine it with other style presets, LoRAs and checkpoints to confirm whether we consistently get the accurate results.
Let us use transfer_images parameter in conjunction with ‘Graffiti’ style preset. We are keeping all other parameter values similar to the payload above.
Example Code Snippet of Photo Merge (transfer_images parameter) with Style Preset:

Let’s now use transfer_images parameter with a pre-trained Style LoRA. We have already imported a pre-trained style based LoRA into OctoAI’s Asset Library. In the payload below, we are using the corresponding asset’s asset id and assigning it a weight of 1.0.
Example Code Snippet of Photo Merge (transfer_images parameter) with pre-trained LoRAs (uploaded from external sources):

Next, we'll create a custom fine-tune for a product and seamlessly integrate our AI-generated human model's image with it.
To create custom fine tunes (SDXL LoRAs), we will upload 10-12 images of our product, which in our case are different colored Lacoste polo shirts for men.


Example Code Snippet of Photo Merge (transfer_images parameter) with custom fine-tuned LoRAs:

Let's now consider a use-case where we want to mix the identities of 2 or more humans. For this let's consider a new set of close up portriats of another human as shown below:

Example Code Snippet of Photo Merge (transfer_images parameter) with multiple human portraits:

Photo Merge offers endless potential - whether in entertainment, gaming, marketing agencies, or fashion and retail sectors, it can help craft personalized avatars, advertisements, and brand ambassador representations. It can also enable virtual try-ons and lifelike digital product showcases.",
    "description": "Seamlessly integrate a photo's subject into AI-generated output and eliminate the need to create time-consuming custom facial finetunes.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.photo-merge-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/image-utilities/photo-merge",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Photo Merge",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities",
        "title": "Supported image utilities",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/upscaling",
    "code_snippets": [
      {
        "code": "import requests
import json
import os
import base64
import time
import io
import PIL.Image
from typing import Optional, Tuple


def _process_test(url):
    
    image = PIL.Image.open("headphones1.jpeg")

    # Create a BytesIO buffer to hold the image data
    image_buffer = io.BytesIO()
    image.save(image_buffer, format='JPEG')
    image_bytes = image_buffer.getvalue()
    encoded_image = base64.b64encode(image_bytes).decode('utf-8')

    OCTOAI_TOKEN = os.environ.get("OCTOAI_TOKEN")

    payload = {
        "model": "real-esrgan-x4-plus",
        "scale": 2,
        "init_image": encoded_image,
        "output_image_encoding": "jpeg"
    }
    headers = {
        "Authorization": f"Bearer {OCTOAI_TOKEN}",
        "Content-Type": "application/json",
        "X-OctoAI-Queue-Dispatch": "true"
    }

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code != 200:
        print(response.text)
    print(response.json())

    img_info = response.json()["image_b64"]

    img_bytes = base64.b64decode(img_info)
    img = PIL.Image.open(io.BytesIO(img_bytes))

    if img.mode == 'RGBA':
     img = img.convert('RGB')

    img.save("result_image.jpeg")

if __name__ == "__main__":
    _process_test("https://image.octoai.run/upscaling")",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "Upscaling takes an existing image you provide and upscales it to a higher resolution.








Example Code of upscaling an image from 1024X1024 to 2048X2048:",
    "description": "Upscale images to higher resolutions.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.upscaling-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/image-utilities/upscaling",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Upscaling",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/media-gen-solution",
        "title": "Media Gen Solution",
      },
      {
        "pathname": "/docs/documentation/media-gen-solution/supported-image-utilities",
        "title": "Supported image utilities",
      },
    ],
    "canonicalPathname": "/docs/media-gen-solution/image-utilities/bg-removal",
    "code_snippets": [
      {
        "code": "import requests
import json
import os
import base64
import time
import io
import PIL.Image
from typing import Optional, Tuple


def _process_test(url):
    
    image = PIL.Image.open("headphones1.jpeg")

    # Create a BytesIO buffer to hold the image data
    image_buffer = io.BytesIO()
    image.save(image_buffer, format='JPEG')
    image_bytes = image_buffer.getvalue()
    encoded_image = base64.b64encode(image_bytes).decode('utf-8')

    OCTOAI_TOKEN = os.environ.get("OCTOAI_TOKEN")

    payload = {
        "init_image": encoded_image,
        "bgcolor":(255, 255, 255, 0)
    }
    headers = {
        "Authorization": f"Bearer {OCTOAI_TOKEN}",
        "Content-Type": "application/json",
        "X-OctoAI-Queue-Dispatch": "true"
    }

    response = requests.post(url, headers=headers, json=payload)

    if response.status_code != 200:
        print(response.text)
    print(response.json())

    img_info = response.json()["image_b64"]

    img_bytes = base64.b64decode(img_info)
    img = PIL.Image.open(io.BytesIO(img_bytes))

    if img.mode == 'RGBA':
     img = img.convert('RGB')

    img.save("result_image.png")

if __name__ == "__main__":
    _process_test("https://image.octoai.run/background-removal")",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "Background removal takes an existing image you provide and removes those parts of the image considered to be “background.”








Example Code of removing background from an image:",
    "description": "Remove parts of the image considered to be background.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.media-gen-solution.supported-image-utilities.background-removal-root-0",
    "org_id": "test",
    "pathname": "/docs/media-gen-solution/image-utilities/bg-removal",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Background Removal",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "description": "OctoAI's GenAI production stack in your environment.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "OctoStack",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "content": "OctoStack is a turnkey generative AI solution allowing you to run your choice of models in your environment. It delivers OctoAI’s industry-leading AI inference service and performance optimizations with the privacy benefits of self-managing within your environment. OctoStack provides a full stack solution for running generative AI at scale - including inference, model customization, load balancing, auto-scaling, and telemetry. OctoStack is compatible with all cloud service providers and container orchestration services. Contact us to get started!",
    "domain": "test.com",
    "hash": "#overview",
    "hierarchy": {
      "h0": {
        "title": "OctoStack",
      },
      "h1": {
        "id": "overview",
        "title": "Overview",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-overview-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Overview",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "content": "OctoStack is designed for portability across any cloud platform or on-premise data center. Prebuilt containers are easily deployed using Kubernetes or your preferred container orchestration service. Maintain complete data privacy and control by processing all generative AI workloads within your environment.",
    "domain": "test.com",
    "hash": "#privately-deploy-in-any-environment",
    "hierarchy": {
      "h0": {
        "title": "OctoStack",
      },
      "h1": {
        "id": "key-benefits",
        "title": "Key Benefits",
      },
      "h3": {
        "id": "privately-deploy-in-any-environment",
        "title": "Privately deploy in any environment",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-privately-deploy-in-any-environment-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Privately deploy in any environment",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "content": "OctoStack includes OctoAI’s performance optimizations delivered through Apache TVM, a compiler framework that optimizes and accelerates inferences. Optimized inference improves GPU utilization and delivers the best possible application experience. OctoStack also provides best-in-class load balancing to operate at scale.",
    "domain": "test.com",
    "hash": "#performance-optimized-inference",
    "hierarchy": {
      "h0": {
        "title": "OctoStack",
      },
      "h1": {
        "id": "key-benefits",
        "title": "Key Benefits",
      },
      "h3": {
        "id": "performance-optimized-inference",
        "title": "Performance optimized inference",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-performance-optimized-inference-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Performance optimized inference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "content": "Application developers can use OctoStack’s industry-standard API’s, including Python and TypeScript SDK’s. Our ergonomic API’s allow rapid development and integration. You can easily load and run open source models, and fine-tuned checkpoints.",
    "domain": "test.com",
    "hash": "#easy-to-use-apis--customization",
    "hierarchy": {
      "h0": {
        "title": "OctoStack",
      },
      "h1": {
        "id": "key-benefits",
        "title": "Key Benefits",
      },
      "h3": {
        "id": "easy-to-use-apis--customization",
        "title": "Easy to use APIs & customization",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-easy-to-use-apis--customization-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Easy to use APIs & customization",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "content": "OctoStack can run a multi-GPU configuration to improve throughput and latency, which is easily enabled with a single configuration setting. We recommend using multiple GPU’s per replica when running models with higher precision and a greater number of parameters. The OctoAI team can help you identify the multi-GPU configuration that best matches your throughput & latency goals.",
    "domain": "test.com",
    "hash": "#gpu-configuration",
    "hierarchy": {
      "h0": {
        "title": "OctoStack",
      },
      "h1": {
        "id": "configuration--deployment",
        "title": "Configuration & Deployment",
      },
      "h3": {
        "id": "gpu-configuration",
        "title": "GPU Configuration",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-gpu-configuration-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "GPU Configuration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "content": "OctoStack is comptable with all cloud environments - these are key system requirements:
Kubernetes, or Docker & Docker Compose

Redis in cluster mode

NVIDIA drivers

GPUs
OctoStack supports a range of hardware including A10G, A100, and H100 GPUs

A100s such as AWS p4d.24xlarge instances can support a broad set of models",
    "domain": "test.com",
    "hash": "#prerequisites",
    "hierarchy": {
      "h0": {
        "title": "OctoStack",
      },
      "h1": {
        "id": "configuration--deployment",
        "title": "Configuration & Deployment",
      },
      "h3": {
        "id": "prerequisites",
        "title": "Prerequisites",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-prerequisites-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Prerequisites",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "content": "OctoStack supports Kubernetes deployment via manifest files, and a Docker Compose deployment compatible with any container orchestration service. There’s a few simple steps to deploy:
OctoAI allow lists your AWS account, so you can access OctoStack containers in OctoAI’s AWS ECR.

Pull the OctoStack containers from OctoAI’s AWS ECR.

Configure and deploy using Kubernetes or Docker Compose, using OctoAI-provided guides.",
    "domain": "test.com",
    "hash": "#deployment",
    "hierarchy": {
      "h0": {
        "title": "OctoStack",
      },
      "h1": {
        "id": "configuration--deployment",
        "title": "Configuration & Deployment",
      },
      "h3": {
        "id": "deployment",
        "title": "Deployment",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-deployment-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Deployment",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/octostack",
    "content": "Reach out to the team to see a live demo and get started on your OctoStack deployment.",
    "domain": "test.com",
    "hash": "#get-started",
    "hierarchy": {
      "h0": {
        "title": "OctoStack",
      },
      "h1": {
        "id": "get-started",
        "title": "Get Started",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.octo-stack-get-started-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/octostack",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Get Started",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/secure-link",
    "description": "Private networking with OctoAI's SecureLink.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.secure-link-guide",
    "org_id": "test",
    "pathname": "/docs/private-deployment/secure-link",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "SecureLink guide",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/secure-link",
    "content": "Keeping our users’ data private and secure is our priority. OctoAI requires token authentication for all API requests, along with TLS to enforce encryption in transit for all connections between the customer and OctoAI. We also use encryption at rest for any data written to disk.
SecureLink is an additional private connectivity security measure, ensuring that network traffic between an OctoAI endpoint and the customer environment is not exposed to the public internet. SecureLink is available for Enterprise customers.",
    "domain": "test.com",
    "hash": "#overview",
    "hierarchy": {
      "h0": {
        "title": "SecureLink guide",
      },
      "h1": {
        "id": "overview",
        "title": "Overview",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.secure-link-guide-overview-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/secure-link",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Overview",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/secure-link",
    "content": "Configure your AWS Account ID in OctoAI

Create and configure a VPC Interface Endpoint

Configure OctoAI’s SDKs & CLI to use the SecureLink subdomain

If you intend to use Asset Library to upload your assets, configure a separate PrivateLink connection for Amazon S3 to ensure the uploads are also completed via a private connection to S3",
    "domain": "test.com",
    "hash": "#setup-steps",
    "hierarchy": {
      "h0": {
        "title": "SecureLink guide",
      },
      "h1": {
        "id": "setup-steps",
        "title": "Setup Steps",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.secure-link-guide-setup-steps-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/secure-link",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Setup Steps",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/private-deployment",
        "title": "Private Deployment",
      },
    ],
    "canonicalPathname": "/docs/private-deployment/secure-link",
    "code_snippets": [
      {
        "code": "curl -X POST "https://api.octoai.cloud/v1/account/securelink" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OCTOAI_TOKEN" \
    --data '{"aws_account_id": "account-ID-value"}'",
      },
      {
        "code": "export OCTO_API_ENDPOINT=https://api.securelink.octo.ai",
      },
      {
        "code": "import { OctoAIClient, OctoAIEnvironment } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
  environment: OctoAIEnvironment.SecureLink,
});",
        "lang": "typescript",
      },
      {
        "code": "import os

from octoai.client import OctoAI
from octoai.environment import OctoAIEnvironment
token=os.environ.get("OCTOAI_TOKEN")

client = OctoAI(api_key=token, environment=OctoAIEnvironment.SECURE_LINK)",
        "lang": "python",
      },
    ],
    "content": "Configure your AWS Account ID in OctoAI
First, run a simple curl command to configure your AWS account ID within OctoAI using the https://api.octoai.cloud/v1/account/securelink endpoint:
This allows OctoAI to generate a VPC Service Name, which you’ll use in the next step. If you don’t receive a successful response, contact us to ensure your OctoAI account is enabled as Enterprise tier.
Create and configure a VPC Interface Endpoint
Now you’ll create the VPC Interface Endpoint in your AWS account using the VPC Service Name. Navigate to the VPC Dashboard, and click Create Endpoint from your AWS console: https://console.aws.amazon.com/vpc/home?#Endpoints

Configure the Service Name value to com.amazonaws.vpce.us-east-1.vpce-svc-0e914445c09bbe700, then click Verify to ensure the service name is found and verified. Contact us for help if the service name is not found.

Next, choose the VPC and subnets that should be peered with the VPC service endpoint. Make sure that Enable DNS name is checked.

Then, choose the security group(s) who can send traffic to the VPC endpoint. The security group must accept inbound traffic on TCP port 443 - you can verify this within the Inbound Rules page. You can now click Create endpoint to create the VPC endpoint. The endpoint maybe take up to 10 minutes to move from Pending to Available. Once it shows Available, it’s ready for use.
Configure OctoAI’s SDKs & CLI to use SecureLink URL
Each OctoAI endpoint uses a SecureLink ingress URL, which will only work with a fully configured VPC Endpoint.
OctoAI CLI
Configure an environment variable by running:
TypeScript SDK
Configure the SecureLink URLs by passing in the SecureLink environment during client instantiation.
Python SDK
For text generation, fine-tuning, or asset library, configure the environment parameter to use OctoAIEnvironment.SECURE_LINK in the client instantiation:


This table summarizes the SecureLink equivalent to each public API URL:
Service Public SecureLink 
Text generation https://text.octoai.run https://text.securelink.octo.ai 
Image generation https://image.octoai.run https://image.securelink.octo.ai 
Asset Library & Fine-tuning https://api.octoai.cloud https://api.securelink.octo.ai 
OctoAI API https://api.octoai.cloud https://api.securelink.octo.ai 
Async Inference https://async.octoai.run https://async.securelink.octoai.run 

Configure private connection for Amazon S3 to upload assets through a private connection
If you are a user of Asset Library, you’ll need to configure a private connection for Amazon S3 to ensure uploads are also secured behind a private connection. Depending on your setup and your needs, you can either create a gateway endpoint, where a route table entry is added to your VPC, or create an interface endpoint, which is similar to configuring an interface endpoint for OctoAI. This guide covers setting up a gateway endpoint for S3.
To create a gateway endpoint, choose AWS services under Service category, and select com.amazonaws.us-east-1.s3. Ensure the type is Gateway.

Choose the route table where the routing entry is added, then click Create Endpoint. For more information, see the S3 gateway endpoint documentation on AWS.",
    "domain": "test.com",
    "hash": "#setup-instructions",
    "hierarchy": {
      "h0": {
        "title": "SecureLink guide",
      },
      "h1": {
        "id": "setup-instructions",
        "title": "Setup Instructions",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.documentation.documentation.private-deployment.secure-link-guide-setup-instructions-0",
    "org_id": "test",
    "pathname": "/docs/private-deployment/secure-link",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Setup Instructions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/compute-getting-started",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.getting-started-root-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/compute-getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Getting started with our Compute Service",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/compute-getting-started",
    "content": "Create your own OctoAI endpoints in two different ways:
Deploy an existing container from a public or private container registry. OctoAI can run any containers with an HTTP server written in any language, as long as your container is built on a GPU and comes with a declarative configuration of which port is exposed for inferences.

Create a container and endpoint using our CLI.",
    "domain": "test.com",
    "hash": "#create-your-own-endpoints",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Compute Service",
      },
      "h2": {
        "id": "create-your-own-endpoints",
        "title": "Create your own endpoints",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.getting-started-create-your-own-endpoints-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/compute-getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Create your own endpoints",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/compute-getting-started",
    "content": "Use our REST API to integrate OctoAI endpoints into your application.",
    "domain": "test.com",
    "hash": "#calling-octoai-endpoints",
    "hierarchy": {
      "h0": {
        "title": "Getting started with our Compute Service",
      },
      "h2": {
        "id": "calling-octoai-endpoints",
        "title": "Calling OctoAI endpoints",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.getting-started-calling-octoai-endpoints-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/compute-getting-started",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Calling OctoAI endpoints",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
      {
        "pathname": "/docs/documentation/compute-service/create-an-endpoint-from-an-existing-container",
        "title": "Create an endpoint from an existing container",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-endpoint-from-existing-container/create-custom-endpoints-from-a-container",
    "code_snippets": [
      {
        "code": "curl -X POST '<your-endpoint-url>/predict' --data '{"prompt": "What state is Los Angeles in?", "max_length": 100}' -H 'content-type: application/json' -H "Authorization: Bearer $YOUR_TOKEN"",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.

OctoAI allows you to create endpoints from custom containers. We support all containers with an HTTP server written in any language, as long as your container comes with a declarative configuration of which port is exposed for inferences and is built for a GPU.
Our coverage includes and is not limited to containers built using Fast API, cog, s2i, Nvidia Triton Inference Server, and Sagemaker/AzureML/Vertex AI based ML containers.

We allow you to pull containers from any private registry, except registries running in a fully private network (e.g. AWS ECR, Gitlab registries in private environments). This means you can pull containers from Docker Hub, Quay, Gitlab, GitHub, etc. that are not in private networks.


If you don't already have a container in hand, see our guide to building a custom endpoint with our CLI.
In this example, we will create a production-grade endpoint for a Flan T5 container pre-built here for a Question Answering application.
In the web UI, navigate to the Custom Endpoints page. Click on “New Endpoint,” then specify the following fields:

Endpoint Name: This name for your endpoint will be part of the endpoint URL you end up integrating into your application-- that URL will be https://<endpoint-name>-<account-id>.octoai.run/<inference-route>

Container Image: The reference to your container needs to be in "[registry/]organization/repository[:tag]" format. In our example, the container image is vanessahlyan/flan-t5-small-pytorch-sanic:latest, hosted here on Docker Hub.

Container Port: The port of the container at which inferences are run is 8000 in our Flan T5 example, as defined here. Make sure to change the default value from 8080 to 8000 in the case of this example!

Registry Credential: Registry Credential defaults to Public, which indicates that your container is available for anyone on the internet to pull. If your container is instead stored in a private registry, follow the guide in Pulling containers from a private registry to store your registry credentials so that we can pull the private container.

Health Check Path: See Health Check Path in custom containers

Enable Public Access: By default, your endpoints are private and require an access token, but you can change this setting such that anyone who knows your endpoint URL can use it.

Active: Whether to spin down all replicas and disable inferences to it.

Specify Secrets: Provide secrets to mount onto the container, such as database secrets that you want to reuse across endpoints. Follow the guide in Setting up account-wide secrets for your custom endpoints to set these up.

Environment Variables: Just like secrets, these variables will be mounted onto the container at runtime. If you have a variable with key foo whose value is bar, you will get a variable foo whose value is bar mounted to the container.

Select hardware: We offer three tiers of hardware-- see Hardware, Pricing, and Billing for more information. Your choice of hardware determines the pricing that you will pay for your endpoint.

Minimum Replicas: The default minimum number of replicas is 0, which means we autoscale down to 0 whenever your endpoint is not receiving requests from your users and the timeout period has passed (this is a way to keep your costs down). Minimum replicas should be set to a higher number if you want to ensure highest uptime for your users and avoid cold starts. Cold start means there is currently no active container instance running inferences on our servers for your application, so we need to incur extra latency to spin up a new one.

Maximum Replicas: This is an important measure for capping your maximum cost. The maximum number of replicas should be set based on how much simultaneous capacity you're willing to support at your heaviest traffic periods. For example, if you set your maximum replicas at 5, and each inference takes 1 second for your model on a GPU, then your application will be able to support a total of 250 inferences per minute without queueing. If your traffic goes above that level (well done you!), requests will be queued until they can be handled, which will increase your average request response time.

Timeout: How many seconds to wait before we scale down your last replica since the time you have no more inferences running.


Now click Create and you’ll be directed to a page for your new endpoint.
To run an inference, use the endpoint URL shown in the UI, with the appropriate inference route appended to it. In our example, we serve inferences at the /predict route as defined in this file, so we should send a CURL to <your-endpont-url>/predict. In my specific case, that ishttps://flan-t5-small-01at11ru3fwy.octoai.run/predict.
You need to edit the curl to use your own API token. If you don't have an existing token, refer to how to create an OctoAI API Token

This model expects the inputs prompt and max_length. We can fill in values "What state is Los Angeles in?" and 100 for those fields respectively, and get a response back.

To query the healthcheck for our endpoint (this is optional), we can hit <your-endpoint-url>/healthcheck


Once you use the curl to make an inference, look at the Events tab on the UI and you’ll see live events coming in informing you of the status of your deployment. Once you see here that the container is running, you inferences should complete successfully.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-an-endpoint-from-an-existing-container.overview-root-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-endpoint-from-existing-container/create-custom-endpoints-from-a-container",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Create an endpoint from an existing container",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
      {
        "pathname": "/docs/documentation/compute-service/create-an-endpoint-from-an-existing-container",
        "title": "Create an endpoint from an existing container",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-endpoint-from-existing-container/create-custom-endpoints-from-a-container",
    "content": "Clone our example container implementation here and customize them for your own use cases!

Contact us if you have any questions.",
    "domain": "test.com",
    "hash": "#next-steps",
    "hierarchy": {
      "h0": {
        "title": "Create an endpoint from an existing container",
      },
      "h2": {
        "id": "next-steps",
        "title": "Next steps",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-an-endpoint-from-an-existing-container.overview-next-steps-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-endpoint-from-existing-container/create-custom-endpoints-from-a-container",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Next steps",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
      {
        "pathname": "/docs/documentation/compute-service/create-an-endpoint-from-an-existing-container",
        "title": "Create an endpoint from an existing container",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-endpoint-from-existing-container/pulling-containers-from-a-private-registry",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.

When you create an endpoint from a custom container, you will see the following page, which has a field for a "Registry Credential." By default, this field is set to "Public," which requires that your container is available for anyone on the internet to pull and no registry credentials will be attached. If your container is instead stored in a private registry, click on the dropdown and then the + sign next to add a "New registry credential".

When you click a model will open where you can set a name for the credential, enter the username and password for the registry.

For example, if you are storing your container in a private repository within DockerHub, you can set the credential name to "docker" and look for your DockerHub username in the top right area of the navigation bar in the DockerHub  UI once you've logged in. To get your DockerHub access token, go to your DockerHub Account Settings Security page, then click the New Access Token button.


Once you've added all the fields in the modal and clicked Save, the Registry Credential field will automatically change to the credential you just set up. Complete the remaining fields to create the endpoint.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-an-endpoint-from-an-existing-container.pulling-containers-from-a-private-registry-root-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-endpoint-from-existing-container/pulling-containers-from-a-private-registry",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Pulling containers from a private registry",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
      {
        "pathname": "/docs/documentation/compute-service/create-an-endpoint-from-an-existing-container",
        "title": "Create an endpoint from an existing container",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-endpoint-from-existing-container/setting-account-wide-secrets-for-custom-endpoints",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.

When creating a new endpoint from your custom containers, you may wish to mount your account-wide secrets onto your container, such as database secrets. Simply click the Select secret(s) dropdown when creating a new endpoint, and then the + sign next to New Secret.

That would bring up a modal in which you can specify a key and a value for the secret. For example, if you create secret with key foo whose value is bar, you will get a variable foo whose value is bar mounted to the container.

Note that this flow currently cannot access a database that is in a private VPC. Please contact us if you have a private database you wish to access.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-an-endpoint-from-an-existing-container.setting-up-account-wide-secrets-for-your-custom-endpoints-root-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-endpoint-from-existing-container/setting-account-wide-secrets-for-custom-endpoints",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Setting up account-wide secrets for your custom endpoints",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-root-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Create a container and endpoint using the CLI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "The octoai command-line interface (CLI) makes it easy for you create a custom endpoint for OctoAI Compute Service. The octoai CLI guides you through the process of creating an initial valid Python application with an example model, building it, and deploying it.
The octoai CLI includes some endpoint scaffolds with example models that you can deploy and try out right away. After you complete that initial workflow, you can follow the instructions in this document to modify the initial application to use the model or code of your choice on OctoAI Compute Service.",
    "domain": "test.com",
    "hash": "#overview",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "overview",
        "title": "Overview",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-overview-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Overview",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "Follow CLI & SDK Installation to make sure you have the CLI and SDK installed. This guide assumes you are running the latest version of the octoai CLI.",
    "domain": "test.com",
    "hash": "#install-the-cli-and-sdk",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "prerequisites",
        "title": "Prerequisites",
      },
      "h3": {
        "id": "install-the-cli-and-sdk",
        "title": "Install the CLI and SDK",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-install-the-cli-and-sdk-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Install the CLI and SDK",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "Docker is required to build and push container images for custom endpoints.
For Mac, follow these instructions: Install Docker Desktop.
For Linux, follow these instructions: Install Docker Engine.
Make sure that docker is running before you proceed! You can confirm by running docker info.
Docker buildx is also required, and it is included with recent versions of Docker. You can confirm that you have it installed by looking for buildx in the output of docker info, or by running docker buildx.",
    "domain": "test.com",
    "hash": "#install-docker",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "prerequisites",
        "title": "Prerequisites",
      },
      "h3": {
        "id": "install-docker",
        "title": "Install Docker",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-install-docker-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Install Docker",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "export OCTOAI_TOKEN=<PasteYourOctoAITokenHere>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "An OctoAI token authenticates you when interacting with the OctoAI Compute Service programatically.
Go to OctoAI Compute Service and log in. Then:
Click Settings (the gear icon on the left).

Provide a name for your token under API Tokens.

Click Generate.

Copy the access token that appears.


On your terminal window, add the token as an environment variable:",
    "domain": "test.com",
    "hash": "#export-your-octoai-token-on-the-terminal",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "prerequisites",
        "title": "Prerequisites",
      },
      "h3": {
        "id": "export-your-octoai-token-on-the-terminal",
        "title": "Export Your OctoAI Token on the Terminal",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-export-your-octoai-token-on-the-terminal-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Export Your OctoAI Token on the Terminal",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "octoai init",
        "lang": "Shell",
        "meta": "Shell",
      },
      {
        "code": "Initialized project in directory. Build your endpoint with:

	cd directory
	octoai build

You can configure your project by editing the octoai.yaml file.

For the OctoAI CLI developer documentation, please visit https://docs.octoai.cloud/docs/cli/cli-configuration-reference",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Use the octoai CLI to create a directory on your computer with the application code for your first endpoint. Run this command:
The CLI offers you to choose one of the existing scaffolds, which are endpoint examples ready to deploy that showcase how to use the service with different models. The following scaffolds are available:
hello-world (a simple example)

flan-t5 (text-in, text-out, adaptable to generative text use cases)

wav2vec (audio-in, text-out, adaptable for speech to text use cases)

yolov8 (image-in, image-out, adaptable for computer vision use cases)


Use the Up/Down arrow keys to navigate and choose a scaffold, then press Enter.
The rest of this section shows the deployment process using the hello-world scaffold. After you complete this tutorial, be sure to check out the other scaffolds to see how use different kinds models in your endpoints.
The CLI then prompts you for more details:
Directory: the directory name to create. Type hello-world and press Enter.

Endpoint name: the name of the endpoint. Type hello-world and press Enter.

Hardware: which hardware to run on. Use the Up/Down arrow keys to select gpu.t4.medium and press Enter

Secrets: the set of secrets to make available to your container. Use the Up/Down arrow keys to select None/No more and press Enter.

Environment variables: the set of environment variables and their values to make available to your container. Use the Up/Down arrow keys to select None/No more and press Enter.


For more information about secrets and environment variables, see Setting up secrets or environment variables for your custom endpoints.
The CLI shows the following guidance:",
    "domain": "test.com",
    "hash": "#create-your-first-endpoint",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "create-your-first-endpoint",
        "title": "Create Your First Endpoint",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-create-your-first-endpoint-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Create Your First Endpoint",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "The hello-world directory contains the following files:
octoai.yaml - Stores the configuration of your endpoint to be used at deployment time.

requirements.txt - Lists any additional Python requirement packages for your application.

service.py - Contains the logic of your endpoint.

test_request.py - Contains example code to send requests to your endpoint.",
    "domain": "test.com",
    "hash": "#directory-structure",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "create-your-first-endpoint",
        "title": "Create Your First Endpoint",
      },
      "h3": {
        "id": "directory-structure",
        "title": "Directory Structure",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-directory-structure-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Directory Structure",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": """"Example OctoAI service scaffold: Hello World."""
from octoai.service import Service

class HelloService(Service):
    """An OctoAI service extends octoai.service.Service."""

    def setup(self):
        """Perform intialization."""
        print("Setting up.")

    def infer(self, prompt: str) -> str:
        """Perform inference."""
        return prompt",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The code in service.py looks like this:
The octoai.service.Service class is an abstract class that any endpoint has to implement.
The octoai.types package contains type definitions that help endpoints and clients work with data formats such as images and audio while communicating over HTTP.
The Service.setup() method is run at endpoint initialization. This method typically contains setup code that should not be run for every inference, such as downloading model weights from the network and making those available in a member variable in memory to be used by the Service.infer() method.
The Service.infer() method is run for every inference request. This method defines the interface of the endpoint (inputs, outputs, and their types) and contains code to perform inference with the model of your choice and return a response. Note that types and type annotations are required, as the OpenAPI specification for the endpoint is automatically generated from the parameters (names and types) and return type in the infer() method definition. The OpenAPI specification is available at /docs once the endpoint is deployed.
The APIs that are available to you (as well as other examples) are covered later in this document. For more information, see the API reference documentation.",
    "domain": "test.com",
    "hash": "#service-code-structure",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "create-your-first-endpoint",
        "title": "Create Your First Endpoint",
      },
      "h3": {
        "id": "service-code-structure",
        "title": "Service Code Structure",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-service-code-structure-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Service Code Structure",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "octoai build",
        "lang": "Shell",
        "meta": "Shell",
      },
      {
        "code": "{"tag":"docker.io/YourDockerUserName/hello-world:SomeTag"}",
        "lang": "JSON",
        "meta": "JSON",
      },
    ],
    "content": "Inside the hello-world directory, run the following command:
This command builds a Docker container for your endpoint. The first time you run this command, it can take a long time (~15 min), since this process has to download large amounts of data. Subsequent builds (of this endpoint or any other endpoint you create) on the same machine are much faster.
When the build completes, the octoai CLI shows the name of the image tag:
The octoai CLI keeps track of this value for you, so you do not need to remember it.",
    "domain": "test.com",
    "hash": "#build-your-endpoint",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "build-your-endpoint",
        "title": "Build your Endpoint",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-build-your-endpoint-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Build your Endpoint",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "octoai run --command "python3 test_request.py"",
        "lang": "Shell",
        "meta": "Shell",
      },
      {
        "code": "{'output': 'Hello World!', ...}",
        "lang": "JSON",
        "meta": "JSON",
      },
    ],
    "content": "Before deploying your endpoint, you can run the container locally and send it a request to verify that it is working properly.


Target Platforms
The images are built for target linux/amd64 so they can be deployed to OctoAI Compute Service. If your computer is of a different architecture (for example, an M1/M2 Macbook), running the container locally can be slow (up to 15 min for yolov8).
The scaffolds included on the octoai CLI all work locally without a GPU. However, other models that require GPU acceleration may not run locally at all if a GPU is not available in your system.
To test locally, run this command:
The command octoai run --command "python3 test_request.py" does two things: it runs your endpoint container locally in the background, and it runs the python script test_request.py to send a request to the endpoint container. After a few seconds, the output should be similar to this:",
    "domain": "test.com",
    "hash": "#run-your-endpoint-locally",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "run-your-endpoint-locally",
        "title": "Run Your Endpoint Locally",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-run-your-endpoint-locally-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Run Your Endpoint Locally",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "$ octoai run -l
Starting container: octoai-local-6585
Initializing container: octoai-local-6585
Running container in foreground
2023-10-10 18:24:46,529 INFO server.py:638 run
2023-10-10 18:24:46,532 INFO server.py:520 Using service in service.HelloService.
2023-10-10 18:24:46,534 INFO server.py:520 Using service in service.HelloService.
INFO:     Started server process [1]
INFO:     Waiting for application startup.
2023-10-10 18:24:46,545 INFO server.py:347 status: State.UNINITIALIZED -> State.LAUNCH_PREDICT_LOOP
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8080 (Press CTRL+C to quit)
2023-10-10 18:24:47,027 INFO server.py:347 status: State.LAUNCH_PREDICT_LOOP -> State.SETUP_SERVICE
2023-10-10 18:24:47,028 INFO server.py:347 status: State.SETUP_SERVICE -> State.RUNNING",
        "lang": "Shell",
        "meta": "Shell",
      },
      {
        "code": "$ python3 test_request.py
{'output': 'Hello world!', 'analytics': {'inference_time_ms': 5.74e-06, 'performance_time_ms': 0.694469}}",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "During development, it is often useful to have one terminal running the endpoint and showing its logs while sending a request from a separate terminal. This helps you iterate on your code and clearly see client and server errors separately, so you can make the appropriate fixes. Follow these steps:
First, run octoai run -l. This runs the endpoint in the foreground and displays the server logs as they occur. If there are any issues with your endpoint implementation code, you will see errors here either at initialization or when processing client requests.
Second, open a new terminal and run python3 test_request.py. This runs the Python client code that sends a request to your endpoint. You typically will see the same response as before, but if there are errors returned to the client, you will see them here.",
    "domain": "test.com",
    "hash": "#two-terminal-workflow",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "run-your-endpoint-locally",
        "title": "Run Your Endpoint Locally",
      },
      "h3": {
        "id": "two-terminal-workflow",
        "title": "Two-Terminal Workflow",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-two-terminal-workflow-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Two-Terminal Workflow",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "octoai deploy",
        "lang": "Shell",
        "meta": "Shell",
      },
      {
        "code": "{"name":"hello-world", "endpoint": "https://hello-world-<hash>.octoai.run"}",
        "lang": "JSON",
        "meta": "JSON",
      },
      {
        "code": "octoai endpoint list",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "To deploy your endpoint, run this command:
This command reads the endpoint configuration from octoai.yaml and uses those settings to create the endpoint. After reading the endpoint configuration, this command pushes your container to Docker Hub. If this is the first endpoint you created, this may take a while depending on the speed of your internet connection. When the push is complete, the CLI shows you the URL of your new endpoint:
The endpoint URL starts with your endpoint name and has additional characters added (shown as <hash> in the example above).
You can now verify your endpoint has been created with:",
    "domain": "test.com",
    "hash": "#deploy-your-endpoint",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "deploy-your-endpoint",
        "title": "Deploy Your Endpoint",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-deploy-your-endpoint-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Deploy Your Endpoint",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "endpoint_config:
  name: hello-world
  hardware: gpu.t4.medium
  registry:
    host: docker.io
    path: <YourDockerHubUserName>/hello-world",
        "lang": "YAML",
        "meta": "YAML",
      },
      {
        "code": "endpoint_config:
  name: hello-world
  hardware: gpu.t4.medium
  max-replicas: 5
  registry:
    host: docker.io
    path: <YourDockerHubUserName>/hello-world",
        "lang": "YAML",
        "meta": "YAML",
      },
      {
        "code": "octoai deploy",
        "lang": "Shell",
        "meta": "Shell",
      },
      {
        "code": "octoai endpoint list",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "The octoai CLI stores your deployment configuration in the octai.yaml configuration file. Initially this file is populated using the answers to the prompts that you provided when running octoai init:
The configuration file supports additional directives that enable you to customize different aspects of your deployment. For example, you can specify your maximum number of replicas:
Then you can redeploy your endpoint:
You can verify that the endpoint has been updated:
Notice that the REPLICAS field now shows [0-5] instead of [0-3] as was the previous default.
For a list of all supported configuration directives, see the Configuration Reference section.",
    "domain": "test.com",
    "hash": "#modify-your-endpoint",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "modify-your-endpoint",
        "title": "Modify Your Endpoint",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-modify-your-endpoint-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Modify Your Endpoint",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "export OCTOAI_TOKEN=(paste your OctoAI API token from octoai.cloud/settings)
python3 test_request.py --endpoint https://hello-world-<hash>.octoai.run",
        "lang": "Shell",
        "meta": "Shell",
      },
      {
        "code": "{'output': 'Hello World!', ...}",
        "lang": "JSON",
        "meta": "JSON",
      },
    ],
    "content": "Ensure you installed the OctoAI Python SDK. Then, to send a request to your endpoint:
When you first send this request, your endpoint is performing a cold start, which means that no replicas were running and the first one has to start and load your container. This may take one to two minutes for the example scaffolds. After this time, you should see a response similar to this:
Once you have an active replica, you can run the test command repeatedly and the endpoint responds quickly. OctoAI Compute Service lets you control the minimum and maximum number of replicas for your endpoint either from the web user interface or from the octoai CLI:
To see your endpoint in the web user interface, go to https://octoai.cloud/endpoints. Then click hello-world. To edit the endpoint, click Edit. On the next screen you can set the minimum and maximum number of replicas.

To use the CLI to update the minimum and maximum number of replicas, you can edit the octoai.yaml configuration file and redeploy your endpoint with octoai deploy.


Congratulations! You have now created your first endpoint on OctoAI Compute Service and ran remote inference.",
    "domain": "test.com",
    "hash": "#send-a-request-to-your-endpoint",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "send-a-request-to-your-endpoint",
        "title": "Send a Request to Your Endpoint",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-a-request-to-your-endpoint-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Send a Request to Your Endpoint",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "from octoai.client import OctoAI

inputs = {"prompt": "Hello world!"}

def main(endpoint):
    """Run inference against the endpoint."""
    # create an OctoAI client
    client = OctoAI()

    # perform inference
    response = client.infer(endpoint_url=f"{endpoint}/infer", inputs=inputs)

    # show the response
    print(response)",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The client code in test_request.py looks like this:
The octoai.client.Client class enables you to query your endpoint as shown here.",
    "domain": "test.com",
    "hash": "#client-code-structure",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "send-a-request-to-your-endpoint",
        "title": "Send a Request to Your Endpoint",
      },
      "h3": {
        "id": "client-code-structure",
        "title": "Client Code Structure",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-client-code-structure-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Client Code Structure",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "$ octoai logs --name hello-world
19s   hello-world-<hash>   octoai server
18s   hello-world-<hash>   Using service in service.HelloService.
18s   hello-world-<hash>   run
18s   hello-world-<hash>   Setting up.
15s   hello-world-<hash>   Started server process [1]
15s   hello-world-<hash>   Waiting for application startup.
15s   hello-world-<hash>   Application startup complete.
15s   hello-world-<hash>   Uvicorn running on http://0.0.0.0:8080
13s   hello-world-<hash>   1.2.3.4:1234 - "POST /infer HTTP/1.1" 200 OK",
        "lang": "Text",
        "meta": "Text",
      },
    ],
    "content": "Now that your endpoint has served one inference, there are some logs available for you to view. OctoAI Compute Service lets you view the server logs for your endpoint using the web user interface or the octoai CLI:
To view the server logs using the web interface, go to https://octoai.cloud/endpoints. Then click hello-world. Then click the Logs button on the top right of the page.

To view the server logs using the CLI, use the octoai logs --name hello-world command.


For example, using the CLI the logs look similar to this:",
    "domain": "test.com",
    "hash": "#view-endpoint-logs",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "view-endpoint-logs",
        "title": "View Endpoint Logs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-view-endpoint-logs-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "View Endpoint Logs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "Once you have built and deployed a hello-world endpoint to OctoAI Compute Service, it is time to customize the endpoint for your use case. To customize your endpoint, you will need:
A list Python dependencies for your model of interest

Example Python code to load your model of interest

Example Python code to perform inference with your model of interest


Then you can modify the Hello World endpoint implementation to call your model of interest instead. Depending on the modality of your model of interest, it may be easier to start with a different scaffold than hello-world (such as flan-t5, wav2vec, or yolov8).",
    "domain": "test.com",
    "hash": "#customize-your-endpoint",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-customize-your-endpoint-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Customize Your Endpoint",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "This section shows you how to modify the hello-world endpoint implementation to call the Flan-T5 model from HuggingFace instead. You can follow the process outlined here to use any other HuggingFace model of your interest.",
    "domain": "test.com",
    "hash": "#use-a-huggingface-model",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-use-a-huggingface-model-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Use a HuggingFace Model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "sentencepiece==0.1.99
torch==2.0.1
transformers==4.29.2",
        "lang": "Text",
        "meta": "Text",
      },
    ],
    "content": "The hello-world endpoint implementation contains an empty requirements.txt file.
Edit this file and add the corresponding requirements for Flan-T5:",
    "domain": "test.com",
    "hash": "#step-1-add-python-requirements",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model",
      },
      "h4": {
        "id": "step-1-add-python-requirements",
        "title": "Step 1: Add Python Requirements",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-1-add-python-requirements-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Step 1: Add Python Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "# add these imports
from transformers import T5ForConditionalGeneration, T5Tokenizer

from octoai.service import Service

class T5Service(Service):
    """An OctoAI service extends octoai.service.Service."""

    # update this method as shown here
    def setup(self):
        """Download model weights to disk."""
        self.tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")
        self.model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small")

    # update this method as shown here
    def infer(self, prompt: str) -> str:
        """Perform inference with the model."""
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.model.generate(**inputs)
        response = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)

        return response[0]",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The hello-world endpoint implementation has an empty Service.setup() method in service.py, since it does not use any model, and a trivial Service.infer() method.
Edit service.py and add the required imports, the model loading code, and the model inferencing code:
In this case, our model of interest is of the same modality as the hello-world examples. For more information on how to use other modalities, see Send and Receive Images and Send and Receive Audio.",
    "domain": "test.com",
    "hash": "#step-2-add-custom-model-code",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model",
      },
      "h4": {
        "id": "step-2-add-custom-model-code",
        "title": "Step 2: Add Custom Model Code",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-2-add-custom-model-code-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Step 2: Add Custom Model Code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "import argparse

from octoai.client import OctoAI

inputs = {"prompt": "What country is California in?"}
...",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The hello-world endpoint implementation has a sample test_request.py that makes a request to your endpoint. In this case, our model of interest is of the same modality as the hello-world example, so you can just change the prompt.",
    "domain": "test.com",
    "hash": "#step-3-modify-sample-client-code",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model",
      },
      "h4": {
        "id": "step-3-modify-sample-client-code",
        "title": "Step 3: Modify Sample Client Code",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-3-modify-sample-client-code-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Step 3: Modify Sample Client Code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "endpoint_config:
  name: flan-t5
  hardware: gpu.t4.medium
  regcred-key: dockerhub
  registry:
    host: docker.io
    path: <YourDockerHubUserName>/flan-t5",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "The hello-world endpoint implementation has a minimal octoai.yaml file.
Update it to change the endpoint name and image tag:",
    "domain": "test.com",
    "hash": "#step-4-modify-the-deployment-configuration",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model",
      },
      "h4": {
        "id": "step-4-modify-the-deployment-configuration",
        "title": "Step 4: Modify the Deployment Configuration",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-4-modify-the-deployment-configuration-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Step 4: Modify the Deployment Configuration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "octoai build
octoai run --command "python3 test_request.py"",
        "lang": "Shell",
        "meta": "Shell",
      },
      {
        "code": "octoai deploy
octoai endpoint list
python3 test_request.py --endpoint https://flan-t5-<hash>.octoai.run",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Once you have modified the endpoint implementation, rebuild and test your endpoint:
Then, deploy your updated endpoint:
Congratulations! You have now customized your first endpoint on OctoAI Compute Service.",
    "domain": "test.com",
    "hash": "#step-5-build-and-deploy",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "use-a-huggingface-model",
        "title": "Use a HuggingFace Model",
      },
      "h4": {
        "id": "step-5-build-and-deploy",
        "title": "Step 5: Build and Deploy",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-step-5-build-and-deploy-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Step 5: Build and Deploy",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "The octoai CLI generates and uses a pre-defined Dockerfile to build a container for your endpoint. The CLI does not expose this Dockerfile by default, since most endpoints can be created successfully without having to view or change this file. However, the octoai CLI provides options for you to view and modify the Dockerfile if your use case requires it. For example, your model may require that you install additional native libraries in the container.
Keep in mind that you do not need to view or modify the Dockerfile to specify Python dependencies, since you can provide those in the requirements.txt file in your project directory.
To view and modify the Dockerfile:
Inside your project directory, run octoai build -g. Instead of building the container, this command generates a Dockerfile inside your project directory.

Inspect and modify the resulting Dockerfile as needed.

Build your container with octoai build. The build command uses the Dockerfile in your project directory to build your container (if one is available), instead of the pre-defined one. Note that using your own Dockerfile can increase cold start times for your endpoint. Contact us if you would like assistance on this subject.




Older versions and the -d flag
Versions of octoai prior to 0.4.5 require that you build with the -d flag to use your custom Dockerfile. Versions 0.4.5 and above always use a custom Dockerfile if it is available in the project directory.
After you have built the container for your endpoint using a customized Dockerfile, you can continue with deployment in the same manner as before by running octoai deploy.",
    "domain": "test.com",
    "hash": "#modify-the-dockerfile",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "modify-the-dockerfile",
        "title": "Modify the Dockerfile",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-modify-the-dockerfile-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Modify the Dockerfile",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "octoai build --setup",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "The Service.setup() method typically contains code that downloads model weights to disk. When you build an endpoint as described so far, this method is called right after the server starts, and the weights are downloaded to disk before the server can serve the first request.
The octoai CLI also enables you to run Service.setup() at container build time, such that the model weights downloaded to the local filesystem become part of the container image instead. In most cases, there is no clear benefit to embedding your weights with the container, since the container image will be larger and take longer to download, even if the server can serve requests soon after starting.
To include your model weights in your container image, add the --setup option to octoai build:",
    "domain": "test.com",
    "hash": "#include-model-weights-in-the-container",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "include-model-weights-in-the-container",
        "title": "Include Model Weights in the Container",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-include-model-weights-in-the-container-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Include Model Weights in the Container",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "from octoai.service import Service

class MyService(Service):

    def infer(self, image: Image) -> str:
        image_pil = image.to_pil()
        output = self.model(image_pil)

        return output[0]",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The octoai.types package contains helpful classes if you are customizing your endpoint to use a model that receives or generates images. To define an endpoint for a model that receives or generates images in service.py, use the Image type as a parameter or return type in the Service.infer() signature. For example, this would be for a model that takes images as Python Pillow objects as input and generates text
For a detailed example on how to send and receive images, see the yolov8 scaffold when running octoai init. For a list of useful methods available to send and receive images, see the API reference documentation.",
    "domain": "test.com",
    "hash": "#send-and-receive-images",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "send-and-receive-images",
        "title": "Send and Receive Images",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-and-receive-images-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Send and Receive Images",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "from octoai.service import Service
from octoai.types import Audio

class MyService(Service):

    def infer(self, audio: Audio) -> str:
        audio_array, sampling_rate = audio.to_numpy()
        output = self.model(audio_array, sampling_rate)

        return output[0]",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The octoai.types package contains helpful classes if you are customizing your endpoint to use a model that receives or generates audio. To define an endpoint for a model that receives or generates audio in service.py, use the Audio type as a parameter or return type in the Service.infer() signature. For example, this would be for a model that takes audio as input and generates text:
For a detailed example on how to send and receive audio, see the wav2vec scaffold when running octoai init. For a list of useful methods available to send and receive audio, see the API Reference documentation.",
    "domain": "test.com",
    "hash": "#send-and-receive-audio",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "send-and-receive-audio",
        "title": "Send and Receive Audio",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-and-receive-audio-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Send and Receive Audio",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "from octoai.service import Service
from octoai.types import Video

class MyService(Service):

    def infer(self, video: Video) -> str:
        video_frames = video.to_numpy()
        output = self.model(video_frames)

        return output[0]",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The octoai.types package contains helpful classes if you are customizing your endpoint to use a model that receives or generates video. To define an endpoint for a model that receives or generates video in service.py, use the Video type as a parameter or return type in the Service.infer() signature. For example, this would be for a model that takes video as input and generates text:
For a list of useful methods available to send and receive video, see the API Reference documentation. The Video type was added in SDK version 0.5.0.",
    "domain": "test.com",
    "hash": "#send-and-receive-video",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "send-and-receive-video",
        "title": "Send and Receive Video",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-and-receive-video-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Send and Receive Video",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "content": "You can create instances of the pre-defined media types (Image, Audio, and Video) from local files and from remote HTTP URLs.
The first approach is the .from_file() function, such as Image.from_file("my_file.jpg"). When you send an Image object created like this to your endpoint, the image data is encoded as base64. This approach is advantageous when your media assets are not available as remote URLs already or when you are working with smaller files, such that the base64 encoding and decoding penalty is small.
The second approach is the .from_url() function, such as Image.from_url("http://myserver.net/my_file.jpg"). When you send an Image object created like this to your endpoint, the image data is not included in the request, just the URL reference is. Inside your endpoint implementation, when you call methods that need the image data, such as Image.to_pil(), the image is downloaded then and data is accessed. This approach is advantageous when your media assets are already available as remote URLs or when you are working with large files, such that the base64 encoding and decoding penalty is noticeable.
For more information, see the API Reference documentation. Support for URL media references was added in SDK version 0.5.0.",
    "domain": "test.com",
    "hash": "#send-media-as-url-references",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "send-media-as-url-references",
        "title": "Send Media as URL References",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-media-as-url-references-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Send Media as URL References",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "from typing import Any, Dict

from octoai.service import Service, path
from octoai.types import File

class FormDataService(Service):
    def setup(self):
        print("Setting up FormDataService")

    def infer(self, data: str) -> str:
        # the endpoint will expose both methods
        return "Use /infer-form-data for form data inference"

    @path("/infer-form-data")
    def infer_form_data(self, file: File, metadata: str) -> Dict[str, Any]:
        # this simple example shows the file information
        # you can instead send the file contents to your model
        return {
            "file_name": file.filename,
            "file_content_type": file.content_type,
            "file_size": file.size,
            "metadata": metadata,
        }",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "import httpx

response = httpx.post(
    "https://<your-endpoint>.octoai.run/infer-form-data",
    files={"file": open("audio.wav", "rb")},
    data={"metadata": "Some info about this audio file"},
)",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "In addition to sending media either as base-64 encoded data or as URL references as described in the previous sections, you can also define an additional API route for your model that accepts form data. You can use this feature to upload any kind of binary file directly to your model.
To support form data uploads, you implement the Service.infer_form_data() method in your endpoint. For example:
You can customize the API route where your implementation of infer_form_data will be exposed within your endpoint using the @path() annotation.


Argument Types for Form Data
The Service.infer_form_data() signature can only contain arguments of types octoai.types.File or str. If you need to provide additional metadata with your files, such as a JSON object, you can serialize it as a string so you can receive it inside the service implementation.
To send a file with some metadata to the endpoint implementation in the previous example, you can use the httpx library as follows.",
    "domain": "test.com",
    "hash": "#send-binary-files-as-form-data",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "send-binary-files-as-form-data",
        "title": "Send Binary Files as Form Data",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-send-binary-files-as-form-data-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Send Binary Files as Form Data",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "from octoai.service import Service, path

class MultipleRoutesService(Service):

  def setup(self):
    print("Setting up MultipleRoutesService")

  # the infer() endpoint is always required
  def infer(self, prompt: str) -> str:
    return prompt

  # this method is exposed as /new-route
  @path("/new-route")
  def my_new_route(self, input_text: str):
    return input_text

  # this method is exposed as /my-new-route2
  def my_new_route2(self, input_text: str):
    return input_text",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The OctoAI SDK enables you to define additional routes in your endpoint. The following example shows how to define two additional endpoints in a Service implementation:
You can customize the path where new methods are exposed by specifying the @path annotation. If you do not provide this annotation, the method will be exposed using the method name with underscores replaced by dashes.
This feature was introduced in SDK version 0.7.2 (CLI version 0.5.8).",
    "domain": "test.com",
    "hash": "#support-multiple-routes",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "customize-your-endpoint",
        "title": "Customize Your Endpoint",
      },
      "h3": {
        "id": "support-multiple-routes",
        "title": "Support Multiple Routes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-support-multiple-routes-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Support Multiple Routes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "endpoint_config:
  name: yolov8                  # Unique endpoint name (required)
  display-name: Yolov8          # User-visible endpoint name. (optional)
  description: Object Detection # User-visible endpoint description. (optional)
  hardware: gpu.t4.medium       # Use a T4 hardware type (required)
  min-replicas: 1               # Keep a minimum of one replica (optional)
  max-replicas: 3               # scale up up to 3 replicas (optional)
  scaledown-in-seconds: 30      # scale down after 30 seconds of inactivity. (optional)
  concurrency-per-replica: 2    # Concurrent requests sent to replica (optional)
  public: false                 # Whether the endpoint is publicly visible (optional)
  regcred-key: dockerhub        # Registry credentials for OctoAI to pull your image (optional)
  secrets:                      # Secrets stored in OctoAI to surface as env. variables (optional)
  - mykey
  - mysecondkey
  env_overrides:                # Environment variables to set in each replica (optional)
    key1: value1
    key2: value2
  registry:
    host: docker.io             # Registry hostname (required)
    path: username/yolov8       # Registry path to image (required)
    tag: v1                     # Tag (optional; not recommended to be set. Defaults to a generated UUID.)
  service-module: app.service   # Path to python module to run (optional); defaults to "service"",
        "lang": "YAML",
        "meta": "YAML",
      },
    ],
    "content": "The following snippet shows a fully populated configuration file with all supported directives you can use to configure your endpoint for deployment.",
    "domain": "test.com",
    "hash": "#reference-octoaiyaml-configuration-file",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "reference-octoaiyaml-configuration-file",
        "title": "Reference: octoai.yaml Configuration File",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-reference-octoaiyaml-configuration-file-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Reference: octoai.yaml Configuration File",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "[{
    'name': 'bus',
    'class': 5,
    'confidence': 0.95,
    'box': {
        'x1': 2.91,
        'x2': 809.51,
        'y1': 230.68,
        'y2': 881.00
    }
}, ...]",
        "lang": "JSON",
        "meta": "JSON",
      },
      {
        "code": "def infer(self, image: Image) -> Dict[str, Any]:",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "class Box(BaseModel):
    """Represents corners of a detection box."""

    x1: float
    x2: float
    y1: float
    y2: float

class Detection(BaseModel):
    """Represents a detection."""

    name: str
    class_: int = Field(..., alias="class")
    confidence: float
    box: Box

class YOLOResponse(BaseModel):
    """Response includes list of detections and rendered image."""

    detections: List[Detection]
    image_with_boxes: Image",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "def infer(self, image: Image) -> YOLOResponse:
    ...
    # Return detection data and a rendered image with boxes
    return YOLOResponse(
        detections=[Detection(**d) for d in detections],
        image_with_boxes=Image.from_pil(img_out_pil),
    )",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "Endpoints that you create using the octoai CLI expose the signature of your Service.infer() method as an OpenAPI specification that is exposed in the /docs HTTP path of your endpoint. Clients of your endpoint can reference this API specification to understand how to query your endpoint using the correct input names and types, as well as identify the type of the output that your endpoint returns.
For most models, you can use primitive Python types and the pre-defined types (Image, Audio, and Video) from the OctoAI Python SDK in your Service.infer() signature. However, some models use custom schemas in some of their inputs or outputs. For example, the YOLOv8 model included in one of the scaffolds returns a list of predictions that have the following schema:
For a model like this, you could define your return type as a list of dictionaries:
However, the OpenAPI specification that the endpoint provides to your client would not give them any information about the schema of the predictions. To address this limitation, the OctoAI Python SDK enables you to define custom entity classes using Pydantic models to capture these schemas and include them in the OpenAPI specification of the endpoint. For example, the YOLOv8 scaffold that you can select when creating a new endpoint with octoai init defines the following entities to capture the structure of the prediction shown above:
Then the infer() signature in the YOLOv8Service implementation returns a YOLOResponse:
The YOLOResponse entity (and any other Pydantic model you use as a return type) is converted automatically to JSON before the endpoint sends it to the client.
For Pydantic models you use as input arguments in infer(), the JSON data that the client provides must conform to the parameters of the entity, and it is converted to a Pydantic object automatically.",
    "domain": "test.com",
    "hash": "#appendix-openapi-specification-and-pydantic-types",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "appendix-openapi-specification-and-pydantic-types",
        "title": "Appendix: OpenAPI Specification and Pydantic Types",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-appendix-openapi-specification-and-pydantic-types-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Appendix: OpenAPI Specification and Pydantic Types",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "code_snippets": [
      {
        "code": "octoai init --registry docker.io",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Endpoints that you create with the octoai CLI use the OctoAI Compute Registry, which works seamlessly with your OctoAI authorization token. However, if you would prefer to use some other registry (such us DockerHub), you can configure your endpoint to do so.
Obtain your credentials for your custom registry. For example, for DockerHub, you can generate an access token from the Account Settings page.

Log into Docker on your terminal. Use the docker login command with your registry username and access token as your password.

Provide your registry credentials to the OctoAI Compute Service. To provide registry credentials using the web UI, see Pulling containers from a private registry. To provide registry credentials using the octoai CLI, use the octoai regcred create command. To verify your registry credentials were created correctly, use the octoai regcred list command.


To create a new endpoint that uses a custom Docker registry, issue the --registry option to the octoai init command with the registry URL. For example, for DockerHub:
The CLI will then prompt you for the image repository namespace (such as your DockerHub username) and the image repository name (which you could set to be the same as your endpoint name, or similar). Then the CLI will let you choose the right credentials to use to authenticate with the registry.",
    "domain": "test.com",
    "hash": "#appendix-using-a-custom-docker-registry",
    "hierarchy": {
      "h0": {
        "title": "Create a container and endpoint using the CLI",
      },
      "h2": {
        "id": "appendix-using-a-custom-docker-registry",
        "title": "Appendix: Using a Custom Docker Registry",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.create-a-container-and-endpoint-using-the-cli-appendix-using-a-custom-docker-registry-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/create-custom-endpoint-using-the-cli",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Appendix: Using a Custom Docker Registry",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.

You can use our CLI to easily create containers for any model written in Python. However, note that OctoAI is able to run any container with an HTTP server, so you are always welcome to build containers in your own ways (with the understanding that using custom containers means potentially longer cold start/ fewer optimizations).
If you prefer to create your own container from scratch, this tutorial will walk you through one example of how to do so.
In this example, we will build a container for a Flan-T5 small model from the Hugging Face transformers library. This model is commonly used for text generation and question answering, but note that because it's small it does not yield outputs that are as high-quality as other OctoAI LLM endpoints.

An equivalent example for Hugging Face diffusers models can be found in the same GitHub repo.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-root-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Advanced: build a container from scratch in Python",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "content": "Sign up for a Docker Hub account

Download Docker desktop on your local machine

Authenticate the Docker CLI on your machine",
    "domain": "test.com",
    "hash": "#prerequisites",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python",
      },
      "h2": {
        "id": "prerequisites",
        "title": "Prerequisites",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-prerequisites-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Prerequisites",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "content": "All the code in this tutorial can be found at this GitHub repo.",
    "domain": "test.com",
    "hash": "#example-code",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python",
      },
      "h2": {
        "id": "example-code",
        "title": "Example code",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-example-code-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Example code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "code_snippets": [
      {
        "code": """"Model wrapper for serving flan-t5-small."""
import argparse
import typing

import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer

_MODEL_NAME = "google/flan-t5-small"
"""The model's name on HuggingFace."""

_DEVICE: str = "cuda:0" if torch.cuda.is_available() else "cpu"
"""Device on which to serve the model."""

class Model:
    """Wrapper for a flan-t5-small Text Generation model."""

    def __init__(self):
        """Initialize the model."""
        self._tokenizer = T5Tokenizer.from_pretrained(_MODEL_NAME)
        self._model = T5ForConditionalGeneration.from_pretrained(_MODEL_NAME).to(
            _DEVICE
        )

    def predict(self, inputs: typing.Dict[str, str]) -> typing.Dict[str, str]:
        """Return a dict containing the completion of the given prompt.

        :param inputs: dict of inputs containing a prompt and optionally the max length
            of the completion to generate.
        :return: a dict containing the generated completion.
        """
        prompt = inputs.get("prompt", None)
        max_length = inputs.get("max_length", 2048)

        input_ids = self._tokenizer(prompt, return_tensors="pt").input_ids.to(_DEVICE)
        output = self._model.generate(input_ids, max_length=max_length)
        result = self._tokenizer.decode(output[0], skip_special_tokens=True)

        return {"completion": result}

    @classmethod
    def fetch(cls) -> None:
        """Pre-fetches the model for implicit caching by Transfomers."""
        # Running the constructor is enough to fetch this model.
        cls()

def main():
    """Entry point for interacting with this model via CLI."""
    parser = argparse.ArgumentParser()
    parser.add_argument("--fetch", action="store_true")
    args = parser.parse_args()

    if args.fetch:
        Model.fetch()

if __name__ == "__main__":
    main()",
        "lang": "py",
        "meta": "model.py",
      },
    ],
    "content": "First, we define how to run an inference on this model in model.py. The core steps include initializing the model and tokenizer using the transformers Python library, then running a predict() function that tokenizes the text input, runs the model, then de-tokenizes the model back into a text format.",
    "domain": "test.com",
    "hash": "#prepare-python-code-for-running-an-inference",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python",
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough",
      },
      "h4": {
        "id": "prepare-python-code-for-running-an-inference",
        "title": "Prepare Python code for running an inference",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-prepare-python-code-for-running-an-inference-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Prepare Python code for running an inference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "code_snippets": [
      {
        "code": """"HTTP Inference serving interface using sanic."""
import os

import model
from sanic import Request, Sanic, response

_DEFAULT_PORT = 8000
"""Default port to serve inference on."""

# Load and initialize the model on startup globally, so it can be reused.
model_instance = model.Model()
"""Global instance of the model to serve."""

server = Sanic("server")
"""Global instance of the web server."""

@server.route("/healthcheck", methods=["GET"])
def healthcheck(_: Request) -> response.JSONResponse:
    """Responds to healthcheck requests.

    :param request: the incoming healthcheck request.
    :return: json responding to the healthcheck.
    """
    return response.json({"healthy": "yes"})

@server.route("/predict", methods=["POST"])
def predict(request: Request) -> response.JSONResponse:
    """Responds to inference/prediction requests.

    :param request: the incoming request containing inputs for the model.
    :return: json containing the inference results.
    """
    inputs = request.json
    output = model_instance.predict(inputs)
    return response.json(output)

def main():
    """Entry point for the server."""
    port = int(os.environ.get("SERVING_PORT", _DEFAULT_PORT))
    server.run(host="0.0.0.0", port=port, workers=1)

if __name__ == "__main__":
    main()",
        "lang": "py",
        "meta": "server.py",
      },
    ],
    "content": "Next, we wrap this model in a Sanic server in server.py. Sanic is a Python 3.7+ web server and web framework that’s written to go fast. In our server file, we define the following:
A default port on which to serve inferences. The port can be any positive number, as long as it's not in use by another application. 80 is commonly used for HTTP, and 443 is often for HTTPS. In this case we choose 8000.

Two server routes that OctoAI containers should have:
a route for inference requests (e.g. "/predict"). This route for inference requests must receive JSON inputs and JSON outputs.

a route for health checks (e.g. "/healthcheck"). See Healthcheck path in custom containers for a detailed explanation.



Number of workers (not required). Typical best practice is to make this number some function of the # of CPU cores that the server has access to and should use.


In our toy example, the line model_instance = model.Model() executes first, so by the time the server is instantiated our model is ready. That is why the code in our "/healthcheck" route is very straightforward in this example. In your own container, make sure your "/healthcheck" returns 200 only after your model is fully loaded and ready to take inferences.",
    "domain": "test.com",
    "hash": "#create-a-server",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python",
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough",
      },
      "h4": {
        "id": "create-a-server",
        "title": "Create a server",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-create-a-server-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Create a server",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "code_snippets": [
      {
        "code": "sanic==23.3.0
torch==2.0.0+cu118
--extra-index-url https://download.pytorch.org/whl/cu118
sentencepiece==0.1.97
transformers==4.27.4",
        "lang": "bash",
        "meta": "requirements.txt",
      },
      {
        "code": "FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

ARG SERVING_PORT=8000
ENV SERVING_PORT=$SERVING_PORT

WORKDIR /

RUN apt update && \
    apt install -y python3-pip

# Upgrade pip and install the copied in requirements.
RUN pip install --no-cache-dir --upgrade pip
ADD requirements.txt requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy in the files necessary to fetch, run and serve the model.
ADD model.py .
ADD server.py .

# Fetch the model and cache it locally.
RUN python3 model.py --fetch

# Expose the serving port.
EXPOSE $SERVING_PORT

# Run the server to handle inference requests.
CMD python3 -u server.py",
        "lang": "Dockerfile",
        "meta": "Dockerfile",
      },
    ],
    "content": "Now we can package the server by defining a requirements.txt file and a Dockerfile:
Along with installing the dependencies, the Dockerfile also downloads the model into the image at build time. Because the model isn't too big, we can cache it in the Docker image for faster startup without impacting the image size too much. If your model is larger, you may want to pull it on container start instead of caching it in the Docker image. This may affect your container startup time, but keeps the image itself slim.",
    "domain": "test.com",
    "hash": "#package-the-server-in-a-dockerfile",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python",
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough",
      },
      "h4": {
        "id": "package-the-server-in-a-dockerfile",
        "title": "Package the server in a Dockerfile",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-package-the-server-in-a-dockerfile-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Package the server in a Dockerfile",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "code_snippets": [
      {
        "code": "$ docker run --gpus=all -d --rm
    -p 8000:8000 --env SERVER_PORT=8000
    --name "flan-t5-small-pytorch-sanic"
  	"$DOCKER_REGISTRY/flan-t5-small-pytorch-sanic"",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": "$ curl -X GET http://localhost:8000/healthcheck",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": "$ curl -X POST http://localhost:8000/predict \
    -H "Content-Type: application/json" \
    --data '{"prompt":"What state is Los Angeles in?","max_length":100}'",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "Run this Docker image locally on a GPU to test that it can run inferences as expected:
..and in a separate terminal run the following command one or more times
... until you see {"healthy":true} in the terminal output. Now, you can get an inference by running:",
    "domain": "test.com",
    "hash": "#test-the-image-locally",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python",
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough",
      },
      "h4": {
        "id": "test-the-image-locally",
        "title": "Test the image locally",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-test-the-image-locally-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Test the image locally",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "code_snippets": [
      {
        "code": "$ docker push "$DOCKER_REGISTRY/flan-t5-small-pytorch-sanic"",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "Push your Docker image to Docker Hub with:
Now that you have your container, create an endpoint to establish your endpoint on OctoAI.
)",
    "domain": "test.com",
    "hash": "#push-the-image-to-a-cloud-registry",
    "hierarchy": {
      "h0": {
        "title": "Advanced: build a container from scratch in Python",
      },
      "h2": {
        "id": "step-by-step-walkthrough",
        "title": "Step-by-step walkthrough",
      },
      "h4": {
        "id": "push-the-image-to-a-cloud-registry",
        "title": "Push the image to a cloud registry",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.advanced-build-a-container-from-scratch-in-python-push-the-image-to-a-cloud-registry-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/advanced-build-a-container-from-scratch-in-python",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Push the image to a cloud registry",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/compute-service",
        "title": "Compute Service",
      },
    ],
    "canonicalPathname": "/docs/compute-service/health-check-path-in-custom-containers",
    "content": "Contact us at customer-success@octo.ai to request access to the Compute Service.

The healthcheck path is be the server route in the container that indicates when the server is ready to receive requests:
It is strongly recommended that you configure a health check path in your container. Otherwise, you may have inference failures whenever you try to make a request before your server is ready.

If you define a healthcheck, your endpoint has 5 minutes from the time the the image is pulled to return a 200 OK response. This will mark the endpoint as available, and the same criteria applies for additional replicas. If there are 3 consecutive calls to the healthcheck endpoint that return a non-200 OK status, then the replica will be restarted.

You can see an example of a health check path in our Flan T5 container in Advanced: Build a Container from Scratch in Python. The health check path exposed by that container is /healthcheck. After the endpoint is created, one should be able to hit https://<endpoint-name>-<account-id>.octoai.run/healthcheck for a 200 response whenever the server is healthy and ready.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.compute-service.healthcheck-path-in-custom-containers-root-0",
    "org_id": "test",
    "pathname": "/docs/compute-service/health-check-path-in-custom-containers",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Healthcheck path in custom containers",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "description": "How to install the OctoAI CLI",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation",
    "org_id": "test",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "CLI installation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "code_snippets": [
      {
        "code": "curl https://s3.amazonaws.com/downloads.octoai.cloud/octoai/install_octoai_cli_and_sdk.sh -sSfL | sh",
        "lang": "bash",
      },
    ],
    "content": "You can install the latest CLI and Python SDK using the script below, with compatibility for both Mac and Linux systems.",
    "domain": "test.com",
    "hash": "#latest-cli-release",
    "hierarchy": {
      "h0": {
        "title": "CLI installation",
      },
      "h2": {
        "id": "latest-cli-release",
        "title": "Latest CLI release",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-latest-cli-release-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Latest CLI release",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "code_snippets": [
      {
        "code": "octoai version",
        "lang": "bash",
      },
      {
        "code": "octoai --help",
        "lang": "bash",
      },
      {
        "code": "python3 -m pip show octoai-sdk",
        "lang": "bash",
      },
      {
        "code": "Name: octoai-sdk
Version: 0.7.1
Summary: A runtime library for OctoAI.
Home-page:
Author: OctoAI
Author-email:
License:
Location: /Users/sliu/anaconda3/envs/sdk-dev/lib/python3.10/site-packages
Requires: boto3, chevron, click, fastapi, httpx, numpy, pillow, pydantic, python-dateutil, python-multipart, pyyaml, soundfile, types-pyyaml, types-requests, uvicorn
Required-by:",
      },
    ],
    "content": "You can also verify the octoai-sdk has successfully installed in your correct environment.
This should provide an output similar to (though likely with different version numbers):",
    "domain": "test.com",
    "hash": "#verify-the-installation",
    "hierarchy": {
      "h0": {
        "title": "CLI installation",
      },
      "h2": {
        "id": "latest-cli-release",
        "title": "Latest CLI release",
      },
      "h3": {
        "id": "verify-the-installation",
        "title": "Verify the installation",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-verify-the-installation-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Verify the installation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "code_snippets": [
      {
        "code": "rm -rf /usr/local/bin/octoai",
        "lang": "bash",
      },
      {
        "code": "python3 -m pip uninstall octoai-sdk",
        "lang": "python",
      },
    ],
    "content": "Our installation script installs the OctoAI binary to usr/local/bin. You can remove it with:
You can uninstall the Python SDK using:
If you used another method of installation such as brew, rpm, etc, please follow the directions from those services.
If you choose not to install the CLI and SDK using the installation script, you can alternatively use brew or apt. For most people, it is recommended you use the installation script.
Please note, if you install the CLI using brew or apt, you will also need to install the SDK directly from PIP. You can review the installation shell script from CLI & SDK Installation to verify a compatible SDK version number if you run into difficulties, and also review there how to verify the installation was successful for both the SDK and CLI.",
    "domain": "test.com",
    "hash": "#uninstall-the-cli-and-python-sdk",
    "hierarchy": {
      "h0": {
        "title": "CLI installation",
      },
      "h2": {
        "id": "uninstall-the-cli-and-python-sdk",
        "title": "Uninstall the CLI and Python SDK",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-uninstall-the-cli-and-python-sdk-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Uninstall the CLI and Python SDK",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "code_snippets": [
      {
        "code": "python3 -m pip install octoai-sdk",
        "lang": "Python",
      },
      {
        "code": "python3 -m pip install --upgrade octoai-sdk",
        "lang": "Python",
      },
    ],
    "content": "Much of the CLI is dependent on the SDK. You can view our full release history on PyPI here.
If you choose not to use the installation script, you can use pip to install the octoai-sdk Python package directly. We strongly recommend that you install OctoAI in a virtualenv, to avoid conflicting with your system packages.
If you've already installed a previous version of the octoai-sdk, you can also upgrade using:",
    "domain": "test.com",
    "hash": "#latest-python-sdk-release",
    "hierarchy": {
      "h0": {
        "title": "CLI installation",
      },
      "h2": {
        "id": "uninstall-the-cli-and-python-sdk",
        "title": "Uninstall the CLI and Python SDK",
      },
      "h3": {
        "id": "latest-python-sdk-release",
        "title": "Latest Python SDK Release",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-latest-python-sdk-release-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Latest Python SDK Release",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-and-sdk-installation",
    "code_snippets": [
      {
        "code": "brew tap octoml/tap
brew install octoai",
        "lang": "Shell",
      },
      {
        "code": "sudo add-apt-repository -y 'deb [trusted=yes] https://repo.fury.io/octoml/ /'
sudo apt update
sudo apt install octoai",
        "lang": "Shell",
      },
    ],
    "content": "After installing the Python SDK, you can then install the CLI.
Mac:
Debian:
Please see the individual package manager for how to remove an installation, as their directions may change over time.",
    "domain": "test.com",
    "hash": "#alternate-cli-installation-options",
    "hierarchy": {
      "h0": {
        "title": "CLI installation",
      },
      "h2": {
        "id": "uninstall-the-cli-and-python-sdk",
        "title": "Uninstall the CLI and Python SDK",
      },
      "h3": {
        "id": "alternate-cli-installation-options",
        "title": "Alternate CLI Installation Options",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-installation-alternate-cli-installation-options-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-and-sdk-installation",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Alternate CLI Installation Options",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "This document provides a detailed description of every command of the OctoAI CLI. Remember that you can install the CLI by following CLI & SDK Installation.",
    "description": "A reference guide for all CLI commands and their behaviors.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-root-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "CLI reference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Commands related to your OctoAI account.",
    "domain": "test.com",
    "hash": "#account",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "account",
        "title": "account",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-account-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "account",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai account info

EMAIL               KEY            NAME           PROVIDER
[[email protected]](/cdn-cgi/l/email-protection)      XXXXXXXXXXXX   First Last     google",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "List account details for identifying yourself, useful for debugging or reporting your account information.",
    "domain": "test.com",
    "hash": "#account-info",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "account",
        "title": "account",
      },
      "h3": {
        "id": "account-info",
        "title": "account info",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-account-info-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "account info",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Build a Docker image from your OctoAI project.",
    "domain": "test.com",
    "hash": "#build",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "build",
        "title": "build",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-build-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "build",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "-d, --dockerfile <string> Dockerfile overload to use for building; if not provided, one will be generated.

-g, --generate Only generate a Dockerfile rather than building the image, which is the default behavior.

-i, --image <IMAGE> Image URI to use for final tagging.

-s, --service-module <MODULE> The Python module containing the octoai.Service implementation.

--setup Run Service.setup() at image build time.",
    "domain": "test.com",
    "hash": "#flags",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "build",
        "title": "build",
      },
      "h3": {
        "id": "flags",
        "title": "Flags",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Flags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Print the configuration of the CLI, useful for checking your token and configuration settings, or for debugging the behavior of your builds.",
    "domain": "test.com",
    "hash": "#check-config",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "check-config",
        "title": "check-config",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-check-config-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "check-config",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai completion <SHELL>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Generate the autocompletion script for the specified shell. Supports one of: bash, fish, powershell, zsh. For more information on how to use it, our CLI is built with Cobra.",
    "domain": "test.com",
    "hash": "#completion",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "completion",
        "title": "completion",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-completion-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "completion",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Deploy your endpoint to OctoAI. By default, we use your settings from .octoai.yaml, but flags can be passed to on this command to override the file settings.",
    "domain": "test.com",
    "hash": "#deploy",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "deploy",
        "title": "deploy",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-deploy-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "deploy",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "--concurrency-per-replica <INT> Maximum concurrency per replica (default is -1).

--description <DESC> Give this endpoint a human readable description.

-d, --display-name Give this endpoint a human readable display name in the web UI.

-e, --env [<KEY>=<VALUE>] Environment variables to pass to the endpoint; issue once per var: --env FOO=bar --env BAZ=qux.

--hardware <HW_TYPE> The hardware instance to deploy to.

-i, --image <IMAGE> A URI to the Image to deploy.

--max-replicas <INT> Maximum number of replicas to scale up to (default -1)

--min-replicas <INT> Minimum number of replicas to scale down to (default -1)

-n, --name <STRING> Name of the endpoint

--regcred <REGCRED_KEY> Registry credentials key for private image pulls.

--scaledown-secs <INT> Number of seconds of inactivity before scaling down the endpoint (default -1).

--secrets <SECRET_KEY> Secrets to load into the endpoint; issue once per secret: --secrets secretKey1 --secrets secretKey2.

-t, --tag <TAG> Tag to use for the image.

--visibility <public|private> Controls whether the endpoint is public or needs authentication.",
    "domain": "test.com",
    "hash": "#flags-1",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "deploy",
        "title": "deploy",
      },
      "h3": {
        "id": "flags-1",
        "title": "Flags:",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-1-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Flags:",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Commands for creating, editing, or updating your endpoints.",
    "domain": "test.com",
    "hash": "#endpoint",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "endpoint",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Create an endpoint with the provided flags.",
    "domain": "test.com",
    "hash": "#endpoint-create",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint",
      },
      "h3": {
        "id": "endpoint-create",
        "title": "endpoint create",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-create-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "endpoint create",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "--concurrency-per-replica <INT> Maximum concurrency per replica (default is -1).

--description <DESC> Give this endpoint a human readable description.

--pause Pause this endpoint on creation.

-d, --display-name Give this endpoint a human readable display name in the web UI.

-e, --env [<KEY>=<VALUE>] Environment variables to pass to the endpoint; issue once per var: --env FOO=bar --env BAZ=qux.

--hardware <HW_TYPE> The hardware instance to deploy to. Options include "gpu.t4.medium" "gpu.a10g.medium" "gpu.a100-80.medium"

--health-check-path <STRING> Server path for health checks

-i, --image <IMAGE> A URI to the Image to deploy.

--max-replicas <INT> Maximum number of replicas to scale up to (default -1)

--min-replicas <INT> Minimum number of replicas to scale down to (default -1)

-n, --name <STRING> Name of the endpoint

-p, --port <INT> Port to route requests to on the container (default 8080)

--regcred <REGCRED_KEY> Registry credentials key for private image pulls.

--scaledown-secs <INT> Number of seconds of inactivity before scaling down the endpoint (default -1).

--secrets <SECRET_KEY> Secrets to load into the endpoint; issue once per secret: --secrets secretKey1 --secrets secretKey2.

-t, --tag <TAG> Tag to use for the image.

--visibility <public|private> Controls whether the endpoint is public or needs authentication.",
    "domain": "test.com",
    "hash": "#flags-2",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint",
      },
      "h3": {
        "id": "flags-2",
        "title": "Flags:",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-2-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Flags:",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai endpoint get --name <NAME> [-o <OUTPUT_FORMAT>]",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Get information on the endpoint",
    "domain": "test.com",
    "hash": "#endpoint-get",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint",
      },
      "h3": {
        "id": "endpoint-get",
        "title": "endpoint get",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-get-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "endpoint get",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai endpoint list  [-o <OUTPUT_FORMAT>]",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "List endpoints within this account",
    "domain": "test.com",
    "hash": "#endpoint-list",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint",
      },
      "h3": {
        "id": "endpoint-list",
        "title": "endpoint list",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-list-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "endpoint list",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Update an endpoint",
    "domain": "test.com",
    "hash": "#endpoint-update",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint",
      },
      "h3": {
        "id": "endpoint-update",
        "title": "endpoint update",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-update-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "endpoint update",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai endpoint start --name <ENDPOINT_NAME>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Start the endpoint, scale to minimum replicas and start routing traffic to it.",
    "domain": "test.com",
    "hash": "#endpoint-start",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint",
      },
      "h3": {
        "id": "endpoint-start",
        "title": "endpoint start",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-start-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "endpoint start",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai endpoint pause --name <ENDPOINT_NAME>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Pause the endpoint, scale down the replicas to 0 and stop routing traffic to it.",
    "domain": "test.com",
    "hash": "#endpoint-pause",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "endpoint",
        "title": "endpoint",
      },
      "h3": {
        "id": "endpoint-pause",
        "title": "endpoint pause",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-endpoint-pause-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "endpoint pause",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Show the events associated with a given endpoint.",
    "domain": "test.com",
    "hash": "#events",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "events",
        "title": "events",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-events-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "events",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "-f, --follow Tail event stream using our streaming events API. You will only events which occur after you start this command.

-n, --name <string> Name of the endpoint to monitor.

-o, --output-format OutputFormat Output format (default table).",
    "domain": "test.com",
    "hash": "#flags-3",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "events",
        "title": "events",
      },
      "h3": {
        "id": "flags-3",
        "title": "Flags",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-3-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Flags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Help about any command, provides a short summary of this page in the CLI.",
    "domain": "test.com",
    "hash": "#help",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "help",
        "title": "help",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-help-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "help",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai init",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Initialize a brand new project from a scaffold, it will prompt the user for new repository configuration. The command will walk you through configuring your endpoint. See below for an example of creating a new project.
An example of running octoai init with the YoloV8 scaffold",
    "domain": "test.com",
    "hash": "#init",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "init",
        "title": "init",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-init-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "init",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Cache auth credentials for the OCTOAI_TOKEN generated from How to create an OctoAI API token.",
    "domain": "test.com",
    "hash": "#login",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "login",
        "title": "login",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-login-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "login",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Show the logs for a given endpoint.",
    "domain": "test.com",
    "hash": "#logs",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "logs",
        "title": "logs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-logs-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "logs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "-f, --follow Tail endpoint logs using our streaming logs API. You will only events which occur after you start this command.

-n, --name <NAME> Name of the endpoint to monitor.

-o, --output-format OutputFormat Output format (default table).",
    "domain": "test.com",
    "hash": "#flags-4",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "logs",
        "title": "logs",
      },
      "h3": {
        "id": "flags-4",
        "title": "Flags",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-4-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Flags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Manage registry credentials for deploying images from private registries.",
    "domain": "test.com",
    "hash": "#regcred",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "regcred",
        "title": "regcred",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "regcred",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai regcred create --name <NAME> --token <TOKEN> --username <USERNAME>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Creates a registry credential in your account.",
    "domain": "test.com",
    "hash": "#regcred-create",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "regcred",
        "title": "regcred",
      },
      "h3": {
        "id": "regcred-create",
        "title": "regcred create",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-create-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "regcred create",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai regcred get --name <NAME> [-o <OUTPUT_FORMAT>]",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Get a registry credential from your account.",
    "domain": "test.com",
    "hash": "#regcred-get",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "regcred",
        "title": "regcred",
      },
      "h3": {
        "id": "regcred-get",
        "title": "regcred get",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-get-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "regcred get",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai regcred list [-o <OUTPUT_FORMAT>]",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "List your account's registry credentials.",
    "domain": "test.com",
    "hash": "#regcred-list",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "regcred",
        "title": "regcred",
      },
      "h3": {
        "id": "regcred-list",
        "title": "regcred list",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-list-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "regcred list",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai regcred update --name <NAME> --token <TOKEN> --username <USERNAME>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Update a registry credential in your account.",
    "domain": "test.com",
    "hash": "#regcred-update",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "regcred",
        "title": "regcred",
      },
      "h3": {
        "id": "regcred-update",
        "title": "regcred update",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-update-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "regcred update",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai regcred delete --name <KEY>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Delete a registry credential from your account.",
    "domain": "test.com",
    "hash": "#regcred-delete",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "regcred",
        "title": "regcred",
      },
      "h3": {
        "id": "regcred-delete",
        "title": "regcred delete",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-regcred-delete-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "regcred delete",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Run the most recently built container locally for testing before deployment.",
    "domain": "test.com",
    "hash": "#run",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "run",
        "title": "run",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-run-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "run",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "-b, --background Whether to run container in the background.

--command <CMD> Inference command to run. Surround in quotes to pass arguments to the command.

-s, --env <stringArray> Environment variables to pass to the container; issue once per var: --env FOO=bar --env BAZ=qux

--gpus <string> Identifier of GPU device, or 'all' to use all GPUs.

-i, --image <string> Image to run.

-p, --port <int> Host port to bind the container port to. (default 8080)

--timeout int How long to wait for container to start and healthcheck to be ready before timing out. (default 300)

--container-name <string> Assign a name to the container.",
    "domain": "test.com",
    "hash": "#flags-5",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "run",
        "title": "run",
      },
      "h4": {
        "id": "flags-5",
        "title": "Flags",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-flags-5-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Flags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Manage your secrets for your endpoints. These allow you to pass extra API keys, or any secret configuration to your containers. See Setting up account-wide secrets for your custom endpoints for more information about this.",
    "domain": "test.com",
    "hash": "#secret",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "secret",
        "title": "secret",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "secret",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai secret create --key <SECRET_KEY> --value <SECRET_VALUE>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Create a secret in your account with a given key and a value.",
    "domain": "test.com",
    "hash": "#secret-create",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "secret",
        "title": "secret",
      },
      "h3": {
        "id": "secret-create",
        "title": "secret create",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-create-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "secret create",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai secret delete --key <SECRET_KEY>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Delete a secret from your account.",
    "domain": "test.com",
    "hash": "#secret-delete",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "secret",
        "title": "secret",
      },
      "h3": {
        "id": "secret-delete",
        "title": "secret delete",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-delete-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "secret delete",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai secret get -key <SECRET_KEY> [--show] [--output-format OutputFormat]",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Fetch a secret from your account.",
    "domain": "test.com",
    "hash": "#secret-get",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "secret",
        "title": "secret",
      },
      "h3": {
        "id": "secret-get",
        "title": "secret get",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-get-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "secret get",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai secret list [--output-format OutputFormat]",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "List the secrets of your account.",
    "domain": "test.com",
    "hash": "#secret-list",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "secret",
        "title": "secret",
      },
      "h3": {
        "id": "secret-list",
        "title": "secret list",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-list-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "secret list",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "code_snippets": [
      {
        "code": "octoai secret update --key <SECRET_KEY> --value <SECRET_VALUE>",
        "lang": "Shell",
        "meta": "Shell",
      },
    ],
    "content": "Update a secret in your account.",
    "domain": "test.com",
    "hash": "#secret-update",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "secret",
        "title": "secret",
      },
      "h3": {
        "id": "secret-update",
        "title": "secret update",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-secret-update-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "secret update",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/cli",
        "title": "CLI",
      },
    ],
    "canonicalPathname": "/docs/cli/cli-reference-guide",
    "content": "Print the CLI version information useful for checking your build information or reporting issues.",
    "domain": "test.com",
    "hash": "#version",
    "hierarchy": {
      "h0": {
        "title": "CLI reference",
      },
      "h2": {
        "id": "version",
        "title": "version",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.cli.cli-reference-version-0",
    "org_id": "test",
    "pathname": "/docs/cli/cli-reference-guide",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "version",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/installation-and-setup",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-installation",
    "org_id": "test",
    "pathname": "/docs/python-sdk/installation-and-setup",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Python SDK installation & setup",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/installation-and-setup",
    "code_snippets": [
      {
        "code": "python3 -m pip install octoai",
        "lang": "shell",
        "meta": "shell",
      },
    ],
    "content": "The SDK currently supports Python versions 3.8.1 and upwards. It is strongly recommended that you use a virtual environment such as Conda or venv to manage Python packages for your development environment. This helps prevent incompatible dependencies with packages installed irrelevant to your current project or that conflict with system dependencies.
You can view our full release history on PyPi for the latest version of the Python SDK.
For Mac and Linux, use pip to install the octoai sdk Python package. We strongly recommend that you install OctoAI in a virtualenv, to avoid conflicting with your system packages.
Please refer to CLI & SDK Installation for how to install the SDK in conjunction with the CLI for the authoring tool.",
    "domain": "test.com",
    "hash": "#installation-guide",
    "hierarchy": {
      "h0": {
        "title": "Python SDK installation & setup",
      },
      "h3": {
        "id": "installation-guide",
        "title": "Installation Guide",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-installation-installation-guide-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/installation-and-setup",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Installation Guide",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/installation-and-setup",
    "code_snippets": [
      {
        "code": "export OCTOAI_TOKEN=YOUR_TOKEN_HERE",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": "from octoai.client import OctoAI

client = OctoAI()",
        "lang": "Python",
        "meta": "Python",
      },
      {
        "code": "from octoai.client import OctoAI

client = OctoAI(api_key="YOUR_OCTOAI_API_TOKEN_HERE")",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "In order to access endpoints, create an OctoAI API token. Set OCTOAI_TOKEN to the token value wherever you set your environment variables, such as your .bashrc or .env file.
Then when you instantiate the client, it will detect the OCTOAI_TOKEN as an envvar and set it for you.
Alternatively, on creation of the OctoAI class, you can set your token, or the Client also accepts a path to where you've stored your API token from the config_path variable. Please see the OctoAI docs for more information.",
    "domain": "test.com",
    "hash": "#setting-api-token-as-an-environment-variable",
    "hierarchy": {
      "h0": {
        "title": "Python SDK installation & setup",
      },
      "h3": {
        "id": "setting-api-token-as-an-environment-variable",
        "title": "Setting API token as an environment variable",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-installation-setting-api-token-as-an-environment-variable-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/installation-and-setup",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Setting API token as an environment variable",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/python-sdk-inferences",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-inference",
    "org_id": "test",
    "pathname": "/docs/python-sdk/python-sdk-inferences",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Python SDK inference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/python-sdk-inferences",
    "code_snippets": [
      {
        "code": "import time
from octoai.client import OctoAI

client = OctoAI()

# It allows you to run inferences
output = client.infer(endpoint_url="your-endpoint-url", inputs={"keyword": "dictionary"})

# It also allows for inference streams for LLMs
for token in client.infer_stream("your-endpoint-url", inputs={"keyword": "dictionary"}):
    if token.get("object") == "chat.completion.chunk":
        # Do stuff with the token
        pass

# And for server-side asynchronous inferences
future = client.infer_async("your-endpoint-url", {"keyword": "dictionary"})
# Typically, you'd collect additional futures then poll for status, but for the sake of example...
while not client.is_future_ready(future):
    time.sleep(1)
# Once the results are ready, you can use them in the same way as you
# typically do for demo endpoints
result = client.get_future_result(future)

# And includes healthChecks
if client.health_check("your-healthcheck-url") == 200:
	# Run some inferences
    pass
",
        "lang": "Python",
        "meta": "Python",
      },
    ],
    "content": "The OctoAI Python SDK is intended to help you use OctoAI endpoints. At its simplest form, it allows you to run inferences against an endpoint by providing a dictionary with the necessary inputs.",
    "domain": "test.com",
    "hash": "#octoai-python-sdk-at-a-glance",
    "hierarchy": {
      "h0": {
        "title": "Python SDK inference",
      },
      "h3": {
        "id": "octoai-python-sdk-at-a-glance",
        "title": "OctoAI Python SDK at a glance",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.python-sdk-inference-octoai-python-sdk-at-a-glance-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/python-sdk-inferences",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "OctoAI Python SDK at a glance",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk",
    "org_id": "test",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Upgrading from the octoai-sdk",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "code_snippets": [
      {
        "code": "from octoai.client import OctoAI

client = OctoAI()

# The various APIs are now accessible from the client
client.text_gen

client.image_gen

client.fine_tuning

client.asset_library",
        "lang": "python",
      },
    ],
    "content": "The various OctoAI APIs are now accessable from a single client:",
    "domain": "test.com",
    "hash": "#upgrading-your-code",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk",
      },
      "h3": {
        "id": "upgrading-your-code",
        "title": "Upgrading your code",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-upgrading-your-code-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Upgrading your code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "content": "In the octoai.image_gen API instead of specifying which
engine to use, use the corresponding generate_* method.",
    "domain": "test.com",
    "hash": "#image-generation-api-changes",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk",
      },
      "h3": {
        "id": "image-generation-api-changes",
        "title": "Image Generation API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-image-generation-api-changes-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Image Generation API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "content": "The text generation models with octoai.text_gen API are now specified with a str model name instead of an enum.
Streaming requests are made with the corresponding octoai.text_gen.*_stream method.",
    "domain": "test.com",
    "hash": "#text-generation-api-changes",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk",
      },
      "h3": {
        "id": "text-generation-api-changes",
        "title": "Text Generation API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-text-generation-api-changes-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text Generation API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "content": "The Asset Orchestrator has been renamed to Asset Library and can be accessed via octoai.asset_library. Assets can be created with the octoai.asset_lirbary.create_from_file.",
    "domain": "test.com",
    "hash": "#asset-library-asset-orchestrator-api-changes",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk",
      },
      "h3": {
        "id": "asset-library-asset-orchestrator-api-changes",
        "title": "Asset Library (Asset Orchestrator) API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-asset-library-asset-orchestrator-api-changes-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Asset Library (Asset Orchestrator) API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/python-sdk",
        "title": "Python SDK",
      },
    ],
    "canonicalPathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "code_snippets": [
      {
        "code": "from octoai.client import OctoAI

client = OctoAI()

# octoai.infer()",
        "lang": "python",
      },
    ],
    "content": "The octoai.service API has been removed. You can make inferences to compute service endpoints via OctoAI client:
However you will need to continue to use the older octoai-sdk for the full octoai.service API which includes service authoring.",
    "domain": "test.com",
    "hash": "#octoaiservice-api-changes",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from the octoai-sdk",
      },
      "h3": {
        "id": "octoaiservice-api-changes",
        "title": "octoai.service API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.python-sdk.upgrading-from-octoai-sdk-octoaiservice-api-changes-0",
    "org_id": "test",
    "pathname": "/docs/python-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "octoai.service API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/installation-and-setup",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.type-script-sdk-installation",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/installation-and-setup",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "TypeScript SDK installation & setup",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/installation-and-setup",
    "content": "The following runtimes are supported:
Node.js 15+

Vercel

Cloudflare Workers

Deno v1.25+

Bun 1.0+



In order to access endpoints, create an OctoAI API token.

Set the token to an environment variable named OCTOAI_TOKEN or pass it to the OctoAIClient class on construction.",
    "domain": "test.com",
    "hash": "#requirements",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK installation & setup",
      },
      "h4": {
        "id": "requirements",
        "title": "Requirements",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.type-script-sdk-installation-requirements-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/installation-and-setup",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Requirements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/installation-and-setup",
    "code_snippets": [
      {
        "code": "npm install @octoai/sdk",
        "lang": "bash",
        "meta": "npm",
      },
      {
        "code": "yarn add @octoai/sdk",
        "lang": "bash",
        "meta": "yarn",
      },
      {
        "code": "pnpm add @octoai/sdk",
        "lang": "bash",
        "meta": "pnpm",
      },
      {
        "code": "npm install @octoai/sdk",
        "lang": "bash",
        "meta": "npm",
      },
      {
        "code": "yarn add @octoai/sdk",
        "lang": "bash",
        "meta": "yarn",
      },
      {
        "code": "pnpm add @octoai/sdk",
        "lang": "bash",
        "meta": "pnpm",
      },
    ],
    "content": "The TypeScript SDK can be installed using your preferred package manager.",
    "domain": "test.com",
    "hash": "#installation",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK installation & setup",
      },
      "h4": {
        "id": "installation",
        "title": "Installation",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.type-script-sdk-installation-installation-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/installation-and-setup",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Installation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/installation-and-setup",
    "code_snippets": [
      {
        "code": "export OCTOAI_TOKEN=YOUR_TOKEN_HERE",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "In order to access endpoints from OctoAI, first create an API token. Set OCTOAI_TOKEN to the token value wherever you set your environment variables, such as your .bashrc or .env file.",
    "domain": "test.com",
    "hash": "#setting-api-token-as-an-environment-variable",
    "hierarchy": {
      "h0": {
        "title": "TypeScript SDK installation & setup",
      },
      "h4": {
        "id": "setting-api-token-as-an-environment-variable",
        "title": "Setting API token as an environment variable",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.type-script-sdk-installation-setting-api-token-as-an-environment-variable-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/installation-and-setup",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Setting API token as an environment variable",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Upgrading from @octoai/client",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "code_snippets": [
      {
        "code": "import { OctoAIClient } from "@octoai/sdk";

const octoai = new OctoAIClient({
  apiKey: process.env.OCTOAI_TOKEN,
});

// The various APIs are now accessible from the client
octoai.textGen;
octoai.imageGen;
octoai.fineTuning;
octoai.assetLibrary;",
        "lang": "typescript",
        "meta": "TypeScript",
      },
    ],
    "content": "The various OctoAI APIs are now accessible from a single client:",
    "domain": "test.com",
    "hash": "#upgrading-your-code",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client",
      },
      "h3": {
        "id": "upgrading-your-code",
        "title": "Upgrading your code",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-upgrading-your-code-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Upgrading your code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "content": "In the octoai.imageGen API, instead of specifying which engine to use, use the corresponding generate* method such as generateSdxl() or generateSd().",
    "domain": "test.com",
    "hash": "#image-generation-api-changes",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client",
      },
      "h3": {
        "id": "image-generation-api-changes",
        "title": "Image Generation API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-image-generation-api-changes-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Image Generation API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "content": "The text generation models with octoai.textGen API are now specified with a string model name instead of an enum.
Streaming requests are made with the corresponding octoai.textGen.*Stream method such as createCompletionStream() or createChatCompletionStream().",
    "domain": "test.com",
    "hash": "#text-generation-api-changes",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client",
      },
      "h3": {
        "id": "text-generation-api-changes",
        "title": "Text Generation API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-text-generation-api-changes-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Text Generation API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "content": "The Asset Orchestrator has been renamed to Asset Library and can be accessed via octoai.assetLibrary. Assets can be created with the octoai.assetLibrary.upload() method.",
    "domain": "test.com",
    "hash": "#asset-library-asset-orchestrator-api-changes",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client",
      },
      "h3": {
        "id": "asset-library-asset-orchestrator-api-changes",
        "title": "Asset Library (Asset Orchestrator) API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-asset-library-asset-orchestrator-api-changes-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Asset Library (Asset Orchestrator) API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/type-script-sdk",
        "title": "TypeScript SDK",
      },
    ],
    "canonicalPathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "content": "The infer() method is not available in this SDK. You will need to continue to use the older @octoai/client for full inferencing support.",
    "domain": "test.com",
    "hash": "#compute-inferencing-api-changes",
    "hierarchy": {
      "h0": {
        "title": "Upgrading from @octoai/client",
      },
      "h3": {
        "id": "compute-inferencing-api-changes",
        "title": "Compute Inferencing API changes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.documentation.documentation.type-script-sdk.upgrading-from-octoai-client-compute-inferencing-api-changes-0",
    "org_id": "test",
    "pathname": "/docs/typescript-sdk/upgrading-from-old-sdk",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Compute Inferencing API changes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/fa-qs",
        "title": "FAQs",
      },
    ],
    "canonicalPathname": "/docs/faqs/rate-limits",
    "content": "Rate limits are restrictions applied by OctoAI on the rate at which an individual account can submit inference requests against an API endpoint. It is a mechanism used to ensure predictable performance of the platform, and to allow all OctoAI customers to experience predictable inference latencies. Inference requests that are not completed because of a rate limit cap will return an HTTP 429 response code, and can be retried after an appropriate backoff period.",
    "description": "Rate limits are restrictions on the rate and individual account can submit inference requests.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.rate-limits-root-0",
    "org_id": "test",
    "pathname": "/docs/faqs/rate-limits",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Rate limits",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/fa-qs",
        "title": "FAQs",
      },
    ],
    "canonicalPathname": "/docs/faqs/rate-limits",
    "content": "API endpoint Free tier Pro tier Enterprise tier 
Text Gen 10 requests per minute 240 requests per minute Contact us 
 
Media Gen 10 requests per minute 60 requests per minute Contact us 

Higher rate limits are available, please reach out if you need an increase.",
    "domain": "test.com",
    "hash": "#octoai-api-rate-limits",
    "hierarchy": {
      "h0": {
        "title": "Rate limits",
      },
      "h2": {
        "id": "octoai-api-rate-limits",
        "title": "OctoAI API rate limits",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.rate-limits-octoai-api-rate-limits-0",
    "org_id": "test",
    "pathname": "/docs/faqs/rate-limits",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "OctoAI API rate limits",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/fa-qs",
        "title": "FAQs",
      },
    ],
    "canonicalPathname": "/docs/faqs/privacy-and-security",
    "content": "We are SOC Type 2 compliant and take data privacy extremely seriously. Our policies around customer data -- anything you upload to the platform and all inputs you send/outputs you generate — are detailed here in sections 8-12. All data in transit and at rest uses SSL encryption.
To summarize, OctoAI can only use your customer data to provide the service to you and not for any other purpose. Providing the service does include debugging, troubleshooting, and ensuring ToS compliance. Customer prompt data is persisted in logs for 15 days and then deleted. Outputs are never stored, and we do not use your data to train models unless you decide to use our fine-tuning service directly. Access to customer logs is controlled on a need to know basis and audited.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.privacy-security-root-0",
    "org_id": "test",
    "pathname": "/docs/faqs/privacy-and-security",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Privacy & security",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/fa-qs",
        "title": "FAQs",
      },
    ],
    "canonicalPathname": "/docs/faqs/rag",
    "content": "OctoAI allows customers to run their choice of LLMs
(like Llama 2 70B, Mixtral 8x7B, Mixtral 8x22B) and embedding models (like gte-large). With these primitives, customers can use their preferred vector database as the reference data store for their RAG application. OctoAI also supports integrations with popular LLM application development frameworks like LangChain, allowing the use of pre-built functions in LangChain to simplify their RAG application development.
Lastly, OctoAI supports integrations into turnkey RAG frameworks like PineCone Canopy for customers to easily implement RAG with their data.",
    "description": "There are multiple ways in which customers can build a RAG application on OctoAI.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.rag-with-octo-ai-root-0",
    "org_id": "test",
    "pathname": "/docs/faqs/rag",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "How to implement RAG with OctoAI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/fa-qs",
        "title": "FAQs",
      },
    ],
    "canonicalPathname": "/docs/faqs/service-regions",
    "content": "OctoAI currently runs on AWS and GCP hardware in several regions - including AWS us-east-1 and us-west-2 regions, and GCP us-central-1 region. We are actively expanding support for more regions.
For customers with data residency requirements, OctoStack can be hosted in your own private cloud environment or on your own hardware. Contact our sales team for more information.",
    "description": "OctoAI currently runs on AWS and GCP hardware in several regions",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.service-regions-root-0",
    "org_id": "test",
    "pathname": "/docs/faqs/service-regions",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Service regions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/fa-qs",
        "title": "FAQs",
      },
    ],
    "canonicalPathname": "/docs/faqs/multi-user-accounts",
    "content": "Contact us on Discord or our in-app chat feature to setup your multi-user account.",
    "description": "The default setup is 1 user profile per 1 OctoAI account. We can easily help you setup multiple users within a single account if you have a team or organization with multiple users. This will allow your team to manage endpoints, view logs & metrics, and securely share access to the account.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.multi-user-accounts-root-0",
    "org_id": "test",
    "pathname": "/docs/faqs/multi-user-accounts",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Multi-user accounts",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/documentation/fa-qs",
        "title": "FAQs",
      },
    ],
    "canonicalPathname": "/docs/faqs/open-source-licenses",
    "content": "Package License 
alabaster 0.7.13 BSD 
annotated-types 0.5.0 MIT) 
anyio 3.7.1 MIT 
av 10.0.0 BSD 
Babel 2.12.1 BSD 
black 23.7.0 MIT 
certifi 2023.7.22 MPL-2.0 
cffi 1.15.1 MIT 
cfgv 3.4.0 MIT 
charset-normalizer 3.2.0 MIT 
chevron 0.14.0 MIT 
click 8.1.6 BSD-3-Clause 
distlib 0.3.7 PSF-2.0 
docutils 0.18.1 public domain, Python, 2-Clause BSD, GPL 3 (see COPYING.txt) 
execnet 2.0.2 MIT 
fastapi 0.100.1 MIT 
filelock 3.12.2 Unlicense 
flake8 6.1.0 MIT 
flake8-docstrings 1.7.0 MIT 
h11 0.14.0 MIT 
httpcore 0.17.3 BSD 
httpx 0.24.1 BSD 
identify 2.5.26 MIT 
idna 3.4 BSD 
imageio 2.31.1 BSD-2-Clause 
imagesize 1.4.1 MIT 
iniconfig 2.0.0 MIT 
isort 5.12.0 MIT 
Jinja2 3.1.2 BSD-3-Clause 
MarkupSafe 2.1.3 BSD-3-Clause 
mccabe 0.7.0 Expat license 
mypy 1.5.1 MIT License 
mypy-extensions 1.0.0 MIT License 
nodeenv 1.8.0 BSD 
numpy 1.24.4 BSD-3-Clause 
packaging 23.1 Apache Software License, BSD License 
pathspec 0.11.2 Mozilla Public License 2.0 (MPL 2.0) 
Pillow 9.5.0 HPND 
pip 23.2.1 MIT 
platformdirs 3.10.0 MIT 
pluggy 1.2.0 MIT 
pre-commit 3.3.3 MIT 
prettytable 3.9.0 BSD (3 clause) 
pycodestyle 2.11.0 MIT 
pycparser 2.21 BSD 
pydantic 2.2.0 MIT 
pydantic-core 2.6.0 MIT 
pydocstyle 6.3.0 MIT 
pyflakes 3.1.0 MIT 
Pygments 2.16.1 BSD-2-Clause 
pytest 7.4.0 MIT 
pytest-xdist 3.3.0 MIT 
python-multipart 0.0.6 Apache Software License 
PyYAML 6.0.1 MIT 
requests 2.31.0 Apache 2.0 
requests-futures 1.0.1 Apache License v2 
respx 0.20.2 BSD-3-Clause 
setuptools 68.1.0 MIT 
sniffio 1.3.0 MIT OR Apache-2.0 
snowballstemmer 2.2.0 BSD-3-Clause 
soundfile 0.12.1 BSD 3-Clause License 
sphinx 6.2.1 BSD 
sphinx-rtd-theme 1.2.2 MIT 
sphinxcontrib-applehelp 1.0.4 BSD-2-Clause 
sphinxcontrib-devhelp 1.0.2 BSD 
sphinxcontrib-htmlhelp 2.0.1 BSD-2-Clause 
sphinxcontrib-jquery 4.1 BSD 
sphinxcontrib-jsmath 1.0.1 BSD 
sphinxcontrib-qthelp 1.0.3 BSD 
sphinxcontrib-serializinghtml 1.1.5 BSD 
starlette 0.27.0 BSD 
types-PyYAML 6.0.12.11 Apache-2.0 license 
types-requests 2.31.0.2 Apache-2.0 license 
types-urllib3 1.26.25.14 Apache-2.0 license 
typing-extensions 4.7.1 (License not found) 
urllib3 2.0.4 MIT 
uvicorn 0.22.0 BSD 
virtualenv 20.24.3 MIT 
wcwidth 0.2.8 MIT 
wheel 0.41.2 MIT",
    "description": "Below are the open source technologies we make use of and their associated licenses",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.documentation.documentation.fa-qs.open-source-licenses-root-0",
    "org_id": "test",
    "pathname": "/docs/faqs/open-source-licenses",
    "tab": {
      "pathname": "/docs/documentation",
      "title": "Documentation",
    },
    "title": "Open source licenses",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/authentication",
    "code_snippets": [
      {
        "code": "Authorization: Bearer $OCTOAI_TOKEN",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "API requests to your endpoints must be authenticated with a token - you can generate a token from the Account Settings page. Be sure to include your token in the header of your requests.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.authentication-root-0",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/authentication",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Authentication",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Inference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "content": "Asynchronous inference request size is currently limited to 10mb. Asynchronous inference output data is stored for 24 hours, then automatically deleted.
A long-running inference with duration greater than 1 minute may occasionally encounter an error. If this happens, re-submit your request or reach out to us for help.",
    "domain": "test.com",
    "hash": "#limitations",
    "hierarchy": {
      "h0": {
        "title": "Inference",
      },
      "h3": {
        "id": "limitations",
        "title": "Limitations",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-limitations-0",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Limitations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "code_snippets": [
      {
        "code": "POST https://image.octoai.run/generate/sdxl",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
 -H "Content-Type: application/json" \
 -H "authorization: Bearer $OCTOAI_TOKEN" \
 --data-raw '{"prompt": "A photo of a cute cat astronaut in space"}'",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "Starts an inference at the specified endpoint URL for the data inputs you provide. The request is synchronous by default, and you can optionally specify the request as asynchronous. Input parameters are included in the cURL example of each endpoint.
API requests to your endpoints must be authenticated with a token - you can generate a token from the Account Settings page. Be sure to include your token in the header of your requests.
Example synchronous cURL request:",
    "domain": "test.com",
    "hash": "#create-inference",
    "hierarchy": {
      "h0": {
        "title": "Inference",
      },
      "h3": {
        "id": "create-inference",
        "title": "Create inference",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-create-inference-0",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create inference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "code_snippets": [
      {
        "code": "curl -X POST "https://image.octoai.run/generate/sdxl" \
 -H "Content-Type: application/json" \
 -H "authorization: Bearer $OCTOAI_TOKEN" \
 -H "X-OctoAI-Async:1" \
 --data-raw '{"prompt": "A photo of a cute cat astronaut in space"}'",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": ""response_id": "778bbfd58c-hz95k",
"poll_url": "https://async.octoai.run/v1/requests/778bbfd58c-hz95k"",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "You can create an asynchronous inference by specifying X-OctoAI-Async: 1 in the request header.
Example asynchronous cURL request:
You’ll receive a response ID and poll URL where you can poll for the status and results:",
    "domain": "test.com",
    "hash": "#asynchronous-inference",
    "hierarchy": {
      "h0": {
        "title": "Inference",
      },
      "h3": {
        "id": "create-inference",
        "title": "Create inference",
      },
      "h4": {
        "id": "asynchronous-inference",
        "title": "Asynchronous inference",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-asynchronous-inference-0",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Asynchronous inference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "code_snippets": [
      {
        "code": "GET https://async.octoai.run/v1/requests/response_id",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": "curl -X GET "https://async.octoai.run/v1/requests/778bbfd58c-hz95k" \
 -H "Authorization: Bearer $OCTOAI_TOKEN"",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": " "status": "pending"",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "Use the poll_url to return the status of an inference, which will be one of these values:
pending: the inference is waiting or starting up

running: the inference is in progress

completed: the inference is finished


Example poll cURL request:
Example pending poll response:",
    "domain": "test.com",
    "hash": "#poll-for-status",
    "hierarchy": {
      "h0": {
        "title": "Inference",
      },
      "h3": {
        "id": "get-inference",
        "title": "Get inference",
      },
      "h4": {
        "id": "poll-for-status",
        "title": "Poll for status",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-poll-for-status-0",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Poll for status",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/inference",
    "code_snippets": [
      {
        "code": "GET https://async.octoai.run/v1/responses/response_id",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": ""status": "completed",
"response_url": "https://async.octoai.run/v1/responses/778bbfd58c-hz95k"",
        "lang": "bash",
        "meta": "bash",
      },
      {
        "code": "curl -X GET "https://async.octoai.run/v1/responses/778bbfd58c-hz95k" \
 -H "Authorization: Bearer $OCTOAI_TOKEN"",
        "lang": "bash",
        "meta": "bash",
      },
    ],
    "content": "When completed, the provided response_url will include the inference data. Asynchronous inference output data is stored for 24 hours, then automatically deleted.
Example completed poll response:
Example cURL request for completed inference data:",
    "domain": "test.com",
    "hash": "#get-inference-data",
    "hierarchy": {
      "h0": {
        "title": "Inference",
      },
      "h3": {
        "id": "get-inference",
        "title": "Get inference",
      },
      "h4": {
        "id": "get-inference-data",
        "title": "Get inference data",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.inference-get-inference-data-0",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/inference",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get inference data",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/overview",
    "content": "LangChain
LangChain provides a framework to easily construct LLM-powered apps.
Langchain developers can leverage OctoAI LLM and embedding endpoints to
easily access efficient compute across a wide selection of LLMs.






Unstructured.io
Unstructured provides components to very easily embed text documents lke
PDFs, HTML, Word Docs, and more. The OctoAIEmbedingEncoder is available, so
documents parsed with Unstructured can easily be embedded with the OctoAI
embeddings endpoint.








Pinecone (Canopy)
Pinecone provides storage and retrieval infrastructure needed for building
and running AI apps. This integration allows a developer using Canopy to
choose from the best LLMs on OctoAI.






OpenRouter
OpenRouter has a unified interface for using various LLMs, allowing users to
find and compare models for their needs. The OpenRouter API users can
leverage OctoAI's best in class LLM endpoints.






LlamaIndex
LlamaIndex aids in the management of interactions between your LLMs and
private data. A developer building AI apps can now access highly optimized
LLMs and Embeddings models on OctoAI.",
    "description": "Browse OctoAI's partner integrations to help you build your custom solution.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.all-root-0",
    "org_id": "test",
    "pathname": "/docs/integrations/overview",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "All OctoAI Integrations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/langchain",
    "description": "Langchain developers can leverage OctoAI LLM and embedding endpoints to easily access efficient compute across a wide selection of LLMs.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.lang-chain",
    "org_id": "test",
    "pathname": "/docs/integrations/langchain",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "LangChain Integration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/langchain",
    "content": "LangChain provides a framework to easily build LLM-powered apps. Developers using LangChain can now utilize OctoAI LLMs and Embedding endpoints
to access efficient, fast, and reliable compute.",
    "domain": "test.com",
    "hash": "#introduction",
    "hierarchy": {
      "h0": {
        "title": "LangChain Integration",
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.lang-chain-introduction-0",
    "org_id": "test",
    "pathname": "/docs/integrations/langchain",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Introduction",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/langchain",
    "code_snippets": [
      {
        "code": "import os

os.environ["OCTOAI_API_TOKEN"] = "OCTOAI_API_TOKEN"
os.environ["ENDPOINT_URL"] = "https://text.octoai.run/v1/chat/completions"",
        "lang": "python",
      },
      {
        "code": "from langchain.chains import LLMChain
from langchain_community.llms.octoai_endpoint import OctoAIEndpoint
from langchain_core.prompts import PromptTemplate

template = """Below is an instruction that describes a task. Write a response that appropriately completes the request.\n Instruction:\n{question}\n Response: """
prompt = PromptTemplate.from_template(template)

llm = OctoAIEndpoint(
    model_kwargs={
        "model": "llama-2-13b-chat-fp16",
        "max_tokens": 128,
        "presence_penalty": 0,
        "temperature": 0.1,
        "top_p": 0.9,
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant. Keep your responses limited to one short paragraph if possible.",
            },
        ],
    },
)

question = "Who was leonardo davinci?"

llm_chain = LLMChain(prompt=prompt, llm=llm)

print(llm_chain.run(question))",
        "lang": "python",
      },
      {
        "code": "  Sure thing! Here's my response:

Leonardo da Vinci was a true Renaissance man - an Italian polymath who excelled in various fields,
including painting, sculpture, engineering, mathematics, anatomy, and geology. He is widely considered
one of the greatest painters of all time, and his inventive and innovative works continue to inspire and
influence artists and thinkers to this day. Some of his most famous works include the Mona Lisa,
The Last Supper, and Vitruvian Man.",
      },
    ],
    "content": "To use OctoAI LLMs with LangChain, first obtain an OcotoAI API Token.
Then paste your API token in the code example below:
Next, run the following Python script:
It should produce the following output:",
    "domain": "test.com",
    "hash": "#using-octoais-llms-and-langchain",
    "hierarchy": {
      "h0": {
        "title": "LangChain Integration",
      },
      "h2": {
        "id": "using-octoais-llms-and-langchain",
        "title": "Using OctoAI's LLMs and LangChain",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.lang-chain-using-octoais-llms-and-langchain-0",
    "org_id": "test",
    "pathname": "/docs/integrations/langchain",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Using OctoAI's LLMs and LangChain",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/langchain",
    "content": "Get started today by following along with one of our demo apps:
DocTalk

Q&A app on a custom PDF",
    "domain": "test.com",
    "hash": "#learn-with-our-demo-apps",
    "hierarchy": {
      "h0": {
        "title": "LangChain Integration",
      },
      "h2": {
        "id": "using-octoais-llms-and-langchain",
        "title": "Using OctoAI's LLMs and LangChain",
      },
      "h3": {
        "id": "learn-with-our-demo-apps",
        "title": "Learn with our demo apps",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.lang-chain-learn-with-our-demo-apps-0",
    "org_id": "test",
    "pathname": "/docs/integrations/langchain",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Learn with our demo apps",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/pinecone",
    "description": "This integration allows a developer using Canopy to choose from the best LLMs on OctoAI.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.pinecone",
    "org_id": "test",
    "pathname": "/docs/integrations/pinecone",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Pinecone (Canopy) Integration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/pinecone",
    "content": "Pinecone provides storage and retrieval infrastructure needed for building AI applications. Pinecone's Canopy is an open-source framework built on top of Pinecone's
vector database to build production-ready chat assistants at any scale.",
    "domain": "test.com",
    "hash": "#introduction",
    "hierarchy": {
      "h0": {
        "title": "Pinecone (Canopy) Integration",
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.pinecone-introduction-0",
    "org_id": "test",
    "pathname": "/docs/integrations/pinecone",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Introduction",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/pinecone",
    "code_snippets": [
      {
        "code": "chat_engine:
  params:
  max_prompt_tokens: 2048
  llm: &llm
  type: OctoAILLM
  params:
    model_name: mistral-7b-instruct

  context_engine:
  knowledge_base:
    record_encoder:
      type: OctoAIRecordEncoder
      params:
        model_name: thenlper/gte-large
        batch_size: 2048",
      },
    ],
    "content": "As a fully open source solution, Canopy+OctoAI is one of the fastest ways and more affordable ways to get started on your
RAG journey. Canopy uses Pinecone vector database for storage and retrieval, which is free to use for up to 100k vectors (that’s about 30k pages of text).
OctoAI offers industry leading pricing at $0.05 / 1M token for its gte-large embedding model, and offers $10 of free credit upon sign up.
To get a Canopy server running with OctoAI's modles, you do not need custom ConvolverNode, simply update the Canopy YAML configureations as follows:",
    "domain": "test.com",
    "hash": "#using-octoais-llms-and-pinecone",
    "hierarchy": {
      "h0": {
        "title": "Pinecone (Canopy) Integration",
      },
      "h2": {
        "id": "using-octoais-llms-and-pinecone",
        "title": "Using OctoAI's LLMs and Pinecone",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.pinecone-using-octoais-llms-and-pinecone-0",
    "org_id": "test",
    "pathname": "/docs/integrations/pinecone",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Using OctoAI's LLMs and Pinecone",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/pinecone",
    "content": "Get started today by following along with one of our demo apps:
DocTalk


Learn more about our partnership:
OctoAI & Pinecone Patnership for GenAI using RAG",
    "domain": "test.com",
    "hash": "#learn-with-our-demo-apps",
    "hierarchy": {
      "h0": {
        "title": "Pinecone (Canopy) Integration",
      },
      "h2": {
        "id": "using-octoais-llms-and-pinecone",
        "title": "Using OctoAI's LLMs and Pinecone",
      },
      "h3": {
        "id": "learn-with-our-demo-apps",
        "title": "Learn with our demo apps",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.pinecone-learn-with-our-demo-apps-0",
    "org_id": "test",
    "pathname": "/docs/integrations/pinecone",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Learn with our demo apps",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/unstructured",
    "description": "The OctoAIEmbedingEncoder is available, so documents parsed with Unstructured can easily be embedded with the OctoAI embeddings endpoint.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.unstructured-io",
    "org_id": "test",
    "pathname": "/docs/integrations/unstructured",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Unstructured.io Integration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/unstructured",
    "content": "Unstructured is both an open-source library and an API service. The library provides components for ingesting and pre-processing images and text documents, such as PDFs, HTML, Word docs, and many more.
It also provides components to very easily embed these documents. In Unstructured’s jargon this component is called an EmbeddingEncoder. The OctoAIEmbedingEncoder is available, so documents parsed with Unstructured can easily be embedded with the OctoAI embeddings endpoint.",
    "domain": "test.com",
    "hash": "#introduction",
    "hierarchy": {
      "h0": {
        "title": "Unstructured.io Integration",
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.unstructured-io-introduction-0",
    "org_id": "test",
    "pathname": "/docs/integrations/unstructured",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Introduction",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/unstructured",
    "code_snippets": [
      {
        "code": "import os

from unstructured.documents.elements import Text
from unstructured.embed.octoai import OctoAiEmbeddingConfig, OctoAIEmbeddingEncoder

embedding_encoder = OctoAIEmbeddingEncoder(
    config=OctoAiEmbeddingConfig(api_key=os.environ["OCTOAI_API_KEY"])
)
elements = embedding_encoder.embed_documents(
    elements=[Text("This is sentence 1"), Text("This is sentence 2")],
)

query = "This is the query"
query_embedding = embedding_encoder.embed_query(query=query)

[print(e.embeddings, e) for e in elements]
print(query_embedding, query)
print(embedding_encoder.is_unit_vector(), embedding_encoder.num_of_dimensions())",
        "lang": "python",
      },
    ],
    "content": "Before you get started let's review some concepts. You will need an OctoAI API Token for this integration.
The OctoAIEmbeddingEncoder class connects to the OctoAI Text&Embedding API to obtain embeddings for pieces of text.

embed_documents will receive a list of Elements, and return an updated list which includes the embeddings attribute for each Element.

embed_query will receive a query as a string, and return a list of floats which is the embedding vector for the given query string.

num_of_dimensions is a metadata property that denotes the number of dimensions in any embedding vector obtained via this class.

is_unit_vector is a metadata property that denotes if embedding vectors obtained via this class are unit vectors.


Now, let's get started with the following code example for how to use OctoAIEmbeddingEncoder.
You will see the updated elements list (with the embeddings attribute included for each element),
the embedding vector for the query string, and some metadata properties about the embedding model.
You will need to set an environment variable named OCTOAI_API_KEY to be able to run this example.
Learn more about our partnership:
OctoAI & Unstructured.io Create New Integration",
    "domain": "test.com",
    "hash": "#using-the-octoaiembeddingencoder",
    "hierarchy": {
      "h0": {
        "title": "Unstructured.io Integration",
      },
      "h2": {
        "id": "using-the-octoaiembeddingencoder",
        "title": "Using the OctoAIEmbeddingEncoder",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.unstructured-io-using-the-octoaiembeddingencoder-0",
    "org_id": "test",
    "pathname": "/docs/integrations/unstructured",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Using the OctoAIEmbeddingEncoder",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/openrouter",
    "description": "The OpenRouter API users can leverage OctoAI's best in class LLM endpoints.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.open-router",
    "org_id": "test",
    "pathname": "/docs/integrations/openrouter",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "OpenRouter Integration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/openrouter",
    "content": "OpenRouter provides a unified interface for using various LLMs and allows users to find and compare models based on PromiseRejectionEvent, latency, throughput.
This let's users find the right LLM and mix of price and performance for their use case.",
    "domain": "test.com",
    "hash": "#introduction",
    "hierarchy": {
      "h0": {
        "title": "OpenRouter Integration",
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.open-router-introduction-0",
    "org_id": "test",
    "pathname": "/docs/integrations/openrouter",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Introduction",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/openrouter",
    "code_snippets": [
      {
        "code": "import OpenAI from "openai"

const openai = new OpenAI({
  baseURL: "https://openrouter.ai/api/v1",
  apiKey: $OPENROUTER_API_KEY,
  defaultHeaders: {
    "HTTP-Referer": $YOUR_SITE_URL, // Optional, for including your app on openrouter.ai rankings.
    "X-Title": $YOUR_SITE_NAME, // Optional. Shows in rankings on openrouter.ai.
  },
  // dangerouslyAllowBrowser: true,
})
async function main() {
  const completion = await openai.chat.completions.create({
    model: "mistralai/mixtral-8x7b-instruct",
    messages: [
      { role: "user", content: "Say this is a test" }
    ],
    provider: {
	    order: ["OctoAI", "Azure", "Together"]
	  }
  })

  console.log(completion.choices[0].message)
}
main()",
        "lang": "python",
      },
    ],
    "content": "To access OctoAI's best in class LLMs via OpenRouter sign into OpenRouter and create an account to obtain an OPENROUTER_API_KEY.
Using the code snippet below you can route your calls to OpenRouter via OpenAI's client API.
Set the providers the OpenRouter will use for your request using the order field. The router will filter this list to only include fproviders that are available for the model you want to use, and then try one at a time. It will fail if none are available.
If you do not set the field, the router will use the default ordering shown on the model page.
In the following code snippet, we assume the order of preference is as follows: OctoAI, Azure, then TogetherAI.
Find all models from OctoAI you can use at https://openrouter.ai.",
    "domain": "test.com",
    "hash": "#using-octoais-llms-and-openrouter",
    "hierarchy": {
      "h0": {
        "title": "OpenRouter Integration",
      },
      "h2": {
        "id": "using-octoais-llms-and-openrouter",
        "title": "Using OctoAI's LLMs and OpenRouter",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.open-router-using-octoais-llms-and-openrouter-0",
    "org_id": "test",
    "pathname": "/docs/integrations/openrouter",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Using OctoAI's LLMs and OpenRouter",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/llamaindex",
    "description": "A developer building AI apps can now access highly optimized LLMs and Embeddings models on OctoAI.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.llama-index",
    "org_id": "test",
    "pathname": "/docs/integrations/llamaindex",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "LlamaIndex Integration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/llamaindex",
    "content": "LlamaIndex strives to help manage the interactions between your language modles and private DataTransfer.
If you are building your application and using LlamaIndex you benefit from the vast ecosystem of integrations, and top LLMs amd Embeddings models hosted by OctoAI.",
    "domain": "test.com",
    "hash": "#introduction",
    "hierarchy": {
      "h0": {
        "title": "LlamaIndex Integration",
      },
      "h2": {
        "id": "introduction",
        "title": "Introduction",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.llama-index-introduction-0",
    "org_id": "test",
    "pathname": "/docs/integrations/llamaindex",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Introduction",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/integrations/integrations",
        "title": "Integrations",
      },
    ],
    "canonicalPathname": "/docs/integrations/llamaindex",
    "code_snippets": [
      {
        "code": "from os import environ
from llama_index.llms.octoai import OctoAI

OCTOAI_API_KEY = environ.get("OCTOAI_TOKEN")

octoai = OctoAI(model="meta-llama-3-8b-instruct", token=OCTOAI_API_KEY)

# Using complete
response = octoai.complete("Octopi can not play chess because...")
print(response)

print("\n=====================\n")

# Using the chat interface
from llama_index.core.llms import ChatMessage

messages = [
    ChatMessage(
        role="system",
        content="Below is an instruction that describes a task. Write a response that appropriately completes the request.",
    ),
    ChatMessage(role="user", content="Write a short blog about Seattle"),
]
response = octoai.chat(messages)
print(response)",
        "lang": "python",
      },
      {
        "code": "from os import environ
from llama_index.embeddings.octoai import OctoAIEmbedding

OCTOAI_API_KEY = environ.get("OCTOAI_TOKEN")
embed_model = OctoAIEmbedding(api_key=OCTOAI_API_KEY)

# Single embedding request
embeddings = embed_model.get_text_embedding("Once upon a time in Seattle.")
assert len(embeddings) == 1024
print(embeddings[:10])


# Batch embedding request
texts = [
    "Once upon a time in Seattle.", 
    "This is a test.", 
    "Hello, world!"
]
embeddings = embed_model.get_text_embedding_batch(texts)
assert len(embeddings) == 3
print(embeddings[0][:10])",
        "lang": "python",
      },
    ],
    "content": "Get started reviewing more about LlamaIndex, and signing up for a free OctoAI account.
LlamaIndex has both Python and TypScript libraries, and OctoAI is available in the Python SDK.
To use OctoAI LLM endpoints with LlamaIndex start with the code below using Llama 3 8B as the LLM.
To use OctoAI Embedding endpoints with llamaindex
you can use the code below to get started. We’re using GTE large in the example below (default model).
If you are using LlamaIndex you can easily switch model provider, and enjoy using models hosted and optimized for scale on OctoAI.",
    "domain": "test.com",
    "hash": "#using-octoais-llms-and-llamaindex",
    "hierarchy": {
      "h0": {
        "title": "LlamaIndex Integration",
      },
      "h2": {
        "id": "using-octoais-llms-and-llamaindex",
        "title": "Using OctoAI's LLMs and LlamaIndex",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.integrations.integrations.integrations.llama-index-using-octoais-llms-and-llamaindex-0",
    "org_id": "test",
    "pathname": "/docs/integrations/llamaindex",
    "tab": {
      "pathname": "/docs/integrations",
      "title": "Integrations",
    },
    "title": "Using OctoAI's LLMs and LlamaIndex",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/may",
    "content": "Improved",
    "description": "OctoAI product updates and release notes for May 2024",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.may-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/may",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "May 2024 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/may",
    "content": "OctoAI releases a redesigned Asset Library.


Improves browsing, filtering, and overall asset management.",
    "domain": "test.com",
    "hash": "#may-3-2024",
    "hierarchy": {
      "h0": {
        "title": "May 2024 Release Notes",
      },
      "h2": {
        "id": "may-3-2024",
        "title": "May 3, 2024",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.may-may-3-2024-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/may",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "May 3, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/april",
    "content": "Improved",
    "description": "OctoAI product updates and release notes for April 2024",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.april-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/april",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "April 2024 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/april",
    "content": "OctoAI releases an improved Billing & Usage dashboard.


Provides more detailed visibility into your OctoAI product usage.








Added",
    "domain": "test.com",
    "hash": "#april-30-2024",
    "hierarchy": {
      "h0": {
        "title": "April 2024 Release Notes",
      },
      "h2": {
        "id": "april-30-2024",
        "title": "April 30, 2024",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.april-april-30-2024-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/april",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "April 30, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/april",
    "content": "OctoAI releases OctoStack: a turnkey generative AI solution allowing you to run your choice of models in your environment.


OctoStack provides a full stack solution for running generative AI at scale,
including inference, model customization, load balancing, auto-scaling, and
telemetry.",
    "domain": "test.com",
    "hash": "#april-24-2024",
    "hierarchy": {
      "h0": {
        "title": "April 2024 Release Notes",
      },
      "h2": {
        "id": "april-24-2024",
        "title": "April 24, 2024",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.april-april-24-2024-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/april",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "April 24, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/february",
    "content": "Added",
    "description": "OctoAI product updates and release notes for February 2024",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.february-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/february",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "February 2024 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/february",
    "content": "OctoAI Text Gen Solution adds several new OSS large language models, including:


Gemma-7B-Instruct, a new open source model from Google.

Smaug-72B-v0.1, a fine tune of the Qwen1.0 family of models that shows impressive leaderboard performance.

Nous-Hermes-2-Mixtral-8x7B-DPO, a fine tune of the powerful Mixtral-8x7b model, offered as the “flagship” checkpoint of Nous Research (the current best producer of open source fine tunes for popular models).




Additionally, OctoAI releases SecureLink, which is a private connectivity security measure, ensuring that network traffic between an OctoAI endpoint and the customer environment is not exposed to the public internet. SecureLink is available for Enterprise customers.






Added",
    "domain": "test.com",
    "hash": "#february-29-2024",
    "hierarchy": {
      "h0": {
        "title": "February 2024 Release Notes",
      },
      "h2": {
        "id": "february-29-2024",
        "title": "February 29, 2024",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.february-february-29-2024-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/february",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "February 29, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/february",
    "content": "OctoAI launches Photo Merge feature for Image Gen Solution


Seamlessly integrate a photos subject (person) into high-quality
AI-generated output.

This eliminates the need for custom facial fine-tunes
and requires only 1-4 images.








Added",
    "domain": "test.com",
    "hash": "#february-16-2024",
    "hierarchy": {
      "h0": {
        "title": "February 2024 Release Notes",
      },
      "h2": {
        "id": "february-16-2024",
        "title": "February 16, 2024",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.february-february-16-2024-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/february",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "February 16, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/february",
    "content": "OctoAI launches the GTE Large embeddings model, available via API.


GTE models were trained by Alibaba DAMO Academy on large-scale corpus of
relevant text pairs covering a myriad of scenarios. - Embedding models extract
meaning from raw data and turn it into a vector representation, which is used
to enhance llms.",
    "domain": "test.com",
    "hash": "#february-1-2024",
    "hierarchy": {
      "h0": {
        "title": "February 2024 Release Notes",
      },
      "h2": {
        "id": "february-1-2024",
        "title": "February 1, 2024",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.february-february-1-2024-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/february",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "February 1, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/january",
    "content": "Added",
    "description": "OctoAI product updates and release notes for January 2024",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.january-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/january",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "January 2024 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/january",
    "content": "Code Llama 70B now available on OctoAI.


Code Llama 70B Instruct now available via API or in the web UI.
Use the same API and token key, simply replace the models with 'codellama-70b-instruct-fp16'










Added",
    "domain": "test.com",
    "hash": "#january-31-2024",
    "hierarchy": {
      "h0": {
        "title": "January 2024 Release Notes",
      },
      "h2": {
        "id": "january-31-2024",
        "title": "January 31, 2024",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.january-january-31-2024-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/january",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "January 31, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2024/january",
    "content": "OctoAI launched a private preview for users who want to "bring your own" fine-tuned LLM.


Host any fine-tuned Llama 2, Code Llama, Mixtral, or Mistral models.

Run your fine-tuned model at the same cost and latency as the default models already on OctoAI.",
    "domain": "test.com",
    "hash": "#january-4-2024",
    "hierarchy": {
      "h0": {
        "title": "January 2024 Release Notes",
      },
      "h2": {
        "id": "january-4-2024",
        "title": "January 4, 2024",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2024.january-january-4-2024-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2024/january",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "January 4, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/december",
    "content": "Added",
    "description": "OctoAI product updates and release notes for December 2023",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.december-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/december",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "December 2023 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/december",
    "content": "OctoAI makes Mixtral 8x7B Instruct available on Text Gen Solution.


You can now run Mixtral 8x7B Instruct using the Text Gen Solution and benefit from high quality competitive with GPT 3.5, a unified API compatible with OpenAI, and a 4x lower price per token over GPT 3.5.",
    "domain": "test.com",
    "hash": "#december-12-2023",
    "hierarchy": {
      "h0": {
        "title": "December 2023 Release Notes",
      },
      "h2": {
        "id": "december-12-2023",
        "title": "December 12, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.december-december-12-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/december",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "December 12, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/november",
    "content": "Added",
    "description": "OctoAI product updates and release notes for November 2023",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/november",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "November 2023 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/november",
    "content": "OctoAI launches it's new Text Gen Solution with Llama 2, Code Llama, and Mistral models.


Run inference against multiple sizes of Llama2 Chat, Code Llama Instruct, and Mistral Instruct all via one unified API.

Reliably scale your app with OctoAI, which is already processing millions of inferences daily.








Added",
    "domain": "test.com",
    "hash": "#november-27-2023",
    "hierarchy": {
      "h0": {
        "title": "November 2023 Release Notes",
      },
      "h2": {
        "id": "november-27-2023",
        "title": "November 27, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-november-27-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/november",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "November 27, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/november",
    "content": "SDXL image generation has greatly been improved with the addition of SSD-1B, a distilled SDXL that is 50% faster, and LCM-LoRA, which enables image generation in less than 1 second.


SSD-1B is an OSS distilled SDXL that is 50% faster due to is smaller size. OctoAI applied our proprietary ML compiler to the model, and are able to generate SDXL images in 1.4 seconds using SSD-1B.

LCM-LoRA is a custom asset that enables high-quality image output requiring as few as 4 steps to generate images in less than 1 second.








Improved",
    "domain": "test.com",
    "hash": "#november-20-2023",
    "hierarchy": {
      "h0": {
        "title": "November 2023 Release Notes",
      },
      "h2": {
        "id": "november-20-2023",
        "title": "November 20, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-november-20-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/november",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "November 20, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/november",
    "content": "Image Gen Solution fine-tuning for SD 1.5 and SDXL via API or web UI now available.


Customers can create their own unique fine-tunes in the web UI with at least 3-6 images, or import custom assets from popular sources to generate one of a kind images.








Added",
    "domain": "test.com",
    "hash": "#november-16-2023",
    "hierarchy": {
      "h0": {
        "title": "November 2023 Release Notes",
      },
      "h2": {
        "id": "november-16-2023",
        "title": "November 16, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-november-16-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/november",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "November 16, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/november",
    "content": "OctoAI Image Gen Solution launched.


The fastest and most customizable GenAI stack for production-grade image generation applications. The new solutions boasts SDXL images in less than 3 seconds, the ability to create or import fine-tuning assets, and reliable scale from a few to thousands of images.",
    "domain": "test.com",
    "hash": "#november-8-2023",
    "hierarchy": {
      "h0": {
        "title": "November 2023 Release Notes",
      },
      "h2": {
        "id": "november-8-2023",
        "title": "November 8, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.november-november-8-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/november",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "November 8, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/october",
    "content": "Improved",
    "description": "OctoAI product updates and release notes for October 2023",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.october-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/october",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "October 2023 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/october",
    "content": "OctoAI is now generally available with two Solution offerings.


OctoAI Image Gen Solution: Build agaist an API endpoint to create, customize, and scale GenAI image generation for your use case.

Compute Service: run your choice of OSS fine-tuned or custom models on the service for your use case.








Improved",
    "domain": "test.com",
    "hash": "#october-31-2023",
    "hierarchy": {
      "h0": {
        "title": "October 2023 Release Notes",
      },
      "h2": {
        "id": "october-31-2023",
        "title": "October 31, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.october-october-31-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/october",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "October 31, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/october",
    "content": "OctoAI is now SOC 2 Type II certified.


Our most recent security enhancement showcases OctoAI provides processes and safeguards that secure customer data as verified by a third party auditor.",
    "domain": "test.com",
    "hash": "#october-26-2023",
    "hierarchy": {
      "h0": {
        "title": "October 2023 Release Notes",
      },
      "h2": {
        "id": "october-26-2023",
        "title": "October 26, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.october-october-26-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/october",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "October 26, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/september",
    "content": "Added",
    "description": "OctoAI product updates and release notes for September 2023",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.september-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/september",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "September 2023 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/september",
    "content": "OctoAI made improvements to SDXL endpoint, and added support to multi-user accounts.


We have improved OctoAI's SDXL. Our latency-optimized SDXL endpoints now take about 2.8s to generate a 30-step image. We also have a cost-optimized SDXL endpoint that takes about 8s for 30 steps.

Added support for multi-user accounts, which allows your team to manage endpoints, view logs & metrics, and securely share access to an account.








Added",
    "domain": "test.com",
    "hash": "#september-20-2023",
    "hierarchy": {
      "h0": {
        "title": "September 2023 Release Notes",
      },
      "h2": {
        "id": "september-20-2023",
        "title": "September 20, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.september-september-20-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/september",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "September 20, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/september",
    "content": "OctoAI made an addition to it's Github repo to help users start building your own app on OctoAI, and a new audio generation endpoint.


Check out our github repo for template applications to help you get started on building your own app with OctoAI. Right now, we have an example using Python and deployable on Streamlit as well as one using TypeScript and deployable on Vercel.

We also have a new audio generation endpoint available under private preview (e.g. Bark, Tortoise TTS).",
    "domain": "test.com",
    "hash": "#september-14-2023",
    "hierarchy": {
      "h0": {
        "title": "September 2023 Release Notes",
      },
      "h2": {
        "id": "september-14-2023",
        "title": "September 14, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.september-september-14-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/september",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "September 14, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/august",
    "content": "Added",
    "description": "OctoAI product updates and release notes for August 2023",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.august-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/august",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "August 2023 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/august",
    "content": "OctoAI made several additions including: Llama 2 70B model, real-time streaming capabilities to Whisper, and updated the domain for newly created endpoints.


Added Llama2 70B quickstart template endpoint. We can also host custom Llama2 LoRAs/ checkpoints.

Enabled users to upload data via URL in the authoring experience (CLI + Python SDK)

Added real-time streaming capabilities to our Whisper audio flow, with a React hook called use Whisper for ease of integration into web/mobile apps. Learn more about this change.

Changed the domain for all newly created endpoints from octoai.cloud to octoai.run Existing endpoints on octoai.cloud will still work, but we suggest that you start changing your code to call endpoints from octoai.run instead of octoai.cloud, since we'll also update existing endpoints in about a month.








Improved",
    "domain": "test.com",
    "hash": "#august-30-2023",
    "hierarchy": {
      "h0": {
        "title": "August 2023 Release Notes",
      },
      "h2": {
        "id": "august-30-2023",
        "title": "August 30, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.august-august-30-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/august",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "August 30, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/august",
    "content": "There were some new additions to our docs about Image Generation, some improvements in the backend, and a faster version of Stable Diffusion 1.5.


Added a new section in our Docs on Image Generation, including how to fine-tune and use Stable Diffusion.

Reduced cold start substantially on endpoints created with our authoring experience (can be multiple minutes of improvement depending on the model). Upgrade to the latest version of the CLI and SDK and author new endpoints to get faster cold start for your custom models.

Improved error boundaries in the UI. Users would be less likely to run into the "Whoops Beta Mode engaged" message in the UI.

Enabled concurrency handling improvements to all new endpoints created from now on. We will also be gradually rolling out this change on previously created endpoints in upcoming weeks.

A faster version of SD XL with dimension 1024x1024 is now available under private preview. We'd be gradually rolling out this new version over the next week or so.

Reminder: OctoAI's quickstart template endpoints are for demo/testing purposes only. On these endpoints, we rate-limit to 15 inferences per hour. If you would like to exceed this limit for production use, please clone the endpoint to your own account.








Added",
    "domain": "test.com",
    "hash": "#august-16-2023",
    "hierarchy": {
      "h0": {
        "title": "August 2023 Release Notes",
      },
      "h2": {
        "id": "august-16-2023",
        "title": "August 16, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.august-august-16-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/august",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "August 16, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/august",
    "content": "There have been some additions to the OctoAI platform: Stable Diffusion 1.5 template feature additions, Whisper feature additions, and private registry updates.


Whisper Template Feature Additions: Multi-hour long audio files are now supported. Furthermore, you can specify a URL to the audio input file (e.g. MP3, WAV, or MP4 formats), instead of uploading a file from your local environment.

Private Registry: OctoAI's container authoring experience has been upgraded. Users are no longer required to provide registry credentials to get started. Images can be uploaded directly to a private OctoAI Registry. User uploaded images to OctoAI's Registry are accessible only to you and OctoAI services i.e. no other user can view or access your images.

Stable Diffusion 1.5 Template Feature Additions: OctoAI's Stable Diffusion endpoint, running on A10Gs, has been upgraded to include the following features to help users customize styling and achieve higher-quality images:
Popular Checkpoints like DreamShaper and Realistic Vision, Low Rank Adaptations (LoRAs), and Textual Inversions. Note: LoRA weights must sum up to 1.

Additional image dimensions.

We updated the web user interface.",
    "domain": "test.com",
    "hash": "#august-10-2023",
    "hierarchy": {
      "h0": {
        "title": "August 2023 Release Notes",
      },
      "h2": {
        "id": "august-10-2023",
        "title": "August 10, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.august-august-10-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/august",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "August 10, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/july",
    "content": "Added",
    "description": "OctoAI product updates and release notes for July 2023",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.july-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/july",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "July 2023 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/july",
    "code_snippets": [
      {
        "code": "    import requests
    import base64

    def download_file(url, filename):
        response = requests.get(url)
        if response.status_code == 200:
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"File downloaded successfully as {filename}.")
        else:
            print(f"Failed to download the file. Status code: {response.status_code}")


    def make_post_request(filename):
        with open(filename, "rb") as f:
            encoded_audio = base64.b64encode(f.read()).decode("utf-8")

        headers = {
            "Content-Type": "application/json"
        }
        data = {
            "audio": encoded_audio,
            "task": "transcribe",
            "diarize": True
        }

        response = requests.post("https://whisper-demo-kk0powt97tmb.octoai.cloud/predict", json=data, headers=headers)

        if response.status_code == 200:
            # Handle the successful response here
            json_response = response.json()

            for seg in json_response["response"]["segments"]:
                print(seg)

        else:
            print(f"Request failed with status code: {response.status_code}")

    if __name__ == "__main__":
        url = "<YOUR_FILE_HERE>.wav"
        filename = "sample.wav"

        download_file(url, filename)

        make_post_request(filename)",
        "lang": "python",
      },
      {
        "code": "    import requests
    import base64

    def download_file(url, filename):
        response = requests.get(url)
        if response.status_code == 200:
            with open(filename, "wb") as f:
                f.write(response.content)
            print(f"File downloaded successfully as {filename}.")
        else:
            print(f"Failed to download the file. Status code: {response.status_code}")


    def make_post_request(filename):
        with open(filename, "rb") as f:
            encoded_audio = base64.b64encode(f.read()).decode("utf-8")

        headers = {
            "Content-Type": "application/json"
        }
        data = {
            "audio": encoded_audio,
            "task": "transcribe",
            "diarize": True
        }

        response = requests.post("https://whisper-demo-kk0powt97tmb.octoai.cloud/predict", json=data, headers=headers)

        if response.status_code == 200:
            # Handle the successful response here
            json_response = response.json()

            for seg in json_response["response"]["segments"]:
                print(seg)

        else:
            print(f"Request failed with status code: {response.status_code}")

    if __name__ == "__main__":
        url = "<YOUR_FILE_HERE>.wav"
        filename = "sample.wav"

        download_file(url, filename)

        make_post_request(filename)",
        "lang": "python",
      },
    ],
    "content": "OctoAI added several new things including better graceful concurrency handling, updated Python SDK, and diarization to Whisper model template.


Added more graceful concurrency handling: when users send more than N concurrent request to an endpoint with N replicas actively running, we will queue all extra requests instead of failing them. This queuing behavior has been activated for selected customers, and will be gradually rolled out over this week and next week. You will temporarily see a new replica spin up while the rollout is occurring on your endpoint.

Updated our Python SDK from 0.1.2 to 0.2.0—it now support both streaming and async inference requests.

Added diarization to our Whisper template endpoint and rectified the list of languages supported. Diarization enables use cases where you'd like to identify the speaker of each segment in a speech recording. You can view the full API specs in the Whisper demo template. Here's an example of how to use the template with diarization:








Improved",
    "domain": "test.com",
    "hash": "#july-26-2023",
    "hierarchy": {
      "h0": {
        "title": "July 2023 Release Notes",
      },
      "h2": {
        "id": "july-26-2023",
        "title": "July 26, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.july-july-26-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/july",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "July 26, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/july",
    "content": "Added an OctoAI template for Llama2-7B Chat.


Added an OctoAI template for Llama2-7B Chat, which is an instruction-tuned model for chatbots.
Users can now work with this brand-new to the market LLM directly in the web UI with limited
token response or programmatically with additional optionality. A similar template for Llama2-70B is coming soon!








Fixed",
    "domain": "test.com",
    "hash": "#july-20-2023",
    "hierarchy": {
      "h0": {
        "title": "July 2023 Release Notes",
      },
      "h2": {
        "id": "july-20-2023",
        "title": "July 20, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.july-july-20-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/july",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "July 20, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/july",
    "content": "Changed the HTTP status code to 201 for the REST API calls for create secret and create registry credentials.  Previously, we returned 200 for these calls.


Changed the HTTP status code to 201 for the REST API calls for create secret and create registry credentials. Previously, we returned 200 for these calls. The behavior of the SDK and web frontend is not affected.",
    "domain": "test.com",
    "hash": "#july-18-2023",
    "hierarchy": {
      "h0": {
        "title": "July 2023 Release Notes",
      },
      "h2": {
        "id": "july-18-2023",
        "title": "July 18, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.july-july-18-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/july",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "July 18, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/june",
    "content": "Added",
    "description": "OctoAI product updates and release notes for June 2023",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.june-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/june",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "June 2023 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/june",
    "content": "New tutorials showing how to use Automatic1111's Stable Diffusion web UI and an updated Falcon template.


Released a Doc tutorial to show users how to use OctoAI's server class GPUs with Automatic1111 Stable Diffusion web user interface.

Released a video tutorial to show users how to apply custom model checkpoints using Automatic1111's Stable Diffusion web user interface on OctoAI.

Updated our Falcon template to use a different server implementation behind the scenes. The inference API is now available at /generate, but inferences at /predict will continue to work.








Added",
    "domain": "test.com",
    "hash": "#june-27-2023",
    "hierarchy": {
      "h0": {
        "title": "June 2023 Release Notes",
      },
      "h2": {
        "id": "june-27-2023",
        "title": "June 27, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.june-june-27-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/june",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "June 27, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/june",
    "content": "We have launched OctoAI into general availability and made several updates to our models and endpoints.


With the launch of our service, changes will be made to our billing. You can find pricing plans and hardware options here. Changes and new user incentives taken into immediate effect are noted below:
Tomorrow, June 13th, any existing endpoints will be set to min replicas=0 so that you are not billed for an instance unintentionally left active and running. Be prepared for a cold start before your first inference and reset to min replicas=1 if you prefer to keep the instance warm.

Every user who logs in during public beta will receive credits for 2 free compute hrs on A100 (or 10+ hrs on A10!) to use in their first two weeks.

The first 500 users to create a new endpoint will receive credits for 12 free compute hrs on A100 (or 50+ hrs on A10!) to use within their first month.



You now have two options to integrate OctoAI endpoints into your application:
Our new Python client (supports synchronous inference).

Our HTTP REST API now supports both synchronous and asynchronous calls allowing users to request inference without persisting a connection, poll for status, and retrieve the completed prediction data. This is most effective when managing longer running requests.



We've updated our Whisper model to be much faster.

We've also added MPT 7B and Vicuña 7B as new quick-start templates as better alternatives to Dolly, which will be removed soon.








Added",
    "domain": "test.com",
    "hash": "#june-14-2023",
    "hierarchy": {
      "h0": {
        "title": "June 2023 Release Notes",
      },
      "h2": {
        "id": "june-14-2023",
        "title": "June 14, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.june-june-14-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/june",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "June 14, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/june",
    "content": "Some additions to OctoAI on June 6, 2023. Added private registry and the ability for users to mount secrets and other environment variables.


Private registry control
Added the ability for users to pull containers from private registries by applying registry credentials to an endpoint. See Pulling containers from a private registry for a guide.


Secrets and environment variables
Added the ability for users to mount secrets and other environment variables into their containers within an endpoint. See Setting up secrets or environment variables for your custom endpoints for a guide.


A100 GPUs
NVIDIA A100s are back online as of 5pm PDT on June 6th. The A100s were temporarily taken down earlier this week as maintenance was being performed to update the version of CUDA. User requests and hardware options are now functioning business as usual.",
    "domain": "test.com",
    "hash": "#june-6-2023",
    "hierarchy": {
      "h0": {
        "title": "June 2023 Release Notes",
      },
      "h2": {
        "id": "june-6-2023",
        "title": "June 6, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.june-june-6-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/june",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "June 6, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/may",
    "content": "Improved",
    "description": "OctoAI product updates and release notes for May 2023",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.may-root-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/may",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "May 2023 Release Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/release-notes/2023",
        "title": "2023",
      },
    ],
    "canonicalPathname": "/docs/release-notes/2023/may",
    "content": "Added support for NVIDIA A100s with 80GB of memory, and the ability for users to specify the health check server path.


Added support for NVIDIA A100s with 80GB of memory enabling faster inference and higher memory bandwidth. OctoAI's compute service allows users to start with a single A100 and scale up as traffic increases without paying for idle hardware.

Released a video tutorial to show users how to apply custom model checkpoints using Automatic1111's Stable Diffusion web user interface on OctoAI.",
    "domain": "test.com",
    "hash": "#may-31-2023",
    "hierarchy": {
      "h0": {
        "title": "May 2023 Release Notes",
      },
      "h2": {
        "id": "may-31-2023",
        "title": "May 31, 2023",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.release-notes.release-notes.2023.may-may-31-2023-0",
    "org_id": "test",
    "pathname": "/docs/release-notes/2023/may",
    "tab": {
      "pathname": "/docs/release-notes",
      "title": "Release Notes",
    },
    "title": "May 31, 2023",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_account.getAccount",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/account",
        "title": "Account",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/account/get-account",
    "default_environment_id": "Default",
    "description": "Return fields on an account",
    "domain": "test.com",
    "endpoint_path": "/v1/account",
    "endpoint_path_alternates": [
      "/v1/account",
      "https://api.octoai.cloud/v1/account",
      "https://api.octoai.cloud/v1/account",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_account.endpoint_account.getAccount",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/account/get-account",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get fields on an account",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_account.patchAccount",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/account",
        "title": "Account",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/account/patch-account",
    "default_environment_id": "Default",
    "description": "Update fields on account",
    "domain": "test.com",
    "endpoint_path": "/v1/account",
    "endpoint_path_alternates": [
      "/v1/account",
      "https://api.octoai.cloud/v1/account",
      "https://api.octoai.cloud/v1/account",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_account.endpoint_account.patchAccount",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/account/patch-account",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Update fields on an account",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.createEndpoint",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/endpoint",
        "title": "Endpoint",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/create-endpoint",
    "default_environment_id": "Default",
    "description": "Create new endpoint",
    "domain": "test.com",
    "endpoint_path": "/v1/endpoint",
    "endpoint_path_alternates": [
      "/v1/endpoint",
      "https://api.octoai.cloud/v1/endpoint",
      "https://api.octoai.cloud/v1/endpoint",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.createEndpoint",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/endpoint/create-endpoint",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create new endpoint",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.getEndpoint",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/endpoint",
        "title": "Endpoint",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/get-endpoint",
    "default_environment_id": "Default",
    "description": "Return description of an endpoint",
    "domain": "test.com",
    "endpoint_path": "/v1/endpoint/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.getEndpoint",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/endpoint/get-endpoint",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get description of an endpoint",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.deleteEndpoint",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/endpoint",
        "title": "Endpoint",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/delete-endpoint",
    "default_environment_id": "Default",
    "description": "Delete an endpoint",
    "domain": "test.com",
    "endpoint_path": "/v1/endpoint/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.deleteEndpoint",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/endpoint/delete-endpoint",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Delete an endpoint",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.patchEndpoint",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/endpoint",
        "title": "Endpoint",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/patch-endpoint",
    "default_environment_id": "Default",
    "description": "Updates an endpoint",
    "domain": "test.com",
    "endpoint_path": "/v1/endpoint/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.patchEndpoint",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/endpoint/patch-endpoint",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Update an endpoint",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.getEndpoints",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/endpoint",
        "title": "Endpoint",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/get-endpoints",
    "default_environment_id": "Default",
    "description": "Return list of endpoints",
    "domain": "test.com",
    "endpoint_path": "/v1/endpoints",
    "endpoint_path_alternates": [
      "/v1/endpoints",
      "https://api.octoai.cloud/v1/endpoints",
      "https://api.octoai.cloud/v1/endpoints",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.getEndpoints",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/endpoint/get-endpoints",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "List endpoints",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.getContainerMetadata",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/endpoint",
        "title": "Endpoint",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/get-container-metadata",
    "default_environment_id": "Default",
    "description": "Return container metadata",
    "domain": "test.com",
    "endpoint_path": "/v1/endpoint/:endpoint_name/container/metadata",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}/container/metadata",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name/container/metadata",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D/container/metadata",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.getContainerMetadata",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/endpoint/get-container-metadata",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get container metadata",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_endpoint.getEndpointVolumeToken",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/endpoint",
        "title": "Endpoint",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/endpoint/get-endpoint-volume-token",
    "default_environment_id": "Default",
    "description": "Returns a token for accessing the volume",
    "domain": "test.com",
    "endpoint_path": "/v1/endpoint/:endpoint_name/volume_token",
    "endpoint_path_alternates": [
      "/v1/endpoint/{endpoint_name}/volume_token",
      "https://api.octoai.cloud/v1/endpoint/:endpoint_name/volume_token",
      "https://api.octoai.cloud/v1/endpoint/%7Bendpoint_name%7D/volume_token",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_endpoint.endpoint_endpoint.getEndpointVolumeToken",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/endpoint/get-endpoint-volume-token",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get token for accessing the volume",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_telemetry.getEndpointLogs",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/telemetry",
        "title": "Telemetry",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-logs",
    "default_environment_id": "Default",
    "description": "Return endpoint logs",
    "domain": "test.com",
    "endpoint_path": "/v1/logs/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/logs/{endpoint_name}",
      "https://api.octoai.cloud/v1/logs/:endpoint_name",
      "https://api.octoai.cloud/v1/logs/%7Bendpoint_name%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_telemetry.endpoint_telemetry.getEndpointLogs",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-logs",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get endpoint logs",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_telemetry.getEndpointLogsStream",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/telemetry",
        "title": "Telemetry",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-logs-stream",
    "default_environment_id": "Default",
    "description": "Return endpoint logs as stream",
    "domain": "test.com",
    "endpoint_path": "/v1/logs/:endpoint_name/stream",
    "endpoint_path_alternates": [
      "/v1/logs/{endpoint_name}/stream",
      "https://api.octoai.cloud/v1/logs/:endpoint_name/stream",
      "https://api.octoai.cloud/v1/logs/%7Bendpoint_name%7D/stream",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_telemetry.endpoint_telemetry.getEndpointLogsStream",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-logs-stream",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get endpoint logs as stream",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_telemetry.getEndpointEvents",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/telemetry",
        "title": "Telemetry",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-events",
    "default_environment_id": "Default",
    "description": "Return endpoint events",
    "domain": "test.com",
    "endpoint_path": "/v1/events/:endpoint_name",
    "endpoint_path_alternates": [
      "/v1/events/{endpoint_name}",
      "https://api.octoai.cloud/v1/events/:endpoint_name",
      "https://api.octoai.cloud/v1/events/%7Bendpoint_name%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_telemetry.endpoint_telemetry.getEndpointEvents",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-events",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get endpoint events",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_telemetry.getEndpointEventsStream",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/telemetry",
        "title": "Telemetry",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-events-stream",
    "default_environment_id": "Default",
    "description": "Return endpoint events as stream",
    "domain": "test.com",
    "endpoint_path": "/v1/events/:endpoint_name/stream",
    "endpoint_path_alternates": [
      "/v1/events/{endpoint_name}/stream",
      "https://api.octoai.cloud/v1/events/:endpoint_name/stream",
      "https://api.octoai.cloud/v1/events/%7Bendpoint_name%7D/stream",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_telemetry.endpoint_telemetry.getEndpointEventsStream",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/telemetry/get-endpoint-events-stream",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get endpoint events as stream",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.createSecret",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/secret",
        "title": "Secret",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/create-secret",
    "default_environment_id": "Default",
    "description": "Create new secret",
    "domain": "test.com",
    "endpoint_path": "/v1/secret",
    "endpoint_path_alternates": [
      "/v1/secret",
      "https://api.octoai.cloud/v1/secret",
      "https://api.octoai.cloud/v1/secret",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.createSecret",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/secret/create-secret",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create new secret",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.getSecret",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/secret",
        "title": "Secret",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/get-secret",
    "default_environment_id": "Default",
    "domain": "test.com",
    "endpoint_path": "/v1/secret/:key",
    "endpoint_path_alternates": [
      "/v1/secret/{key}",
      "https://api.octoai.cloud/v1/secret/:key",
      "https://api.octoai.cloud/v1/secret/%7Bkey%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.getSecret",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/secret/get-secret",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve secret by key",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.updateSecret",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/secret",
        "title": "Secret",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/update-secret",
    "default_environment_id": "Default",
    "domain": "test.com",
    "endpoint_path": "/v1/secret/:key",
    "endpoint_path_alternates": [
      "/v1/secret/{key}",
      "https://api.octoai.cloud/v1/secret/:key",
      "https://api.octoai.cloud/v1/secret/%7Bkey%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PUT",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.updateSecret",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/secret/update-secret",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Update secret by key",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.deleteSecret",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/secret",
        "title": "Secret",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/delete-secret",
    "default_environment_id": "Default",
    "domain": "test.com",
    "endpoint_path": "/v1/secret/:key",
    "endpoint_path_alternates": [
      "/v1/secret/{key}",
      "https://api.octoai.cloud/v1/secret/:key",
      "https://api.octoai.cloud/v1/secret/%7Bkey%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.deleteSecret",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/secret/delete-secret",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Delete secret by key",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_secret.getSecrets",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/secret",
        "title": "Secret",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/secret/get-secrets",
    "default_environment_id": "Default",
    "description": "Return list of secrets",
    "domain": "test.com",
    "endpoint_path": "/v1/secrets",
    "endpoint_path_alternates": [
      "/v1/secrets",
      "https://api.octoai.cloud/v1/secrets",
      "https://api.octoai.cloud/v1/secrets",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_secret.endpoint_secret.getSecrets",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/secret/get-secrets",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get secret",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.createRegistryCredential",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/registry-credential",
        "title": "Registry Credential",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/create-registry-credential",
    "default_environment_id": "Default",
    "description": "Create new registry credentials",
    "domain": "test.com",
    "endpoint_path": "/v1/registry-credential",
    "endpoint_path_alternates": [
      "/v1/registry-credential",
      "https://api.octoai.cloud/v1/registry-credential",
      "https://api.octoai.cloud/v1/registry-credential",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.createRegistryCredential",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/create-registry-credential",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create new registry credentials",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.getRegistryCredential",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/registry-credential",
        "title": "Registry Credential",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/get-registry-credential",
    "default_environment_id": "Default",
    "domain": "test.com",
    "endpoint_path": "/v1/registry-credential/:key",
    "endpoint_path_alternates": [
      "/v1/registry-credential/{key}",
      "https://api.octoai.cloud/v1/registry-credential/:key",
      "https://api.octoai.cloud/v1/registry-credential/%7Bkey%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.getRegistryCredential",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/get-registry-credential",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve registry credentials by key",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.updateRegistryCredential",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/registry-credential",
        "title": "Registry Credential",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/update-registry-credential",
    "default_environment_id": "Default",
    "domain": "test.com",
    "endpoint_path": "/v1/registry-credential/:key",
    "endpoint_path_alternates": [
      "/v1/registry-credential/{key}",
      "https://api.octoai.cloud/v1/registry-credential/:key",
      "https://api.octoai.cloud/v1/registry-credential/%7Bkey%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PUT",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.updateRegistryCredential",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/update-registry-credential",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Update registry credentials",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.deleteRegistryCredential",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/registry-credential",
        "title": "Registry Credential",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/delete-registry-credential",
    "default_environment_id": "Default",
    "domain": "test.com",
    "endpoint_path": "/v1/registry-credential/:key",
    "endpoint_path_alternates": [
      "/v1/registry-credential/{key}",
      "https://api.octoai.cloud/v1/registry-credential/:key",
      "https://api.octoai.cloud/v1/registry-credential/%7Bkey%7D",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.deleteRegistryCredential",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/delete-registry-credential",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Delete registry credentials",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_registryCredential.getRegistryCredentials",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/registry-credential",
        "title": "Registry Credential",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/registry-credential/get-registry-credentials",
    "default_environment_id": "Default",
    "description": "Return list of registry credentials",
    "domain": "test.com",
    "endpoint_path": "/v1/registry-credentials",
    "endpoint_path_alternates": [
      "/v1/registry-credentials",
      "https://api.octoai.cloud/v1/registry-credentials",
      "https://api.octoai.cloud/v1/registry-credentials",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_registryCredential.endpoint_registryCredential.getRegistryCredentials",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/registry-credential/get-registry-credentials",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get registry credentials",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_publicEndpoint.getPublicEndpoints",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/public-endpoint",
        "title": "Public Endpoint",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/public-endpoint/get-public-endpoints",
    "default_environment_id": "Default",
    "description": "Return list of public, OctoAI-deployed endpoints",
    "domain": "test.com",
    "endpoint_path": "/v1/public-endpoints",
    "endpoint_path_alternates": [
      "/v1/public-endpoints",
      "https://api.octoai.cloud/v1/public-endpoints",
      "https://api.octoai.cloud/v1/public-endpoints",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_publicEndpoint.endpoint_publicEndpoint.getPublicEndpoints",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/public-endpoint/get-public-endpoints",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "List public, OctoAI-deployed endpoints",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "cb786748-d42f-427d-9c21-00e6c59bd56c",
    "api_endpoint_id": "endpoint_instanceTypes.getInstanceTypes",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/octoai-api/instance-types",
        "title": "Instance Types",
      },
    ],
    "canonicalPathname": "/docs/api-reference/octoai-api/instance-types/get-instance-types",
    "default_environment_id": "Default",
    "description": "Return list of available instance types to deploy an endpoint to",
    "domain": "test.com",
    "endpoint_path": "/v1/instance-types",
    "endpoint_path_alternates": [
      "/v1/instance-types",
      "https://api.octoai.cloud/v1/instance-types",
      "https://api.octoai.cloud/v1/instance-types",
    ],
    "environments": [
      {
        "id": "Default",
        "url": "https://api.octoai.cloud",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octoai-api.octo-ai-api.subpackage_instanceTypes.endpoint_instanceTypes.getInstanceTypes",
    "org_id": "test",
    "pathname": "/docs/api-reference/octoai-api/instance-types/get-instance-types",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "List hardware instance types",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.list",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/asset-library",
        "title": "Asset Library",
      },
    ],
    "canonicalPathname": "/docs/api-reference/asset-library/list",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/assets",
    "endpoint_path_alternates": [
      "/v1/assets",
      "https://api.octoai.cloud/v1/assets",
      "https://api.securelink.octo.ai/v1/assets",
      "https://api.octoai.cloud/v1/assets",
      "https://api.securelink.octo.ai/v1/assets",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.list",
    "org_id": "test",
    "pathname": "/docs/api-reference/asset-library/list",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "List Assets",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.create",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/asset-library",
        "title": "Asset Library",
      },
    ],
    "canonicalPathname": "/docs/api-reference/asset-library/create",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/assets",
    "endpoint_path_alternates": [
      "/v1/assets",
      "https://api.octoai.cloud/v1/assets",
      "https://api.securelink.octo.ai/v1/assets",
      "https://api.octoai.cloud/v1/assets",
      "https://api.securelink.octo.ai/v1/assets",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.create",
    "org_id": "test",
    "pathname": "/docs/api-reference/asset-library/create",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create Asset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.delete",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/asset-library",
        "title": "Asset Library",
      },
    ],
    "canonicalPathname": "/docs/api-reference/asset-library/delete",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/assets/:asset_id",
    "endpoint_path_alternates": [
      "/v1/assets/{asset_id}",
      "https://api.octoai.cloud/v1/assets/:asset_id",
      "https://api.securelink.octo.ai/v1/assets/:asset_id",
      "https://api.octoai.cloud/v1/assets/%7Basset_id%7D",
      "https://api.securelink.octo.ai/v1/assets/%7Basset_id%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.delete",
    "org_id": "test",
    "pathname": "/docs/api-reference/asset-library/delete",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Delete Asset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.completeUpload",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/asset-library",
        "title": "Asset Library",
      },
    ],
    "canonicalPathname": "/docs/api-reference/asset-library/complete-upload",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/assets/:asset_id/complete-upload",
    "endpoint_path_alternates": [
      "/v1/assets/{asset_id}/complete-upload",
      "https://api.octoai.cloud/v1/assets/:asset_id/complete-upload",
      "https://api.securelink.octo.ai/v1/assets/:asset_id/complete-upload",
      "https://api.octoai.cloud/v1/assets/%7Basset_id%7D/complete-upload",
      "https://api.securelink.octo.ai/v1/assets/%7Basset_id%7D/complete-upload",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.completeUpload",
    "org_id": "test",
    "pathname": "/docs/api-reference/asset-library/complete-upload",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Complete Asset Upload",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_asset-library.get",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/asset-library",
        "title": "Asset Library",
      },
    ],
    "canonicalPathname": "/docs/api-reference/asset-library/get",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/assets/:asset_owner_and_name_or_id",
    "endpoint_path_alternates": [
      "/v1/assets/{asset_owner_and_name_or_id}",
      "https://api.octoai.cloud/v1/assets/:asset_owner_and_name_or_id",
      "https://api.securelink.octo.ai/v1/assets/:asset_owner_and_name_or_id",
      "https://api.octoai.cloud/v1/assets/%7Basset_owner_and_name_or_id%7D",
      "https://api.securelink.octo.ai/v1/assets/%7Basset_owner_and_name_or_id%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_asset-library.endpoint_asset-library.get",
    "org_id": "test",
    "pathname": "/docs/api-reference/asset-library/get",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Asset",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.create",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/fine-tuning",
        "title": "Fine Tuning",
      },
    ],
    "canonicalPathname": "/docs/api-reference/fine-tuning/create",
    "default_environment_id": "Production",
    "description": "Spawn a tune.",
    "domain": "test.com",
    "endpoint_path": "/v1/tune",
    "endpoint_path_alternates": [
      "/v1/tune",
      "https://api.octoai.cloud/v1/tune",
      "https://api.securelink.octo.ai/v1/tune",
      "https://api.octoai.cloud/v1/tune",
      "https://api.securelink.octo.ai/v1/tune",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.create",
    "org_id": "test",
    "pathname": "/docs/api-reference/fine-tuning/create",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create Tune",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.get",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/fine-tuning",
        "title": "Fine Tuning",
      },
    ],
    "canonicalPathname": "/docs/api-reference/fine-tuning/get",
    "default_environment_id": "Production",
    "description": "Get the specific tune.",
    "domain": "test.com",
    "endpoint_path": "/v1/tune/:tune_id",
    "endpoint_path_alternates": [
      "/v1/tune/{tune_id}",
      "https://api.octoai.cloud/v1/tune/:tune_id",
      "https://api.securelink.octo.ai/v1/tune/:tune_id",
      "https://api.octoai.cloud/v1/tune/%7Btune_id%7D",
      "https://api.securelink.octo.ai/v1/tune/%7Btune_id%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.get",
    "org_id": "test",
    "pathname": "/docs/api-reference/fine-tuning/get",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Get Tune",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.delete",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/fine-tuning",
        "title": "Fine Tuning",
      },
    ],
    "canonicalPathname": "/docs/api-reference/fine-tuning/delete",
    "default_environment_id": "Production",
    "description": "Delete the specified tune.",
    "domain": "test.com",
    "endpoint_path": "/v1/tune/:tune_id",
    "endpoint_path_alternates": [
      "/v1/tune/{tune_id}",
      "https://api.octoai.cloud/v1/tune/:tune_id",
      "https://api.securelink.octo.ai/v1/tune/:tune_id",
      "https://api.octoai.cloud/v1/tune/%7Btune_id%7D",
      "https://api.securelink.octo.ai/v1/tune/%7Btune_id%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.delete",
    "org_id": "test",
    "pathname": "/docs/api-reference/fine-tuning/delete",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Delete Tune",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.cancel",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/fine-tuning",
        "title": "Fine Tuning",
      },
    ],
    "canonicalPathname": "/docs/api-reference/fine-tuning/cancel",
    "default_environment_id": "Production",
    "description": "Cancel the specified tune.",
    "domain": "test.com",
    "endpoint_path": "/v1/tune/:tune_id/cancel",
    "endpoint_path_alternates": [
      "/v1/tune/{tune_id}/cancel",
      "https://api.octoai.cloud/v1/tune/:tune_id/cancel",
      "https://api.securelink.octo.ai/v1/tune/:tune_id/cancel",
      "https://api.octoai.cloud/v1/tune/%7Btune_id%7D/cancel",
      "https://api.securelink.octo.ai/v1/tune/%7Btune_id%7D/cancel",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.cancel",
    "org_id": "test",
    "pathname": "/docs/api-reference/fine-tuning/cancel",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Cancel Tune",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_fine-tuning.list",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/fine-tuning",
        "title": "Fine Tuning",
      },
    ],
    "canonicalPathname": "/docs/api-reference/fine-tuning/list",
    "default_environment_id": "Production",
    "description": "List all tunes owned by the current user.",
    "domain": "test.com",
    "endpoint_path": "/v1/tunes",
    "endpoint_path_alternates": [
      "/v1/tunes",
      "https://api.octoai.cloud/v1/tunes",
      "https://api.securelink.octo.ai/v1/tunes",
      "https://api.octoai.cloud/v1/tunes",
      "https://api.securelink.octo.ai/v1/tunes",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.octoai.cloud",
      },
      {
        "id": "SecureLink",
        "url": "https://api.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "GET",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_fine-tuning.endpoint_fine-tuning.list",
    "org_id": "test",
    "pathname": "/docs/api-reference/fine-tuning/list",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "List Tunes",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateSsd",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/image-gen",
        "title": "Image Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/image-gen/generate-ssd",
    "default_environment_id": "Production",
    "description": "Generate images in response to the given request.",
    "domain": "test.com",
    "endpoint_path": "/generate/ssd",
    "endpoint_path_alternates": [
      "/generate/ssd",
      "https://image.octoai.run/generate/ssd",
      "https://image.securelink.octo.ai/generate/ssd",
      "https://image.octoai.run/generate/ssd",
      "https://image.securelink.octo.ai/generate/ssd",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateSsd",
    "org_id": "test",
    "pathname": "/docs/api-reference/image-gen/generate-ssd",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Generate SSD",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateControlnetSdxl",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/image-gen",
        "title": "Image Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/image-gen/generate-controlnet-sdxl",
    "default_environment_id": "Production",
    "description": "Generate images in response to the given request.",
    "domain": "test.com",
    "endpoint_path": "/generate/controlnet-sdxl",
    "endpoint_path_alternates": [
      "/generate/controlnet-sdxl",
      "https://image.octoai.run/generate/controlnet-sdxl",
      "https://image.securelink.octo.ai/generate/controlnet-sdxl",
      "https://image.octoai.run/generate/controlnet-sdxl",
      "https://image.securelink.octo.ai/generate/controlnet-sdxl",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateControlnetSdxl",
    "org_id": "test",
    "pathname": "/docs/api-reference/image-gen/generate-controlnet-sdxl",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Generate ControlNet SDXL",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateControlnetSd15",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/image-gen",
        "title": "Image Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/image-gen/generate-controlnet-sd-15",
    "default_environment_id": "Production",
    "description": "Generate images in response to the given request.",
    "domain": "test.com",
    "endpoint_path": "/generate/controlnet-sd15",
    "endpoint_path_alternates": [
      "/generate/controlnet-sd15",
      "https://image.octoai.run/generate/controlnet-sd15",
      "https://image.securelink.octo.ai/generate/controlnet-sd15",
      "https://image.octoai.run/generate/controlnet-sd15",
      "https://image.securelink.octo.ai/generate/controlnet-sd15",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateControlnetSd15",
    "org_id": "test",
    "pathname": "/docs/api-reference/image-gen/generate-controlnet-sd-15",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Generate ControlNet SD1.5",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateSdxl",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/image-gen",
        "title": "Image Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/image-gen/generate-sdxl",
    "default_environment_id": "Production",
    "description": "Generate images in response to the given request.",
    "domain": "test.com",
    "endpoint_path": "/generate/sdxl",
    "endpoint_path_alternates": [
      "/generate/sdxl",
      "https://image.octoai.run/generate/sdxl",
      "https://image.securelink.octo.ai/generate/sdxl",
      "https://image.octoai.run/generate/sdxl",
      "https://image.securelink.octo.ai/generate/sdxl",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateSdxl",
    "org_id": "test",
    "pathname": "/docs/api-reference/image-gen/generate-sdxl",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Generate SDXL",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateSd",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/image-gen",
        "title": "Image Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/image-gen/generate-sd",
    "default_environment_id": "Production",
    "description": "Generate images in response to the given request.",
    "domain": "test.com",
    "endpoint_path": "/generate/sd",
    "endpoint_path_alternates": [
      "/generate/sd",
      "https://image.octoai.run/generate/sd",
      "https://image.securelink.octo.ai/generate/sd",
      "https://image.octoai.run/generate/sd",
      "https://image.securelink.octo.ai/generate/sd",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateSd",
    "org_id": "test",
    "pathname": "/docs/api-reference/image-gen/generate-sd",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Generate SD1.5",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_image-gen.generateSvd",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/image-gen",
        "title": "Image Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/image-gen/generate-svd",
    "default_environment_id": "Production",
    "description": "Generate videos in response to the given request.",
    "domain": "test.com",
    "endpoint_path": "/generate/svd",
    "endpoint_path_alternates": [
      "/generate/svd",
      "https://image.octoai.run/generate/svd",
      "https://image.securelink.octo.ai/generate/svd",
      "https://image.octoai.run/generate/svd",
      "https://image.securelink.octo.ai/generate/svd",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://image.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://image.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_image-gen.endpoint_image-gen.generateSvd",
    "org_id": "test",
    "pathname": "/docs/api-reference/image-gen/generate-svd",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Generate SVD Animations",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_text-gen.createChatCompletion",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/text-gen",
        "title": "Text Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/text-gen/create-chat-completion",
    "default_environment_id": "Production",
    "description": "Create a Chat Completion.",
    "domain": "test.com",
    "endpoint_path": "/v1/chat/completions",
    "endpoint_path_alternates": [
      "/v1/chat/completions",
      "https://text.octoai.run/v1/chat/completions",
      "https://text.securelink.octo.ai/v1/chat/completions",
      "https://text.octoai.run/v1/chat/completions",
      "https://text.securelink.octo.ai/v1/chat/completions",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://text.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://text.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_text-gen.endpoint_text-gen.createChatCompletion",
    "org_id": "test",
    "pathname": "/docs/api-reference/text-gen/create-chat-completion",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create Chat Completion",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_text-gen.createChatCompletion_stream",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/text-gen",
        "title": "Text Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/text-gen/create-chat-completion",
    "default_environment_id": "Production",
    "description": "Create a Chat Completion.",
    "domain": "test.com",
    "endpoint_path": "/v1/chat/completions",
    "endpoint_path_alternates": [
      "/v1/chat/completions",
      "https://text.octoai.run/v1/chat/completions",
      "https://text.securelink.octo.ai/v1/chat/completions",
      "https://text.octoai.run/v1/chat/completions",
      "https://text.securelink.octo.ai/v1/chat/completions",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://text.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://text.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "stream",
      "ChatCompletionChunk",
      "HTTPValidationError",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_text-gen.endpoint_text-gen.createChatCompletion_stream",
    "org_id": "test",
    "pathname": "/docs/api-reference/text-gen/create-chat-completion-stream",
    "response_type": "stream",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create Chat Completion",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_text-gen.createCompletion",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/text-gen",
        "title": "Text Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/text-gen/create-completion",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/completions",
    "endpoint_path_alternates": [
      "/v1/completions",
      "https://text.octoai.run/v1/completions",
      "https://text.securelink.octo.ai/v1/completions",
      "https://text.octoai.run/v1/completions",
      "https://text.securelink.octo.ai/v1/completions",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://text.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://text.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_text-gen.endpoint_text-gen.createCompletion",
    "org_id": "test",
    "pathname": "/docs/api-reference/text-gen/create-completion",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create Completion",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "270fcb14-fecb-45b5-8075-3e207a0d4b31",
    "api_endpoint_id": "endpoint_text-gen.createCompletion_stream",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference",
        "title": "OctoAI API",
      },
      {
        "pathname": "/docs/api-reference/text-gen",
        "title": "Text Gen",
      },
    ],
    "canonicalPathname": "/docs/api-reference/text-gen/create-completion",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/completions",
    "endpoint_path_alternates": [
      "/v1/completions",
      "https://text.octoai.run/v1/completions",
      "https://text.securelink.octo.ai/v1/completions",
      "https://text.octoai.run/v1/completions",
      "https://text.securelink.octo.ai/v1/completions",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://text.octoai.run",
      },
      {
        "id": "SecureLink",
        "url": "https://text.securelink.octo.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "stream",
      "CompletionResponse",
      "HTTPValidationError",
      "ErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.octo-ai-api.subpackage_text-gen.endpoint_text-gen.createCompletion_stream",
    "org_id": "test",
    "pathname": "/docs/api-reference/text-gen/create-completion-stream",
    "response_type": "stream",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Create Completion",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "2eeac53f-90cc-492b-9d29-e3ed711d7704",
    "api_endpoint_id": "endpoint_.upscale",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/media-util",
        "title": "Media Utilities API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/media-util/upscale",
    "description": "Upscale the given image.",
    "domain": "test.com",
    "endpoint_path": "/upscaling",
    "endpoint_path_alternates": [
      "/upscaling",
    ],
    "environments": [],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.media-util.endpoint_.upscale",
    "org_id": "test",
    "pathname": "/docs/api-reference/media-util/upscale",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Upscaling",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "2eeac53f-90cc-492b-9d29-e3ed711d7704",
    "api_endpoint_id": "endpoint_.remove_background",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/media-util",
        "title": "Media Utilities API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/media-util/remove-background",
    "description": "Remove background from the given image.",
    "domain": "test.com",
    "endpoint_path": "/background-removal",
    "endpoint_path_alternates": [
      "/background-removal",
    ],
    "environments": [],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.media-util.endpoint_.remove_background",
    "org_id": "test",
    "pathname": "/docs/api-reference/media-util/remove-background",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Remove Background",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "2eeac53f-90cc-492b-9d29-e3ed711d7704",
    "api_endpoint_id": "endpoint_.generate_images",
    "api_type": "http",
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/docs/api-reference/media-util",
        "title": "Media Utilities API",
      },
    ],
    "canonicalPathname": "/docs/api-reference/media-util/generate-images",
    "description": "Detail the given image.",
    "domain": "test.com",
    "endpoint_path": "/adetailer",
    "endpoint_path_alternates": [
      "/adetailer",
    ],
    "environments": [],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "HTTPValidationError",
    ],
    "method": "POST",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.media-util.endpoint_.generate_images",
    "org_id": "test",
    "pathname": "/docs/api-reference/media-util/generate-images",
    "response_type": "json",
    "tab": {
      "pathname": "/docs/api-reference",
      "title": "API Reference",
    },
    "title": "Adetailer",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
]