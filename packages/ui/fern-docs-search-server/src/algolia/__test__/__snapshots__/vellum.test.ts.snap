[
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/welcome",
        "title": "Welcome",
      },
    ],
    "canonicalPathname": "/help-center/welcome/welcome",
    "content": "Welcome üëã Vellum helps bring LLM-powered features to production with tools for prompt engineering,
semantic search, version control, quantitative testing, and performance monitoring across
all major LLM providers and open source models.
Here you'll find resources and guides for the Vellum platform and our APIs. Please don't hesitate to contact
us at support@vellum.ai if you can't find what you're looking for.",
    "description": "Discover Vellum for prompt engineering, semantic search, and more. Get resources and support for all major LLM providers.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.welcome.welcome-root-0",
    "org_id": "test",
    "pathname": "/help-center/welcome/welcome",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Welcome to Vellum ‚Äì AI product development platform",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/welcome",
        "title": "Welcome",
      },
    ],
    "canonicalPathname": "/help-center/welcome/getting-support",
    "content": "Having trouble some trouble using Vellum? Don't worry, we're here to help ‚Äì there are many ways to get unstuck!",
    "description": "Discover ways to contact the Vellum team and receive assistance with your LLM features.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.welcome.getting-support-root-0",
    "org_id": "test",
    "pathname": "/help-center/welcome/getting-support",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Vellum's Help Center",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/welcome",
        "title": "Welcome",
      },
    ],
    "canonicalPathname": "/help-center/welcome/getting-support",
    "content": "We respond quickly to emails, so don't hesitate to reach out to us at support@vellum.ai. When you do, please describe your use case and the specific problem you're encountering so we can better assist you.",
    "domain": "test.com",
    "hash": "#-email-us",
    "hierarchy": {
      "h0": {
        "title": "Vellum's Help Center",
      },
      "h2": {
        "id": "-email-us",
        "title": "üìß Email us",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.welcome.getting-support--email-us-0",
    "org_id": "test",
    "pathname": "/help-center/welcome/getting-support",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "üìß Email us",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/welcome",
        "title": "Welcome",
      },
    ],
    "canonicalPathname": "/help-center/welcome/getting-support",
    "content": "If you prefer a self-service option, check out the articles here in our Help Center. We have plenty of articles with detailed explanations, screenshots, and videos to help you troubleshoot common issues. We're constantly adding new resources, so be sure to check back often!",
    "domain": "test.com",
    "hash": "#help-center",
    "hierarchy": {
      "h0": {
        "title": "Vellum's Help Center",
      },
      "h2": {
        "id": "help-center",
        "title": "‚ùìHelp Center",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.welcome.getting-support-help-center-0",
    "org_id": "test",
    "pathname": "/help-center/welcome/getting-support",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "‚ùìHelp Center",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/welcome",
        "title": "Welcome",
      },
    ],
    "canonicalPathname": "/help-center/welcome/getting-support",
    "content": "Another great way to get help and connect with other Vellum users is to join our Discord community. Our community is a great resource for getting advice and tips from other Vellum users. We're constantly monitoring Discord and answering questions to help you get unstuck. We're all here to help each other out, so don't hesitate to join and start chatting!",
    "domain": "test.com",
    "hash": "#-discord",
    "hierarchy": {
      "h0": {
        "title": "Vellum's Help Center",
      },
      "h2": {
        "id": "-discord",
        "title": "üßë‚Äçüíª Discord",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.welcome.getting-support--discord-0",
    "org_id": "test",
    "pathname": "/help-center/welcome/getting-support",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "üßë‚Äçüíª Discord",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Prompts are the "instructions" that you give to Large Language Models (LLMs) to generate a response.
If you've used ChatGPT, then you've actually already interacted with an LLM that was provided with a
Prompt on how it should respond in a helpful, polite manner!
When building your own AI-powered application, you'll very likely need to come up with your own Prompts.
Furthermore, Prompts are rarely static strings. Instead, most Prompts are "templates" with dynamic
sections whose contents are determined at runtime. For example, you might need to include information about
the user that's interacting with your application, some relevant section of a knowledge base, etc.
Vellum encourages the development of dynamic prompts by providing a powerful
prompt syntax that supports variable substitution, jinja templating, and function calling.
At a high level, you define "Input Variables" and reference those variables in your Prompt.
You can experiment with different values for these variables via "Scenarios" to determine
what the LLM's output would be.
You can reference input variables in your Prompt in one of two ways using different types
of "blocks."
Rich Text Blocks: Great for most use-cases where a simple variable substitution is needed.
Begin type {{  or / to get a dropdown of available variables.

Jinja Blocks: Used for more complex use-cases where you need the power of Jinja templating syntax
to perform conditional logic, loops, etc.",
    "description": "Discover how Vellum's prompt engineering enhances LLMs with dynamic templates, jinja templating, and function calling for smarter prompts.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-root-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Prompt Engineering",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Most of the time, you'll use Rich Text blocks for simple variable substitution.
These blocks are easy to use and are great for most use-cases. You can reference
variables by typing {{  or / to get a dropdown of available variables.
Here's an example of a Rich Text block:
Rich Text Block Example",
    "domain": "test.com",
    "hash": "#rich-text-blocks",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "rich-text-blocks",
        "title": "Rich Text Blocks",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-rich-text-blocks-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Rich Text Blocks",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Jinja is a powerful templating syntax useful for dynamic content.
In its most basic form, you might use it to reference Prompt Variables.
However, if all you need is variable substitution, consider using a Rich
Text block instead.
Below are the most common things you‚Äôre likely to want to do,
but you can find jinja‚Äôs complete documentation
here.",
    "domain": "test.com",
    "hash": "#jinja-blocks",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "jinja-blocks",
        "title": "Jinja Blocks",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-jinja-blocks-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Jinja Blocks",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Reference variables using double-curly-brackets. For example,


You are a {{ personality_type }} AI assistant.


Note that all prompt variables are treated as strings!",
    "domain": "test.com",
    "hash": "#variables",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "jinja-blocks",
        "title": "Jinja Blocks",
      },
      "h3": {
        "id": "variables",
        "title": "Variables",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-variables-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Variables",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "code_snippets": [
      {
        "code": "You are a {{ personality_type }} AI assistant.
{% if personality_type == "rude" %}
You end every message with a frowning emoji.
{% else %}
You end every message with a smiling emoji.
{% endif %}",
      },
      {
        "code": "You are a {{ personality_type }} AI assistant.
{% if personality_type == "rude" %}
You end every message with a frowning emoji.
{% else %}
You end every message with a smiling emoji.
{% endif %}",
      },
    ],
    "content": "Perform conditional logic based on your input variables using if/else statements",
    "domain": "test.com",
    "hash": "#conditionals",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "jinja-blocks",
        "title": "Jinja Blocks",
      },
      "h3": {
        "id": "conditionals",
        "title": "Conditionals",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-conditionals-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Conditionals",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "code_snippets": [
      {
        "code": "{# This is a comment #}
Hello, world!",
      },
      {
        "code": "{# This is a comment #}
Hello, world!",
      },
    ],
    "content": "You can use jinja to leave comments in your prompt that don‚Äôt use up any
tokens when compiled and sent to the LLM. For example,",
    "domain": "test.com",
    "hash": "#comments",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "jinja-blocks",
        "title": "Jinja Blocks",
      },
      "h3": {
        "id": "comments",
        "title": "Comments",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-comments-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Comments",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "code_snippets": [
      {
        "code": "{
  "hair_color": "brown",
  "personality": "happy go lucky"
}",
        "lang": "json",
      },
      {
        "code": "You are a {{ traits.personality }} AI assistant.
",
      },
    ],
    "content": "Vellum supports JSON input variables. When you supply a JSON variable, you can use this trick to access specific key/value pairs.
For example, say you have a variable called traits whose value in a Scenario looked like:
Then you can access "happy go lucky" by using a Jinja template block and referencing the JSON variable like so:",
    "domain": "test.com",
    "hash": "#json-inputs",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "jinja-blocks",
        "title": "Jinja Blocks",
      },
      "h3": {
        "id": "json-inputs",
        "title": "JSON Inputs",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-json-inputs-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "JSON Inputs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "code_snippets": [
      {
        "code": "You are an AI chat bot working at the registry of motor vehicles.
The person who just stepped up to the counter
{% if age | float > 16 %}
is of legal driving age.
{% else %}
isn't yet old enough to drive.
{% endif %}",
      },
    ],
    "content": "Vellum currently treats all input variables to prompts as strings. However, you may use jinja filters to convert your variables to specific types and then use them accordingly. For example:",
    "domain": "test.com",
    "hash": "#casting-variable-types",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "jinja-blocks",
        "title": "Jinja Blocks",
      },
      "h3": {
        "id": "casting-variable-types",
        "title": "Casting Variable Types",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-casting-variable-types-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Casting Variable Types",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Vellum uses blocks to separate key pieces of a prompt, such as System/Assistant/User messages of a Chat model¬†(e.g. gpt-3.5-turbo or claude-v1) or the special $chat_history variable.
Blocks are more noticeable when using a Chat model than when using a Text model.",
    "domain": "test.com",
    "hash": "#blocks",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "blocks",
        "title": "Blocks",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-blocks-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Blocks",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Here‚Äôs what a sequence of blocks might look like for a Chat model
Chat Model Prompt",
    "domain": "test.com",
    "hash": "#chat-model-example",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "blocks",
        "title": "Blocks",
      },
      "h3": {
        "id": "chat-model-example",
        "title": "Chat Model Example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-chat-model-example-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Chat Model Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Text models typically use a single block, which might look like this:
Text Model Prompt",
    "domain": "test.com",
    "hash": "#text-model-example",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "blocks",
        "title": "Blocks",
      },
      "h3": {
        "id": "text-model-example",
        "title": "Text Model Example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-text-model-example-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Text Model Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "When switching from a Chat Model to a Text model, blocks are converted as best they can be.
Chat ‚Üí Text
Here‚Äôs an example going from Chat to Text.
Converting from Chat Models to Text Models
Text ‚Üí Chat
Here‚Äôs an example going from Text to Chat.
Converting from Text Models to Chat Models",
    "domain": "test.com",
    "hash": "#switching-from-chat--text-models",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "blocks",
        "title": "Blocks",
      },
      "h3": {
        "id": "switching-from-chat--text-models",
        "title": "Switching from Chat ‚Üî Text Models",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-switching-from-chat--text-models-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Switching from Chat ‚Üî Text Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Function Calling allows you to provide function definitions within your prompts the the model could use in deciding how to respond with each user prompt. It‚Äôs best to think of functions as a type of classifier prompt that pushes to model to respond with either:
The name of one of those functions, with associated parameter values.

A standard text response.


This definitive response from the model allows developers building LLM features into their applications to know when to call a function or when to return a message to the user. This removes the need to try parsing JSONs are other formats from the LLM text response, leading to more stable user experiences.
To define a function, you first need to choose a model that supports function calling and click the + Add ‚Üí Function button at the bottom of your prompt:
Add Prompt Block Button
Then, click the block that‚Äôs created and a modal will appear where you can start defining your function! There are three important sections to consider:
Name - This is a single identifier that the model will use to instruct you which function to call next

Description - This is a natural language description of what your function does. This is the part most used by the model to decide which function to call and should be considered counting towards your token count.

Parameters - The set of parameters your functions accept. Each parameter will also have a Name, Description, & Type that the model uses to decide what values the function should be called with.


Function Placeholder
Edit Function Dialog
When you then call the model with a prompt along with these function definitions, the model will then decide whether it makes sense to call one of the defined functions or return the standard text response. If it decides to call a defined function, the response will be a JSON directing which function to call and with which parameter values:
Function Call Response
Notice that in this example, the model is directing us to call the get_current_weather function with the location parameter set to Boston, MA. At this point, it is up to the app developer to actually invoke the function - the model itself does not have access to the execution logic. These functions should represent public or private APIs that the app developer supports and could invoke once instructed by the model.
Once the function is called and a response is observed, the response should be fed back to the LLM as a function message so that the model knows what was the outcome of calling that function. The model will then be able to use the response of that function when deciding how to respond next in order to satisfy the original prompt.
Assistant Response Following FUnction Call
Notice that we need to specify the original response from the model as an assistant message, before following up with a function message. The final output from the model then represents its understanding of the user‚Äôs prompt and the output of the function it had access to.
Once you have reached this point, you‚Äôll have everything you‚Äôll need to add functions to models! Function calling is best used for incorporating the following types of data into your prompts:
Runtime or recent data - New data that has become available that the developer‚Äôs APIs have access to but the model was not trained on

Proprietary data - Data the developer has collected that is specialized to their business proposition that gives their application a comparative advantage

Weighted data - Data that the model already has been trained on but that the developer would like to re-prioritize based on some special insights that could be inferred from their API",
    "domain": "test.com",
    "hash": "#function-calling",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h2": {
        "id": "blocks",
        "title": "Blocks",
      },
      "h3": {
        "id": "function-calling",
        "title": "Function Calling",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-function-calling-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Function Calling",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "Given the powerful and dynamic nature of Vellum‚Äôs prompt syntax, you may want to see what the final, compiled payload sent to the model provider after all variable substitutions and jinja templating is applied. You can do this in the Playground UI like so:
Previewing Compiled Prompts",
    "domain": "test.com",
    "hash": "#previewing-compiled-prompts",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h1": {
        "id": "previewing-compiled-prompts",
        "title": "Previewing Compiled Prompts",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-previewing-compiled-prompts-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Previewing Compiled Prompts",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompt-engineering",
    "content": "You can view how much token, character or compute time usage your prompts are costing you by enabling the "Track Usage" toggle in your Prompt Sandbox's settings.
Usage Tracking Sandbox",
    "domain": "test.com",
    "hash": "#viewing-prompt-usage",
    "hierarchy": {
      "h0": {
        "title": "Prompt Engineering",
      },
      "h1": {
        "id": "viewing-prompt-usage",
        "title": "Viewing Prompt Usage",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompt-engineering-viewing-prompt-usage-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompt-engineering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Viewing Prompt Usage",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/collaboration",
    "content": "The Vellum Prompt Playground is a powerful tool for rapid iteration and
collaboration between multiple models and prompts. Save, tag,
and share your work with ease using the features outlined below.",
    "description": "Explore Vellum Prompt Playground for rapid iteration, collaboration, and sharing of model-generated prompts. Save, tag, and track progress easily.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.collaboration-root-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/collaboration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Collaborate on Prompts with Vellum Prompt Playground",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/collaboration",
    "content": "Every model-generated response and respective prompt are saved
as history items, giving you access to a detailed record of your
work. To access history items, simply activate the toggle button
located at the top right of the Playground, and all history items
will appear on the left side of your screen.
Playground History",
    "domain": "test.com",
    "hash": "#history",
    "hierarchy": {
      "h0": {
        "title": "Collaborate on Prompts with Vellum Prompt Playground",
      },
      "h3": {
        "id": "history",
        "title": "History",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.collaboration-history-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/collaboration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "History",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/collaboration",
    "content": "The Playground is designed to help you iterate on prompts and model
providers until you find the perfect fit for your needs. With the
history feature, you can keep track of your team's work in an organized
way by only keeping the iterations you choose to, through the save button.
Everyone working on the same sandbox can see each other's history items,
and you can also tag them to keep better track of your work.
Tagging History",
    "domain": "test.com",
    "hash": "#tracking-progress-collaborating-and-tagging",
    "hierarchy": {
      "h0": {
        "title": "Collaborate on Prompts with Vellum Prompt Playground",
      },
      "h3": {
        "id": "tracking-progress-collaborating-and-tagging",
        "title": "Tracking Progress, Collaborating, and Tagging",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.collaboration-tracking-progress-collaborating-and-tagging-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/collaboration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Tracking Progress, Collaborating, and Tagging",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/collaboration",
    "content": "At any point in time, you can easily share your work with anyone in
your organization through a URL by using the ‚Äúinvite‚Äù button located
at the top right of the page.
Inviting Teammates",
    "domain": "test.com",
    "hash": "#share-your-work",
    "hierarchy": {
      "h0": {
        "title": "Collaborate on Prompts with Vellum Prompt Playground",
      },
      "h3": {
        "id": "share-your-work",
        "title": "Share Your Work",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.collaboration-share-your-work-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/collaboration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Share Your Work",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/custom-models",
    "content": "Vellum supports several of the industry's most popular models by default available in your workspace right away. However, you may wish to use a custom model that gives your business some additional advantage not provided by these off the shelf models, such as higher rate limits or more domain-specific training. These models can also be set up for use within Vellum!
Custom models fall under two categories: private models and public models. Both could be added via the Models tab within Vellum.",
    "description": "Learn how to add both private and public custom models to your Vellum workspace for enhanced functionality and domain-specific advantages.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.custom-models-root-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/custom-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Integrate Custom Models in Your Vellum Workspace Easily",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/custom-models",
    "content": "Private models are new instances of models that were created by you outside of Vellum and are looking to integrate into the platform. When you navigate to the models page, the supported types of private models will be accessible from a section on the top of the page:
Adding Private Custom Models
Clicking on one of the templates will take you to an onboarding flow on how to connect your private model to Vellum. Once you've completed the pre-requisite steps and add in the requested form info, your model should be successfully added to your workspace!
We currently support the following private Model Templates:
OpenAI models hosted on Azure

OpenAI fine-tuned models

Fine-tuned models hosted on Fireworks AI",
    "domain": "test.com",
    "hash": "#adding-private-models",
    "hierarchy": {
      "h0": {
        "title": "Integrate Custom Models in Your Vellum Workspace Easily",
      },
      "h2": {
        "id": "adding-private-models",
        "title": "Adding Private Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.custom-models-adding-private-models-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/custom-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Adding Private Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/custom-models",
    "content": "Public models are shared instances of models that are hosted by model providers and are granted access to them by various authentication schemes, most commonly via an API Token. Some are enabled in your workspace by default when you create a new workspace in Vellum. To find other public models not yet enabled in your workspace, navigate to the models page and scroll down to the Available Models section:
Adding Public Custom Models
To help filter the options, you could select just Available in the drop down on the right or use the search bar to look for the specific model of interest.
While most of these models require just adding your API key from the relevant model provider, some like those from AWS Bedrock will require some additional steps taken within your account. These directions will be laid out within each model's onboarding modal when you click to enable them in your workspace.",
    "domain": "test.com",
    "hash": "#adding-public-models",
    "hierarchy": {
      "h0": {
        "title": "Integrate Custom Models in Your Vellum Workspace Easily",
      },
      "h2": {
        "id": "adding-public-models",
        "title": "Adding Public Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.custom-models-adding-public-models-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/custom-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Adding Public Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/custom-models",
    "content": "Don't see a custom model listed here but want to try it within Vellum? Reach out to us on Slack for support!",
    "domain": "test.com",
    "hash": "#request-a-model",
    "hierarchy": {
      "h0": {
        "title": "Integrate Custom Models in Your Vellum Workspace Easily",
      },
      "h2": {
        "id": "request-a-model",
        "title": "Request a Model",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.custom-models-request-a-model-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/custom-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Request a Model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/images",
    "content": "Leverage the power of multimodal models to process both natural language and visual inputs within your LLM-applications using Vellum.
Vellum supports images for OpenAI‚Äôs vision models like GPT-4 Turbo with Vision - both via API and in the Vellum UI.
Images in Vellum UI
Read on to learn how to get started using images in Vellum!",
    "description": "Learn how to send images to multimodal models like GPT-4 Turbo with Vision from within Vellum‚Äôs UI",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.images-root-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/images",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Leverage Images in Your Vellum Prompts and Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/images",
    "content": "Vellum supports images as inputs to both your Prompts and Workflows. In either Sandbox, you can add images inside of scenario Chat History messages.
Begin by selecting the correct model, GPT-4 Turbo with vision, in your Prompt. In Workflows, you can set the model within a Prompt Node. Before you do, you'll want to add a Chat History block as an input to your Workflow first.
Vision Model Selection
Next, add a Chat History block and some messages to your template so you can drag images within them.
Here's how to do it:
In the Prompt Sandbox, add a Chat History block by typing in $chat_history. This is a special Prompt Variable name that will add an empty Chat History block component in each scenario
Prompt Sandbox Steps

In the Workflow Sandbox, a Chat History block can be added directly from the "Add" dropdown on the bottom left of the Input Variables modal. After adding this block, configure your Prompt Node to use Chat History as an input
Workflow Sandbox Steps


Now you're ready to add images! Drag and drop a valid image into a Chat History message that's being used as an input to define a Prompt or Workflow scenario. This converts the Chat Message into a draggable array that can be easily re-ordered and can contain multiple image and/or text items.


Valid image URLs: Images must have their absolute path including the image
filetype in their URL and must be publicly visible (example:
https://storage.googleapis.com/vellum-public/help-docs/release-tags-on-deploy.png)
Here‚Äôs what images look like in the Prompt Sandbox:
Images in Prompt Scenarios
And images in the Workflow Sandbox:
Images in Workflow Scenarios
Once you‚Äôve added in your image, you can configure its settings by clicking the small gear icon to the right of the image. Here you'll be able to adjust things like the Image Detail which can have a big impact on token usage (more on that below).
Image Configuration Steps
You can also switch out an image you‚Äôve dragged in for a new one by updating the image URL in the settings.
Image Configuration Modal",
    "domain": "test.com",
    "hash": "#using-images-in-the-ui",
    "hierarchy": {
      "h0": {
        "title": "Leverage Images in Your Vellum Prompts and Workflows",
      },
      "h2": {
        "id": "using-images-in-the-ui",
        "title": "Using Images in the UI",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.images-using-images-in-the-ui-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/images",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Using Images in the UI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/images",
    "code_snippets": [
      {
        "code": "image_link = "https://storage.googleapis.com/vellum-public/help-docs/add_prompt_block_button.png"
response = client.execute_prompt(
    prompt_deployment_name="github-loom-demo",
    inputs=[
        PromptDeploymentInputRequest_ChatHistory(
            name="$chat_history",
            value=[
                ChatMessageRequest(
                    role=ChatMessageRole.USER,
                    content={
                        "type": "ARRAY",
                        "value": [
                            {"type": "STRING", "value": "What's in this image?"},
                            {"type": "IMAGE", "value": {"src": image_link}},
                        ],
                    },
                )
            ],
            type=VellumVariableType.CHAT_HISTORY,
        ),
    ],
)
print(response.outputs[0].value)",
        "lang": "python",
      },
    ],
    "content": "Here are some important model specifications for GPT-4 Turbo with Vision to keep in mind as you‚Äôre incorporating images into your Prompts and Workflows:
Number of Images: No set limit
There is no fixed number here but token and image size restrictions still apply to determine the number of images that can be sent

Image Size: Less than 32MB
For prompts and workflows with multiple images, the combined image size should not exceed this limit

Supported Image Formats:
JPEG (.jpeg / .jpg)

PNG (.png)

Non-animated GIF (.gif)

WEBP (.webp)



Other Notes:
GPT-4 Turbo with Vision does not currently support tool calls so be sure there are no function blocks in your $chat_history messages

The Vellum UI currently supports only publicly hosted image urls. To send a base64 image file, you can use Vellum's API instead.
Here's a short example on how to send an image to the model, using Vellum's Python SDK:",
    "domain": "test.com",
    "hash": "#image-specifications",
    "hierarchy": {
      "h0": {
        "title": "Leverage Images in Your Vellum Prompts and Workflows",
      },
      "h2": {
        "id": "image-specifications",
        "title": "Image Specifications",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.images-image-specifications-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/images",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Image Specifications",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/images",
    "content": "When working with image models, token usage is an important factor to consider. For GPT-4 Turbo with Vision, the two main factors for token count are the image‚Äôs size and it‚Äôs detail setting.
There are three possible settings for the image detail: low, high, or auto
In Vellum, we default the detail to be low to prevent unintended token usage. OpenAI's default setting is auto where the model decides whether to use low or high detail based on the size of the input image.
Image Details
The low setting processes a lower resolution 512x512 version of the image. With low, the response time is faster and there‚Äôs a fixed token consumption per image. At the time of this writing, that amount is 85 tokens. The low setting is great when the fine details of the image are not required.
The high setting on the other hand is the high resolution mode. In this mode, the input image is tiled and a detailed segment is created from it. Token usage is calculated based on the number of these segments which correlates to the image size. High resolution allows for a more comprehensive interpretation of your image.
You can learn more about the image detail setting and OpenAI Vision models on their site


Are you looking for greater multimodal model support in Vellum beyond
gpt-4-vision-preview? Please don't hesitate to let us know at
support@vellum.ai!",
    "domain": "test.com",
    "hash": "#image-detail-and-token-usage",
    "hierarchy": {
      "h0": {
        "title": "Leverage Images in Your Vellum Prompts and Workflows",
      },
      "h2": {
        "id": "image-detail-and-token-usage",
        "title": "Image Detail and Token Usage",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.images-image-detail-and-token-usage-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/images",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Image Detail and Token Usage",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompting-tips-and-examples",
    "description": "Learn prompting techniques for common use-cases.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompting-tips-and-examples",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompting-tips-and-examples",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Prompting Tips and Examples",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/help-center/prompts/prompting-tips-and-examples",
    "code_snippets": [
      {
        "code": "Provide a JSON response for the following transcript information and use the formatting
below:
 
```
{
  "meeting_type": "board meeting" || "special meeting" || "work session" // if none, return "general",
  "speakers": string[], // list of speaker names
  "meeting_location": "virtual" || "in_person",
  "date": "datetime", // ISO 8601 format with date and time and default to "null" if datetime is unknown
  "summary": "string" // concise description of key topics
}
```
 
Transcript:
"""
[transcript contents go here]
"""",
        "lang": "plaintext",
      },
    ],
    "content": "You can efficiently specify the shape of JSON objects you'd like your LLM to produce with the following recipe:
Note the code fencing used here to add descriptions for the key-value pairs so the model knows what to extract.
We also provide a default value so we can handle that downstream in our system rather than getting inconsistent values from the LLM when the data is not found.",
    "domain": "test.com",
    "hash": "#producing-json",
    "hierarchy": {
      "h0": {
        "title": "Prompting Tips and Examples",
      },
      "h2": {
        "id": "producing-json",
        "title": "Producing JSON",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.prompts.prompting-tips-and-examples-producing-json-0",
    "org_id": "test",
    "pathname": "/help-center/prompts/prompting-tips-and-examples",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Producing JSON",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/introduction",
    "content": "Vellum Workflows help you quickly prototype, deploy, version, and monitor complex chains of LLM calls and the business logic that tie them together.
It provides a low-code interface for defining these chains so that you get rapid feedback on how they work across a variety of test cases that you define. Once you‚Äôre happy with the Workflow, you can ‚Äúdeploy‚Äù it and hit an API to invoke that Workflow from your application.
Once deployed, future changes to the Workflow definition are versioned and invocations made from your application are logged. For a given invocation, you can view the inputs, outputs, and latency of each step along the way.",
    "description": "Discover how Vellum Workflows streamline LLM call chains with a low-code interface, easy testing, and versioned deployments.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.introduction-root-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/introduction",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Build multi-step AI apps with Vellum‚Äôs Worfklows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/introduction",
    "content": "Workflows make heavy use of the following concepts:
Input Variables

Scenarios

Nodes

Edges

Final Outputs


Let‚Äôs take a look at each",
    "domain": "test.com",
    "hash": "#concepts",
    "hierarchy": {
      "h0": {
        "title": "Build multi-step AI apps with Vellum‚Äôs Worfklows",
      },
      "h1": {
        "id": "concepts",
        "title": "Concepts",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.introduction-concepts-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/introduction",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Concepts",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/introduction",
    "content": "The behavior of most Workflows depend on 1 or more dynamic Input. For example, you could define a single Input named query that your Workflow depends on.
Workflow Input Variables",
    "domain": "test.com",
    "hash": "#input-variables",
    "hierarchy": {
      "h0": {
        "title": "Build multi-step AI apps with Vellum‚Äôs Worfklows",
      },
      "h1": {
        "id": "concepts",
        "title": "Concepts",
      },
      "h3": {
        "id": "input-variables",
        "title": "Input Variables",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.introduction-input-variables-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/introduction",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Input Variables",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/introduction",
    "content": "A Scenario is a set of values for your Input Variables. In the above example, we have Scenario 1 which assigns a value of What is fine tuning? to the query Input Variable.
You can define as many Scenarios as you want and swap between them to test that your Workflows behaves the way you expect for each.",
    "domain": "test.com",
    "hash": "#scenarios",
    "hierarchy": {
      "h0": {
        "title": "Build multi-step AI apps with Vellum‚Äôs Worfklows",
      },
      "h1": {
        "id": "concepts",
        "title": "Concepts",
      },
      "h3": {
        "id": "scenarios",
        "title": "Scenarios",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.introduction-scenarios-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/introduction",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Scenarios",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/introduction",
    "content": "Nodes are the steps in your Workflow where some action will take place. Some Nodes generate Outputs, whereas some Nodes are used purely to direct the flow of execution.
For example, the Prompt Node is used to pass Input Variables into a Prompt and execute an LLM. It generates an output that can then be used as an input to other downstream Nodes.
Workflow Nodes",
    "domain": "test.com",
    "hash": "#nodes",
    "hierarchy": {
      "h0": {
        "title": "Build multi-step AI apps with Vellum‚Äôs Worfklows",
      },
      "h1": {
        "id": "concepts",
        "title": "Concepts",
      },
      "h3": {
        "id": "nodes",
        "title": "Nodes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.introduction-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/introduction",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/introduction",
    "content": "Edges connect Nodes and define the order in which they are executed. The are represented as the lines in between Nodes.
Workflow Edges
Note that a Node has access to the output data from all upstream Nodes, not just the Node(s) that it‚Äôs directly connected to via an Edge.",
    "domain": "test.com",
    "hash": "#edges",
    "hierarchy": {
      "h0": {
        "title": "Build multi-step AI apps with Vellum‚Äôs Worfklows",
      },
      "h1": {
        "id": "concepts",
        "title": "Concepts",
      },
      "h3": {
        "id": "edges",
        "title": "Edges",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.introduction-edges-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/introduction",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Edges",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/introduction",
    "content": "There‚Äôs a special Node called a ‚ÄúFinal Output Node.‚Äù They‚Äôre used to indicate which Node output you actually care about and want to surface as the overall output for the Workflow.
In the below example, I have a Final Output Node named final-output that subscribes to a string output that comes from the OpenAI Help Center Prompt Node.
Workflow Final Output
Final Output Nodes are particularly important when you Deploy a Workflow and invoke it via API. By default, only the data that Final Output Nodes subscribe to will be returned by the API.
Note that you can have as many Final Output Nodes in a Workflow and can assign each a name to differentiate the data associated with each in API calls.",
    "domain": "test.com",
    "hash": "#final-output",
    "hierarchy": {
      "h0": {
        "title": "Build multi-step AI apps with Vellum‚Äôs Worfklows",
      },
      "h1": {
        "id": "concepts",
        "title": "Concepts",
      },
      "h3": {
        "id": "final-output",
        "title": "Final Output",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.introduction-final-output-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/introduction",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Final Output",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/experimentation",
    "description": "Discover how Vellum's Workflows simplifies building AI apps by managing complex LLM call chains and business logic easily.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.experimentation",
    "org_id": "test",
    "pathname": "/help-center/workflows/experimentation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Streamline AI App Development with Vellum's Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/experimentation",
    "content": "Workflows help you quickly prototype, deploy, and manage complex chains of LLM calls and business logic. We solve the "whack-a-mole" problem encountered by companies that use popular open source frameworks to build AI applications, but are scared to make changes for fear of introducing regressions in production.
The Workflows UI consists of a graphical app builder where you can string together various nodes and test various input values through this system. Each prompt can also be tested extensively through Playground & Test Suites. When implemented effectively, Workflows can help you build advanced LLM applications",
    "domain": "test.com",
    "hash": "#about-workflows",
    "hierarchy": {
      "h0": {
        "title": "Streamline AI App Development with Vellum's Workflows",
      },
      "h2": {
        "id": "about-workflows",
        "title": "About Workflows",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.experimentation-about-workflows-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/experimentation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "About Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/experimentation",
    "content": "Workflow nodes are connected by linking the output of one node to the input of another node. For any node the variables can be populated either by the results of an upstream node or the values of global variables.
When 2 nodes are successfully connected there‚Äôs a solid purple line between the nodes and the connection points turn blue. Here‚Äôs an example of a workflow that‚Äôs connected successfully:
Connecting Workflow Nodes and Defining Variables",
    "domain": "test.com",
    "hash": "#connecting-workflow-nodes-and-defining-variables",
    "hierarchy": {
      "h0": {
        "title": "Streamline AI App Development with Vellum's Workflows",
      },
      "h2": {
        "id": "connecting-workflow-nodes-and-defining-variables",
        "title": "Connecting Workflow Nodes and Defining Variables",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.experimentation-connecting-workflow-nodes-and-defining-variables-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/experimentation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Connecting Workflow Nodes and Defining Variables",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/experimentation",
    "content": "Each variable in a node can either take the value of an upstream node or the value can be defined globally. To define them globally, you can populate them in the Input Variables dropdown before running a workflow. You can define as many scenarios as you want, each scenario is a unique set of input values that will be sent to the workflow.
Variables can be added one-by-one using the Add button or automatically using Auto-Add. Auto-Add looks at all the variables in the workflow and adds them to the scenario.
Workflow Inputs
Once all the variables are selected for each prompt (either as values of upstream nodes or defined globally), you are now ready to Run your workflow!
When you Run the Workflow (purple button on the top right corner), you will see the execution path of the Workflow in green and the intermediate results at each step of the workflow. If the results at the end of the Workflow look surprising then may be a good idea to check what the responses look like at each step.
Here‚Äôs an example of a workflow that‚Äôs executed successfully:
Executed Workflow",
    "domain": "test.com",
    "hash": "#running-a-workflow",
    "hierarchy": {
      "h0": {
        "title": "Streamline AI App Development with Vellum's Workflows",
      },
      "h2": {
        "id": "running-a-workflow",
        "title": "Running a Workflow",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.experimentation-running-a-workflow-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/experimentation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Running a Workflow",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/experimentation",
    "content": "Workflow development is best done iteratively. However, this can become prohibitively expensive both in terms of token consumption and runtime if there are Prompt Nodes defined early in the Workflow that you have to frequently re-run just to get to the part of the Workflow that you actually want to test. To help speed up Workflow development, you can mock out the execution of a given node. This will skip the node's execution and return the hard-coded output(s) you define rather than running the node itself.
Workflow Node Mocking
Once defined, you can easily toggle the mock on and off to go back and forth between mocking the node and actually executing the Prompt to see your Workflow work end-to-end. This also allows you to save your mocks without needing to delete them when you'd like to actually execute the node. During a workflow run, nodes that are mocked will be outlined in yellow to differentiate from nodes that are actually executed.
Workflow Node Mocking
These mocks are only defined within the context of Workflow Sandboxes, and are defined per Scenario. They do not get deployed with your Workflow Deployments and do not affect behavior when invoking Workflow Deployment APIs.
The following nodes support mocking:
Prompt Nodes

Subworkflow Nodes


Check out the video below for a full demo of Workflow Node Mocking.",
    "domain": "test.com",
    "hash": "#node-mocking",
    "hierarchy": {
      "h0": {
        "title": "Streamline AI App Development with Vellum's Workflows",
      },
      "h2": {
        "id": "node-mocking",
        "title": "Node Mocking",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.experimentation-node-mocking-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/experimentation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Node Mocking",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "description": "Discover the different types of Workflow Nodes provided by Vellum to build complex LLM Workflows with ease.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Build Powerful Workflows with Vellum Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "Vellum offers over a dozen Node types that you can use to build any Workflow you can imagine. On this page, we'll outline the purpose of each Node.
For additional examples of Node usage, check out our Common Workflow Architectures, which we update regularly.",
    "domain": "test.com",
    "hash": "#supported-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "supported-nodes",
        "title": "Supported Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-supported-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Supported Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "Node Description 
Prompt Node Invoke LLMs with your prompts, optionally using variables from other nodes 
Templating Node Apply Jinja templating to perform lightweight data transformations 
Search Node Search against a Document Index, great for RAG 
API Node Make an HTTP request to an API endpoint 
Code Execution Node Run custom Python or Typescript code 
Subworkflow Node Makes Workflows reusable and more maintanable as they get more complex 
Map Node Iterate over an array, executing a sub-workflow for each item 
Guardrail Node Run an inline evaluation using a pre-defined Metric 
Conditional Node Branch your workflow based on a condition, also useful for error handling 
Merge Node Wait for one or multiple branches to complete before continuing 
Final Output Node Exposes values you can use in your application, you may have more than one! 
Error Node Stop workflow execution and raise an error 
Note Node A simple node that displays text to help annotate your Workflow",
    "domain": "test.com",
    "hash": "#quick-reference",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-quick-reference-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Quick Reference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "A core part of any LLM application. This node represents a call to a Large Language Model. Similar to Vellum Prompts, you can use models from any of the major providers or open source community, including: OpenAI, Anthropic, Meta, Cohere, Google, Mosaic, and Falcon-40b.
Upon creating a Prompt Node you‚Äôll be asked to import a prompt from an existing Deployment, Sandbox, or create one from scratch. Prompts are defined by their variables, prompt template, model provider, and parameters. Refer to this help center article to learn more about our prompt syntax (Vellum Prompt Template Syntax).
Prompt Node",
    "domain": "test.com",
    "hash": "#prompt-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "prompt-nodes",
        "title": "Prompt Nodes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-prompt-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Prompt Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "code_snippets": [
      {
        "code": "{# this example will have invisible whitespace #} 
{% if some_condition %}
    {{ result A }}
{% else %}
    {{ result B }}
{% endif %}

{# this will give the result you expect #} 
{%- if some_condition -%}
    {{- result A -}}
{%- else -%}
    {{- result B -}}
{%- endif -%}",
        "lang": "jinja",
      },
      {
        "code": "{# this example will have invisible whitespace #} 
{% if some_condition %}
    {{ result A }}
{% else %}
    {{ result B }}
{% endif %}

{# this will give the result you expect #} 
{%- if some_condition -%}
    {{- result A -}}
{%- else -%}
    {{- result B -}}
{%- endif -%}",
        "lang": "jinja",
      },
    ],
    "content": "The Templating Node allows you to perform custom data transformations on a set of defined inputs to create a new output. You can use this to define constants, manipulate data before feeding into a prompt, or massage a response to a format of your liking.
Check out our Common Data Transformation Templates for some common examples.
Templating Node




You may have a templating node that outputs JSON which seems valid, but yields the following error when you click ‚ÄúTest‚Äù or run your workflow:
Tips - Using Jinja
Jinja has a tendency to leave hard-to-see whitespace which can cause issues when doing equality checks in places like Metrics or Conditional Nodes.




Use double quotes when working with JSON

Jinja has a tendency to leave hard-to-see whitespace which can cause issues when doing equality checks in places like Metrics or Conditional Nodes.",
    "domain": "test.com",
    "hash": "#templating-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "templating-nodes",
        "title": "Templating Nodes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-templating-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Templating Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "The Search Node returns results from a Document Index stored inside Vellum Search. Once your documents are uploaded in an index (details on how to do that here: Uploading Documents), you can start using them in a Workflow.
The index in a Search Node can be fixed for the Workflow or chosen dynamically based on the output of an upstream node. Additional configuration options, similar to the ones in Vellum Search are also available in the Search Node.
Search Node",
    "domain": "test.com",
    "hash": "#search-node",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "search-node",
        "title": "Search Node",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-search-node-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Search Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "The API Node invokes an API endpoint and returns back the status code, raw output, and JSON output if applicable. These APIs can be either publicly accessible or privately defined within your backend through the help of Authorization headers and Secrets. Simply define a URL, HTTP Method, relevant additional headers, and the body that you would like to send to the desired endpoint.
API Node


You can use a Templating Node and the "Dynamic" field of an API Node to quickly and flexibly make API calls in your Workflows. See the example below for more details. Notice how we do string concatenation in the Templating Node using Jinja2's ~ syntax.


It's better to use API Nodes over Code Execution Nodes for API calls. Code Execution Nodes add more latency to your Workflow. Reserve them for more complex tasks.",
    "domain": "test.com",
    "hash": "#api-node",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "api-node",
        "title": "API Node",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-api-node-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "API Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "The Code Execution Node empowers you to include custom logic defined directly in the workflow. You can even import custom public packages within the node's logic. We support the following languages:
Python

TypeScript


Code Execution Node




Set your output type to JSON when returning string arrays from Code Execution Nodes, or you'll get the following error: Failed to execute node Code Execution Node: Mismatched output type. Output[0]: Expected to deserialize a 'dict', got 'str'




The main scenario in which you'd use Array as your Code Execution Node output type is when you're processing Prompt Execution Node outputs with Function Calls.
For more on that, see Quirks and Tips for Handling Functions",
    "domain": "test.com",
    "hash": "#code-execution-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "code-execution-nodes",
        "title": "Code Execution Nodes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-code-execution-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Code Execution Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "Subworkflow Nodes are essential for managing giant, complex workflows. Define reusable groups of nodes in one Workflow Sandbox and have them directly accessible upon deployment from any other workflow in your workspace. Subworkflow nodes also support release tag specification, allowing you the option to always invoke the latest workflow, or pinning to a specific release tag defined by you.
Check out the video below to see Subworkflow Nodes in action!",
    "domain": "test.com",
    "hash": "#subworkflow-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "subworkflow-nodes",
        "title": "Subworkflow Nodes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-subworkflow-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Subworkflow Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "Subworkflow Nodes could also be defined directly within the existing Workflow editor! This spawns a new editor within the existing parent Workflow that supports many of
the same features as the parent Workflow such as all existing nodes and copy/paste. This could be used to help organize complex Workflow architectures into separate,
digestable groups. Check out the video below to see it in action!",
    "domain": "test.com",
    "hash": "#inline-subworkflow-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "subworkflow-nodes",
        "title": "Subworkflow Nodes",
      },
      "h4": {
        "id": "inline-subworkflow-nodes",
        "title": "Inline Subworkflow Nodes",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-inline-subworkflow-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Inline Subworkflow Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "Map Nodes allow you to easily run a Subworkflow multiple times in a row. Map Nodes work in the same way that array map functions do in many common programming languages.
The Nodes take a JSON array as an input and iterate over it, running a Subworkflow for each item. The Subworkflow is provided with three input variables for the iteration item, index and the array.
The output of every Subworkflow is then combined into a single array as a Node output. Map Nodes also support up to 96 concurrent iterations.




Two tips here:
At the time of writing, you'll need to cast items into strings at the beginning of your Map Node Subworkflow. Expand the Map Node in the Subworkflow below for more:




Make sure you're using the JSON type as output, even if it's an array, from whichever Node output you're passing to the Map Node. ARRAY types won't be recognized.",
    "domain": "test.com",
    "hash": "#map-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "map-nodes",
        "title": "Map Nodes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-map-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Map Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "Guardrail Nodes allow you to use Evaluation Metrics from within a Workflow. Guardrail Nodes let you run pre-defined evaluation criteria at runtime as part of a Workflow execution so that you can drive downstream behavior based on that Metric's score.
For example, if building a RAG application, you might determine whether the generated response passes some threshold for Ragas Faithfulness and if not, loop around to try again.
Guardrail Nodes",
    "domain": "test.com",
    "hash": "#guardrail-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "guardrail-nodes",
        "title": "Guardrail Nodes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-guardrail-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Guardrail Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "Conditional Nodes are extremely powerful because they can help you diverge the execution path of your Workflow based on the results of an upstream node. The Conditional Node supports as many if-else-if conditions as you‚Äôd like and the rules can be grouped / nested within each other.
The number of exit options from a conditional node equal the number of if-else-if conditions created on the node
Conditional Node",
    "domain": "test.com",
    "hash": "#conditional-node",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "conditional-node",
        "title": "Conditional Node",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-conditional-node-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Conditional Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "See our tips about invisible whitespace in Jinja
Wrong Way To Check Equality From Templating Nodes
The issue in the above check is that we're not using the Jinja2 syntax to remove unintentional whitespace: {%- and {{- rather than {% or {{
Right Way To Check Equality From Templating Nodes
You can see here that adding the - character fixes the issue and gets the correct branch to execute after the conditional.",
    "domain": "test.com",
    "hash": "#tip---equality-checking-templating-nodes",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "conditional-node",
        "title": "Conditional Node",
      },
      "h4": {
        "id": "tip---equality-checking-templating-nodes",
        "title": "Tip - Equality Checking Templating Nodes",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-tip---equality-checking-templating-nodes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Tip - Equality Checking Templating Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "Merge Nodes are used when the goal is to bring back the execution of divergent paths into one path. You can configure the number of inputs to a Merge Node and choose between ‚ÄúAwait All‚Äù or ‚ÄúAwait Any‚Äù as your merge strategy. The merge strategy determines the logic that will continue workflow execution.
Merge Node",
    "domain": "test.com",
    "hash": "#merge-node",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "merge-node",
        "title": "Merge Node",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-merge-node-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Merge Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "The Final Output Node represents the end of your workflow. Your workflow may have multiple Final Output Nodes if the execution has been branched off from an upstream node.
A name for the output and an output type must be configured here because the response streamed back from the endpoint (when the workflow is taken to production) has this information included.
Final Output Node",
    "domain": "test.com",
    "hash": "#final-output-node",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "final-output-node",
        "title": "Final Output Node",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-final-output-node-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Final Output Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "The Error Node enables you to reject the full workflow, terminating execution with an error event wherever you define it in your execution flow. There are two types of errors you could raise with this node:
Pass-through - Use an Error output from an upstream node and pass it through to this node.

Custom - Define your own String output that this node will use as an error message


Error Node",
    "domain": "test.com",
    "hash": "#error-node",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "error-node",
        "title": "Error Node",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-error-node-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Error Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/node-types",
    "content": "The Note Node helps you keep your workflow organized and maintainable. You can use it to add context, related links, or other pieces of information in your workflow. They don't alter any functionality in your workflow, and are purely for your team and you. You can change the font size and even use colors!
Note Node",
    "domain": "test.com",
    "hash": "#note-node",
    "hierarchy": {
      "h0": {
        "title": "Build Powerful Workflows with Vellum Nodes",
      },
      "h2": {
        "id": "quick-reference",
        "title": "Quick Reference",
      },
      "h3": {
        "id": "note-node",
        "title": "Note Node",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.node-types-note-node-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/node-types",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Note Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-architectures",
    "content": "With a large number of supported node types (full details here: Experimenting with Workflows) and few limits on how they can be connected to each other, the types of architectures/ applications you can create using Workflows is very large.
The list of architectures below is not exhaustive, we‚Äôre continuing to build it out. If you come up with an interesting architecture that you think the community might benefit from, please reach out so we can add it to the list here.",
    "description": "Discover how to build dynamic architectures using Workflows, from RAG systems to message routing and looping",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-architectures-root-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-architectures",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Building Common LLM architectures with Vellum Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-architectures",
    "content": "LLM applications often require specific context from a Vector DB which is added into the prompt. Forget signing up for multiple systems and being stuck on various micro decisions, with Vellum you can prototype a RAG system in minutes
Walkthrough


Create a Document Index and upload your documents
Follow this article for tips: Uploading Documents)
Add a Search Node in your Workflow
Place this anywhere and connect it to the "entrypoint"
Add a Prompt Node
The prompt node should take the results of your Search Node as an input variable
Link to a Final Output or other downstream node
For example, if the Prompt Node result is a certain value branch execution based on a Conditional Node)
Set up input variables and hit Run!
Workflow",
    "domain": "test.com",
    "hash": "#create-a-retrieval-augmented-generation-rag-system",
    "hierarchy": {
      "h0": {
        "title": "Building Common LLM architectures with Vellum Workflows",
      },
      "h2": {
        "id": "create-a-retrieval-augmented-generation-rag-system",
        "title": "Create a Retrieval Augmented Generation (RAG) system",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-architectures-create-a-retrieval-augmented-generation-rag-system-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-architectures",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Create a Retrieval Augmented Generation (RAG) system",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-architectures",
    "content": "If you‚Äôre building an agent that answers questions coming from users (e.g., a support chatbot), you may want to set up rules such that anytime the incoming message from a user is sensitive (e.g., the user is angry or in a dangerous situation) then the LLM automatically escalates it to a human. With Workflows you‚Äôd be able to build that out real quick.
Walkthrough


Add a classification prompt
Use a Prompt Node to filter out incoming messages
Add a downstream prompt
Use another prompt node for the LLM to respond to messages that don‚Äôt need to be escalated
Add and connect two Final Output Nodes
Connect the classification prompt outputs to two separate Final Output Nodes
Set up variables and hit Run!
Workflow",
    "domain": "test.com",
    "hash": "#route-messages-to-a-human",
    "hierarchy": {
      "h0": {
        "title": "Building Common LLM architectures with Vellum Workflows",
      },
      "h2": {
        "id": "route-messages-to-a-human",
        "title": "Route messages to a Human",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-architectures-route-messages-to-a-human-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-architectures",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Route messages to a Human",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-architectures",
    "content": "Prompt nodes support two selectable outputs - one from the model in case of a valid output and one in case of a non deterministic error. Model hosts fail for all sorts of reasons that include timeouts, rate limits, or server overload. You could make your production-grade LLM features resilient to these features by adding retry logic into your Workflows!
Walkthrough


Add a standard Prompt Node
Add a Conditional Node (Error Check)
This node will read from the new Error output from the Prompt Node and check to see if it's not null.
Define another Conditional Node (Count Check)
This node will read from the Prompt Node's Execution Counter, and check if it's been invoked more than your desired limit (3).
Loop back to the Prompt Node
Loop back to the Prompt Node if it's under the limit, or exit with some error message if it's over the limit. In the case that the error is null, exit with the Prompt Node's response.
Workflow",
    "domain": "test.com",
    "hash": "#retrying-a-prompt-node-in-case-of-non-deterministic-failure",
    "hierarchy": {
      "h0": {
        "title": "Building Common LLM architectures with Vellum Workflows",
      },
      "h2": {
        "id": "retrying-a-prompt-node-in-case-of-non-deterministic-failure",
        "title": "Retrying a Prompt Node in case of non-deterministic failure",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-architectures-retrying-a-prompt-node-in-case-of-non-deterministic-failure-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-architectures",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Retrying a Prompt Node in case of non-deterministic failure",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-architectures",
    "content": "Vellum Document Indexes are typically used to power RAG systems via Search Nodes. However, they can also be used to operate on the entirety of a single file's contents.
In this example, we make use of Vellum Document Indexes not for the purpose of search, instead, to leverage the OCR that's performed and operate on the raw text that's extracted
from a PDF file.
Prerequisites:
You need to have ....
Created a Document Index. Note: it doesn't matter what embedding model or chunking strategy you choose, since we're only leveraging the OCR capabilities of the Document Index.

Uploaded a PDF file to the Document Index and noted down its ID.

Generated a Vellum API Token and saved its value as a Workspace Secret.


Walkthrough


Set the input to the workflow
This will be the ID of a Document that was previously uploaded to a Document Index
Add a Templating Node (Document API URL)
This will construct the url of a Vellum API we want to hit.
Add an API Node (Document API)
This will ping the Vellum API and retrieve metadata about the Document.
Add a Templating Node (Processed Document URL)
This will extract the url of the processed document from the API response.
Add an API Node (Processed Document Contents)
This will retrieve the text contents of the Document.
Pass those contents to a Prompt Node that summarizes the text.
Workflow",
    "domain": "test.com",
    "hash": "#summarizing-the-contents-of-a-pdf-file",
    "hierarchy": {
      "h0": {
        "title": "Building Common LLM architectures with Vellum Workflows",
      },
      "h2": {
        "id": "summarizing-the-contents-of-a-pdf-file",
        "title": "Summarizing the contents of a PDF file",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-architectures-summarizing-the-contents-of-a-pdf-file-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-architectures",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Summarizing the contents of a PDF file",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-data-transforms",
    "content": "The Templating Node supports Jinja2 syntax and is a flexible way of performing light-weight data transformations as part of your Workflow. Here are some common data manipulations you may want to make in a Workflow and how you define them via Templating Nodes.",
    "description": "Learn how to manipulate strings, JSON, chat history, and search results using Templating Nodes for efficient AI workflows.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-data-transforms-root-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-data-transforms",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Guide to Data Transformation with Templating Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-data-transforms",
    "code_snippets": [
      {
        "code": "{{ user_input[:10] }}",
        "lang": "jinja2",
      },
      {
        "code": "{{ user_input[:10] }}",
        "lang": "jinja2",
      },
      {
        "code": "Inputs:
-------
user_input = "Hello, world!"

Output:
-------
"Hello, wor"",
      },
      {
        "code": "Inputs:
-------
user_input = "Hello, world!"

Output:
-------
"Hello, wor"",
      },
    ],
    "content": "Useful if you want to ensure that you‚Äôre not providing too much context to a prompt.
String Manipulation",
    "domain": "test.com",
    "hash": "#output-only-the-first-n-characters",
    "hierarchy": {
      "h0": {
        "title": "Guide to Data Transformation with Templating Nodes",
      },
      "h1": {
        "id": "string-manipulation",
        "title": "String Manipulation",
      },
      "h3": {
        "id": "output-only-the-first-n-characters",
        "title": "Output Only the First n Characters",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-data-transforms-output-only-the-first-n-characters-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-data-transforms",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Output Only the First n Characters",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-data-transforms",
    "code_snippets": [
      {
        "code": "{% if maybe_json|is_valid_json_string %}
    {{ maybe_json }}
    
    ## to extract specific properties from the JSON
    {{ json.loads(maybe_json).property }}
{% else %}
    {{ {} }}
{% endif %}",
        "lang": "jinja2",
      },
      {
        "code": "{% if maybe_json|is_valid_json_string %}
    {{ maybe_json }}
    
    ## to extract specific properties from the JSON
    {{ json.loads(maybe_json).property }}
{% else %}
    {{ {} }}
{% endif %}",
        "lang": "jinja2",
      },
      {
        "code": "Inputs:
-------
maybe_json = '{"key": "value"}'

Output:
-------
{"key": "value"}",
      },
      {
        "code": "Inputs:
-------
maybe_json = '{"key": "value"}'

Output:
-------
{"key": "value"}",
      },
      {
        "code": "Inputs:
-------
maybe_json = 'not valid json'

Output:
-------
{}",
      },
      {
        "code": "Inputs:
-------
maybe_json = 'not valid json'

Output:
-------
{}",
      },
    ],
    "content": "If you‚Äôre trying to extract structured JSON from unstructed text using a prompt, or if you want to use OpenAI‚Äôs function-calling functionality, it‚Äôs likely you‚Äôll need to check whether an LLM‚Äôs response is valid JSON and if so, convert the output string as proper JSON.
You can also extract specific properties from valid JSON strings.
Here‚Äôs how to do it:
JSON Manipulation",
    "domain": "test.com",
    "hash": "#checking-llm-output-for-valid-json",
    "hierarchy": {
      "h0": {
        "title": "Guide to Data Transformation with Templating Nodes",
      },
      "h1": {
        "id": "json-manipulation",
        "title": "JSON Manipulation",
      },
      "h3": {
        "id": "checking-llm-output-for-valid-json",
        "title": "Checking LLM Output for Valid JSON",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-data-transforms-checking-llm-output-for-valid-json-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-data-transforms",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Checking LLM Output for Valid JSON",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-data-transforms",
    "code_snippets": [
      {
        "code": "{{ chat_history[-2:] }}",
        "lang": "jinja2",
      },
      {
        "code": "{{ chat_history[-2:] }}",
        "lang": "jinja2",
      },
      {
        "code": "Inputs:
-------
chat_history = [
    {"role": "USER", "text": "What color is the sky?"},
    {"role": "ASSISTANT", "text": "Blue"},
    {"role": "USER", "text": "But why"}
]

Output:
-------
[
    {"role": "ASSISTANT", "text": "Blue"},
    {"role": "USER", "text": "But why"}
]",
      },
      {
        "code": "Inputs:
-------
chat_history = [
    {"role": "USER", "text": "What color is the sky?"},
    {"role": "ASSISTANT", "text": "Blue"},
    {"role": "USER", "text": "But why"}
]

Output:
-------
[
    {"role": "ASSISTANT", "text": "Blue"},
    {"role": "USER", "text": "But why"}
]",
      },
    ],
    "content": "If you‚Äôre building a chatbot and conversations can be long-lived, you may find that your chat histories are too long to fit within the context window of a prompt.
Once simple solution is to only ever include the most recent n messages from the conversation. Here‚Äôs how you can do this:
Chat History Manipulation",
    "domain": "test.com",
    "hash": "#output-the-most-recent-n-messages-in-chat-history",
    "hierarchy": {
      "h0": {
        "title": "Guide to Data Transformation with Templating Nodes",
      },
      "h1": {
        "id": "chat-history-manipulation",
        "title": "Chat History Manipulation",
      },
      "h3": {
        "id": "output-the-most-recent-n-messages-in-chat-history",
        "title": "Output the Most Recent n Messages in Chat History",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-data-transforms-output-the-most-recent-n-messages-in-chat-history-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-data-transforms",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Output the Most Recent n Messages in Chat History",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-data-transforms",
    "code_snippets": [
      {
        "code": "{% for result in search_results -%}
Source:
{{ result.document.label }}

Content:
{{ result.text }}
{% if not loop.last %}

#####

{% endif %}
{% endfor %}",
        "lang": "jinja2",
      },
      {
        "code": "{% for result in search_results -%}
Source:
{{ result.document.label }}

Content:
{{ result.text }}
{% if not loop.last %}

#####

{% endif %}
{% endfor %}",
        "lang": "jinja2",
      },
      {
        "code": "Inputs:
-------
search_results = [
    {
        "text": "Hello, world!",
        "score": 0.015,
        "keywords": ["hello", "world‚Äù],
        "document": {
            "id": "22df06cf-c876-45ef-a162-4836c410e37b",
            "label": "introduction.txt",
            "external_id": "introduction.txt"
        }
    },
    {
        "text": "The sky is blue.",
        "score": 0.005,
        "keywords": ["sky", "blue‚Äù],
        "document": {
            "id": "d9655f5f-885e-400e-b000-00b605a03a99",
            "label": "description.txt",
            "external_id": "description.txt"
        }
    }
]

Output:
-------
Source:
introduction.txt

Content:
Hello, world!


#####


Source:
description.txt

Content:
The sky is blue.",
      },
      {
        "code": "Inputs:
-------
search_results = [
    {
        "text": "Hello, world!",
        "score": 0.015,
        "keywords": ["hello", "world‚Äù],
        "document": {
            "id": "22df06cf-c876-45ef-a162-4836c410e37b",
            "label": "introduction.txt",
            "external_id": "introduction.txt"
        }
    },
    {
        "text": "The sky is blue.",
        "score": 0.005,
        "keywords": ["sky", "blue‚Äù],
        "document": {
            "id": "d9655f5f-885e-400e-b000-00b605a03a99",
            "label": "description.txt",
            "external_id": "description.txt"
        }
    }
]

Output:
-------
Source:
introduction.txt

Content:
Hello, world!


#####


Source:
description.txt

Content:
The sky is blue.",
      },
    ],
    "content": "Search Nodes make it easy to query a vector store for text that‚Äôs semantically similar to some input. By default, the chunks of text that are returned are concatenated together into a single string using a configurable separator (e.g. \n\n#####\n\n). The flattened string can then be fed directly to Prompt Nodes as an input variable and referenced within your prompt template.
However, if you want your Prompt to cite its sources and say where it got the info it used to generate its response, then you‚Äôll need more than just the chunk text. You need the name/id/url/etc of the document each chunk came from and you need to provide this info to your Prompt in a consumable form. This is where Templating Nodes come in.
The template below takes in the raw search results and performs custom chunk concatenation, but also pulls in info from the document associated with each chunk.
Search Result Manipulation",
    "domain": "test.com",
    "hash": "#citing-sources-via-chunk-concatenation-customization",
    "hierarchy": {
      "h0": {
        "title": "Guide to Data Transformation with Templating Nodes",
      },
      "h1": {
        "id": "search-result-manipulation",
        "title": "Search Result Manipulation",
      },
      "h3": {
        "id": "citing-sources-via-chunk-concatenation-customization",
        "title": "Citing Sources via Chunk Concatenation Customization",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-data-transforms-citing-sources-via-chunk-concatenation-customization-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-data-transforms",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Citing Sources via Chunk Concatenation Customization",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/common-data-transforms",
    "content": "Templating nodes are flexible and powerful, but admittedly not the most intuitive. If you‚Äôd like to see additional examples here, or have ideas for custom filters that we should add (like the is_valid_json_string filter used above), please don‚Äôt hesitate to reach out to us on discord!",
    "domain": "test.com",
    "hash": "#need-help",
    "hierarchy": {
      "h0": {
        "title": "Guide to Data Transformation with Templating Nodes",
      },
      "h1": {
        "id": "need-help",
        "title": "Need Help?",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.common-data-transforms-need-help-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/common-data-transforms",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Need Help?",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/code-execution-node-examples",
    "content": "The Code Execution Node supports running arbitrary Python or TypeScript code to perform data transformations in your Workflow. It can simplify your workflow, especially in cases where a combination of Conditional, Templating, and Merge nodes are used. Below are some example use cases:",
    "description": "Discover how to use Python or TypeScript for data transformations in Vellum Workflows, including arithmetic and JSON manipulation.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.code-execution-node-examples-root-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/code-execution-node-examples",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Code Execution Node Examples",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/code-execution-node-examples",
    "code_snippets": [
      {
        "code": "import * as _ from "lodash";

async function main(inputs: {
  test: string,
}): Promise<number> {
  return inputs.test.length + _.floor(5.452);
}",
        "lang": "typescript",
      },
    ],
    "content": "You can add both pip packages for Python code and npm packages for TypeScript code. You must provide exact package versions and add the import to your code yourself.
Note that whenever you update your packages list, the first execution after doing so may be slow due to our system creating and caching the custom runtime.
Code Package example",
    "domain": "test.com",
    "hash": "#code-packages",
    "hierarchy": {
      "h0": {
        "title": "Code Execution Node Examples",
      },
      "h1": {
        "id": "code-packages",
        "title": "Code Packages",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.code-execution-node-examples-code-packages-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/code-execution-node-examples",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Code Packages",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/api-integration",
    "content": "Once you have your Workflow built in Vellum‚Äôs UI, we provide an easy way to use it in production. Vellum handles the execution of the Workflow ‚Äî all you need to provide are the input variables to call the Workflow. Vellum abstracts away the need to store the prompts, semantic search & the business logic tying together these prompts in your code base. Using Workflows in production becomes a matter of minutes, not days.
This help center article covers how to make the integration, and the monitoring options you have once in production.",
    "description": "Learn how to integrate and monitor your Workflow with Vellum's API, making production deployment quick and easy.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.api-integration-root-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/api-integration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Easy Integration with Vellum's API for Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/api-integration",
    "content": "Once you Deploy the Workflow from the UI, you‚Äôre taken to a code snippet which you need to use to call this Workflow in production. The adjacent screenshot shows the Workflow Deployment‚Äôs name & its input variables
Workflow Details
Workflow API Code Snippet",
    "domain": "test.com",
    "hash": "#workflow-code-snippet-integration",
    "hierarchy": {
      "h0": {
        "title": "Easy Integration with Vellum's API for Workflows",
      },
      "h2": {
        "id": "workflow-code-snippet-integration",
        "title": "Workflow Code Snippet Integration",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.api-integration-workflow-code-snippet-integration-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/api-integration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Workflow Code Snippet Integration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/api-integration",
    "content": "Once you start making requests to the Workflow, all the executions are stored in the Executions tab for monitoring purposes. Any time you find an edge case in production, you can save that specific Execution back as a Scenario for future testing. This is typically used to build out your test bank and debugging unexpected behavior. By running this Scenario in the UI you can see what the responses were at each step and tweak the Workflow logic (prompts, semantic search, business logic tying together the prompts)
Workflow Execution Observability",
    "domain": "test.com",
    "hash": "#workflow-executions",
    "hierarchy": {
      "h0": {
        "title": "Easy Integration with Vellum's API for Workflows",
      },
      "h2": {
        "id": "workflow-executions",
        "title": "Workflow Executions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.api-integration-workflow-executions-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/api-integration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Workflow Executions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/api-integration",
    "content": "Clicking the View Details button on the Execution brings you to a UI where you can see the inputs, outputs and latency at each step of the Workflow when it was run in production",
    "domain": "test.com",
    "hash": "#workflow-executions-details",
    "hierarchy": {
      "h0": {
        "title": "Easy Integration with Vellum's API for Workflows",
      },
      "h2": {
        "id": "workflow-executions-details",
        "title": "Workflow Executions Details",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.api-integration-workflow-executions-details-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/api-integration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Workflow Executions Details",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "Function calling (aka ‚Äútool calling‚Äù) helps you get consistent structured data from LLMs. It lets you call custom functions, interact with external APIs, and generally turn natural language into something code can understand.
But, don't be misled by the name‚Äîthis feature doesn't actually call functions for you.
Instead, it creates a JSON object with the name of the function to call and the arguments to pass, which you can use to trigger functions in your code. OpenAI models generate this JSON based on the tools you define with the tools parameter in the API.
So now that we‚Äôve cleared that part, let‚Äôs learn how you can use function calling with Chat Models in Vellum.",
    "description": "Learn how to use function calling with Chat Models in Vellum Workflows",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-root-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Function Calling with Chat Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "In this tutorial, we cover how function calling works and how to use it in Vellum Workflows:
By the end of this tutorial, you'll learn to do four tasks in Vellum:
Define function calls in a Prompt Node within Workflows;

Enable your models to auto-select or enforce the execution of a function response;

Pass Function Call outputs from a Workflow to your code;

Run functions with the provided arguments.




üí° Keep in mind that although we‚Äôll run some arbitrary functions to close
the response loop, we won't go into much detail on how to call external APIs
using the generated arguments.",
    "domain": "test.com",
    "hash": "#using-function-calling-in-vellum",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-using-function-calling-in-vellum-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Using Function Calling in Vellum",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "For this tutorial, we‚Äôll create an AI-powered customer chatbot for a smartphone outlet, that will have two tools (or functions) defined:
delivery_data
This function will require three parameters: Phone, which returns the phone's name, Location, which returns the user's location, and Condition, which returns the phone's condition.



call_agent
This one accepts one parameter: Question which is the user's query.






üí° This tutorial demonstrates how to define function calling messages in
Prompt Nodes within Workflows. However, they can also be configured in Prompt
Variants within a Prompt Sandbox.",
    "domain": "test.com",
    "hash": "#outline",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "outline",
        "title": "Outline",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-outline-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Outline",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "To setup an OpenAI API call in Vellum, we‚Äôll be using a Prompt Node. This node enables the configuration of all parameters required for an OpenAI API call.
We begin by adding the Prompt Node in the Workflow:
Add Prompt Node
To edit the Prompt Node, just click on the ‚Äúexpand‚Äù icon like on the image above.
Now let‚Äôs set up the messages.",
    "domain": "test.com",
    "hash": "#step-1-setting-up-messages-and-conversation-history",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-1-setting-up-messages-and-conversation-history",
        "title": "Step 1: Setting up Messages and Conversation History",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-step-1-setting-up-messages-and-conversation-history-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 1: Setting up Messages and Conversation History",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "First we‚Äôll start by adding a system message that tells the model to classify user intents. These models can hallucinate, so we‚Äôll explicitly tell the model to ask for clarification if there's incomplete information. Here‚Äôs the system message we used:


üí¨ You're great at classifying user intents. Don't assume which values to use
in functions‚Äîask for clarification if needed.",
    "domain": "test.com",
    "hash": "#system-message",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-1-setting-up-messages-and-conversation-history",
        "title": "Step 1: Setting up Messages and Conversation History",
      },
      "h3": {
        "id": "system-message",
        "title": "System Message",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-system-message-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "System Message",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "Next, we'll create a dynamic variable msg , and add it in the user message to ensure it automatically updates with the variable's content.
Here‚Äôs what the Prompt Node looks like once we've added the System and the User message:
Prompt Node Setup",
    "domain": "test.com",
    "hash": "#user-message",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-1-setting-up-messages-and-conversation-history",
        "title": "Step 1: Setting up Messages and Conversation History",
      },
      "h3": {
        "id": "user-message",
        "title": "User Message",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-user-message-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "User Message",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "To use function calling with Chat Models, you first need to set up three things in the Prompt Node:
Chat history variable: Simply click on Add Variable and write $chat_history

Chat generation model: Click on the model dropdown and select the latest GPT-4 turbo model, or GPT-4 Turbo 04/09/2024

Chat history block: Click on the ‚ÄúAdd‚Äù dropdown on the right, and select ‚ÄúAdd Chat History‚Äù


Here‚Äôs what the prompt should look like after you've completed the final step:
Prompt Node With Chat History
Now let‚Äôs set up the functions.",
    "domain": "test.com",
    "hash": "#chat-mode",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-1-setting-up-messages-and-conversation-history",
        "title": "Step 1: Setting up Messages and Conversation History",
      },
      "h3": {
        "id": "chat-mode",
        "title": "Chat Mode",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-chat-mode-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Chat Mode",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "code_snippets": [
      {
        "code": "{
  "name": "delivery_data",
  "description": "Calls a function with phone and location details",
  "parameters": {
    "type": "object",
    "properties": {
      "type": {
        "description": "returns if the model is used or new",
        "type": "string"
      },
      "phone": {
        "description": "Description about the phone",
        "type": "string"
      },
      "location": {
        "description": "The location of the user",
        "type": "string"
      }
    },
    "required": [
      "type",
      "phone",
      "location"
    ]
  }
}",
        "lang": "jsx",
      },
    ],
    "content": "To include a Function block, simply select it from the "Add" dropdown menu. Once added, you‚Äôll notice a grayed out box that says ‚Äútodo‚Äù:
Add Function
When you click that box, you can change the function's name and add a custom description and parameters:
Edit Function
You'll notice there are two options available to define these functions: Form and Upload. The Form option allows you to define your function calling message using the UI, while the Upload option enables you to upload a JSON or YAML file. The uploaded file will automatically map its values to the fields.
You might upload a file if you already have your functions defined in in your codebase and want to track their source of truth there.
In this tutorial, we‚Äôll upload this JSON file for the delivery_data function that has three properties (condition, phone, location):
Also, notice that all of these are required, so the model needs to collect all three in order to generate the response. Here‚Äôs what the UI returned:
Uploaded Function Definition
Have in mind that you can also check the "Forced" box to ensure the model always uses this function. If left unchecked, the model will decide which function to use based on the user query. OpenAI's latest models are quite good at making the right choice, so we‚Äôll leave it unchecked.
Toggle Forced Function
Now let‚Äôs create the other Function block call_agent. Using the same process as earlier, we add this function as well, which has one required parameter, "question" which is the user's query:
Call Agent Function
Now that we defined everything, our setup looks like this:
Prompt Node With Functions


üí° With this setup, our OpenAI API calls will take into account the system and user message, the chat history, and two function calling messages to generate responses to a given user query.",
    "domain": "test.com",
    "hash": "#step-2-defining-function-calls",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-2-defining-function-calls",
        "title": "Step 2: Defining Function Calls",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-step-2-defining-function-calls-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 2: Defining Function Calls",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "To test the API call, we‚Äôll use the Chat History simulation, where you can run user-assistant messages. You can find this option in the top-left corner of your Workflow sandbox:
Workflow Chat History",
    "domain": "test.com",
    "hash": "#step-3-testing-the-api-call",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-3-testing-the-api-call",
        "title": "Step 3: Testing the API call",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-step-3-testing-the-api-call-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 3: Testing the API call",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "In the image below, you'll notice that the model recognizes the user's inquiry about delivery options for a particular mobile phone. Consequently, it prompts the user for the other required parameters, "location," & "condition".
This corresponds to the function call we defined earlier: delivery_data, and the model successfully determined which function to call for this request:
Workflow Chat History
After the user provides the missing information, the model then proceeds to generate the JSON object with the function parameters. Below, you can see that the model successfully gathered all required parameters and displayed them in the output of the Prompt Node:
Prompt Node Function Called
Next, you‚Äôll probably want to pass these values directly to your code. Let‚Äôs learn how to do that in the next section.",
    "domain": "test.com",
    "hash": "#testing-the-delivery_data-function",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-3-testing-the-api-call",
        "title": "Step 3: Testing the API call",
      },
      "h3": {
        "id": "testing-the-delivery_data-function",
        "title": "Testing the delivery_data function",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-testing-the-delivery_data-function-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Testing the delivery_data function",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "code_snippets": [
      {
        "code": "{"state":"FULFILLED","arguments":{"type":"new","phone":"iPhone 15 Pro","location":"San Diego"},"id":"call_2wVIb9fBFDYh5rPP6QOOemok","name":"delivery_data"}",
        "lang": "jsx",
      },
    ],
    "content": "To ensure consistent output across different models, you can use Vellum‚Äôs standardized Function Call output type.
To do this, you need to add a Templating Node that receives all model outputs, and extracts a specific one.
In our case, we add a Templating Node, where we extract the Prompt Node‚Äôs outputs as an Array, and we set the output type to be a Function Call:
Extract Function from Templating Node
Finally, you can add a Final Output node to pass the function call into your code.
Here‚Äôs what the final Workflow looks like:
Final Workflow
And here‚Äôs the raw format from the Final Output node:
This is beneficial if you want your Workflow to exclusively produce function call responses. But in real situations, you might have a Workflow that should generate both Assistant and Function call outputs.
Let‚Äôs look at how you can conditionally branch out those outputs with Vellum.",
    "domain": "test.com",
    "hash": "#step-4-pass-standardized-function-call-outputs-to-your-code",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-4-pass-standardized-function-call-outputs-to-your-code",
        "title": "Step 4: Pass standardized Function Call outputs to your code",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-step-4-pass-standardized-function-call-outputs-to-your-code-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 4: Pass standardized Function Call outputs to your code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "To branch out the model's responses, you‚Äôll first need to add a Templating Node and extract the output type from the Prompt Node:
Extract Output Type Templating Node
Next, you‚Äôll add a Conditional Node, where you‚Äôll verify if the output type is a FUNCTION_CALL. If it is, you can divert to the function calling flow; otherwise, proceed with the alternative path.
Here‚Äôs what the setup should look like:
Conditional Node Branching
Finally, we add the Function calling and the alternate path:
Function Calling with Alternative Workflow Branch
The first path will verify if the model generates a function call message and then pass that value as the final output. The second path will check if the model produces an Assistant message and similarly pass that value as the final output.
Once all of this is connected, we get this Workflow:
Full Workflow Diagram
You can now deploy your Workflow in your code.
Once deployed, you‚Äôll need to invoke it using two input variables:
chat_history ‚Äì CHAT_HISTORY

user ‚Äì STRING


And the API call will return, two output variables:
function_call ‚Äì FUNCTION_CALL

answer ‚Äì STRING",
    "domain": "test.com",
    "hash": "#step-4-branching-out-prompt-node-outputs",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-4-branching-out-prompt-node-outputs",
        "title": "Step 4: Branching out Prompt Node Outputs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-step-4-branching-out-prompt-node-outputs-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 4: Branching out Prompt Node Outputs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "code_snippets": [
      {
        "code": "import json

def main(input_str):
    # Convert the string to a dictionary
    data = json.loads(input_str)

    # Check the 'name' field and output the corresponding message
    if data.get("name") == "delivery_data":
        return "For delivery options in San Diego: You can get same-day delivery if you order before noon, with deliveries happening between 1:00 PM and 8:00 PM. If you miss the deadline, don't worry! You can still get your order the next day if you place it after noon, with deliveries scheduled between 10:00 AM and 6:00 PM."
    elif data.get("name") == "call_agent":
        return "Please wait until I connect you with an agent‚Ä¶"
    else:
        return "unknown operation"",
        "lang": "jsx",
      },
    ],
    "content": "To showcase how this Workflow works, in our code, we wrote arbitrary functions that will print some static messages once the model passes the required parameters.
For the delivery_data function we‚Äôll retrieve static message:


üí¨ For delivery options in San Diego: You can get same-day delivery if you
order before noon, with deliveries happening between 1:00 PM and 8:00 PM. If
you miss the deadline, don't worry! You can still get your order the next day
if you place it after noon, with deliveries scheduled between 10:00 AM and
6:00 PM.
For the call_agent function we send another static message:


üí¨ Please wait until I connect you with an agent‚Ä¶
This is what our code looks like:",
    "domain": "test.com",
    "hash": "#step-5-adding-arbitrary-functions",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-5-adding-arbitrary-functions",
        "title": "Step 5: Adding arbitrary functions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-step-5-adding-arbitrary-functions-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 5: Adding arbitrary functions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "Now, let‚Äôs see how this works!",
    "domain": "test.com",
    "hash": "#step-6-testing-the-workflow",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-6-testing-the-workflow",
        "title": "Step 6: Testing the workflow",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-step-6-testing-the-workflow-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 6: Testing the workflow",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "Works as intended, the model asks about the other required parameter: _location_.
Test Case 1",
    "domain": "test.com",
    "hash": "#case-1-ask-incomplete-delivery-questions",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-6-testing-the-workflow",
        "title": "Step 6: Testing the workflow",
      },
      "h3": {
        "id": "case-1-ask-incomplete-delivery-questions",
        "title": "Case 1: Ask incomplete delivery questions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-case-1-ask-incomplete-delivery-questions-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Case 1: Ask incomplete delivery questions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "Works as intended; the model provides the arbitrary answer we added in our function for the delivery_data function:
Test Case 2",
    "domain": "test.com",
    "hash": "#case-2-closing-the-loop-call-the-delivery_data-function",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-6-testing-the-workflow",
        "title": "Step 6: Testing the workflow",
      },
      "h3": {
        "id": "case-2-closing-the-loop-call-the-delivery_data-function",
        "title": "Case 2: Closing the loop, Call the delivery_data function",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-case-2-closing-the-loop-call-the-delivery_data-function-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Case 2: Closing the loop, Call the delivery_data function",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "The model accurately follows the instructions and asks the user if they want to be connected with an agent:
Test Case 3",
    "domain": "test.com",
    "hash": "#case-3-ask-other-questions",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-6-testing-the-workflow",
        "title": "Step 6: Testing the workflow",
      },
      "h3": {
        "id": "case-3-ask-other-questions",
        "title": "Case 3: Ask other questions",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-case-3-ask-other-questions-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Case 3: Ask other questions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "The model successfully runs the call_agent function and outputs the arbitrary answer we defined:
Test Case 4",
    "domain": "test.com",
    "hash": "#case-4-call-the-call_agent-function-if-user-says-yes",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "step-6-testing-the-workflow",
        "title": "Step 6: Testing the workflow",
      },
      "h3": {
        "id": "case-4-call-the-call_agent-function-if-user-says-yes",
        "title": "Case 4: Call the call_agent function if user says yes",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-case-4-call-the-call_agent-function-if-user-says-yes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Case 4: Call the call_agent function if user says yes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "Hopefully, through this guide, you've learned how to effectively set up and utilize function calling with Chat Models in Vellum Workflows.
By following the steps outlined, you can define functions or tools, enable models to auto-select or enforce function execution, pass function call outputs to your code, and run functions with the provided arguments. This powerful feature allows you to create sophisticated, AI-powered workflows that can handle complex user interactions and seamlessly integrate with your existing systems.",
    "domain": "test.com",
    "hash": "#conclusion",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "using-function-calling-in-vellum",
        "title": "Using Function Calling in Vellum",
      },
      "h2": {
        "id": "conclusion",
        "title": "Conclusion",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-conclusion-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Conclusion",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/function-calling-with-chat-models",
    "content": "See below for a quick overview of some common scenarios for handling functions and passing them between nodes.
In all three examples, the Prompt Node returns text beyond just the function call. We show how to filter that away and pass the Function Call as part of an Array or on its own.
Extracting an Array of multiple Function Calls with a Code Execution Node

Extracting a single Function Call with a Code Execution Node

Extracting a single Function Call with a Templating Node",
    "domain": "test.com",
    "hash": "#quirks-and-tips-for-handling-functions",
    "hierarchy": {
      "h0": {
        "title": "Function Calling with Chat Models",
      },
      "h1": {
        "id": "quirks-and-tips-for-handling-functions",
        "title": "Quirks and Tips for Handling Functions",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.function-calling-with-chat-models-quirks-and-tips-for-handling-functions-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/function-calling-with-chat-models",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Quirks and Tips for Handling Functions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/examples-and-walkthroughs",
    "content": "Below you'll find many example architectures for common use-cases, alongside video walkthroughs that explain the architecture of each. You're free to use these as a starting point for your own applications!",
    "description": "See interactive Vellum architectures and watch video walkthroughs of them! Use these as a great starting point for your own applications.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.examples-and-walkthroughs-root-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/examples-and-walkthroughs",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Examples and Walkthroughs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/examples-and-walkthroughs",
    "content": "Concepts: Routing, Classification, Chatbot, Statefulness, State Management, Function Calling, Dynamic API URLs",
    "domain": "test.com",
    "hash": "#customer-support-bot-with-escalation-to-human",
    "hierarchy": {
      "h0": {
        "title": "Examples and Walkthroughs",
      },
      "h4": {
        "id": "customer-support-bot-with-escalation-to-human",
        "title": "Customer Support Bot with Escalation To Human",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.examples-and-walkthroughs-customer-support-bot-with-escalation-to-human-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/examples-and-walkthroughs",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Customer Support Bot with Escalation To Human",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/examples-and-walkthroughs",
    "code_snippets": [
      {
        "code": "  import requests

  # Replace with your actual Vellum API key
  VELLUM_API_KEY = "..."

  url = "https://predict.vellum.ai/v1/execute-workflow"

  headers = {
      "Content-Type": "application/json",
      "X_API_KEY": VELLUM_API_KEY
  }

  data = {
      "workflow_deployment_name": "vellum-customer-support-q-a-demos",
      "release_tag": "LATEST",
      "inputs": [
          {
              "type": "STRING",
              "name": "question",
              "value": input_data["user_question"] # whatever Zapier values you want to use here
          }
      ]
  }

  response = requests.post(url, headers=headers, json=data)

  # Print the response from the server
  print(response.status_code)
  print(response.json())

  return response.json()",
        "lang": "python",
      },
      {
        "code": "  import requests

  # Replace with your actual Vellum API key
  VELLUM_API_KEY = "..."

  url = "https://predict.vellum.ai/v1/execute-workflow"

  headers = {
      "Content-Type": "application/json",
      "X_API_KEY": VELLUM_API_KEY
  }

  data = {
      "workflow_deployment_name": "vellum-customer-support-q-a-demos",
      "release_tag": "LATEST",
      "inputs": [
          {
              "type": "STRING",
              "name": "question",
              "value": input_data["user_question"] # whatever Zapier values you want to use here
          }
      ]
  }

  response = requests.post(url, headers=headers, json=data)

  # Print the response from the server
  print(response.status_code)
  print(response.json())

  return response.json()",
        "lang": "python",
      },
    ],
    "content": "Concepts: Document Indexes, Metadata, Zapier, Slack, Citing Sources, Combining Sources








Passing JSON Arrays in Zapier can be tricky. Instead, you can use Code Blocks to make it easier.
You can use the follow code snippet as inspiration for your own API calls with Zapier Code Blocks and Python.",
    "domain": "test.com",
    "hash": "#slack-support-bot-cites-sources-using-multiple-indexes",
    "hierarchy": {
      "h0": {
        "title": "Examples and Walkthroughs",
      },
      "h4": {
        "id": "slack-support-bot-cites-sources-using-multiple-indexes",
        "title": "Slack Support Bot, Cites Sources using Multiple Indexes",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.examples-and-walkthroughs-slack-support-bot-cites-sources-using-multiple-indexes-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/examples-and-walkthroughs",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Slack Support Bot, Cites Sources using Multiple Indexes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/examples-and-walkthroughs",
    "content": "Concepts: Chatbots, Chat History Modification, Conditionals, Adversarial Debate, Academic Research",
    "domain": "test.com",
    "hash": "#chatbots-debating-each-other",
    "hierarchy": {
      "h0": {
        "title": "Examples and Walkthroughs",
      },
      "h4": {
        "id": "chatbots-debating-each-other",
        "title": "Chatbots Debating Each Other",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.examples-and-walkthroughs-chatbots-debating-each-other-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/examples-and-walkthroughs",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Chatbots Debating Each Other",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/examples-and-walkthroughs",
    "content": "Concepts: PDF Parsing, CSV Generation, Data Extraction, Data Transformation, Document Indexes, Map Nodes
Blog post",
    "domain": "test.com",
    "hash": "#convert-pdf-to-csv",
    "hierarchy": {
      "h0": {
        "title": "Examples and Walkthroughs",
      },
      "h4": {
        "id": "convert-pdf-to-csv",
        "title": "Convert PDF to CSV",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.examples-and-walkthroughs-convert-pdf-to-csv-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/examples-and-walkthroughs",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Convert PDF to CSV",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/examples-and-walkthroughs",
    "content": "Concepts: Parallel Function Calls, Concurrency, Map Nodes, Chat History, API Calls",
    "domain": "test.com",
    "hash": "#multiple-parallelized-function-calls",
    "hierarchy": {
      "h0": {
        "title": "Examples and Walkthroughs",
      },
      "h4": {
        "id": "multiple-parallelized-function-calls",
        "title": "Multiple Parallelized Function Calls",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.examples-and-walkthroughs-multiple-parallelized-function-calls-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/examples-and-walkthroughs",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Multiple Parallelized Function Calls",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/help-center/workflows/examples-and-walkthroughs",
    "content": "This example takes a URL, looks up all people mentioned on the page, and sorts them in accordance with how involved they or their affiliates are with AI / LLMs.
Concepts: Perplexity, SERP, Research Automation, Structured Data Extraction, Parallelization",
    "domain": "test.com",
    "hash": "#lookup-conference-attendees-with-perplexity",
    "hierarchy": {
      "h0": {
        "title": "Examples and Walkthroughs",
      },
      "h4": {
        "id": "lookup-conference-attendees-with-perplexity",
        "title": "Lookup Conference Attendees with Perplexity",
      },
    },
    "level": "h4",
    "objectID": "test:test.com:root.uv.help-center.help-center.workflows.examples-and-walkthroughs-lookup-conference-attendees-with-perplexity-0",
    "org_id": "test",
    "pathname": "/help-center/workflows/examples-and-walkthroughs",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Lookup Conference Attendees with Perplexity",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "Ensuring model quality is challenging, because prompts need to work effectively and
consistently over a wide range of potential inputs.
When modifying a Prompt, adjusting parameters, or switching models, the likelihood of regression is high.
This is where quantitative evaluation comes in.
Vellum's answer to quantitative evaluation are Test Suites and Metrics. Unlike Comparison Mode and Chat Mode,
where the output of each Scenario is qualitatively evaluated by visual inspection,
Test Suites use Metrics that return scores between 0 and 1 to provide a more objective measure of quality over a wider range of scenarios.
This is especially important when your prompt's coverage needs scale and visual inspection is no longer feasible,
which typically happens when you have 10+ scenarios.
Test Suites cover common use cases in LLM development:
Test Driven Development of Prompts/Workflows in a Sandbox

Performance testing on large numbers of Scenarios

Regression testing before deploying a change to a Prompt or Workflow

Evaluating external entities


If you're looking to improve the velocity or quality of your LLM development, Test Suites could be your answer.",
    "description": "Discover how Vellum's Evaluations feature quantitatively evaluates LLM outputs, ensuring model quality across numerous scenarios.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-root-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Maximize LLM Development Quality with Vellum's Evaluations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "You can create standalone Test Suites through the Test Suites page in the Evaluations tab.
You can also create Test Suites starting from a Prompt or Workflow Sandbox.
Here, we'll begin by creating a Test Suite from a Prompt Sandbox.
From your Prompt Sandbox, click on Evaluations sub-tab

Click "Create New Test Suite" if you haven't set up a Test Suite for this Prompt Sandbox yet or click the gray "Add Test Suite" button on the right of the page

The "label" and "name" fields are autopopulated with the name of your Sandbox

Open the "Interface Configuration" to see the expected inputs/outputs for this Test Suite. This is also autopopulated from your Sandbox.

If you are creating a Test Suite from scratch, you can customize the inputs/outputs to meet your needs. For now, leave this as it is and click next to start setting up your Metrics.

Click the "Add Metric" button to select one or more Metrics to evaluate your output against

Select "Exact Match" from the list of available Metrics. To learn how to create Custom Metrics that appear in this list, see Vellum's Metrics

Press "Confirm" to start mapping the Metric to this Test Suite

First, select "Completion" to map the input to the output of your Prompt

Next, select "Target" and then "Add New" in the dropdown to create a new expected output variable that's mapped to this input

Press "Next" in the bottom right


Well done! You've now created a Test Suite with the Exact Match Metric to check for whether the output is desirable.
Test Suite Creation - Metric Configuration",
    "domain": "test.com",
    "hash": "#create-a-test-suite",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "getting-started",
        "title": "Getting Started",
      },
      "h2": {
        "id": "create-a-test-suite",
        "title": "Create a Test Suite",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-create-a-test-suite-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Create a Test Suite",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "Test Cases are analogous to Scenarios. A Test Case by default will have a column for each input and output variable expected by your Test Suite,
as well as columns for any expected output variables required by your selected Metric.
When a Test Suite is run, Vellum will iterate over every Test Case it contains.
For each Test Case, it will feed the provided inputs into the Prompt, Workflow, or entity being tested.
The output will be passed to the Metric based on your mapping in the previous step, and a score will be provided based on the Metric being used.
With this Test Suite created, you should be on a step that asks you how you want to initialize your Test Cases.
Select "Start from Scenarios" which will autopopulate test cases for you from Scenarios in your Sandbox.
To create additional Test Cases, follow these steps:
Click the blue "+ Add Test Case" button just below the tab to add a new Test Case row

You can leave the "Label" field alone for now - it's used to help you visually identify your Test Cases

Enter a value for each of your input variables. For example, if you have an input variable user_age, you may enter "28".

Enter a value for each of your expected output variables

Click outside of your Test Case row to save. You should get a notification that it was successful.

Click "Finish" to exit the Test Suite creation wizard


Almost there! Now that you have your Test Cases added, you're ready to run it against your Prompt.
Test Suite Creation - Test Cases",
    "domain": "test.com",
    "hash": "#create-a-test-case",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "getting-started",
        "title": "Getting Started",
      },
      "h2": {
        "id": "create-a-test-case",
        "title": "Create a Test Case",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-create-a-test-case-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Create a Test Case",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "This Test Suite is automatically added to the current Prompt Sandbox you are in.
To evaluate your Prompt, simply click the blue Run button.
The Test Suite can also be run by any Prompt or Workflow that uses the same input variables you configured earlier. Let's try it.
Navigate to any Prompt Sandbox in the "Prompts" tab

Click the gray "Add Test Suite" at the top right of the page

Attach a Test Suite by clicking on "Use Existing Test Suite" and selecting your Test Suite from the dropdown

Press the blue "Run" to execute the Test Suite


Link Test Suite to Prompt
Run Test Suite on Prompt",
    "domain": "test.com",
    "hash": "#run-a-test-suite",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "getting-started",
        "title": "Getting Started",
      },
      "h2": {
        "id": "run-a-test-suite",
        "title": "Run a Test Suite",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-run-a-test-suite-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Run a Test Suite",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "We also support viewing your Test Suite Run results via the API. Check out our docs on Test Suite Runs to learn how to view existing runs. We also embed ready-to-use code snippets within the app itself for each executable column on the evaluations table.
Test Suite Runs via API",
    "domain": "test.com",
    "hash": "#download-via-api",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "getting-started",
        "title": "Getting Started",
      },
      "h2": {
        "id": "download-via-api",
        "title": "Download via API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-download-via-api-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Download via API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "It's possible for a Test Suite to have multiple Metrics run simultaneously.
This is often desirable when the output is complex and must meet multiple criterion.
For example, you may want to validate that an output semantically means "I'm very happy",
but must contain the word "ecstatic".
Additional Metrics can be configured on an existing Test Suite under the "Metric Setup" section
of the "Test Suite Details" page. Any additional columns needed by that Metric will be editable
under the "Test Cases" tab.
When running the Test Suite, you'll see columns displaying the results for each Metric that's been added to the Test Suite.


For more on premade Vellum Metrics, see Out of the Box Metrics. To learn how to create your own, see Custom Metrics",
    "domain": "test.com",
    "hash": "#multiple-metrics",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "multiple-metrics",
        "title": "Multiple Metrics",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-multiple-metrics-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Multiple Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "To help you migrate your Test Cases into Vellum, we provide two methods for bulk Test Case uploads.",
    "domain": "test.com",
    "hash": "#uploading-test-cases",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "uploading-test-cases",
        "title": "Uploading Test Cases",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-uploading-test-cases-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Uploading Test Cases",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "Under the "Test Cases" tab on the "Test Suite Details" page, there is a blue "Upload Test Cases" button.
Clicking that button will open a modal that allows you to bulk upload test cases via a CSV file.",
    "domain": "test.com",
    "hash": "#via-csv",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "uploading-test-cases",
        "title": "Uploading Test Cases",
      },
      "h3": {
        "id": "via-csv",
        "title": "Via CSV",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-via-csv-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Via CSV",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "To upload test cases via API, check out the Test Cases API documentation.",
    "domain": "test.com",
    "hash": "#via-api",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "uploading-test-cases",
        "title": "Uploading Test Cases",
      },
      "h3": {
        "id": "via-api",
        "title": "Via API",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-via-api-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Via API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "Prompts can output different modalities, which at Vellum we call input and output types. One increasingly popular output type for models if function/tool calling. You can use Vellum Test Suites to ensure that your model is producing the correct function call based on any given combination of inputs.
First, you will want to edit the Output Variable type of the test suite to the "Function Call" type:
Output Function Type
Next, define the Expected Output type to be of type "Function Call" too:
Expected Function Type
You can then use any Metric to help ensure that your prompt is outputting function calls that perform well across your test cases! Here's an example of a test suite using Exact Match as the Metric:
Function Call Tests",
    "domain": "test.com",
    "hash": "#function-calling",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "function-calling",
        "title": "Function Calling",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-function-calling-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Function Calling",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "Just like Prompts, Workflows can be tested too.
The key difference Workflow Test Suites have is that Workflows may have multiple outputs.
You may choose to test some of them, or all of them.
Workflow Test Suites can be run from the "Evaluations" tab in the Workflow Builder.
It works very similarly as testing prompts, but you can create new test cases inline.
One common pitfall is trying to attach a Test Suite where the output variable is named completion
to a Workflow where the output variable is something else. You will receive a warning when this happens.
Interface Mismatch
Interface Match",
    "domain": "test.com",
    "hash": "#testing-workflows",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "testing-workflows",
        "title": "Testing Workflows",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-testing-workflows-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Testing Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "To help you diagnose issues with workflows you can click on the "View Workflow Details" button located within a test case's value cell to view the details.
Workflow Executions2",
    "domain": "test.com",
    "hash": "#viewing-workflow-test-case-entitys-execution-details",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "testing-workflows",
        "title": "Testing Workflows",
      },
      "h3": {
        "id": "viewing-workflow-test-case-entitys-execution-details",
        "title": "Viewing Workflow Test Case Entity's Execution Details",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-viewing-workflow-test-case-entitys-execution-details-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Viewing Workflow Test Case Entity's Execution Details",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/quantitative-evaluation",
    "content": "Not only can Vellum's evaluation framework be used to test Prompts & Workflows hosted in Vellum, it can also be used
to evaluate the outputs of arbitrary functions hosted externally to Vellum.
For example, you might test a prompt chain that lives in your codebase and that's defined using another third party
library. This can be particularly useful if you want to incrementally migrate to Vellum Prompts/Workflows, but ensure
that the outputs remain consistent.
For a detailed example of how to use Vellum's evaluation framework to test external functions, see the
python example here",
    "domain": "test.com",
    "hash": "#testing-functions-external-to-vellum",
    "hierarchy": {
      "h0": {
        "title": "Maximize LLM Development Quality with Vellum's Evaluations",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "testing-functions-external-to-vellum",
        "title": "Testing Functions External to Vellum",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.quantitative-evaluation-testing-functions-external-to-vellum-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/quantitative-evaluation",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Testing Functions External to Vellum",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "content": "Retrieval Augmented Generation (RAG) is a powerful technique to improve your LLM output quality by providing it relevant data - usually retrieved from an external knowledgebase.
In Vellum, setting up your RAG pipeline is straightforward using our Documents feature. Upload and vectorize your knowledgebase for use in minutes.
For more details on how to set up a RAG pipeline in Vellum, see common architectures here.
Once you have your RAG workflow set up, a challenging but important and often overlooked aspect is evaluating quality. When it comes to RAG, you're not only interested in the quality of your LLM response (the Generation), but also the context being returned from your vector database (the Retrieval).
Essentially, you're checking to see how effectively your system retrieves relevant information from a knowledge base and then uses it to produce reliable and precise responses or content.
RAG evaluation is a continuous process - running these evaluations gives confidence when initially deploying your RAG into production but the benefits continue post-deployment.
Running these evals in production help you understand your system's current performance and identify areas for optimization.
Using Out-of-Box RAG Metrics in Test Suites, Vellum makes it easy to evaluate, monitor, and continuously improve your RAG pipeline over time without concern of introducing regressions.
Read on to see how you can evaluate your RAG pipeline in Vellum!",
    "description": "How to evaluate and maintain high quality RAG pipelines in Vellum",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.evaluating-rag-pipelines-root-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Evaluating RAG Pipelines",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "content": "Step 1: Create a Document Index and upload your documents (follow this article for tips: Uploading Documents)

Step 2: Add a Search Node in your Workflow

Step 3: Add a Prompt Node that takes the results of your Search Node as an input variable

Step 4: Link the output of the Prompt Node to a Final Output Node (for evaluating Generation)

Step 5: Link the output of the Search Node to a Final Output Node (for evaluating Retrieval)

Step 5: Set up your Workflow variables and hit Run!


Once your RAG pipeline runs and passes visual inspection, it's time to set up your Test Suite.
RAG Pipeline Workflow",
    "domain": "test.com",
    "hash": "#set-up-your-rag-pipeline-for-evaluation",
    "hierarchy": {
      "h0": {
        "title": "Evaluating RAG Pipelines",
      },
      "h1": {
        "id": "rag-evaluation-in-vellum",
        "title": "RAG Evaluation in Vellum",
      },
      "h2": {
        "id": "set-up-your-rag-pipeline-for-evaluation",
        "title": "Set up your RAG pipeline for Evaluation",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.evaluating-rag-pipelines-set-up-your-rag-pipeline-for-evaluation-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Set up your RAG pipeline for Evaluation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "content": "Step 1: Create a Test Suite for this Workflow (follow this article for more info: Quantitatively Evaluating Outputs

Step 2: Add the following Ragas Metrics: Ragas - Faithfulness, Ragas - Answer Relevance, Ragas - Context Relevancy

Step 3: Map the Test Suite variables to the Metric Inputs

Step 4: Add your Test Cases and hit Run!


Ragas Metrics
Now you can see how well your RAG pipeline performs across a bank of test cases!
Depending on your results, you can adjust the appropriate component in your RAG system.
Ragas Test Suite Results",
    "domain": "test.com",
    "hash": "#set-up-your-test-suite",
    "hierarchy": {
      "h0": {
        "title": "Evaluating RAG Pipelines",
      },
      "h1": {
        "id": "rag-evaluation-in-vellum",
        "title": "RAG Evaluation in Vellum",
      },
      "h2": {
        "id": "set-up-your-test-suite",
        "title": "Set up your Test Suite",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.evaluating-rag-pipelines-set-up-your-test-suite-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Set up your Test Suite",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "content": "Answer Relevance and Faithfulness are Generation evals that measure the quality of the LLM response and guard against hallucinations.
If you see low performance here, you can optimize your Prompt.
Try adjusting the Prompt itself, tweak Model parameters, or try a different Model for better performance.
Read more about Prompt Engineering best practices from the Vellum team:
Prompt Engineering Tips for Claude

Prompt Engineering Tips for GPT",
    "domain": "test.com",
    "hash": "#generation",
    "hierarchy": {
      "h0": {
        "title": "Evaluating RAG Pipelines",
      },
      "h1": {
        "id": "rag-evaluation-in-vellum",
        "title": "RAG Evaluation in Vellum",
      },
      "h2": {
        "id": "set-up-your-test-suite",
        "title": "Set up your Test Suite",
      },
      "h3": {
        "id": "generation",
        "title": "Generation",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.evaluating-rag-pipelines-generation-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Generation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "content": "Context Relevancy is a Retrieval eval. If you see low performance here, you can optimize your Document Indexes.
Try different embedding strategies, chunking sizes, and add metadata filtering so the context returned is precise and relevant to the question being asked.
If these methods don't improve performance enough, make sure the documents you've uploaded are clean from any noise or extraneous elements that can negatively impact their vector representation and your results.
This includes:
Header / footer info

Extra or special characters

New Lines

Inconsistent formatting (including capitalizations)


Once the documents are processed, you can also try more sophisticated Workflow methods such as splitting your knowledge base into separate Document Indexes and dynamically selecting the right Document Index to use in your Search Node.
This method often involves an additional LLM call from a simpler model (think GPT-3.5 turbo) that is used to categorize and select the correct Document Index base on the question being asked.


To learn more about Retrieval Augmented Generation and the most effective
Metrics to use in your RAG pipelines, check out our blog
article",
    "domain": "test.com",
    "hash": "#retrieval",
    "hierarchy": {
      "h0": {
        "title": "Evaluating RAG Pipelines",
      },
      "h1": {
        "id": "rag-evaluation-in-vellum",
        "title": "RAG Evaluation in Vellum",
      },
      "h2": {
        "id": "set-up-your-test-suite",
        "title": "Set up your Test Suite",
      },
      "h3": {
        "id": "retrieval",
        "title": "Retrieval",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.evaluating-rag-pipelines-retrieval-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/evaluating-rag-pipelines",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Retrieval",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/online-evaluations",
    "content": "Online Evaluations in Vellum provide a powerful way to continuously assess the quality of your deployed LLM applications.
This feature allows you to monitor and evaluate the performance of your prompts or workflows in real-time as they're being used in production.",
    "description": "Learn how Vellum's Online Evaluations feature continuously assesses LLM outputs, ensuring model quality across diverse deployment scenarios.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.online-evaluations-root-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/online-evaluations",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Leveraging Online Evaluations for LLM Development with Vellum",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/online-evaluations",
    "content": "Start by creating either a Workflow in the Workflow Sandbox or a Prompt in the Prompt Sandbox.

Once you're satisfied with what you've created, deploy your Workflow or Prompt.",
    "domain": "test.com",
    "hash": "#step-1-create-and-deploy-your-llm-application",
    "hierarchy": {
      "h0": {
        "title": "Leveraging Online Evaluations for LLM Development with Vellum",
      },
      "h1": {
        "id": "getting-started-with-online-evaluations",
        "title": "Getting Started with Online Evaluations",
      },
      "h2": {
        "id": "step-1-create-and-deploy-your-llm-application",
        "title": "Step 1: Create and Deploy Your LLM Application",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.online-evaluations-step-1-create-and-deploy-your-llm-application-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/online-evaluations",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 1: Create and Deploy Your LLM Application",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/online-evaluations",
    "content": "After deployment, you can configure Metrics to evaluate your LLM application's performance:
Configure Metrics for use in Online Evals
Navigate to your Prompt or Workflow Deployment.

Locate the "Metrics" tab in the tab bar.

In the Metrics tab, configure which Metrics you'd like like to use to evaluate the performance of your Deployment.

Save your changes. From this point forward, every execution of your Deployment will be automatically evaluated against these Metrics.


See results of Metrics alongside Execution details


For information on using and defining Metrics in Vellum, see our Metrics page.",
    "domain": "test.com",
    "hash": "#step-2-configure-metrics",
    "hierarchy": {
      "h0": {
        "title": "Leveraging Online Evaluations for LLM Development with Vellum",
      },
      "h1": {
        "id": "getting-started-with-online-evaluations",
        "title": "Getting Started with Online Evaluations",
      },
      "h2": {
        "id": "step-2-configure-metrics",
        "title": "Step 2: Configure Metrics",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.online-evaluations-step-2-configure-metrics-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/online-evaluations",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 2: Configure Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/online-evaluations",
    "content": "Online Evaluations offer several key benefits for LLM application development:
Real-time Performance Monitoring: Continuously assess your Deployment's performance as it handles live requests.

Quality Assurance: Ensure your LLM application maintains high standards even as input patterns may shift over time.

Regression Detection: Quickly identify any degradation in performance, allowing for swift corrective action.

Insight-Driven Improvement: Use the gathered data to inform future iterations and improvements of your LLM application.",
    "domain": "test.com",
    "hash": "#step-3-understanding-online-evaluations",
    "hierarchy": {
      "h0": {
        "title": "Leveraging Online Evaluations for LLM Development with Vellum",
      },
      "h1": {
        "id": "getting-started-with-online-evaluations",
        "title": "Getting Started with Online Evaluations",
      },
      "h2": {
        "id": "step-3-understanding-online-evaluations",
        "title": "Step 3: Understanding Online Evaluations",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.online-evaluations-step-3-understanding-online-evaluations-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/online-evaluations",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 3: Understanding Online Evaluations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/online-evaluations",
    "content": "When configuring Metrics for use with Online Evaluations, it's essential to choose the right ones to align with your specific use case and quality standards. Here are some key considerations to keep in mind:
You should start by defining what "good" means to you and how you might decompose your definition of "good" into multiple smaller dimensions that are easier to measure individually.

From there, you can select Metrics provided by Vellum that align with these dimensions, or you can define your own.

Note that for now, Metrics are only able to operate on the inputs sent to a Deployment and the outputs generated by it.
In the future, Metrics will also be able to operate on Actuals
(i.e. end-user feedback send back to Vellum), such that they can more effectively measure accuracy.

If you'd like advice on which Metrics to use, please free to reach out to the Vellum team for guidance!",
    "domain": "test.com",
    "hash": "#selecting-the-right-metrics",
    "hierarchy": {
      "h0": {
        "title": "Leveraging Online Evaluations for LLM Development with Vellum",
      },
      "h1": {
        "id": "selecting-the-right-metrics",
        "title": "Selecting the Right Metrics",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.online-evaluations-selecting-the-right-metrics-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/online-evaluations",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Selecting the Right Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/online-evaluations",
    "content": "To access your Online Evaluation results:
Go to your Prompt or Workflow's Deployment details page.

Navigate to the "Executions" tab.

Click on an individual Execution ID to view its details.

In the Execution Details page, you'll find the evaluation results based on your configured metrics.


You can analyze these results to gain insights into your Deployment's strengths and areas for improvement.",
    "domain": "test.com",
    "hash": "#viewing-evaluation-results",
    "hierarchy": {
      "h0": {
        "title": "Leveraging Online Evaluations for LLM Development with Vellum",
      },
      "h1": {
        "id": "viewing-evaluation-results",
        "title": "Viewing Evaluation Results",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.online-evaluations-viewing-evaluation-results-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/online-evaluations",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Viewing Evaluation Results",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/online-evaluations",
    "content": "You can configure multiple Metrics within a single Deployment to evaluate its performance across multiple dimensions. This allows for a more comprehensive assessment of your Deployment's capabilities.
For example, you might configure a Metric to evaluate whether your LLM application produced a response of an appropriate length and another Metric to assess whether it used the proper tone of voice.",
    "domain": "test.com",
    "hash": "#multiple-metrics",
    "hierarchy": {
      "h0": {
        "title": "Leveraging Online Evaluations for LLM Development with Vellum",
      },
      "h1": {
        "id": "advanced-usage",
        "title": "Advanced Usage",
      },
      "h2": {
        "id": "multiple-metrics",
        "title": "Multiple Metrics",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.online-evaluations-multiple-metrics-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/online-evaluations",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Multiple Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/evaluation",
        "title": "Evaluation & Test Suites",
      },
    ],
    "canonicalPathname": "/help-center/evaluation/online-evaluations",
    "content": "Online Evaluations in Vellum offer a robust, automated way to ensure the ongoing quality and performance of your LLM applications. By providing continuous, metric-based assessments, this feature empowers you to maintain high standards and make data-driven improvements to your Prompts and Workflows.
Remember, the key to leveraging Online Evaluations effectively is in thoughtfully configuring your Metrics to align with your specific use case and quality standards. Regularly reviewing and adjusting these Metrics will help you get the most out of this powerful feature.",
    "domain": "test.com",
    "hash": "#conclusion",
    "hierarchy": {
      "h0": {
        "title": "Leveraging Online Evaluations for LLM Development with Vellum",
      },
      "h1": {
        "id": "conclusion",
        "title": "Conclusion",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.evaluation.online-evaluations-conclusion-0",
    "org_id": "test",
    "pathname": "/help-center/evaluation/online-evaluations",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Conclusion",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "description": "Learn how to add both private and public custom models to your Vellum workspace for enhanced functionality and domain-specific advantages.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "content": "Vellum comes with a set of Metrics that you can use right away within your Test Suites. We are continually adding new Metrics based on the needs of Vellum users.
Here are the default Metrics currently available within Vellum:",
    "domain": "test.com",
    "hash": "#metrics",
    "hierarchy": {
      "h0": {
        "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
      },
      "h1": {
        "id": "metrics",
        "title": "Metrics",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics-metrics-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "content": "Check that the output is exactly equal to the target.
Returns a score of 1 if the output is an exact match, and 0 otherwise.",
    "domain": "test.com",
    "hash": "#exact-match",
    "hierarchy": {
      "h0": {
        "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
      },
      "h1": {
        "id": "metrics",
        "title": "Metrics",
      },
      "h2": {
        "id": "exact-match",
        "title": "Exact Match",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics-exact-match-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Exact Match",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "content": "Check that the specified regular expression can be found in the output.
Returns a score of 1 if the regular expression matches, and 0 otherwise.
Note that unless the regular expression is explicitly anchored, it can match anywhere in the output.",
    "domain": "test.com",
    "hash": "#regex-match",
    "hierarchy": {
      "h0": {
        "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
      },
      "h1": {
        "id": "metrics",
        "title": "Metrics",
      },
      "h2": {
        "id": "regex-match",
        "title": "Regex Match",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics-regex-match-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Regex Match",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "content": "Check that the output is semantically similar to the target.
Returns a score between 0 and 1, where 1 is a perfect match.
Uses a cross encoder to compute the similarity.",
    "domain": "test.com",
    "hash": "#semantic-similarity",
    "hierarchy": {
      "h0": {
        "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
      },
      "h1": {
        "id": "metrics",
        "title": "Metrics",
      },
      "h2": {
        "id": "semantic-similarity",
        "title": "Semantic Similarity",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics-semantic-similarity-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Semantic Similarity",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "content": "Check that the output is valid JSON.
Returns a score of 1 if the output is valid JSON, and 0 otherwise.




The Metrics below are Ragas
Metrics designed to evaluate your
Retrieval Augmented Generation (RAG) systems. For tips on evaluating your RAG
pipeline in Vellum, check out this help center
article",
    "domain": "test.com",
    "hash": "#json-validity",
    "hierarchy": {
      "h0": {
        "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
      },
      "h1": {
        "id": "metrics",
        "title": "Metrics",
      },
      "h2": {
        "id": "json-validity",
        "title": "JSON Validity",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics-json-validity-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "JSON Validity",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "content": "Faithfulness measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.
For details, see: https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html",
    "domain": "test.com",
    "hash": "#ragas---faithfulness",
    "hierarchy": {
      "h0": {
        "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
      },
      "h1": {
        "id": "metrics",
        "title": "Metrics",
      },
      "h2": {
        "id": "ragas---faithfulness",
        "title": "Ragas - Faithfulness",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics-ragas---faithfulness-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Ragas - Faithfulness",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "content": "The Metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy.
For details, see: https://docs.ragas.io/en/latest/concepts/metrics/answer_relevance.html",
    "domain": "test.com",
    "hash": "#ragas---answer-relevance",
    "hierarchy": {
      "h0": {
        "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
      },
      "h1": {
        "id": "metrics",
        "title": "Metrics",
      },
      "h2": {
        "id": "ragas---answer-relevance",
        "title": "Ragas - Answer Relevance",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics-ragas---answer-relevance-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Ragas - Answer Relevance",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/out-of-the-box-metrics",
    "content": "This Metric gauges the relevancy of the retrieved context, calculated based on both the question and contexts. The values fall within the range of (0, 1), with higher values indicating better relevancy.
For details, see: https://docs.ragas.io/en/v0.1.5/concepts/metrics/context_relevancy.html",
    "domain": "test.com",
    "hash": "#ragas--context-relevancy",
    "hierarchy": {
      "h0": {
        "title": "Evaluate your LLM Workflows with Dozens of Premade Vellum Metrics",
      },
      "h1": {
        "id": "metrics",
        "title": "Metrics",
      },
      "h2": {
        "id": "ragas--context-relevancy",
        "title": "Ragas ‚Äì Context Relevancy",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.out-of-the-box-metrics-ragas--context-relevancy-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/out-of-the-box-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Ragas ‚Äì Context Relevancy",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "content": "In addition to the default Metrics, Vellum makes it easy to define custom Reusable Metrics tailored to your specific business logic and use-case.
This saves you time and ensures standardized evaluation criteria for your Prompts, Workflows, or external entities you'd like to test.
Let's create your first Reusable Metric
Visit the Evaluations tab in Vellum and open the Metrics page

Click the blue Create Metric button at the top-right of the page to open the Create Metric modal

From the Metric type dropdown, select JSON Schema Match. To learn about Metric types other than JSON Schema Match, see Vellum's Available Metric Types.

In the "Label" field at the top left, enter "My First Metric". The "Name" field should autopopulate. This is a unique name that you can use to programmatically identify this Metric.

In the "Description" field, type in "My first Metric description"

Click next to configure your Metric and define what the expected output should match

Add "name" and "email" properties to the JSON schema

Click Finish to exit the modal and see your newly added Metric card on the Metrics page


Congrats! You've now created a Reusable Metric that will be visible when selecting and configuring Metrics within any Test suite.
Create New Reusable Metric",
    "description": "Learn how to create custom Metrics to evaluate your LLM Workflows with ease. Catch edge-cases, prevent regressions, and ship AI features faster with more confidence!",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-root-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Create Custom Reusable Metrics for LLM Evaluation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "content": "Check that the output matches a specified JSON schema.
Returns a score of 1 if the output matches the schema, and 0 otherwise.",
    "domain": "test.com",
    "hash": "#json-schema-match",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h2": {
        "id": "available-metric-types",
        "title": "Available Metric Types",
      },
      "h3": {
        "id": "json-schema-match",
        "title": "JSON Schema Match",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-json-schema-match-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "JSON Schema Match",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "content": "Run a Workflow to evaluate the output.
See Workflow Metric for more details.",
    "domain": "test.com",
    "hash": "#workflow",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h2": {
        "id": "available-metric-types",
        "title": "Available Metric Types",
      },
      "h3": {
        "id": "workflow",
        "title": "Workflow",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-workflow-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Workflow",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "code_snippets": [
      {
        "code": "def main(input_1, input_2, target, completion):
    return {
        "score": 10
    }",
        "lang": "python",
      },
      {
        "code": "def main(input_1, input_2, target, completion):
    return {
        "score": 10
    }",
        "lang": "python",
      },
    ],
    "content": "Run custom Python code to evaluate the output.
The code must include a function named main that takes the function arguments specified when creating the Metric and returns a dictionary with the key score.",
    "domain": "test.com",
    "hash": "#code",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h2": {
        "id": "available-metric-types",
        "title": "Available Metric Types",
      },
      "h3": {
        "id": "code",
        "title": "Code",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-code-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "content": "The Code Execution Metric allows arbitrary Python code execution to be used to produce scores for LLM outputs.
It is intended as a quick and powerful way to format outputs and write conditionals without the restrictions of Jinja or Regex.
After selecting the "Code Execution" Metric in the UI, a code editor will be provided.
There will be a template with the bare minimum for the Metric to run: A main() function that returns a score.",
    "domain": "test.com",
    "hash": "#code-execution-metric",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h1": {
        "id": "code-execution-metric",
        "title": "Code Execution Metric",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-code-execution-metric-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Code Execution Metric",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "code_snippets": [
      {
        "code": "def main(
    completion: str,
    target: str,
) -> dict:
    """Produces a dict containing at least a "score" key with a numerical value."""
    completion_dict = json.loads(completion)
    target_dict = json.loads(target)
    completion_set = set(completion_dict.items())
    target_set = set(target_dict.items())
    is_equal = completion_set == target_set

    return {
        "score": 1.0 if is_equal else 0.0,
    }",
        "lang": "python",
      },
    ],
    "content": "While JSON Validity checks for whether the output is JSON and JSON Schema Match checks if the output conforms to a structure,
neither checks for exact key/value matches per test case. Using the following Python code,
it's possible to check that the output matches a known JSON regardless of order or spacing.",
    "domain": "test.com",
    "hash": "#json-comparison",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h1": {
        "id": "code-execution-metric",
        "title": "Code Execution Metric",
      },
      "h2": {
        "id": "examples",
        "title": "Examples",
      },
      "h3": {
        "id": "json-comparison",
        "title": "JSON Comparison",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-json-comparison-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "JSON Comparison",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "code_snippets": [
      {
        "code": "def main(
    completion: str,
    target: str,
) -> dict:
    """Produces a dict containing at least a "score" key with a numerical value."""
    is_equal = completion.strip() == target.strip()

    return {
        "score": 1.0 if is_equal else 0.0,
    }",
        "lang": "python",
      },
    ],
    "content": "A common problem with exact match comparison using LLM outputs is that often there is additional leading or trailing whitespace.
We can create an exact match Metric that ignores such whitespace with a few short lines of Python.",
    "domain": "test.com",
    "hash": "#ignore-whitespace",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h1": {
        "id": "code-execution-metric",
        "title": "Code Execution Metric",
      },
      "h2": {
        "id": "examples",
        "title": "Examples",
      },
      "h3": {
        "id": "ignore-whitespace",
        "title": "Ignore Whitespace",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-ignore-whitespace-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Ignore Whitespace",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "code_snippets": [
      {
        "code": "import * as _ from "lodash"

async function main(variables: {
  completion: string,
  target: string,
}): Promise<{ score: number }> {
  return {
    score: variables.target.length + _.floor(7.55),
  }
}",
        "lang": "typescript",
      },
    ],
    "content": "You can add both pip packages for Python code and npm packages for TypeScript code. You must provide exact package versions and add the import to your code yourself.
Note that whenever you update your packages list, the first execution after doing so may be slow due to our system creating and caching the custom runtime.
Code Package example",
    "domain": "test.com",
    "hash": "#code-packages",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h1": {
        "id": "code-execution-metric",
        "title": "Code Execution Metric",
      },
      "h2": {
        "id": "examples",
        "title": "Examples",
      },
      "h3": {
        "id": "code-packages",
        "title": "Code Packages",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-code-packages-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Code Packages",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "content": "The Workflow Metric allows you to use a Workflow to evaluate outputs, allowing LLM based evaluation for outputs that may be hard to score via traditional methods.",
    "domain": "test.com",
    "hash": "#workflow-metric-using-llms-to-evaluate-llms",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h1": {
        "id": "workflow-metric-using-llms-to-evaluate-llms",
        "title": "Workflow Metric (using LLMs to evaluate LLMs)",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-workflow-metric-using-llms-to-evaluate-llms-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Workflow Metric (using LLMs to evaluate LLMs)",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "content": "We've built a Metric that you can use in your test suites to evaluate the outputs of your Prompts and LLMs using another LLM. This example simply takes a rubric, or set of rules, outputs a 1 if the output passes the criteria in the rubric, or a 0 if it does not, and also outputs a reason for the provided score. You can extend this to give scores between 0 and 1, or to provide more detailed feedback.








Add the Metric to your test suite and add a new input which we'll use to tell the Metric how it should score the output that it's evaluating.




Option #1: Different assertions on different test cases
Add a new input to your test suite, connect it to this Metric, and use different rules in that input for each test case.  For example: one row checks that the user is addressed by name during introductions, but this isn't a condition we'd want to test on every test case. It will ultimately depend how you split up your Workflows and Prompts (unit testing vs. integration testing).
Option #2: Same assertion on every row
Add multiple copies of this Metric to your test suite, rename each one according to its purpose, and hardcode a different rule for each.  For example: one could check that every output of a Q&A bot cites a source, another could check that every output of a Math Assistant shows its work.


Avoid putting too many rules in a single scoring rubric. Split into multiple Metrics if you have many rules and notice your evaluations aren't performing well.",
    "domain": "test.com",
    "hash": "#the-generic-llm-metric",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h1": {
        "id": "workflow-metric-using-llms-to-evaluate-llms",
        "title": "Workflow Metric (using LLMs to evaluate LLMs)",
      },
      "h2": {
        "id": "the-generic-llm-metric",
        "title": "The Generic LLM Metric",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-the-generic-llm-metric-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "The Generic LLM Metric",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/metrics",
        "title": "Metrics",
      },
    ],
    "canonicalPathname": "/help-center/metrics/custom-metrics",
    "content": "Create a new Workflow Sandbox.

Add one input variable for each Test Suite variable you want to pass to the Workflow.
You'll map these to the Test Suite variables when setting up the Metric later, so you can name them anything you want.
Examples of variables you may want to include: the output to be evaluated, the desired output, the inputs to the evaluated prompt.

Create a Final Output, set the name to score, and set the output type to Number.

[Optional] - create additional outputs to provide more context about the score ("rationale" or "summary" or "chat history" etc.). Great for debugging!

Fill in the logic of your Workflow!

Deploy your Workflow using the Deploy button in the top right corner of the Workflow Sandbox.",
    "domain": "test.com",
    "hash": "#setting-up-an-metric-workflow",
    "hierarchy": {
      "h0": {
        "title": "Create Custom Reusable Metrics for LLM Evaluation",
      },
      "h1": {
        "id": "workflow-metric-using-llms-to-evaluate-llms",
        "title": "Workflow Metric (using LLMs to evaluate LLMs)",
      },
      "h2": {
        "id": "setting-up-an-metric-workflow",
        "title": "Setting up an Metric Workflow",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.metrics.custom-metrics-setting-up-an-metric-workflow-0",
    "org_id": "test",
    "pathname": "/help-center/metrics/custom-metrics",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Setting up an Metric Workflow",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/prompt-lifecycle-management",
    "description": "Discover how Vellum simplifies prompt deployment with observability, version control, and easy integration for better performance.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.prompt-lifecycle-management",
    "org_id": "test",
    "pathname": "/help-center/deployments/prompt-lifecycle-management",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Streamline Your Prompt Deployment with Vellum",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/prompt-lifecycle-management",
    "content": "Now that you‚Äôve used Vellum Playground for prompt engineering and have a prompt that clears your test cases, you‚Äôre ready to start making requests against it. In production, Vellum acts as a high reliability, low latency proxy between your application and the underlying model provider.
By deploying a Prompt through Vellum and integrating a 10-line code snippet you get:
Observability into individual completions and their quality: Tracking completions & measuring quality

Version Controlled changes to prompts/model without updating code: Changing prompts in production & versioning

Request Replay to back-test changes and avoid regressions: Backtesting with Vellum

Monitoring of aggregate data to spot trends: Monitoring production traffic


Let's take a look at how to actually deploy a Prompt in Vellum",
    "domain": "test.com",
    "hash": "#introduction-to-prompt-deployments",
    "hierarchy": {
      "h0": {
        "title": "Streamline Your Prompt Deployment with Vellum",
      },
      "h2": {
        "id": "introduction-to-prompt-deployments",
        "title": "Introduction to Prompt Deployments",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.prompt-lifecycle-management-introduction-to-prompt-deployments-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/prompt-lifecycle-management",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Introduction to Prompt Deployments",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/prompt-lifecycle-management",
    "content": "Deploy Prompt Button
Deploy Prompt Options",
    "domain": "test.com",
    "hash": "#creating-a-prompt-deployment",
    "hierarchy": {
      "h0": {
        "title": "Streamline Your Prompt Deployment with Vellum",
      },
      "h2": {
        "id": "creating-a-prompt-deployment",
        "title": "Creating a Prompt Deployment",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.prompt-lifecycle-management-creating-a-prompt-deployment-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/prompt-lifecycle-management",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Creating a Prompt Deployment",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/prompt-lifecycle-management",
    "content": "The Deployment Overview page shows you details about the currently live version of the Prompt.
Prompt Deployment Details",
    "domain": "test.com",
    "hash": "#view-deployment-details",
    "hierarchy": {
      "h0": {
        "title": "Streamline Your Prompt Deployment with Vellum",
      },
      "h2": {
        "id": "view-deployment-details",
        "title": "View Deployment Details",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.prompt-lifecycle-management-view-deployment-details-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/prompt-lifecycle-management",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "View Deployment Details",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/prompt-lifecycle-management",
    "content": "The Deployment Overview page also contains code snippets to make integration simple. We support Python & Typescript clients and have an option to make Curl requests. Optionally, you can also integrate with our Actuals Endpoint to start keeping track of output quality for monitoring and eventually fine tuning. More details about this in the completions & quality help center article.
Generate API Code Snippet
Note that our full API docs can be found at docs.vellum.ai",
    "domain": "test.com",
    "hash": "#integrating-w-vellums-api",
    "hierarchy": {
      "h0": {
        "title": "Streamline Your Prompt Deployment with Vellum",
      },
      "h2": {
        "id": "integrating-w-vellums-api",
        "title": "Integrating w/ Vellum's API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.prompt-lifecycle-management-integrating-w-vellums-api-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/prompt-lifecycle-management",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Integrating w/ Vellum's API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/prompt-lifecycle-management",
    "content": "With Vellum, you can make changes to your prompts in production without having to make any code changes! This might be useful for a variety of reasons:
When you encounter edge cases in production, you may want to tweak the prompt to accommodate for them

A new model comes out and can provide similar quality at lower cost or lower latency

Product requirements change and a non-technical member of the team with the proper permissions wants to make changes


You can do this by updating a Prompt Deployment. All updates are version-controlled and past versions can be immediately reverted to at any time (no code chnages required).",
    "domain": "test.com",
    "hash": "#changing-prompts-in-production",
    "hierarchy": {
      "h0": {
        "title": "Streamline Your Prompt Deployment with Vellum",
      },
      "h2": {
        "id": "changing-prompts-in-production",
        "title": "Changing Prompts in Production",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.prompt-lifecycle-management-changing-prompts-in-production-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/prompt-lifecycle-management",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Changing Prompts in Production",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/prompt-lifecycle-management",
    "content": "Find the Prompt Sandbox you'd like to deploy and click the "Deploy" button.
Deploy Prompt Button
This'll provide the option to update an existing deployment or create a new one. Select "Update Existing Deployment" and choose the deployment you'd like to update.
Update Deployment Option


Note that code changes will likely be required if you change which input variables the Prompt replies on.",
    "domain": "test.com",
    "hash": "#updating-a-prompt-deployment",
    "hierarchy": {
      "h0": {
        "title": "Streamline Your Prompt Deployment with Vellum",
      },
      "h2": {
        "id": "changing-prompts-in-production",
        "title": "Changing Prompts in Production",
      },
      "h3": {
        "id": "updating-a-prompt-deployment",
        "title": "Updating a Prompt Deployment",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.prompt-lifecycle-management-updating-a-prompt-deployment-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/prompt-lifecycle-management",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Updating a Prompt Deployment",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/prompt-lifecycle-management",
    "content": "After a Prompt Deployment is updated, you'll find a new entry in the "History" tab. You can visually inspect how the Prompt has changed
over time across versions. You can also revert to prior versions at any time. After reverting to a prior version, it's immediately live ‚Äì
no code changes required.
Prompt Versioning",
    "domain": "test.com",
    "hash": "#prompt-versioning",
    "hierarchy": {
      "h0": {
        "title": "Streamline Your Prompt Deployment with Vellum",
      },
      "h2": {
        "id": "prompt-versioning",
        "title": "Prompt Versioning",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.prompt-lifecycle-management-prompt-versioning-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/prompt-lifecycle-management",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Prompt Versioning",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/observability",
    "content": "After using a Prompt in prompt with your application, you'll likely wonder what the contents of the requests were and whether the model provided reasonable responses. A big benefit of using Vellum's proxy layer via Prompt Deployments is that we automatically keep track of every request and the details you need to debug issues.",
    "description": "Discover how Vellum's observability in production helps track requests and improve AI model responses with user feedback integration.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.observability-root-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/observability",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Enhance AI Model Accuracy with Vellum's Observability Tools",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/observability",
    "content": "You can go to the "Completions" tab of any prompt Deployment to see the requests that were made. Columns can be hidden, shown, filtered, and sorted.
Completions
As you apply filters/sorting, the page's url is updated. You can bookmark this link or share with others to return to the same view later.
Completion Columns",
    "domain": "test.com",
    "hash": "#prompt-deployment-completions",
    "hierarchy": {
      "h0": {
        "title": "Enhance AI Model Accuracy with Vellum's Observability Tools",
      },
      "h2": {
        "id": "prompt-deployment-completions",
        "title": "Prompt Deployment Completions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.observability-prompt-deployment-completions-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/observability",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Prompt Deployment Completions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/observability",
    "content": "Vellum has the concept of "Completion Actuals" where you can say, for a given request, what the output should have been and what its quality was.
This is particularly using for monitoring quality, and later, for usage as training data to fine-tune your own custom model.
Capturing Actuals works best if your end users have some mechanism (usually via a UI) to provide feedback on the output of the model.
For example, you're creating an AI Recruiting Email Generator for recruiters where they can use AI to generate rough draft, you might:
Infer that if they hit "Send" without making edits, the quality was great (a 1.0 in Vellum)

Infer that if they hit "Discard" then the quality was bad (a 0.0 in Vellum)

Or you might have a 5-star "Rating" system that they can use to explicitly provide feedback on the quality of the output.


In all cases, you could integrate with Vellum's Completion Actuals API to capture this feedback. You can find a code snippet for this in a Prompt Deployment's "Overview" tab. It'll look like this:
Deployment Actuals
Note that you reference a Completion made previously by the ID that Vellum generates and returns, or by some UUID that you track and provide via the "external_id" property.",
    "domain": "test.com",
    "hash": "#capturing-end-user-feedback",
    "hierarchy": {
      "h0": {
        "title": "Enhance AI Model Accuracy with Vellum's Observability Tools",
      },
      "h2": {
        "id": "capturing-end-user-feedback",
        "title": "Capturing End-User Feedback",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.observability-capturing-end-user-feedback-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/observability",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Capturing End-User Feedback",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/monitoring",
    "content": "All the row-level Completions found in the "Completions" tab of a Prompt Deployment can
be monitored in aggregate via the "Monitoring" tab.
This is especially useful for spotting trends in things like request volume, latency,
quality, and more. If there are other visualizations you'd like to see here, please share that feedback with us!
The charts you see can be filtered down to specific time ranges using the ‚ÄúRelative Date‚Äù button.
Prompt Deployment Monitoring
Number of Completions: Number of requests made against the Generate endpoint
Average Quality over Time: Quality tracked for each completion. This is only
visible if Quality is filled out either through the UI or Actuals Endpoint API
Number of Completions w/ Actuals Submitted: Number of requests that have an
associated quality / Actuals indication
Average Latency Over Time: Time taken for the request to complete
Num LLM Provider Errors Over Time: Number of errors from the LLM provider",
    "description": "Easily monitor request volume, latency, and quality trends in your Prompt Deployment with our intuitive charts and filters.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.monitoring-root-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/monitoring",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Track Production Trends with Prompt Deployment Monitoring",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "description": "Discover how to manage Vellum deployments with release tags for better version control in your development workflow.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "content": "As you start having many versions of a Prompt or Workflow, being able to manage these deployment releases becomes an important consideration.
In Vellum, you have multiple ways to manage releases so you can easily promote prompt / workflow changes from your development or staging environments into production with as much flexibility or control as required by your team.
Let's take a look at some of the different release strategies in Vellum!",
    "domain": "test.com",
    "hash": "#introduction-to-deployment-release-tags",
    "hierarchy": {
      "h0": {
        "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
      },
      "h2": {
        "id": "introduction-to-deployment-release-tags",
        "title": "Introduction to Deployment Release Tags",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases-introduction-to-deployment-release-tags-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Introduction to Deployment Release Tags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "content": "This page highlights Prompt Deployments but the same holds true for managing
your Workflow Deployment releases",
    "domain": "test.com",
    "hash": "#tldr-watch-a-video-walkthrough-instead",
    "hierarchy": {
      "h0": {
        "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
      },
      "h2": {
        "id": "tldr-watch-a-video-walkthrough-instead",
        "title": "TL;DR Watch a Video Walkthrough Instead",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases-tldr-watch-a-video-walkthrough-instead-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "TL;DR Watch a Video Walkthrough Instead",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "content": "Every time you create a new Prompt or Workflow Deployment or update an existing one, Vellum will automatically assign it as latest.
In this approach, you always point to the latest prompt or workflow available for a given Deployment by making your Vellum API requests without providing a specific Release Tag.
This method simplifies your deployment process by automatically incorporating any changes that you deploy from your Prompt or Workflow Sandbox.",
    "domain": "test.com",
    "hash": "#1-dont-specify-a-release-tag-and-always-use-latest",
    "hierarchy": {
      "h0": {
        "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
      },
      "h2": {
        "id": "three-ways-to-release-prompts-or-workflows",
        "title": "Three Ways to Release Prompts or Workflows",
      },
      "h3": {
        "id": "1-dont-specify-a-release-tag-and-always-use-latest",
        "title": "1. Don‚Äôt Specify a Release Tag and Always Use Latest",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases-1-dont-specify-a-release-tag-and-always-use-latest-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Don‚Äôt Specify a Release Tag and Always Use Latest",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "content": "For more flexibility, Vellum lets you define Custom Release Tags. Custom Release Tags are floating tags that you define and can be moved by authorized users directly within the Vellum app.
You can assign multiple Custom Release Tags to a single Prompt or Workflow Deployment, providing flexibility to granularly assign and update versions across different environments.
For example, you can assign Custom Release Tags to point to a version of your prompt in both your ‚ÄúProduction‚Äù and ‚ÄúStaging‚Äù environments.
This method offers you the ability to manage your releases according to your own specific workflow and versioning requirements.


Note that Custom Release Tags follow a specific format: less than or equal to
150 characters, begin and end with a Regex word character (i.e., letter,
number, or underscore), contain only word characters, dashes or dots, and do
not contain more than one dot in a row",
    "domain": "test.com",
    "hash": "#2-use-custom-release-tags",
    "hierarchy": {
      "h0": {
        "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
      },
      "h2": {
        "id": "three-ways-to-release-prompts-or-workflows",
        "title": "Three Ways to Release Prompts or Workflows",
      },
      "h3": {
        "id": "2-use-custom-release-tags",
        "title": "2. Use Custom Release Tags",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases-2-use-custom-release-tags-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Use Custom Release Tags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "content": "Every time you create a new Deployment or update an existing one, Vellum creates a unique Static Release Tag tied to that version.
When using Static Release Tags to manage your releases, you can control version increments through code to ensure a structured and controlled release process.
This method is ideal if your organization prioritizes strict version management and you want to avoid accidental updates or changes to prompts.
Release Tags Types",
    "domain": "test.com",
    "hash": "#3-use-static-release-tags",
    "hierarchy": {
      "h0": {
        "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
      },
      "h2": {
        "id": "three-ways-to-release-prompts-or-workflows",
        "title": "Three Ways to Release Prompts or Workflows",
      },
      "h3": {
        "id": "3-use-static-release-tags",
        "title": "3. Use Static Release Tags",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases-3-use-static-release-tags-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Use Static Release Tags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "content": "Release tags can be created:
Within the Create New or Update Existing tabs of the Deploy Prompt modal in a Prompt Sandbox or Deploy Workflow modal in a Workflow Sandbox
Creating Release Tags from Deploy Modal

By opening the Assign Release Tags modal from the Releases tab of the Deployment Details page",
    "domain": "test.com",
    "hash": "#creating-a-custom-release-tag",
    "hierarchy": {
      "h0": {
        "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
      },
      "h2": {
        "id": "creating-a-custom-release-tag",
        "title": "Creating a Custom Release Tag",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases-creating-a-custom-release-tag-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Creating a Custom Release Tag",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "content": "Release tags can be updated:
Within the Update Existing tab of the Deploy Prompt modal in a Prompt Sandbox or Deploy Workflow modal in a Workflow Sandbox
Updating Release Tags from Deploy Modal

By opening the Assign Release Tags modal from the Releases tab of the Deployment Details page
Creating Release Tags Post Deployment",
    "domain": "test.com",
    "hash": "#updating-a-custom-release-tag",
    "hierarchy": {
      "h0": {
        "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
      },
      "h2": {
        "id": "updating-a-custom-release-tag",
        "title": "Updating a Custom Release Tag",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases-updating-a-custom-release-tag-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Updating a Custom Release Tag",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/deployments",
        "title": "Deployments",
      },
    ],
    "canonicalPathname": "/help-center/deployments/managing-releases",
    "code_snippets": [
      {
        "code": "curl -X POST \
--url "https://predict.vellum.ai/v1/execute-prompt" \
--header "Content-Type: application/json" \
--header "X_API_KEY: $VELLUM_API_KEY" \
--header "Accept: application/json" \
--data '
{
  "inputs": [
    {
      "type": "STRING",
      "name": "string",
      "value": "string"
    }
  ],
  ‚Äúrelease_tag‚Äù: ‚Äústaging‚Äù
}
  '
",
      },
      {
        "code": "curl -X POST \
--url "https://predict.vellum.ai/v1/execute-prompt" \
--header "Content-Type: application/json" \
--header "X_API_KEY: $VELLUM_API_KEY" \
--header "Accept: application/json" \
--data '
{
  "inputs": [
    {
      "type": "STRING",
      "name": "string",
      "value": "string"
    }
  ],
  ‚Äúrelease_tag‚Äù: ‚Äústaging‚Äù
}
  '
",
      },
      {
        "code": "",
      },
    ],
    "content": "Both the Vellum execute_prompt and execute_workflow API‚Äôs accept an optional release_tag parameter to pin your request to a specific release tag. When release tags are updated via the Vellum app to point to new versions of a prompt or workflow, these changes will automatically reflect in these requests.
If no release_tag parameter is provided, the request will default to the latest version of that Deployment.
More details on how to do this: Vellum API Docs",
    "domain": "test.com",
    "hash": "#pinning-to-a-release-tag-in-code",
    "hierarchy": {
      "h0": {
        "title": "Mastering Vellum Deployment ‚Äì Release Tags Explained",
      },
      "h2": {
        "id": "updating-a-custom-release-tag",
        "title": "Updating a Custom Release Tag",
      },
      "h3": {
        "id": "pinning-to-a-release-tag-in-code",
        "title": "Pinning to a Release Tag in Code",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.deployments.managing-releases-pinning-to-a-release-tag-in-code-0",
    "org_id": "test",
    "pathname": "/help-center/deployments/managing-releases",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Pinning to a Release Tag in Code",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/uploading-documents",
    "content": "Any document that you want to query against should be uploaded ahead
of time at https://app.vellum.ai/document-indexes.",
    "description": "Learn how to upload and manage documents on Vellum AI for efficient document indexing and searching. Supports multiple file types.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.uploading-documents-root-0",
    "org_id": "test",
    "pathname": "/help-center/documents/uploading-documents",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Easy Guide to Uploading Documents on Vellum AI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/uploading-documents",
    "content": "Document indexes act as a collection of documents grouped together
for performing searches against for a specific use case. For example,
if you are creating a chatbot to query against OpenAI‚Äôs help center
documents, the text files of each article in the help center would be
stored in one index. Here's how it looks in Vellum's UI:
Document Details",
    "domain": "test.com",
    "hash": "#what-is-a-document-index",
    "hierarchy": {
      "h0": {
        "title": "Easy Guide to Uploading Documents on Vellum AI",
      },
      "h2": {
        "id": "what-is-a-document-index",
        "title": "What is a Document Index?",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.uploading-documents-what-is-a-document-index-0",
    "org_id": "test",
    "pathname": "/help-center/documents/uploading-documents",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "What is a Document Index?",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/uploading-documents",
    "content": "You can manually upload files through the UI
or via API.
Upload Documents
Each document has a Name and an External ID which are
initially populated with the name of the file that you upload.
Name - Human readable text which is how the document will be visible in Vellum's UI (in documents tab)
External ID - As the contents of a document change and the old documents becomes out of date, you can submit the updated document for reindexing re-uploading it and specifying the same External ID.",
    "domain": "test.com",
    "hash": "#how-to-upload-documents",
    "hierarchy": {
      "h0": {
        "title": "Easy Guide to Uploading Documents on Vellum AI",
      },
      "h2": {
        "id": "how-to-upload-documents",
        "title": "How to upload documents?",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.uploading-documents-how-to-upload-documents-0",
    "org_id": "test",
    "pathname": "/help-center/documents/uploading-documents",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "How to upload documents?",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/uploading-documents",
    "content": "In addition to sending plain strings via API, Vellum also supports uploading files of the following types:
.csv

.doc

.docx

.pdf

.png

.txt

.xls

.xlsx


For .pdf and .png files, we apply an OCR process to convert the file to a text representation. If you need another file type, please reach out!",
    "domain": "test.com",
    "hash": "#supported-file-types",
    "hierarchy": {
      "h0": {
        "title": "Easy Guide to Uploading Documents on Vellum AI",
      },
      "h2": {
        "id": "how-to-upload-documents",
        "title": "How to upload documents?",
      },
      "h3": {
        "id": "supported-file-types",
        "title": "Supported File Types",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.uploading-documents-supported-file-types-0",
    "org_id": "test",
    "pathname": "/help-center/documents/uploading-documents",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Supported File Types",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/uploading-documents",
    "content": "Each document can be up to 32MB and 2.5M characters",
    "domain": "test.com",
    "hash": "#document-size-limits",
    "hierarchy": {
      "h0": {
        "title": "Easy Guide to Uploading Documents on Vellum AI",
      },
      "h2": {
        "id": "how-to-upload-documents",
        "title": "How to upload documents?",
      },
      "h3": {
        "id": "document-size-limits",
        "title": "Document Size Limits",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.uploading-documents-document-size-limits-0",
    "org_id": "test",
    "pathname": "/help-center/documents/uploading-documents",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Document Size Limits",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/uploading-documents",
    "content": "Vellum currently uses a static chunking strategy.
Chunking strategy: Overlapping windows w/ sentence splitting
Min overlap: 50%
Max characters: 1000
This configuration has proven to work well for most use cases. These settings will become configurable in future updates. Please reach out to support@vellum.ai if this chunking strategy doesn‚Äôt work for you and we can work on a solution for you.",
    "domain": "test.com",
    "hash": "#out-of-box-chunking-strategy",
    "hierarchy": {
      "h0": {
        "title": "Easy Guide to Uploading Documents on Vellum AI",
      },
      "h2": {
        "id": "out-of-box-chunking-strategy",
        "title": "Out-of-box Chunking Strategy",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.uploading-documents-out-of-box-chunking-strategy-0",
    "org_id": "test",
    "pathname": "/help-center/documents/uploading-documents",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Out-of-box Chunking Strategy",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/running-searches",
    "code_snippets": [
      {
        "code": "Answer questions based on the context provided below without
using any other knowledge. If the question can't be answered
using the provided context say "Sorry, I don't know."
Answer in the following format:

Question: ..
Answer: ..

---
{context_str}
---
Question: {query_str}
Answer:",
      },
    ],
    "content": "Vellum Search can be used to include relevant context in a prompt at run-time that fits within LLM token window limits. Typically, the query that produces the search results comes from an end-user.
For example, here‚Äôs a prompt used to answer questions, pulling the source materials for an answer from a document index. The remainder of this article references variables from this prompt to explain the mechanics.",
    "description": "Learn to add relevant context to your searches in Vellum Playground with simple steps for better query results.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.running-searches-root-0",
    "org_id": "test",
    "pathname": "/help-center/documents/running-searches",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Using Search to retrieve context in a prompt",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/running-searches",
    "content": "In Vellum Playground, each variable has a üîç icon to include search results in Playground
Prompt Search Icon
Clicking this button opens up a modal to return Search results for a given query. Follow these steps in the modal:
Enter the Document Index that should be queried against

Type the search query (note: this should be the same as the query_str in the prompt variable)

Choose the number of chunks to be returned, the separators between each chunk and hit Run. By default, Vellum returns 3 chunks and uses new lines as separators.

Clicking Apply adds these results to the context_str variable


Prompt Search Dialog",
    "domain": "test.com",
    "hash": "#step-1-how-to-add-relevant-context-to-context_str-variable",
    "hierarchy": {
      "h0": {
        "title": "Using Search to retrieve context in a prompt",
      },
      "h2": {
        "id": "step-1-how-to-add-relevant-context-to-context_str-variable",
        "title": "Step 1: How to add relevant context to context_str variable",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.running-searches-step-1-how-to-add-relevant-context-to-context_str-variable-0",
    "org_id": "test",
    "pathname": "/help-center/documents/running-searches",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 1: How to add relevant context to context_str variable",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/running-searches",
    "content": "Copy/paste the same user query into the query_str variable. Hit Run on the Playground and see the results against a variety of prompts.
Prompt Search Results",
    "domain": "test.com",
    "hash": "#step-2-how-to-use-the-context-to-get-results",
    "hierarchy": {
      "h0": {
        "title": "Using Search to retrieve context in a prompt",
      },
      "h2": {
        "id": "step-2-how-to-use-the-context-to-get-results",
        "title": "Step 2: How to use the context to get results",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.running-searches-step-2-how-to-use-the-context-to-get-results-0",
    "org_id": "test",
    "pathname": "/help-center/documents/running-searches",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Step 2: How to use the context to get results",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/api-integration",
    "content": "Once in production, there‚Äôs a 3 step process to add search results in your queries at run-time:
Call Search API to obtain relevant context (details below)

Format the returned context and include as a single variable value when making requests to a Vellum Deployment

Pass search results to request endpoint while calling the LLM


For Step 3, make sure you have a variable in Vellum Playground where search results are entered. Details to set that up are here: Running Searches in Playground",
    "description": "Learn to integrate search results in your queries with our easy 3-step process, including API calls and formatting tips.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.api-integration-root-0",
    "org_id": "test",
    "pathname": "/help-center/documents/api-integration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Easily integrate with Velum‚Äôs Search API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/api-integration",
    "content": "There‚Äôs a code snippet for the Search API in the Document Index. There are 3 variables to call the API:
index_name - Index that is searched across

query - Search query (usually a user input)

options - Optional configuration that drives search behavior. Namely used to
determine the max number of results returned in the response. You can also use:

weights - to change the prioritization between keyword matches vs semantic similarity

result_merging - to automatically merge overlapping chunks into larger chunks without redundant content

filters - to perform rule-based filtering prior to matching on keywords / semantic similarity.
For more info, see Metadata Filtering


Search API Code Snippet",
    "domain": "test.com",
    "hash": "#search-api",
    "hierarchy": {
      "h0": {
        "title": "Easily integrate with Velum‚Äôs Search API",
      },
      "h2": {
        "id": "search-api",
        "title": "Search API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.api-integration-search-api-0",
    "org_id": "test",
    "pathname": "/help-center/documents/api-integration",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Search API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/metadata-filtering",
    "content": "Some use-cases of Vellum Search require you to narrow in on a subset of documents prior to searching based on keyword match / semantic similarity. For example, you might want to search across historical conversations for a specific user or only across documents that have specific tags.
You can do this through metadata filtering.
Metadata filtering requires that you:
Provide structured metadata for your documents either upon initial upload or later; and

Provide filter criteria when performing a search.


Let‚Äôs see how to do each.",
    "description": "Learn how to refine searches using metadata filtering for precise document retrieval. Perfect for targeted searches.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.metadata-filtering-root-0",
    "org_id": "test",
    "pathname": "/help-center/documents/metadata-filtering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Improve Retrieval Results with Metadata Filtering",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/metadata-filtering",
    "content": "You can specify metadata for documents through both the UI and API.",
    "domain": "test.com",
    "hash": "#specifying-metadata",
    "hierarchy": {
      "h0": {
        "title": "Improve Retrieval Results with Metadata Filtering",
      },
      "h1": {
        "id": "specifying-metadata",
        "title": "Specifying Metadata",
      },
    },
    "level": "h1",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.metadata-filtering-specifying-metadata-0",
    "org_id": "test",
    "pathname": "/help-center/documents/metadata-filtering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Specifying Metadata",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/metadata-filtering",
    "content": "You can provide metadata upon initial upload.
Metadata Specification
You can also view metadata associated with a document and edit it after it‚Äôs been uploaded.
Viewing Metadata",
    "domain": "test.com",
    "hash": "#through-the-ui",
    "hierarchy": {
      "h0": {
        "title": "Improve Retrieval Results with Metadata Filtering",
      },
      "h1": {
        "id": "specifying-metadata",
        "title": "Specifying Metadata",
      },
      "h3": {
        "id": "through-the-ui",
        "title": "Through the UI",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.metadata-filtering-through-the-ui-0",
    "org_id": "test",
    "pathname": "/help-center/documents/metadata-filtering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Through the UI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/metadata-filtering",
    "content": "You can provide metadata as stringified JSON upon initial upload using the upload Documents API here.
You can also update a document‚Äôs metadata after-the-fact using the the Document - Partial Update endpoint here.
Note that in this endpoint, you can simply provide a JSON object (rather than a stringified JSON object as is required during initial upload).",
    "domain": "test.com",
    "hash": "#through-the-api",
    "hierarchy": {
      "h0": {
        "title": "Improve Retrieval Results with Metadata Filtering",
      },
      "h1": {
        "id": "specifying-metadata",
        "title": "Specifying Metadata",
      },
      "h3": {
        "id": "through-the-api",
        "title": "Through the API",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.metadata-filtering-through-the-api-0",
    "org_id": "test",
    "pathname": "/help-center/documents/metadata-filtering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Through the API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/metadata-filtering",
    "content": "You use the search endpoint to perform a search against an index (documented here). This endpoint exposes an options.filters.metadata field for filtering against your provided metadata prior to matching on keywords/semantic similarity.
The syntax of the metadata property supports complex boolean logic and was borrowed from React Query Builder. You can use their demo here to get a feel for the query syntax.
Note that values for fields must be JSON-deserializable. If you're looking to filter against a string, then the value passed in should contain escaped double quotes.",
    "domain": "test.com",
    "hash": "#filtering-against-metadata",
    "hierarchy": {
      "h0": {
        "title": "Improve Retrieval Results with Metadata Filtering",
      },
      "h1": {
        "id": "specifying-metadata",
        "title": "Specifying Metadata",
      },
      "h2": {
        "id": "filtering-against-metadata",
        "title": "Filtering Against Metadata",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.metadata-filtering-filtering-against-metadata-0",
    "org_id": "test",
    "pathname": "/help-center/documents/metadata-filtering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Filtering Against Metadata",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/help-center/documents/metadata-filtering",
    "code_snippets": [
      {
        "code": "// Document A
{
	"tags": [
		"customer-facing", "needs-triage", "bug"
	],
	"priority": "high"
}
// Document B
{
	"tags": [
		"needs-triage", "bug"
	],
	"priority": "low"
}",
        "lang": "json",
      },
      {
        "code": "{
		...,
		"options": {
			"filters": {
				"metadata": {
					"combinator": "AND",
					"rules": [
						{
							"field": "tags",
							"operator": "contains",
							"value": "\"customer-facing\""
						},
						{
							"field": "tags",
							"operator": "contains",
							"value": "\"bug\""
						},
						{
							"priority": "tags",
							"operator": "+",
							"value": "high"
						}
					],
					"negated": false
				}
			}
		}
}",
        "lang": "json",
      },
    ],
    "content": "Suppose you have two documents with the following metadata:
And you wanted to perform a search across all documents that are marked as high priority, customer-facing bugs, you would use the following query:",
    "domain": "test.com",
    "hash": "#example",
    "hierarchy": {
      "h0": {
        "title": "Improve Retrieval Results with Metadata Filtering",
      },
      "h1": {
        "id": "specifying-metadata",
        "title": "Specifying Metadata",
      },
      "h2": {
        "id": "filtering-against-metadata",
        "title": "Filtering Against Metadata",
      },
      "h3": {
        "id": "example",
        "title": "Example",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.help-center.help-center.documents.metadata-filtering-example-0",
    "org_id": "test",
    "pathname": "/help-center/documents/metadata-filtering",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Example",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/security",
        "title": "Security",
      },
    ],
    "canonicalPathname": "/help-center/security/hmac-authentication",
    "content": "This guide will walk you through the process of setting up and using HMAC authentication in Vellum. HMAC authentication provides an additional layer of security for outgoing API calls and webhooks.",
    "description": "This guide will walk you through the process of setting up and using HMAC authentication in Vellum.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.help-center.help-center.security.hmac-authentication-root-0",
    "org_id": "test",
    "pathname": "/help-center/security/hmac-authentication",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "HMAC Authetication",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/security",
        "title": "Security",
      },
    ],
    "canonicalPathname": "/help-center/security/hmac-authentication",
    "code_snippets": [
      {
        "code": "import secrets
print(secrets.token_hex(16))",
        "lang": "python",
      },
      {
        "code": "import secrets
print(secrets.token_hex(16))",
        "lang": "python",
      },
    ],
    "content": "Create a new secret token securely: You can do this in Python using the secrets module. Here's a simple example:




Provide your secret token to Vellum: Navigate to the API keys page. Click the "Provide HMAC Token" button and enter your secret token.",
    "domain": "test.com",
    "hash": "#setup",
    "hierarchy": {
      "h0": {
        "title": "HMAC Authetication",
      },
      "h2": {
        "id": "setup",
        "title": "Setup",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.security.hmac-authentication-setup-0",
    "org_id": "test",
    "pathname": "/help-center/security/hmac-authentication",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Setup",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/help-center/security",
        "title": "Security",
      },
    ],
    "canonicalPathname": "/help-center/security/hmac-authentication",
    "code_snippets": [
      {
        "code": "message = f"{timestamp}\n{method}\n{url}\n{body}"",
        "lang": "python",
      },
      {
        "code": "message = f"{timestamp}\n{method}\n{url}\n{body}"",
        "lang": "python",
      },
      {
        "code": "import hmac
import hashlib

def verify(message: str, secret: str, signature: str) -> bool:
    hash_object = hmac.new(secret.encode(), msg=message.encode(), digestmod=hashlib.sha256)
    expected_signature = hash_object.hexdigest()
    return hmac.compare_digest(expected_signature, signature)",
        "lang": "python",
      },
      {
        "code": "import hmac
import hashlib

def verify(message: str, secret: str, signature: str) -> bool:
    hash_object = hmac.new(secret.encode(), msg=message.encode(), digestmod=hashlib.sha256)
    expected_signature = hash_object.hexdigest()
    return hmac.compare_digest(expected_signature, signature)",
        "lang": "python",
      },
    ],
    "content": "Only outgoing webhooks and API calls from Vellum include HMAC authentication.
Each request will contain two headers: X-Vellum-Timestamp and X-Vellum-Signature.
Verify the timestamp: Check that the value of X-Vellum-Timestamp is within the last 60 seconds.

Create the message string: Concatenate the following values together, separated by one newline character, into a new string message:
X-Vellum-Timestamp

The request method (GET, POST, etc)

The request URL

The request body






Verify the signature. Use the HMAC algorithm with SHA-256 to verify the authenticity of X-Vellum-Signature.",
    "domain": "test.com",
    "hash": "#usage",
    "hierarchy": {
      "h0": {
        "title": "HMAC Authetication",
      },
      "h2": {
        "id": "usage",
        "title": "Usage",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.help-center.help-center.security.hmac-authentication-usage-0",
    "org_id": "test",
    "pathname": "/help-center/security/hmac-authentication",
    "tab": {
      "pathname": "/help-center",
      "title": "Help Center",
    },
    "title": "Usage",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/api-reference/overview",
        "title": "Overview",
      },
    ],
    "canonicalPathname": "/api-reference/overview/getting-started",
    "content": "Welcome to Vellum's API documentation! Here you'll find information about the various endpoints available to you,
as well as the parameters and responses that they accept and return.
We will be exposing more and more of our APIs over time as they stabilize. If there is some action you can perform
via the UI that you wish you could perform via API, please let us know and we can expose the relevant API here.",
    "description": "Dive into Vellum's API docs for endpoint details, parameters, and responses. Use our official Python, Node, or Go clients for stable interaction.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.overview.getting-started-root-0",
    "org_id": "test",
    "pathname": "/api-reference/overview/getting-started",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Welcome to Vellum",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/api-reference/overview",
        "title": "Overview",
      },
    ],
    "canonicalPathname": "/api-reference/overview/getting-started",
    "content": "Some of the APIs documented within are undergoing active development. Use the 

 and 


tags to differentiate between those that are stable and those that are not. GA stands for generally available.",
    "domain": "test.com",
    "hash": "#api-stability",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Vellum",
      },
      "h3": {
        "id": "api-stability",
        "title": "API Stability",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.overview.getting-started-api-stability-0",
    "org_id": "test",
    "pathname": "/api-reference/overview/getting-started",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "API Stability",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/api-reference/overview",
        "title": "Overview",
      },
    ],
    "canonicalPathname": "/api-reference/overview/getting-started",
    "content": "Some endpoints are hosted separately from the main Vellum API and therefore have a different base url. If this is
the case, they will say so in their description.
Unless otherwise specified, all endpoints use https://api.vellum.ai as their base URL.",
    "domain": "test.com",
    "hash": "#base-urls",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Vellum",
      },
      "h3": {
        "id": "base-urls",
        "title": "Base URLs",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.overview.getting-started-base-urls-0",
    "org_id": "test",
    "pathname": "/api-reference/overview/getting-started",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Base URLs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/api-reference/overview",
        "title": "Overview",
      },
    ],
    "canonicalPathname": "/api-reference/overview/getting-started",
    "content": "Vellum maintains official API clients for Python, Node/Typescript, and Go. We recommend using these clients to interact
with all stable endpoints. You can find them here:",
    "domain": "test.com",
    "hash": "#official-api-clients",
    "hierarchy": {
      "h0": {
        "title": "Welcome to Vellum",
      },
      "h3": {
        "id": "official-api-clients",
        "title": "Official API Clients",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.overview.getting-started-official-api-clients-0",
    "org_id": "test",
    "pathname": "/api-reference/overview/getting-started",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Official API Clients",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/api-reference/overview",
        "title": "Overview",
      },
    ],
    "canonicalPathname": "/api-reference/overview/authentication",
    "description": "Learn how to authenticate with the Vellum API using API tokens.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.overview.authentication",
    "org_id": "test",
    "pathname": "/api-reference/overview/authentication",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Authentication",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/api-reference/overview",
        "title": "Overview",
      },
    ],
    "canonicalPathname": "/api-reference/overview/authentication",
    "content": "The Vellum API uses API keys to authenticate requests. You can view and manage your API keys in the Vellum here.",
    "domain": "test.com",
    "hash": "#generating-api-keys",
    "hierarchy": {
      "h0": {
        "title": "Authentication",
      },
      "h2": {
        "id": "generating-api-keys",
        "title": "Generating API Keys",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.overview.authentication-generating-api-keys-0",
    "org_id": "test",
    "pathname": "/api-reference/overview/authentication",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Generating API Keys",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/api-reference/overview",
        "title": "Overview",
      },
    ],
    "canonicalPathname": "/api-reference/overview/authentication",
    "content": "Authentication is performed using headers. You should include your API key as the value associated with the X_API_KEY header in your requests.
Note that all API requests must be made over HTTPS. Calls made over plain HTTP will fail. API requests without authentication will also fail.",
    "domain": "test.com",
    "hash": "#authentication",
    "hierarchy": {
      "h0": {
        "title": "Authentication",
      },
      "h2": {
        "id": "authentication",
        "title": "Authentication",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.overview.authentication-authentication-0",
    "org_id": "test",
    "pathname": "/api-reference/overview/authentication",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Authentication",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/api-reference/overview",
        "title": "Overview",
      },
    ],
    "canonicalPathname": "/api-reference/overview/authentication",
    "content": "The API keys you generate should be treated like passwords. Do not share your API keys in publicly accessible areas such as GitHub, client-side code, etc.",
    "domain": "test.com",
    "hash": "#api-key-best-practices",
    "hierarchy": {
      "h0": {
        "title": "Authentication",
      },
      "h2": {
        "id": "api-key-best-practices",
        "title": "API Key Best Practices",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.api-reference.api-reference.overview.authentication-api-key-best-practices-0",
    "org_id": "test",
    "pathname": "/api-reference/overview/authentication",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "API Key Best Practices",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | October, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 31st, 2024
We've added support for Anthropic's Claude-3-5-sonnet-20241022-v2:0 model on AWS Bedrock.
AWS Bedrock Support",
    "domain": "test.com",
    "hash": "#aws-bedrock-support-for-anthropics-claude-35-sonnet",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "aws-bedrock-support-for-anthropics-claude-35-sonnet",
        "title": "AWS Bedrock Support for Anthropic's Claude 3.5 Sonnet",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-aws-bedrock-support-for-anthropics-claude-35-sonnet-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "AWS Bedrock Support for Anthropic's Claude 3.5 Sonnet",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 31st, 2024
We've added support for Anthropic's Claude-3-5-sonnet-20241022-v2:0 model on Google Cloud Vertex AI.
Google Cloud Vertex AI Support",
    "domain": "test.com",
    "hash": "#google-cloud-vertex-ai-support-for-anthropics-claude-35-sonnet",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "google-cloud-vertex-ai-support-for-anthropics-claude-35-sonnet",
        "title": "Google Cloud Vertex AI Support for Anthropic's Claude 3.5 Sonnet",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-google-cloud-vertex-ai-support-for-anthropics-claude-35-sonnet-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Google Cloud Vertex AI Support for Anthropic's Claude 3.5 Sonnet",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 31st, 2024
We've added two new API endpoints for retrieving a Workspace Secret and updating a Workspace Secret.
For retrieving a Workspace Secret, check out our GET API here.

For updating a Workspace Secret, check out our PATCH API here.


This API is available in our SDKs beginning with version 0.8.30.",
    "domain": "test.com",
    "hash": "#retrieve-workspace-secret-or-update-workspace-secret",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "retrieve-workspace-secret-or-update-workspace-secret",
        "title": "Retrieve Workspace Secret or Update Workspace Secret",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-retrieve-workspace-secret-or-update-workspace-secret-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Retrieve Workspace Secret or Update Workspace Secret",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 29th, 2024
You can now set timeouts for Prompt Deployments. With this, you can ensure that any Prompt Execution will timeout if it lasts longer than the specified amount of time.
To set a timeout for a Prompt Deployment, navigate to the "Parameters" section within the Prompt Sandbox and scroll down to toggle the "Timeout" setting on. You can then set the timeout duration in seconds. After deploying your Prompt, your Prompt Deployment will respect your configured timeout.
Visit Prompt Parameters to set Timeout
Set Timeout Duration",
    "domain": "test.com",
    "hash": "#prompt-timeout-enabled-for-prompt-deployments",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "prompt-timeout-enabled-for-prompt-deployments",
        "title": "Prompt Timeout Enabled for Prompt Deployments",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-prompt-timeout-enabled-for-prompt-deployments-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Timeout Enabled for Prompt Deployments",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 24th, 2024
You can now reorder Input and Evaluation Variables within a Test Suite's settings page. Drag and drop the variables into the order you prefer. This new order will automatically be reflected in your Evaluation Reports.",
    "domain": "test.com",
    "hash": "#reorder-test-suite-variables",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "reorder-test-suite-variables",
        "title": "Reorder Test Suite Variables",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-reorder-test-suite-variables-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Reorder Test Suite Variables",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 24th, 2024
We've added support for Perplexity AI's newest Sonar Online models, which provide real-time web search capabilities integrated directly into the language model.
Check out Perplexity AI's online models here:
The supported models are:
LLama 3.1 Sonar Small 128k Online

LLama 3.1 Sonar Large 128k Online

LLama 3.1 Sonar Huge 128k Online


These models offer several key features:
Real-time web search: The models can perform live internet searches to retrieve up-to-date information.

Contextual understanding: They can interpret search results in the context of the user's query.

Source citation: The models provide citations for information sourced from the web.

Multilingual support: They can understand and generate content in multiple languages.

Long-context understanding: The models can handle extended conversations and complex queries.


To use these models in Vellum, simply select the appropriate Sonar Online model when configuring your Prompt or Workflow. The model will automatically perform web searches when needed to supplement its knowledge and provide the most current and relevant information.
Note: Using these models may result in slightly longer processing times due to the real-time web search functionality, but they offer significantly enhanced capabilities for tasks requiring up-to-date information.",
    "domain": "test.com",
    "hash": "#support-for-perplexity-ais-online-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "support-for-perplexity-ais-online-models",
        "title": "Support for Perplexity AI's Online Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-support-for-perplexity-ais-online-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Perplexity AI's Online Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 24th, 2024
We've added support for Perplexity AI as one of our newest model hosts!
Along with the launch of the Perplexity AI integration, we've added the following models:
Perplexity AI: LLama 3.1 Sonar Small 128k Chat

Perplexity AI: LLama 3.1 Sonar Large 128k Chat

Perplexity AI: LLama 3.1 8B Instruct

Perplexity AI: LLama 3.1 70B Instruct


Perplexity AI Models",
    "domain": "test.com",
    "hash": "#support-for-perplexity-ai-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "support-for-perplexity-ai-models",
        "title": "Support for Perplexity AI Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-support-for-perplexity-ai-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Perplexity AI Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 24th, 2024
We've added support for the LLama 3.1 Lumimaid 70B and Magnum v4 72B models on OpenRouter!",
    "domain": "test.com",
    "hash": "#support-for-llama-31-lumimaid-70b-and-magnum-v4-72b-models-on-openrouter",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "support-for-llama-31-lumimaid-70b-and-magnum-v4-72b-models-on-openrouter",
        "title": "Support for LLama 3.1 Lumimaid 70B and Magnum v4 72B models on OpenRouter",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-support-for-llama-31-lumimaid-70b-and-magnum-v4-72b-models-on-openrouter-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for LLama 3.1 Lumimaid 70B and Magnum v4 72B models on OpenRouter",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 23nd, 2024
In addition to the existing support for the Gemini 1.5 models, we've added support for the Gemini 1.5 Flash 8B model to Vellum!
Gemini 1.5 Flash 8B",
    "domain": "test.com",
    "hash": "#support-for-gemini-15-flash-8b-model",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "support-for-gemini-15-flash-8b-model",
        "title": "Support for Gemini 1.5 Flash 8B Model",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-support-for-gemini-15-flash-8b-model-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Gemini 1.5 Flash 8B Model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 22nd, 2024
We've added support for Anthropic's latest 10/22/2024 snapshot of Claude 3.5 Sonnet to Vellum!
Claude 3.5 Sonnet",
    "domain": "test.com",
    "hash": "#claude-35-sonnet-2024-10-22-live",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "claude-35-sonnet-2024-10-22-live",
        "title": "Claude 3.5 Sonnet 2024-10-22 Live!",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-claude-35-sonnet-2024-10-22-live-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Claude 3.5 Sonnet 2024-10-22 Live!",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 22nd, 2024
We've added support for Cerebras-AI to Vellum!
Along with the launch of the Cerebras-AI API, we've added the following models:
Cerebras-AI: llama3.1-8b

Cerebras-AI: llama3.1-70b


Cerebras-AI Models",
    "domain": "test.com",
    "hash": "#support-for-cerebras-ai-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "support-for-cerebras-ai-models",
        "title": "Support for Cerebras-AI Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-support-for-cerebras-ai-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Cerebras-AI Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 22nd, 2024
You can now set a max timeout for Prompt Nodes within Workflows. With this, you can ensure that no one LLM invocation will run for too long and slow down the Workflow overall and instead, fail early if it does.
To set a timeout for a Prompt Node, simply navigate to the new "Settings" section and toggle the "Timeout" setting on. You can then set the timeout duration in seconds.
Prompt Settings",
    "domain": "test.com",
    "hash": "#configurable-prompt-node-timeouts",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "configurable-prompt-node-timeouts",
        "title": "Configurable Prompt Node Timeouts",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-configurable-prompt-node-timeouts-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Configurable Prompt Node Timeouts",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 20th, 2024
We now have a new API endpoint for listing all entities in a folder. This endpoint allows you to retrieve all entities in a folder, including subfolders, with a single API call. You can use this endpoint to quickly get a list of all entities in a folder with high-level metadata about them.
For details, check out our API Reference here.
This API is available in our SDKs beginning version 0.8.25.",
    "domain": "test.com",
    "hash": "#new-api-for-listing-entities-in-a-folder",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "new-api-for-listing-entities-in-a-folder",
        "title": "New API for Listing Entities in a Folder",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-new-api-for-listing-entities-in-a-folder-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New API for Listing Entities in a Folder",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 15th, 2024
Logs for your Prompts, Workflows and Documents can now be streamed to Datadog and external Webhooks. This is useful if you want deeper insight into key events that happen in Vellum in your external systems. For example, you might set up a Datadog alert that fires when there are multiple subsequent failures when executing a Workflow Deployment.
These integrations are currently in beta. If you'd like to participate in the beta period and want help setting up the integration, please contact Vellum Support.",
    "domain": "test.com",
    "hash": "#datadog-and-webhook-logging-beta-integrations",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "datadog-and-webhook-logging-beta-integrations",
        "title": "Datadog and Webhook Logging Beta Integrations",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-datadog-and-webhook-logging-beta-integrations-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Datadog and Webhook Logging Beta Integrations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 13th, 2024
We've added 2 additional new models to Vellum via our OpenRouter integration!
Eva Qwen 2.5 14B - A powerful model based on the Qwen architecture.

Rocinante 12B - A versatile 12 billion parameter model.",
    "domain": "test.com",
    "hash": "#eva-qwen-and-rocinante-added-to-openrouter-integration",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "eva-qwen-and-rocinante-added-to-openrouter-integration",
        "title": "Eva Qwen and Rocinante Added to OpenRouter Integration",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-eva-qwen-and-rocinante-added-to-openrouter-integration-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Eva Qwen and Rocinante Added to OpenRouter Integration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 15th, 2024
We're excited to announce the addition of the Vertex AI embedding models text-embedding-004 and text-multilingual-embedding-002 to Vellum!
These models can be selected when creating a Document Index.
Vertex AI Embeddings",
    "domain": "test.com",
    "hash": "#vertex-ai-embedding-model-support",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "vertex-ai-embedding-model-support",
        "title": "Vertex AI Embedding Model Support",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-vertex-ai-embedding-model-support-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Vertex AI Embedding Model Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 11th, 2024
We now have the addition of 8 new models integrated into Vellum via our OpenRouter integration:
Magnum v2 72B - A powerful model designed to achieve prose quality similar to Claude 3 models.

Nous: Hermes 3 405B Instruct - A frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model.

NousResearch: Hermes 2 Pro - Llama-3 8B - An upgraded version of Nous Hermes 2 with improved capabilities.

Nous: Hermes 3 405B Instruct (extended) - An extended context version of Hermes 3 405B Instruct.

Goliath 120B - A large LLM created by combining two fine-tuned Llama 70B models.

Dolphin 2.9.2 Mixtral 8x22B - An uncensored model designed for instruction following, conversation, and coding.

Anthropic: Claude 3.5 Sonnet (self-moderated) - A faster, self-moderated endpoint of Claude 3.5 Sonnet.

Liquid: LFM 40B MoE - A 40.3B Mixture of Experts (MoE) model for general-purpose AI tasks.


These new models offer a wide range of capabilities, from improved prose quality and instruction following to extended context lengths and specialized tasks like coding. Users can now leverage these models in their Vellum projects, expanding the possibilities for AI-powered applications.",
    "domain": "test.com",
    "hash": "#new-models-added-to-openrouter-integration",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "new-models-added-to-openrouter-integration",
        "title": "New Models Added to OpenRouter Integration",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-new-models-added-to-openrouter-integration-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New Models Added to OpenRouter Integration",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 10th, 2024
In the past, it could be quite difficult to achieve a perfectly straight line between two Nodes in a Workflow with the "smooth-step" edge type, but those days are behind us, friends.
You'll now see that your edges will automagically snap into straight-line connectors whenever they're close-to-horizontal.",
    "domain": "test.com",
    "hash": "#workflow-edge-type-improvements",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "workflow-edge-type-improvements",
        "title": "Workflow Edge Type Improvements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-workflow-edge-type-improvements-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Edge Type Improvements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 10th, 2024
Two exciting new features have been added to Workflows ‚Äî AutoLayout and AutoConnect.
AutoLayout allows you to instantly organize your workflow via algorithm, making it easier than ever to tame even the most-unruly of Workflows.
AutoConnect will automatically connect any unconnected Nodes in your Workflow by creating edges from left to right (more-or-less).
Both of these features are accessible via new buttons in the bottom left toolbar in your Workflow Sandboxes.
In the event that you only want to use AutoConnect or AutoLayout on a specific subset of Nodes, simply drag to select and you'll see a new temporary toolbar that allows you to do just that.",
    "domain": "test.com",
    "hash": "#autolayout-and-autoconnect-for-workflows",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "autolayout-and-autoconnect-for-workflows",
        "title": "AutoLayout and AutoConnect for Workflows",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-autolayout-and-autoconnect-for-workflows-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "AutoLayout and AutoConnect for Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 9th, 2024
You can now reorder entities in the Evaluation Report table. Simply select the "Reorder" option in the entity column's menu to adjust the order to your preference.
Evaluation Report Entity Reorder",
    "domain": "test.com",
    "hash": "#reorder-entities-in-evaluation-reports",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "reorder-entities-in-evaluation-reports",
        "title": "Reorder Entities in Evaluation Reports",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-reorder-entities-in-evaluation-reports-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Reorder Entities in Evaluation Reports",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 3rd, 2024
We're excited to announce the launch of Online Evaluations for Workflow and Prompt Deployments! This new feature allows you to configure Metrics for your Deployments to be evaluated in real-time as they're executed. Key highlights include:
Continuous Assessment: Automatically evaluate the quality of your deployed LLM applications as they handle live requests.

Flexible Configuration: Set up multiple Metrics to assess different aspects of your Deployment's performance.

Easy Access to Results: View evaluation results directly in the execution details of your Deployments.


It works by configuring Metrics for your Workflow or Prompt Deployment in the new "Metrics" tab.
Configure Metrics for use in Online Evals
Once configured, every execution of your Deployment will be evaluated against these Metrics. You can then view the results alongside the execution details.
See results of Metrics alongside Execution details
For more details on how to get started with Online Evaluations, check out our help documentation.",
    "domain": "test.com",
    "hash": "#online-evaluations-for-workflow-and-prompt-deployments",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "online-evaluations-for-workflow-and-prompt-deployments",
        "title": "Online Evaluations for Workflow and Prompt Deployments",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-online-evaluations-for-workflow-and-prompt-deployments-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Online Evaluations for Workflow and Prompt Deployments",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 2nd, 2024
We've added OpenRouter as a new model host in Vellum! OpenRouter provides access to a wide range of AI models through a single API, expanding the options of models available to our users.
As part of our new OpenRouter integration, we're pleased to introduce the WizardLM-2 8x22B model to our platform. WizardLM-2 8x22B is known for its strong performance across various natural language processing tasks and is now available for use in your Vellum projects.
OpenRouter Model Host",
    "domain": "test.com",
    "hash": "#openrouter-model-hosting--wizardlm-2-8x22b",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "openrouter-model-hosting--wizardlm-2-8x22b",
        "title": "OpenRouter Model Hosting + WizardLM-2 8x22B",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-openrouter-model-hosting--wizardlm-2-8x22b-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "OpenRouter Model Hosting + WizardLM-2 8x22B",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 2nd, 2024
Today OpenAI introduced Prompt Caching for GPT-4o and o1 models. Subsequent invocations of the same prompt will produce outputs with lower latency and up to 50% reduced costs.
To follow this, we've begun capturing cache tokens in Vellum's monitoring layer. With this update, you'll now see the number of Prompt Cache Tokens used by a Prompt Deployment's executions if it's backed by an OpenAI model.
This new monitoring data can be used to help analyze your cache hit rate with OpenAI and optimize your LLM spend.",
    "domain": "test.com",
    "hash": "#prompt-caching-support-for-openai",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "prompt-caching-support-for-openai",
        "title": "Prompt Caching Support for OpenAI",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-prompt-caching-support-for-openai-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Caching Support for OpenAI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-10",
    "content": "October 1st, 2024
You can now filter and sort on a Metric's score within Evaluation Reports. This makes it easy to find all Test Cases that failed below a given threshold for a given Metric.",
    "domain": "test.com",
    "hash": "#filter-and-sort-on-metric-scores",
    "hierarchy": {
      "h0": {
        "title": "Changelog | October, 2024",
      },
      "h2": {
        "id": "filter-and-sort-on-metric-scores",
        "title": "Filter and Sort on Metric Scores",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-10-filter-and-sort-on-metric-scores-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-10",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Filter and Sort on Metric Scores",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | September, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 30th, 2024
Meta's most recent open source vision model, Llama 3.2 Vision Instruct, is now available in Vellum.
This model excels in visual recognition, image reasoning, captioning, and answering diverse questions related to images and is a great open source option if you're looking for a vision model.",
    "domain": "test.com",
    "hash": "#fireworks-llama-32-90b-vision-instruct",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "fireworks-llama-32-90b-vision-instruct",
        "title": "Fireworks Llama 3.2 90B Vision Instruct",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-fireworks-llama-32-90b-vision-instruct-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Fireworks Llama 3.2 90B Vision Instruct",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 26th, 2024
Models that are now created through the Custom Model Carousel on the models page will have
cost tracking for prompt sandboxes and cost tracking for prompt deployments.
This means that you'll be able to see the dollar cost of LLM calls to model providers even for your custom models.",
    "domain": "test.com",
    "hash": "#private-models-cost-tracking",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "private-models-cost-tracking",
        "title": "Private Models Cost Tracking",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-private-models-cost-tracking-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Private Models Cost Tracking",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 24th, 2024
Google Gemini's newest 002 models gemini-1.5-pro-002 & gemini-1.5-flash-002 are now available in Vellum! They offer 50% reduced pricing, 2x higher rate limits, and 3x lower latency than the previous Gemini 1.5 models.",
    "domain": "test.com",
    "hash": "#google-gemini-15-002-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "google-gemini-15-002-models",
        "title": "Google Gemini 1.5 002 Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-google-gemini-15-002-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Google Gemini 1.5 002 Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 24th, 2024
You can now view and filter on release tags attached to your prompt executions within the Prompt Deployment Execution Table!
This addition allows for quick identification of the release version associated with each execution.
You can enable this new column in the Columns dropdown.
Prompt Deployment Executions Table with Release Tag Filter",
    "domain": "test.com",
    "hash": "#release-tag-column-and-filter-for-prompt-deployment-execution-table",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "release-tag-column-and-filter-for-prompt-deployment-execution-table",
        "title": "Release Tag Column and Filter for Prompt Deployment Execution Table",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-release-tag-column-and-filter-for-prompt-deployment-execution-table-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Release Tag Column and Filter for Prompt Deployment Execution Table",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 23rd, 2024
A while back Anthropic added support for Prompt Caching. With this update, you'll now see the number of Prompt Cache Read and Cache Creation Tokens used by a Prompt Deployment's executions if it's backed by an Anthropic model.
This new monitoring data can be used to help analyze your cache hit rate with Anthropic and optimize your LLM spend.
Prompt Executions with Cache Tokens",
    "domain": "test.com",
    "hash": "#new-prompt-caching-columns-for-prompt-deployment-execution-table",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "new-prompt-caching-columns-for-prompt-deployment-execution-table",
        "title": "New Prompt Caching Columns for Prompt Deployment Execution Table",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-new-prompt-caching-columns-for-prompt-deployment-execution-table-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New Prompt Caching Columns for Prompt Deployment Execution Table",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 23rd, 2024
You can now sort and filter by the Latency field in the Workflow Executions Table! This update allows for better prioritization and
identification of executions with higher or lower latencies, as well as targeting executions within a range of latencies.
We believe these improvements will greatly aid in monitoring and managing workflow executions and their performance and metrics!",
    "domain": "test.com",
    "hash": "#improved-latency-filter-and-sorting-for-workflow-executions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "improved-latency-filter-and-sorting-for-workflow-executions",
        "title": "Improved Latency Filter and Sorting for Workflow Executions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-improved-latency-filter-and-sorting-for-workflow-executions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Improved Latency Filter and Sorting for Workflow Executions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 23rd, 2024
It used to be difficult to debug problematic iterations when a Map Node failed. We now keep track of each iteration's execution and make it easy to view them. You can page through a Map Node's iterations one-by-one.
Map Node Rejected Pagination
Each of these iterations, included the any that failed, are now also show in a Map Node's full screen editor.
Map Node Rejected Editor
The full screen editor now also allows you to cycle through each of an executed Map Node's iterations, making it easy to debug problematic iterations and iterate on the subworkflow used to produce that iteration's execution.",
    "domain": "test.com",
    "hash": "#improved-debugging-for-map-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "improved-debugging-for-map-nodes",
        "title": "Improved Debugging for Map Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-improved-debugging-for-map-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Improved Debugging for Map Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 20th, 2024
For those of you using the new Workflow Builder, you'll now be able to resize the Node Editor Panel. This update makes it much easier to edit complex Conditional Node rules, Chat History Messages, JSON values, and more.
Resizable editor panel",
    "domain": "test.com",
    "hash": "#resizable-node-editor-panel",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "resizable-node-editor-panel",
        "title": "Resizable Node Editor Panel",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-resizable-node-editor-panel-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Resizable Node Editor Panel",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 17th, 2024
While not as flashy as some of our other updates, we've undergone a major overhaul of our Evaluations backend resulting
in significant performance improvements to the Evaluations page. Test Suites consisting of thousands of Test Cases
used to feel sluggish and sometimes not load, but now load successfully and should feel much more responsive.",
    "domain": "test.com",
    "hash": "#evaluations-performance-improvements",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "evaluations-performance-improvements",
        "title": "Evaluations Performance Improvements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-evaluations-performance-improvements-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Evaluations Performance Improvements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 17th, 2024
You can now see the cost of each Prompt Execution in the Prompt Executions Table.
Cost tracking prompt executions
This is the next step of many we have planned for improving visibility into LLM costs in Vellum. You might use this to audit expensive calls and optimize your prompts to reduce costs.",
    "domain": "test.com",
    "hash": "#cost-tracking-for-prompt-deployment-executions-table",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "cost-tracking-for-prompt-deployment-executions-table",
        "title": "Cost Tracking for Prompt Deployment Executions Table",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-cost-tracking-for-prompt-deployment-executions-table-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Cost Tracking for Prompt Deployment Executions Table",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 13th, 2024
This update brings a reduction in load times for filters and sorts; in some instances, dropping 2 minute load times to a
few seconds.
We've achieved this by switching to a more efficient data source, enabling more effective filtering and sorting
capabilities. You'll notice faster page load times across the board, resulting in a smoother, more responsive experience
when working with Prompt Deployment Executions.
This optimization sets the stage for exciting new features we have in the works. Stay tuned for more updates that
will enhance your ability to analyze, and optimize your prompt executions.",
    "domain": "test.com",
    "hash": "#optimized-prompt-deployment-executions-table",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "optimized-prompt-deployment-executions-table",
        "title": "Optimized Prompt Deployment Executions Table",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-optimized-prompt-deployment-executions-table-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Optimized Prompt Deployment Executions Table",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 13th, 2024
Previously, when filtering workflow deployment executions by external IDs, you had to provide the exact string match
to retrieve relevant results.
Now, you can filter external IDs using a variety of string patterns. You can specify that the external ID
should start with, end with, or contain certain substrings. This enhancement allows for more flexible filtering,
making it easier to locate specific workflow deployment executions based on partial matches.
new_external_id_filter_options",
    "domain": "test.com",
    "hash": "#external-id-filtering-for-workflow-deployment-executions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "external-id-filtering-for-workflow-deployment-executions",
        "title": "External ID Filtering for Workflow Deployment Executions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-external-id-filtering-for-workflow-deployment-executions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "External ID Filtering for Workflow Deployment Executions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 13th, 2024
We have given the Workflow Execution Timeline View a bit of a facelift. Along with a more modern look, we have added a couple quality of life improvements:
Subworkflows: Instead of needing to navigate to a separate page, you can now expand subworkflows to view their executions details within the same page.

Node Pages: Instead of cluttering the page with the details of all nodes at once, we now display the details for just one node at a time. Click on a node to view its inputs, outputs, and more. Each node has its own permalink so that you can share the url with others.


Workflow Execution Timeline",
    "domain": "test.com",
    "hash": "#workflow-execution-timeline-view-revamp",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "workflow-execution-timeline-view-revamp",
        "title": "Workflow Execution Timeline View Revamp",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-workflow-execution-timeline-view-revamp-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Execution Timeline View Revamp",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 12th, 2024
OpenAI's newest Strawberry (o1) models o1-preview, o1-mini, o1-preview-2024-09-12, & o1-mini-2024-09-12 are now available in Vellum and have been added to all workspaces!",
    "domain": "test.com",
    "hash": "#openai-strawberry-o1-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "openai-strawberry-o1-models",
        "title": "OpenAI Strawberry (o1) Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-openai-strawberry-o1-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "OpenAI Strawberry (o1) Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 7th, 2024
It used to be that when two people were on the same Prompt/Workflow Sandbox, only one person could edit and interact with the page.
If you were a Viewer, you were unable to interact with the page at all and were blocked with a big page overlay.
Now, the page overlay is gone and Viewers can interact with the page in a read-only mode and perform actions that
don't affect the state of the page. This includes things like scrolling, opening modals, copying text, etc.",
    "domain": "test.com",
    "hash": "#interactive-pages-in-single-editor-mode",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "interactive-pages-in-single-editor-mode",
        "title": "Interactive Pages in Single Editor Mode",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-interactive-pages-in-single-editor-mode-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Interactive Pages in Single Editor Mode",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "code_snippets": [
      {
        "code": "{
  ...,
  "expand_meta" : {
    "cost": true
  }
}",
        "lang": "json",
      },
      {
        "code": "{
  ...,
  "meta": {
    "cost" : {
        "value" : 0.000450003,
        "unit" : "USD"
    }
  }
}",
        "lang": "json",
      },
    ],
    "content": "September 4th, 2024
You can now opt in to receive the cost of a Prompt's execution in the response of the Execute Prompt and
Execute Prompt Stream APIs.
This is helpful if you want to capture the cost of executing a Prompt in your own system or if you want to provide cost
transparency to your end users.
To opt in, you can pass the expand_meta field in the request body with the cost key set to true.
You can expect a corresponding value to be included in the meta field on the response:
This functionality is available in our SDKs beginning v0.8.9.",
    "domain": "test.com",
    "hash": "#expand-cost-in-execute-prompt-apis",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "expand-cost-in-execute-prompt-apis",
        "title": "Expand Cost in Execute Prompt APIs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-expand-cost-in-execute-prompt-apis-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Expand Cost in Execute Prompt APIs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 4th, 2024
You can now set a default Block type to use when defining Prompts in Vellum. Whenever you see the "Add Block" or "Add Message" options in a Prompt Editor, your preferred Block type will be used.
By default, the Block type is set to "Rich Text," the newer option that supports Variable Chips. You can still switch between Block types for individual Blocks within the Prompt Editor.
default block type toggle",
    "domain": "test.com",
    "hash": "#default-block-type-preference",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "default-block-type-preference",
        "title": "Default Block Type Preference",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-default-block-type-preference-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Default Block Type Preference",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 3rd, 2024
We now use Monaco Editor for our code editor that is used by Workflow Code Nodes and custom Code Evaluation Metrics.
Monaco is the same editor that Visual Studio Code uses under the hood.
This offers a number of improvements including IntelliSense, semantic validation and syntax validation. Additionally we now inject Vellum Value types into the editor,
so you can now have fully typed input values for things such as Chat History. Some of these improvements are currently only available for TypeScript and not Python.",
    "domain": "test.com",
    "hash": "#new-and-improved-code-editor",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "new-and-improved-code-editor",
        "title": "New and Improved Code Editor",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-new-and-improved-code-editor-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New and Improved Code Editor",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 3rd, 2024
VPC customers of Vellum can now disable gVisor sandboxing for code execution in self-hosted environments to significantly improve the performance of Code Nodes in Workflows.
gVisor is needed for secure sandboxing in our Managed SASS platform, but in a self hosted environment where you're the only organization,
it's not strictly required if you trust that users within your org won't run malicious code.
gVisor self hosted flag",
    "domain": "test.com",
    "hash": "#vpc-disable-gvisor-option-for-code-execution",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "vpc-disable-gvisor-option-for-code-execution",
        "title": "VPC Disable gVisor Option for Code Execution",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-vpc-disable-gvisor-option-for-code-execution-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "VPC Disable gVisor Option for Code Execution",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-09",
    "content": "September 2nd, 2024
You can now download a file that was originally uploaded as a Document to a Document Index from the UI.
You'll find a new "Download Original" option in a Document's ‚Ä¢‚Ä¢‚Ä¢ More Menu.",
    "domain": "test.com",
    "hash": "#download-original-document-from-ui",
    "hierarchy": {
      "h0": {
        "title": "Changelog | September, 2024",
      },
      "h2": {
        "id": "download-original-document-from-ui",
        "title": "Download Original Document from UI",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-09-download-original-document-from-ui-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-09",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Download Original Document from UI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | August, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 30th, 2024
We now support using Anthropic's Claude 3.5 Sonnet, Claude 3 Opus and Claude 3 Haiku Models with Google Vertex AI. You can add them to your workspace from the models page.",
    "domain": "test.com",
    "hash": "#anthropic-google-vertex-ai-support",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "anthropic-google-vertex-ai-support",
        "title": "Anthropic Google Vertex AI Support",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-anthropic-google-vertex-ai-support-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Anthropic Google Vertex AI Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 30th, 2024
We now support using Anthropic's Tool Use API for function calling with Claude 3.5 Sonnet, Claude 3 Opus and Claude 3 Haiku Models. Previously Anthropic function calling had been supported by shimming function call XML into the prompt.",
    "domain": "test.com",
    "hash": "#anthropic-tool-use-api-for-function-calling",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "anthropic-tool-use-api-for-function-calling",
        "title": "Anthropic Tool Use API for Function Calling",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-anthropic-tool-use-api-for-function-calling-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Anthropic Tool Use API for Function Calling",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 29th, 2024
We have reworked the relationship of how Prompt Node's interact with Deployments. Previously, there was:
No way to update a Prompt in one spot and have it update in multiple Workflows

Confusing UX around what it meant to import a Prompt


Today we are releasing this new setup modal that appears when you create a Prompt Node:
New Prompt Node Setup
The setup modal contains a new Link to Deployment option. This is a Prompt Node that references a Prompt Deployment directly with a Release Tag. This
allows for Workflows both in the Sandbox and as a Deployment to automatically pick up changes to the underlying Prompt without needing to update the Workflow
by pointing to LATEST. To maintain a specific version of a Prompt Deployment, you can specify a user-defined Release Tag to keep the Prompt Node pinned to
a specific version. In this way, they now work exactly as Subworkflow Nodes when you select Link to Deployment there:
Prompt Node Linked Deployments",
    "domain": "test.com",
    "hash": "#prompt-node-linked-deployments",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "prompt-node-linked-deployments",
        "title": "Prompt Node Linked Deployments",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-prompt-node-linked-deployments-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Node Linked Deployments",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 29th, 2024
Earlier this month, we restricted the Workflow Deployment Executions table to only show executions invoked via API requests. This helped to filter out all of the noise
from other contexts in which a Workflow Deployment could be invoked, bringing focus to only data from production traffic. However, we've found that are still other contexts in which it's useful to see Workflow Executions.
You'll now find a new Executed By column that shows what the immediate "parent" context was in which the Workflow was executed. This table is filtered down to just API Request by default, but you can opt in to include additional contexts, such invocation as a Subworkflow via a parent Workflow:
Executed By Workflow Filter",
    "domain": "test.com",
    "hash": "#workflow-executed-by-filterable-column",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "workflow-executed-by-filterable-column",
        "title": "Workflow Executed By Filterable Column",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-workflow-executed-by-filterable-column-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Executed By Filterable Column",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 28th, 2024
We are excited to announce that you can now natively specify how prompts handle functions using OpenAI's Tool Choice
parameter. With the Tool Choice parameter, you can now dictate exactly when tools are used, allowing more precise and effective control of your prompt tools.
This feature is now available across all OpenAI models that support functions.
Tool Choice Enablement",
    "domain": "test.com",
    "hash": "#tool-choice-parameter-support-for-openai",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "tool-choice-parameter-support-for-openai",
        "title": "Tool Choice Parameter Support for OpenAI",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-tool-choice-parameter-support-for-openai-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Tool Choice Parameter Support for OpenAI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 27th, 2024
You can now add metadata to your Workflow Executions through the API. This is useful for tracking additional information
about your executions, such as the source of the request or any other custom data you want to associate with the
execution.
This metadata is visible in the Workflow Execution Details page in the Vellum UI.
You can view more information at the API documentation.
Workflow Execution Details Metadata",
    "domain": "test.com",
    "hash": "#add-metadata-to-workflow-executions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "add-metadata-to-workflow-executions",
        "title": "Add Metadata to Workflow Executions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-add-metadata-to-workflow-executions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Add Metadata to Workflow Executions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 26th, 2024
Our new Workflow Editor is now available as an opt-in beta release. Next time you open the Workflow Editor, you'll see an announcement with the option to turn on the new Editor experience.
We've made a ton of improvements to the Editor UI, and more improvements are in the works. You should find that your Workflows are easier to navigate and edit, and more performant.
The beta can also be toggled on or off in the workflow builder settings at any time.
We'd love to get your feedback about the new experience, so please let us know what you think!",
    "domain": "test.com",
    "hash": "#new-workflow-editor-beta-release",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "new-workflow-editor-beta-release",
        "title": "New Workflow Editor Beta Release",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-new-workflow-editor-beta-release-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New Workflow Editor Beta Release",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 26th, 2024
You can now view the compiled provider payload on a Workflow's Prompt Node. This is useful for debugging and understanding the
exact data that was sent to the provider during a run, especially if you got some unexpected results.
Workflow Provider Payload",
    "domain": "test.com",
    "hash": "#view-the-provider-payload-on-a-workflows-prompt-node",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "view-the-provider-payload-on-a-workflows-prompt-node",
        "title": "View the Provider Payload on a Workflow's Prompt Node",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-view-the-provider-payload-on-a-workflows-prompt-node-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "View the Provider Payload on a Workflow's Prompt Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 26th, 2024
Merging two adjacent prompt blocks in the prompt editor is now possible! This feature is especially useful when you want to combine two prompt long prompt blocks into one.
You can find this button in the top right drop down in the prompt editor.
Only blocks of the same type can be merged. For example, you can merge two rich text blocks or two Jinja blocks, but you cannot merge a rich text block with a Jinja block.
You can easily convert between the two, however, by clicking the three dots in the top right of the block and selecting "Convert to Jinja" or "Convert to Rich Text".
Merging Two Adjacent Blocks Dropdown",
    "domain": "test.com",
    "hash": "#merging-two-adjacent-prompt-blocks",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "merging-two-adjacent-prompt-blocks",
        "title": "Merging Two Adjacent Prompt Blocks",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-merging-two-adjacent-prompt-blocks-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Merging Two Adjacent Prompt Blocks",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 26th, 2024
Exports of evaluation reports are now asynchronous. You can export your evaluation report along with its results in CSV or JSON format, and an email will be sent to you once the export is done.
This change is especially useful for large evaluation reports, where the export process and download can take some time.
Evaluation Report Export",
    "domain": "test.com",
    "hash": "#asynchronous-exports-of-evaluation-reports",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "asynchronous-exports-of-evaluation-reports",
        "title": "Asynchronous Exports of Evaluation Reports",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-asynchronous-exports-of-evaluation-reports-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Asynchronous Exports of Evaluation Reports",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 26th, 2024
Vellum let's you define JSON Schemas in a few different places throughout the app to do things like define Structured Outputs and Function Calls. Previously this UI was just a simple form that allowed you to define basic JSON schemas. This UI has been improved to support direct edits via a raw JSON editor.
Raw Schema Button
From here, you can edit your JSON schema directly. This raw editor allows you to make use of all features supported by the JSON Schema spec, even if they may not yet be supported by our basic form UI. For example, you can now defined references (i.e. $ref) like this:
as references:
Raw Editor References",
    "domain": "test.com",
    "hash": "#json-schema-editor-with-ref-support",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "json-schema-editor-with-ref-support",
        "title": "JSON Schema Editor with $ref Support",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-json-schema-editor-with-ref-support-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "JSON Schema Editor with $ref Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 23rd, 2024
We now support uploading .xls and .xlsx files to Document Indexes for indexing and searching.",
    "domain": "test.com",
    "hash": "#support-for-excel-files-in-document-indexes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "support-for-excel-files-in-document-indexes",
        "title": "Support for Excel Files in Document Indexes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-support-for-excel-files-in-document-indexes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Excel Files in Document Indexes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 22nd, 2024
Anthropic recently released some exciting API changes that allow for Prompt Caching.
This new feature allows for caching of frequently used portions of your Prompt for up to 5 minutes; which reduces the latency and cost of subsequent executions that include the same Prompt context.
This powerful feature is now natively supported within Vellum! In order to use it, simply toggle the new cache options on a given Prompt Block for the supported
Claude Sonnet 3.5 and Claude Haiku 3.0 models.
Vellum Prompt Caching",
    "domain": "test.com",
    "hash": "#prompt-caching-support-for-anthropic",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "prompt-caching-support-for-anthropic",
        "title": "Prompt Caching Support for Anthropic",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-prompt-caching-support-for-anthropic-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Caching Support for Anthropic",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 22nd, 2024
If you wanted to drill into a single Prompt Execution, previously you‚Äôd have to navigate to the Prompt Deployment's Executions table and try to filter for the specific Execution ID
you're looking for. Now each row has a navigable link accessible from the table:
Prompt Execution Link
This will navigate you to a dedicated page representing that specific Prompt Execution. From here, you can see details about the Execution like the raw HTTP data sent to and from the provider,
any actuals recorded, the Vellum inputs and outputs to the prompt, and more!
Prompt Execution Page",
    "domain": "test.com",
    "hash": "#prompt-execution-pages",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "prompt-execution-pages",
        "title": "Prompt Execution Pages",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-prompt-execution-pages-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Execution Pages",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 21st, 2024
Earlier this month, we introduced Evaluation Report History, which allows you to view a history of all Evaluation runs and revisit the results of any prior state. We‚Äôve now enhanced this feature by adding the ability to preview or navigate directly to the version of the Workflow or Prompt as it existed during that specific run.
Evaluation Report History",
    "domain": "test.com",
    "hash": "#historical-versions-of-entities-in-evaluation-reports",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "historical-versions-of-entities-in-evaluation-reports",
        "title": "Historical Versions of Entities in Evaluation Reports",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-historical-versions-of-entities-in-evaluation-reports-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Historical Versions of Entities in Evaluation Reports",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 19th, 2024
OpenAI's newest GPT-4o models gpt-4o-2024-08-06 and gpt-4o-mini-2024-07-18 are now available as base models to add as OpenAI finetuned models.",
    "domain": "test.com",
    "hash": "#gpt-4o-finetuning",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "gpt-4o-finetuning",
        "title": "GPT-4o Finetuning",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-gpt-4o-finetuning-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "GPT-4o Finetuning",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 18th, 2024
You can now replay and scrub through the execution of a Workflow in Workflow Sandbox and Deployment Execution Details pages.
This feature is particularly useful for debugging and understanding the flow of your Workflow, especially if it
contains loops where a single node might be run more than once.
Workflow Execution Replay & Scrubbing",
    "domain": "test.com",
    "hash": "#workflow-execution-replay--scrubbing",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "workflow-execution-replay--scrubbing",
        "title": "Workflow Execution Replay & Scrubbing",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-workflow-execution-replay--scrubbing-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Execution Replay & Scrubbing",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 15th, 2024
OpenAI released some API changes that allow their newest models to support Structured Outputs. This powerful new feature
enables developers to strictly define the expected JSON object schemas from the model as part of the response through a model parameter, or through a function call. This new functionality is now natively integrated within Vellum!
To use within the context of Function Calling, simply toggle on the Strict checkbox for any given Function Call:
Function Call Strict
To enable Structured Outputs as part of a general OpenAI response, configure the JSON Schema setting as part of model parameters:
JSON Schema Strict
Both places come with upload/download functionality built into the form. Note that for function calling, this means we've reduced the scope of the upload/download to be just the Parameters
JSON schema field. This allows schemas to be cross-compatible between either location since we are working with an open specification.
JSON Schema Strict",
    "domain": "test.com",
    "hash": "#openai-structured-outputs-support",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "openai-structured-outputs-support",
        "title": "OpenAI Structured Outputs Support",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-openai-structured-outputs-support-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "OpenAI Structured Outputs Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 14th, 2024
Vellum Prompts have historically been able to accept strings and chat histories as dynamic inputs to their template.
If you wanted to operate on JSON, you'd have to pass it as a string and then parse it within the Prompt itself
(i.e. perform json.loads() within a Jinja Block).
Vellum Prompts now support native JSON as inputs! When you add an input variable to a Prompt, you can now select the new "JSON" type.
JSON Variables Dropdown
JSON input values will render as prettified JSON objects when referenced in Rich Text Blocks and can be operated on directly
without the need for json.loads() when referenced in Jinja Blocks.",
    "domain": "test.com",
    "hash": "#native-json-input-variable-support-for-prompts",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "native-json-input-variable-support-for-prompts",
        "title": "Native JSON Input Variable Support for Prompts",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-native-json-input-variable-support-for-prompts-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Native JSON Input Variable Support for Prompts",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 12th, 2024
Our Workflow Deployment Executions page used to list all executions of a Workflow Deployment, no matter where they were invoked from. However, this
would often get confusing because you'd see a mix of results from both eval runs and production traffic in the same view.
Our Workflow Deployment Executions page now filters down to just those executions that were invoked via the API. Executions from evaluations are still accessible from within the Evaluations UI by hovering over a row and clicking the "View Workflow Details" button:
View Workflow Details",
    "domain": "test.com",
    "hash": "#workflow-deployment-executions-filtered-to-just-api-executions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "workflow-deployment-executions-filtered-to-just-api-executions",
        "title": "Workflow Deployment Executions Filtered to Just API Executions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-workflow-deployment-executions-filtered-to-just-api-executions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Deployment Executions Filtered to Just API Executions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 12th, 2024
We've updated Evaluation Reports to give you more control over the releases you evaluate. Previously, you could only add the latest release of a Deployment to your reports. Now, you can select specific releases by their tag, allowing you to compare different versions within your Evaluation Reports.
Add Deployment",
    "domain": "test.com",
    "hash": "#add-specific-releases-to-evaluation-reports",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "add-specific-releases-to-evaluation-reports",
        "title": "Add Specific Releases to Evaluation Reports",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-add-specific-releases-to-evaluation-reports-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Add Specific Releases to Evaluation Reports",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 9th, 2024
You can now view the latency of Workflow Sandboxes and their Nodes. To enable viewing latency click the Workflow Sandbox settings gear icon in the top right and turn on the "View Latency" option.
Workflow Sandbox Latency
Workflow Sandbox Latency Settings",
    "domain": "test.com",
    "hash": "#workflow-sandbox-latency",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "workflow-sandbox-latency",
        "title": "Workflow Sandbox Latency",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-workflow-sandbox-latency-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Sandbox Latency",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 9th, 2024
You can now see the dollar cost of a Prompt's execution within both a Prompt Sandbox's Prompt Editor and Comparison Mode views.
These costs are calculated using model providers' publicly available pricing data in conjunction with the number of input/output tokens used.
Prompt Sandbox With Cost Tracking
Prompt Sandbox Comparison With Cost Tracking
If you're curious about a given model's pricing, you can view details in the Model's detail page.
MLModel Detail Page with Billing Config
Most popular models already have pricing information populated, with support for even more models following in the coming days.
Showing cost information in Prompt Sandboxes is just the first step! We'll expose cost details throughout more of Vellum over time.",
    "domain": "test.com",
    "hash": "#prompt-sandbox-cost-tracking",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "prompt-sandbox-cost-tracking",
        "title": "Prompt Sandbox Cost Tracking",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-prompt-sandbox-cost-tracking-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Sandbox Cost Tracking",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 6th, 2024
OpenAI's newest GPT-4o model gpt-4o-2024-08-06 is now available in Vellum and has been added to all workspaces!",
    "domain": "test.com",
    "hash": "#gpt-4o-2024-08-06",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "gpt-4o-2024-08-06",
        "title": "GPT-4o 2024-08-06",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-gpt-4o-2024-08-06-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "GPT-4o 2024-08-06",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 2nd, 2024
You can now update your Prompt and Workflow Deployments to include a human-readable description. This is useful for giving other members of your team a high-level summary
of what the Prompt or Workflow does without needing to parse through the configuration or control flow.
Update Deployment Description
Once set, the description will appear as part of the Deployment Details page within the Deployment Info section:
Display Deployment Description",
    "domain": "test.com",
    "hash": "#deployment-descriptions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "deployment-descriptions",
        "title": "Deployment Descriptions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-deployment-descriptions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Deployment Descriptions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-08",
    "content": "August 1st, 2024
It used to be that you could only view the latest set of Evaluation results for a given Prompt or Workflow. But now,
you can view a history of all Evaluation runs and go back to view the results of any prior state.
Evaluation Report History
This is particularly helpful if you want to do things like compare the results of two different Evaluation runs,
download the results of a past Evaluation run, or simply view the Test Cases that existed at that time.",
    "domain": "test.com",
    "hash": "#evaluation-report-history",
    "hierarchy": {
      "h0": {
        "title": "Changelog | August, 2024",
      },
      "h2": {
        "id": "evaluation-report-history",
        "title": "Evaluation Report History",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-08-evaluation-report-history-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-08",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Evaluation Report History",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | July, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 31st, 2024
For a while now you've been able to supply structured JSON metadata alongside Documents and then filtering on that
metadata when making an API call to search across Documents in a Document index (see here for more info).
However, Search Nodes within Workflows didn't offer this same functionality through the UI. The workaround has been to use a
Code Node or API Node and invoke Vellum's Search API manually.
We're happy to share that the UI has reached parity with the API and you can now filter on metadata natively in Search Nodes.
You'll be able to construct arbitrarily complex boolean logic using the new Metadata Filters section of the Search Node's
Advanced settings.
Search Node Metadata Filtering",
    "domain": "test.com",
    "hash": "#metadata-filtering-in-search-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "metadata-filtering-in-search-nodes",
        "title": "Metadata Filtering in Search Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-metadata-filtering-in-search-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Metadata Filtering in Search Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 30th, 2024
We've added a new feature to Test Suites that allows you to optionally assign an external ID to each Test Case.
This is useful if you track your Test Cases in an external system and you want to periodically sync them with Vellum.
You assign an external ID to each Test Case upon creation and then later upsert Test Cases to that Test Suite,
keying off of the external ID.
Upload Test Suite Test Cases Modal",
    "domain": "test.com",
    "hash": "#test-suite-test-case-external-ids",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "test-suite-test-case-external-ids",
        "title": "Test Suite Test Case External IDs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-test-suite-test-case-external-ids-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Test Suite Test Case External IDs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 30th, 2024
We've added another quality-of-life improvement for the index/file browser pages for Prompts, Documents, Test Suites, and Workflows. You'll now see a "Sort by" dropdown next to the other page-level controls. You can now sort both folders' and files' by created date, modified date, and label. If there are other sort fields that you'd find useful, please let us know!
Index page sort dropdown
Index page sort options",
    "domain": "test.com",
    "hash": "#index-page-sorting",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "index-page-sorting",
        "title": "Index Page Sorting",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-index-page-sorting-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Index Page Sorting",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 30, 2024
Building on our recent update that introduced Prompt Variable Chips, we've improved the experience by adding support for copy/pasting variables across blocks of different types. Now, when you copy text that includes a {{ my_var }} variable reference from a Jinja block and paste it into a Rich Text block, it's seamlessly converted into a variable chip.",
    "domain": "test.com",
    "hash": "#auto-conversion-to-variable-chips-on-paste",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "auto-conversion-to-variable-chips-on-paste",
        "title": "Auto-Conversion to Variable Chips on Paste",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-auto-conversion-to-variable-chips-on-paste-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Auto-Conversion to Variable Chips on Paste",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 29th, 2024
We now support Google Vertex AI models. Previously you could only use Google AI Studio for using Google's models. You can add them to your workspace from the models page.
Vertex AI Usage",
    "domain": "test.com",
    "hash": "#google-vertex-ai-support",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "google-vertex-ai-support",
        "title": "Google Vertex AI Support",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-google-vertex-ai-support-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Google Vertex AI Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 26th, 2024
For a while now we've had an API for compiling a Prompt and retrieving the exact payload that Vellum would send
to a model provider on your behalf. We now support a new parameter in this API ‚Äì expand_meta. With expand_meta,
you can opt-in to return additional metadata relating to the compiled prompt payload. Learn more about which
fields are expandable in our API docs here.
This new field is available in our SDKs starting v0.7.3.",
    "domain": "test.com",
    "hash": "#expandable-meta-params-in-retrieve-provider-payload-endpoint",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "expandable-meta-params-in-retrieve-provider-payload-endpoint",
        "title": "Expandable Meta Params in Retrieve Provider Payload Endpoint",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-expandable-meta-params-in-retrieve-provider-payload-endpoint-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Expandable Meta Params in Retrieve Provider Payload Endpoint",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "code_snippets": [
      {
        "code": "stream = client.execute_workflow_stream(
  workflow_deployment_name="demo",
  inputs=[
    WorkflowRequestInputRequest_String(
      type="STRING",
      name="foo",
      value="bar",
    ),
  ],
  event_types=["WORKFLOW", "NODE"],
  expand_meta=WorkflowExpandMetaRequest(
    usage=True
  )
)

for event in stream:
  if event.type == "NODE" and event.data.state == "FULFILLED":
    node_result_data = event.data.data
    if node_result_data and node_result_data.type == "PROMPT":
      print(node_result_data.data.execution_meta.usage)",
        "lang": "python",
      },
    ],
    "content": "July 25th, 2024
You can now see token counts and other usage metrics appear in Prompt Node results when invoking Workflows in the Workflow Sandbox:
Prompt Node Usage
This setting is now on by default, but can be toggled off in the Workflow Builder Settings.
You can also now return usage data when invoking a Workflow Deployment via API, by passing in True
to the expand_meta.usage parameter on either Execute Workflow endpoints.",
    "domain": "test.com",
    "hash": "#prompt-node-usage-in-workflows",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "prompt-node-usage-in-workflows",
        "title": "Prompt Node Usage in Workflows",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-prompt-node-usage-in-workflows-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Node Usage in Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 25th, 2024
Mocking Prompt Nodes helps to save token usage and time when developing the later stages of your Workflow. However, once the Workflow is in a good state, it's often useful to run
the full Workflow end-to-end without mocks to make sure it all comes together. Previously, you had to enable/disable each mock individually. Now, beneath the scenario inputs there
is a toggle that allows you to enable/disable all mocks in a workflow at once.
Enable Disable All Mocks",
    "domain": "test.com",
    "hash": "#enabledisable-all-workflow-node-mocks",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "enabledisable-all-workflow-node-mocks",
        "title": "Enable/Disable All Workflow Node Mocks",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-enabledisable-all-workflow-node-mocks-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Enable/Disable All Workflow Node Mocks",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 24th, 2024
For a while now we've had an API for creating, replacing, and deleting Test Cases in a Test Suite in bulk.
We now support a fourth operation in this API ‚Äì upsert. With upsert, you can provide an external_id and a Test Case
payload. If there is already a Test Case with that external_id, it'll be replaced. Otherwise, it'll be created.
This new operation is available in our SDKs starting v0.6.12.",
    "domain": "test.com",
    "hash": "#support-for-bulk-upserting-test-suite-test-cases-via-api",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "support-for-bulk-upserting-test-suite-test-cases-via-api",
        "title": "Support for Bulk Upserting Test Suite Test Cases via API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-support-for-bulk-upserting-test-suite-test-cases-via-api-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Bulk Upserting Test Suite Test Cases via API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 23rd, 2024
Meta's newest Llama 3.1 models are now available in Vellum through our Groq integration!",
    "domain": "test.com",
    "hash": "#llama-31-on-groq",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "llama-31-on-groq",
        "title": "Llama 3.1 on Groq",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-llama-31-on-groq-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Llama 3.1 on Groq",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 19th, 2024
When on the Prompt Deployment Overview page, you can now see the name of the Prompt Variant that's been deployed.
This is useful if your Prompt Sandbox has multiple Prompt Variants that you were comparing against one another
and you're not sure which one is currently deployed.
Deployed Prompt Variant Display",
    "domain": "test.com",
    "hash": "#deployed-prompt-variant-display",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "deployed-prompt-variant-display",
        "title": "Deployed Prompt Variant Display",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-deployed-prompt-variant-display-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Deployed Prompt Variant Display",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 18th, 2024
It used to be that Prompts that accepted a dynamic Chat History required an input variable whose name was specifically $chat_history.
This nomenclature caused frequent confusion and was a bit cumbersome to work with.
Now, you can name Chat History input variables whatever you want and even rename them after-the-fact. As part of this,
we've also centralized input variable definitions so that whether you want to create a String variable or a Chat History variable,
you can do so via the "Add" button in the "Input Variables" section of the Prompt Editor.
Add Prompt Input Variable Button",
    "domain": "test.com",
    "hash": "#improvements-to-prompt-chat-history-variables",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "improvements-to-prompt-chat-history-variables",
        "title": "Improvements to Prompt Chat History Variables",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-improvements-to-prompt-chat-history-variables-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Improvements to Prompt Chat History Variables",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 18th, 2024
We‚Äôve introduced the ability to copy Prompt Variant IDs, Document Indexes, Models, Workflow Deployment Names and IDs, Document Keys, and Prompt Deployment Names and IDs to clipboard.
This feature comes with an enhanced UI with intuitive indicators and tooltips for copyable fields.
Copy Text to Clipboard
Copy Text to Clipboard",
    "domain": "test.com",
    "hash": "#copyable-text-to-clipboard",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "copyable-text-to-clipboard",
        "title": "Copyable Text to Clipboard",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-copyable-text-to-clipboard-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Copyable Text to Clipboard",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 18th, 2024
OpenAI's newest GPT-4o Mini models gpt-4o-mini & gpt-4o-mini-2024-07-18 are now available in Vellum and have been added to all workspaces!",
    "domain": "test.com",
    "hash": "#gpt-4o-mini",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "gpt-4o-mini",
        "title": "GPT-4o Mini",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-gpt-4o-mini-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "GPT-4o Mini",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 18th, 2024
It used to be that any time you wanted to reference a variable in a Prompt, you did so using {{ myVariable }} syntax.
While powerful if you need to use more complex Jinja templating syntax, using double-curlies for simple variable substitution
can be a bit cumbersome.
They are harder to visually parse from the rest of your Prompt

They can get confusing when dealing with json, which also uses double-curly brackets

Whenever you rename a variable, you need to hunt down its usages.


To make this easier, we've introduced a new way to reference variables in Prompts: Variable Chips.
Variable Chips are small, clickable chips that you can reference in your Prompt text. You can add them by beginning to
type {{  or by typing /. Renaming a variable automatically renames all of its references.
Variable chips can be used in the new "Rich Text" block type. New Prompt blocks will default to Rich Text, but you can
change existing blocks to Rich Text by clicking the block type dropdown in the block's toolbar and converting from Jinja
to Rich Text and vice versa.
Check out a full video demo here:",
    "domain": "test.com",
    "hash": "#prompt-variable-chips",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "prompt-variable-chips",
        "title": "Prompt Variable Chips",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-prompt-variable-chips-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Variable Chips",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 17th, 2024
Previously, when a Prompt/Workflow had multiple Test Suites associated with it, we'd shown them all on the page at once. This made navigation difficult (you had to scroll up and down to see each) and could also lead to performance issues. We've addressed these issues updating the page layout to display just one Test Suite at a time with a searchable select input that allows you to easily load and view each table one at a time.
Sandbox Evaluation Select",
    "domain": "test.com",
    "hash": "#new-layout-for-sandbox-evaluations",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "new-layout-for-sandbox-evaluations",
        "title": "New Layout for Sandbox Evaluations",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-new-layout-for-sandbox-evaluations-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New Layout for Sandbox Evaluations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 16th, 2024
We've introduced a new API for adding previously uploaded Documents to a Document Index. This API is useful when you have a Document that
had previously been added to one Document Index and you want to add it to another without having to re-upload its contents altogether. It's available in our SDKs
beginning version 0.6.10. You can find docs for this new API here.",
    "domain": "test.com",
    "hash": "#new-add-document-to-document-index-api",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "new-add-document-to-document-index-api",
        "title": "New "Add Document to Document Index" API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-new-add-document-to-document-index-api-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New "Add Document to Document Index" API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 12th, 2024
We've made several quality-of-life enhancements to the Prompt Deployment Executions table, simplifying the process of adding and editing 'Desired Output' values. The entire table has been updated to align with the design of our other tables, such as Evaluations, ensuring a familiar editing experience. Additionally, it is now easier than ever to expand/collapse and copy values.
Moreover, we've significantly improved the consistency and usability of the 'Quality' column. You can now edit quality ratings with a single click, and the 'Desired Output' column will automatically update to reflect your rating where applicable.
Prompt Deployment Executions Table",
    "domain": "test.com",
    "hash": "#prompt-deployment-executions-table-improvements",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "prompt-deployment-executions-table-improvements",
        "title": "Prompt Deployment Executions Table Improvements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-prompt-deployment-executions-table-improvements-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Deployment Executions Table Improvements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 11th, 2024
It's often the case that you might want to specify a constant value as a Workflow Node Input, either as the input's primary value or as its fallback value.
The solution up until now was to specify a Templating Node, have it output a constant value, and then feed its output to the downstream Node.
Today, we are releasing the ability to inline constant values directly within Workflow Node inputs! First, start typing in the Node Input until the no options modal shows:
New Constant Link
A modal will appear to specify your value:
New Constant Modal
Upon confirming, Vellum will use an icon to denote that the input value represents a constant. As part of this work, we also added icons for all other Node Input types:
Constant Value Display
Note that constant values will always drop to the last fallback option of a given Node input, and there can only be maximum one constant defined per input.
This is due to the nature fallback values ‚Äì fallbacks are only used if other values aren't available (i.e. the node that produced the value hadn't executed yet). In the case of constants, their values are always present.",
    "domain": "test.com",
    "hash": "#constant-values-in-workflow-node-inputs",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "constant-values-in-workflow-node-inputs",
        "title": "Constant Values in Workflow Node Inputs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-constant-values-in-workflow-node-inputs-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Constant Values in Workflow Node Inputs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 9th, 2024
We‚Äôve introduced the ability to upload Test Cases to a Test Suite directly from within the Evaluations tab of a Prompt or Workflow. Now, you'll find an "Upload Test Cases" button in the table header of every Evaluations table, for both Workflows and Prompt Sandboxes whereas previously, you needed to first navigate to the Test Suite itself and upload from there.
Test Case Upload in Evaluation Reports",
    "domain": "test.com",
    "hash": "#test-case-csv-upload-in-evaluation-reports",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "test-case-csv-upload-in-evaluation-reports",
        "title": "Test Case CSV Upload in Evaluation Reports",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-test-case-csv-upload-in-evaluation-reports-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Test Case CSV Upload in Evaluation Reports",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 3rd, 2024
We‚Äôve introduced a list-view toggle to the index/file browser pages for Prompts, Documents, Test Suites, and Workflows. Your preferred view will be saved automatically by entity type, allowing you to, for instance, default to list view for Prompts and grid view for Documents.
Index Page List View",
    "domain": "test.com",
    "hash": "#index-page-list-view",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "index-page-list-view",
        "title": "Index Page List View",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-index-page-list-view-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Index Page List View",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-07",
    "content": "July 2nd, 2024
You can now collapse sections on the index/file browser pages for Prompts, Documents, Test Suites, and Workflows. Simply click the heading of any section to toggle the visibility of all folders and items within that section.
Collapsible Index Page Sections",
    "domain": "test.com",
    "hash": "#collapsible-index-page-sections",
    "hierarchy": {
      "h0": {
        "title": "Changelog | July, 2024",
      },
      "h2": {
        "id": "collapsible-index-page-sections",
        "title": "Collapsible Index Page Sections",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-07-collapsible-index-page-sections-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-07",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Collapsible Index Page Sections",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | June, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 27th, 2024
Often times when designing a workflow you need to iterate over an array and run the same operation on each item.
Previously this was only accomplishable by manually creating the loop by connecting Nodes in a tedious layout.
To make this process easier, we are now introducing Map Nodes! Map Nodes work in the same way that array map functions do in many common programming languages.
The Nodes take a JSON array as an input and iterate over it, running a Subworkflow for each item. The Subworkflow is provided with three input variables for the iteration item, index and the array.
The output of every Subworkflow is then combined into a single array as a Node output.
Map Nodes also support up to 12 concurrent iterations.",
    "domain": "test.com",
    "hash": "#map-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "map-nodes",
        "title": "Map Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-map-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Map Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 26th, 2024
Subworkflow nodes are a powerful node within Vellum Workflows that allow users to create reusable units of node logic. However up until now, they necessitated
developing the Workflow in a separate Sandbox, and for that Workflow to be deployed in order to reference it in a particular Workflow.
Today, we are releasing Inline Subworkflows! They empower users to create and group together modular units of nodes directly within the context of an existing
Workflow. The node spawns its own editor and supports similar UX as the parent Workflow such as all existing nodes and copy/paste.

For more details, check out our new help center doc.",
    "domain": "test.com",
    "hash": "#inline-subworkflow-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "inline-subworkflow-nodes",
        "title": "Inline Subworkflow Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-inline-subworkflow-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Inline Subworkflow Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 20th, 2024
We now support the new Claude 3.5 Sonnet model. It has already been automatically added to all workspaces.
We also support the model hosted through AWS Bedrock. You can add it to your workspace from the models page.",
    "domain": "test.com",
    "hash": "#claude-35-sonnet-support",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "claude-35-sonnet-support",
        "title": "Claude 3.5 Sonnet Support",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-claude-35-sonnet-support-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Claude 3.5 Sonnet Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 13th, 2024
To help you organize and document your Workflows we've added Workflow Notes with customizable colors and font sizes. You can find Workflow Notes in the Workflow Nodes drag and drop selector.
Workflow Notes",
    "domain": "test.com",
    "hash": "#workflow-notes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "workflow-notes",
        "title": "Workflow Notes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-workflow-notes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Notes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 13th, 2024
You can now add a comment to any Workflow Node to help you document your Workflow's logic. To add a comment click the chat bubble icon on the top right of the Node to open up the comment section.
Node Comments",
    "domain": "test.com",
    "hash": "#workflow-node-comments",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "workflow-node-comments",
        "title": "Workflow Node Comments",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-workflow-node-comments-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Node Comments",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 11th, 2024
You'll now see breadcrumbs that show the folder path whenever visiting the details of an entity in Vellum. This is helpful for seeing the file structure and easily navigating up to a parent folder.
With this, you can also rename a parent folder by right-clicking on its breadcrumb rather than having to first navigate to its parent.
Lastly, can also now access all of an entity's "More Menu" options by right-clicking its card when on the entity's grid view.
Breadcrumb Context Menus",
    "domain": "test.com",
    "hash": "#breadcrumb-context-menus",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "breadcrumb-context-menus",
        "title": "Breadcrumb Context Menus",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-breadcrumb-context-menus-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Breadcrumb Context Menus",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 10th, 2024
You can now provide your own API keys for models that Vellum provides API keys for such as Fireworks hosted models. To do so, click the 3 dot menu on a Model card and click the "Set API Key" option.
API Key Override",
    "domain": "test.com",
    "hash": "#override-vellum-provided-api-keys",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "override-vellum-provided-api-keys",
        "title": "Override Vellum Provided API Keys",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-override-vellum-provided-api-keys-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Override Vellum Provided API Keys",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 7th, 2024
Made a mistake while editing a workflow you want to undo? Good news, you can now undo and redo from within Workflow Sandboxes by using keyboard shortcuts or by clicking the new undo and redo buttons.
Workflow Undo Redo",
    "domain": "test.com",
    "hash": "#undo-and-redo-for-workflow-sandboxes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "undo-and-redo-for-workflow-sandboxes",
        "title": "Undo and Redo for Workflow Sandboxes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-undo-and-redo-for-workflow-sandboxes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Undo and Redo for Workflow Sandboxes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 7th, 2024
Using Vellum Workflows to power custom LLM Metrics (i.e. have one AI grade another AI) is super powerful, but to date,
you've only been able to use Workflows that produce a single score output.
We now have official support for Workflow Metrics that produce multiple outputs! As long as the Workflow used as
your Metric has at least one Final Output Node of type NUMBER named score, you can add as many additional Final Output Nodes
with custom names and types as you like.
All outputs are shown when the Metric is used in an Evaluation Report.
Workflow Sandbox and Variant IDs
Note: only the score output is aggregated and shown in the aggregate view.",
    "domain": "test.com",
    "hash": "#support-for-multiple-outputs-in-workflow-metrics",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "support-for-multiple-outputs-in-workflow-metrics",
        "title": "Support for Multiple Outputs in Workflow Metrics",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-support-for-multiple-outputs-in-workflow-metrics-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Multiple Outputs in Workflow Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 6th, 2024
For a while now you've been able to programmatically upsert
and delete Test Cases in a Test suite individually.
However, this can be problematic if you want to operate on many Test Cases at once. To solve this, we've added an API to create, replace, and delete Test Cases in bulk.
Check out the new Bulk Test Case Operations API in our docs here.
Note: this API is available in our SDKs beginning version 0.6.4.",
    "domain": "test.com",
    "hash": "#api-for-updating-a-test-suites-test-cases-in-bulk",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "api-for-updating-a-test-suites-test-cases-in-bulk",
        "title": "API for Updating a Test Suite's Test Cases in Bulk",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-api-for-updating-a-test-suites-test-cases-in-bulk-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "API for Updating a Test Suite's Test Cases in Bulk",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 5th, 2024
To follow up the release of APIs for programmatically deploying Prompts and Workflows,
we're excited to also announce APIs for programmatically moving Release Tags.
With these APIs, you can create a CI/CD pipeline that automatically moves a Release Tag for one environment from one version of a Prompt/Workflow to another.
For example, you might run certain tests or QA processes before promoting STAGING to PRODUCTION.
To move a Prompt Deployment Release Tag, check out the API docs here.
To move a Workflow Deployment Release Tag, see the API docs here.
Note: these APIs are available in our SDKs beginning version 0.6.3.",
    "domain": "test.com",
    "hash": "#apis-for-programmatically-moving-release-tags",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "apis-for-programmatically-moving-release-tags",
        "title": "APIs for Programmatically Moving Release Tags",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-apis-for-programmatically-moving-release-tags-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "APIs for Programmatically Moving Release Tags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 5th, 2024
Thanks to the desires of a few very forward-thinking customers, we now have APIs to support programmatically deploying prompts and workflows.
These APIs can be used as the basis for CI/CD pipelines for Vellum-managed entities.
We're super bullish on integrating Vellum with existing release management systems (think, Github Actions) and you can expect
to see more from us here, soon!
To deploy a Prompt, you'll need the IDs of the Prompt Sandbox and the Prompt Variant shown here:
Prompt Sandbox and Variant IDs
And can then hit the Deploy Prompt endpoint found here.
Similarly, to deploy a Workflow, you'll need the IDs of the Workflow Sandbox and the Workflow shown here:
Workflow Sandbox and Variant IDs
And can then hit the Deploy Workflow endpoint found here.
Note: these APIs are available in our SDKs beginning version 0.6.3.",
    "domain": "test.com",
    "hash": "#apis-for-programmatically-deploying-promptsworkflows",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "apis-for-programmatically-deploying-promptsworkflows",
        "title": "APIs for Programmatically Deploying Prompts/Workflows",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-apis-for-programmatically-deploying-promptsworkflows-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "APIs for Programmatically Deploying Prompts/Workflows",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-06",
    "content": "June 3rd, 2024
Until now, GPT-4 was the only multi-modal family of models supported in Vellum that let you parse images and return text.
Vellum now also supports multi-modality for Claude 3 and Gemini models. This means you can now use Vellum's prompt comparison UI and normalized API layer to compare and easily swap between multi-modal models.
This is particularly useful if you're trying to extract text from images, classify pictures, and more, and need to find the best model for your specific use-case.
For more on how to work with images in Vellum, see our help docs here.",
    "domain": "test.com",
    "hash": "#image-support-in-claude-3-and-gemini-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | June, 2024",
      },
      "h2": {
        "id": "image-support-in-claude-3-and-gemini-models",
        "title": "Image Support in Claude 3 and Gemini Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-06-image-support-in-claude-3-and-gemini-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-06",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Image Support in Claude 3 and Gemini Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | May, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 31th, 2024
You can now right-click on Workflow Edges to open a context menu to allow you to delete them without having to hunt down the trash icon. You can also now right-click on Workflow Nodes to delete them as well.
Workflow Context Menu",
    "domain": "test.com",
    "hash": "#context-menu-for-workflow-edges-and-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "context-menu-for-workflow-edges-and-nodes",
        "title": "Context Menu for Workflow Edges and Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-context-menu-for-workflow-edges-and-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Context Menu for Workflow Edges and Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 31th, 2024
We've significantly improved folder and page breadcrumbs throughout the app. Prompts, Test Suites, Workflows, and Documents now display the entire folder path of your current page, making it much easier to navigate through your folder structure. We've also updated the overflow styling for breadcrumbs: instead of an ellipsis, you'll now see a count of hidden breadcrumbs, which can be accessed via a dropdown menu.
Additionally, the pages mentioned above, along with Workflow/Prompt Evaluations and Deployments, now feature the same updated header design.
Breadcrumbs and header updates",
    "domain": "test.com",
    "hash": "#breadcrumbs-and-page-header-improvements",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "breadcrumbs-and-page-header-improvements",
        "title": "Breadcrumbs and Page Header Improvements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-breadcrumbs-and-page-header-improvements-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Breadcrumbs and Page Header Improvements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 31st, 2024
When viewing the execution details of a Workflow, Subworkflow nodes executed as part of that run will now have a link to its execution page.
Subworkflow Navigation",
    "domain": "test.com",
    "hash": "#subworkflow-node-navigation",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "subworkflow-node-navigation",
        "title": "Subworkflow Node Navigation",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-subworkflow-node-navigation-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Subworkflow Node Navigation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 29th, 2024
When submitting execution Actuals for Prompts, you can now optionally include a metadata field. This field can contain arbitrary data, and will be saved and shown in the Executions tab of your Prompt Deployment.
Prompt Actuals Metadata
This is particularly helpful if you want to capture feedback/quality across multiple custom dimensions. Learn more in our
API docs here.",
    "domain": "test.com",
    "hash": "#prompt-deployment-actuals-metadata",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "prompt-deployment-actuals-metadata",
        "title": "Prompt Deployment Actuals Metadata",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-prompt-deployment-actuals-metadata-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Deployment Actuals Metadata",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 29th, 2024
One of the biggest burdens when developing Workflows in Vellum is having to rerun your entire Workflow whenever you want to make
a change to just a single node and want to see its downstream effects.
You can now re-run a Workflow from a specific Node! After running a Workflow for the first time, you'll see this new play icon above each Node.
Replay From Node Icon
Doing so will re-use results from the previous execution for all upstream nodes and only actually execute the target node and all nodes downstream of it.
Replay From Node Execution
We hope this helps you decrease iteration cycles and save on LLM costs!",
    "domain": "test.com",
    "hash": "#replay-workflow-from-node",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "replay-workflow-from-node",
        "title": "Replay Workflow from Node",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-replay-workflow-from-node-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Replay Workflow from Node",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 29th, 2024
Saving Prompt/Workflow Deployment Executions from production API calls to an Evaluation dataset as Test Cases is a great
way to close the feedback loop between monitoring and experimentation. However, this process has historically been
time-consuming when you have many Executions to save.
We've made a number of improvements to this process:
You can now multi-select to bulk save Executions as Test Cases

We now default to the correct Sandbox/Test Suite when saving Executions as Scenarios/Test Cases

You'll now see warnings if the Sandbox/Test Suite you're saving to has required variables that are missing from the Execution


Check out a full demo here:",
    "domain": "test.com",
    "hash": "#improvements-to-saving-executions-as-scenarios--test-cases",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "improvements-to-saving-executions-as-scenarios--test-cases",
        "title": "Improvements to Saving Executions as Scenarios & Test Cases",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-improvements-to-saving-executions-as-scenarios--test-cases-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Improvements to Saving Executions as Scenarios & Test Cases",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 28th, 2024
Previously, editing past versions of a Prompt Sandbox could be confusing, with unclear indications of which version you were modifying and how it was being saved.
Now, the history view for a Prompt Sandbox is read-only. To edit a previous version, simply click the Restore button, and a new editable version will be created from that specific version.",
    "domain": "test.com",
    "hash": "#prompt-sandbox-history-update",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "prompt-sandbox-history-update",
        "title": "Prompt Sandbox History Update",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-prompt-sandbox-history-update-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Sandbox History Update",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "code_snippets": [
      {
        "code": "curl -X POST https://predict.vellum.ai/v1/submit-workflow-execution-actuals \
     -H "X_API_KEY: "$VELLUM_API_KEY"" \
     -H "Content-Type: application/json" \
     -d '{
  "execution_id": "be975a69-33c7-4ff0-b6ac-d8008198db1e",
  "actuals": [
    {
      "output_type": "STRING",
      "output_key": "final-output",
      "quality": 0.8,
      "metadata": {
        "user_score": 1.0,
        "internal_score": 1.0,
        "internal_notes": "The output was not factually correct."
      }
    }
  ]
}'",
      },
    ],
    "content": "May 28th, 2024
When submitting execution Actuals for Workflows, you can now optionally include a metadata field. This field can contain arbitrary data, and will be saved and shown in the Executions tab of your Workflow Deployment.
This is particularly helpful if you want to capture feedback/quality across multiple custom dimensions.",
    "domain": "test.com",
    "hash": "#workflow-deployment-actuals-metadata",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "workflow-deployment-actuals-metadata",
        "title": "Workflow Deployment Actuals Metadata",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-workflow-deployment-actuals-metadata-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Deployment Actuals Metadata",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 23rd, 2024
You can now use Metrics inside of Workflows with the new Guardrail Node! Guardrail Nodes let you run pre-defined evaluation criteria at runtime as part of a Workflow execution so that you can drive downstream behavior based on that Metric's score.
For example, if building a RAG application, you might determine whether the generated response passes some threshold for Ragas Faithfulness and if not, loop around to try again.
Guardrail Nodes",
    "domain": "test.com",
    "hash": "#guardrail-workflow-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "guardrail-workflow-nodes",
        "title": "Guardrail Workflow Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-guardrail-workflow-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Guardrail Workflow Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 22th, 2024
Chat Mode in Prompt Sandboxes has received a major facelift! The left side of the new interface will be familiar to anyone using the Prompt Editor, while the rest of the interface retains its functionality with a fresh new look. We've also fixed some UX wonk and minor bugs during the restyling process.
Chat Mode Styling Update",
    "domain": "test.com",
    "hash": "#chat-mode-revamp",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "chat-mode-revamp",
        "title": "Chat Mode Revamp",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-chat-mode-revamp-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Chat Mode Revamp",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 22th, 2024
You can now double-click on resizable row and column edges in both Comparison and Chat modes to auto-expand that row/column to its maximum size. If already at maximum size, double-clicking will reset them to their default size. Additionally, in Comparison mode, double-clicking on cell corners will auto-resize both dimensions simultaneously.",
    "domain": "test.com",
    "hash": "#double-click-to-resize-rows--columns-in-prompt-sandboxes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "double-click-to-resize-rows--columns-in-prompt-sandboxes",
        "title": "Double-Click to Resize Rows & Columns in Prompt Sandboxes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-double-click-to-resize-rows--columns-in-prompt-sandboxes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Double-Click to Resize Rows & Columns in Prompt Sandboxes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 22th, 2024
We've made several changes to enhance the UX of working with images. Chat History messages now include an explicit content-type selector, making it easier to work with image content using supported models. You can now add publicly-hosted images in multiple ways: by pasting an image URL, pasting a copied image, or dragging and dropping an image from another window.
Additionally, we've added limited support for embedded images. You can embed an image directly into the prompt by copy/pasting or dragging/dropping an image file from your computer's file browser. This method has a 1MB size limit and is an interim solution as we continue to explore image upload and hosting options.",
    "domain": "test.com",
    "hash": "#improved-image-support-in-chat-history-fields",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "improved-image-support-in-chat-history-fields",
        "title": "Improved Image Support in Chat History Fields",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-improved-image-support-in-chat-history-fields-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Improved Image Support in Chat History Fields",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 20th, 2024
Google's Gemini 1.5 Flash model is now available in Vellum. You can add it to your workspace from the models page.",
    "domain": "test.com",
    "hash": "#gemini-15-flash",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "gemini-15-flash",
        "title": "Gemini 1.5 Flash",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-gemini-15-flash-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Gemini 1.5 Flash",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 14th, 2024
We now support both of the Llama 3 models on AWS Bedrock. You can add them to your workspace from the models page.",
    "domain": "test.com",
    "hash": "#llama-3-models-on-bedrock",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "llama-3-models-on-bedrock",
        "title": "Llama 3 Models on Bedrock",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-llama-3-models-on-bedrock-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Llama 3 Models on Bedrock",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 13th, 2024
OpenAI's newest GPT-4o models gpt-4o & gpt-4o-2024-05-13 are now available in Vellum and have been added to all workspaces!
GPT 4o",
    "domain": "test.com",
    "hash": "#gpt-4o-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "gpt-4o-models",
        "title": "GPT-4o Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-gpt-4o-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "GPT-4o Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 13th, 2024
You can now view the active Organization's name and the active Workspace's name in the left sidebar navigation.
Workspace and Org Name Nav",
    "domain": "test.com",
    "hash": "#organization-and-workspace-names-in-side-nav",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "organization-and-workspace-names-in-side-nav",
        "title": "Organization and Workspace Names in Side Nav",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-organization-and-workspace-names-in-side-nav-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Organization and Workspace Names in Side Nav",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 10th, 2024
There's now a "Run All" button on evaluation reports that runs a test suite for all variants. Instead of running each variant individually, you can now run them all with one click.
Prompt Node Execution",
    "domain": "test.com",
    "hash": "#run-all-button-on-evaluation-reports",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "run-all-button-on-evaluation-reports",
        "title": "Run All Button on Evaluation Reports",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-run-all-button-on-evaluation-reports-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Run All Button on Evaluation Reports",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 9th, 2024
Vellum is now capturing monitoring data for deployed Prompt Nodes. Whenever a deployed Workflow invokes a Prompt Node, it will now show a link displaying the Prompt Deployment label:
Prompt Node Monitoring
Clicking on the link will take you to the Prompt's executions page, where you can then see all metadata captured for the execution, including the raw request data sent to the model:
Prompt Node Execution",
    "domain": "test.com",
    "hash": "#prompt-node-monitoring",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "prompt-node-monitoring",
        "title": "Prompt Node Monitoring",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-prompt-node-monitoring-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Node Monitoring",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 9th, 2024
Vellum now has a native integration with the LPU Inference Engine, Groq. All public models on Groq are now available to add to your workspace. Be sure to add your API key as a Secret named GROQ_API_KEY on the API Keys page.
Groq is an LLM hosting provider that offers incredible inference speed for open source LLMs, including the recently released (and very hyped!) Llama 3 model.
Groq Support",
    "domain": "test.com",
    "hash": "#groq-support",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "groq-support",
        "title": "Groq Support",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-groq-support-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Groq Support",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 8th, 2024
Prompts that output function calls can now be evaluated via Test Suites. This allows you to define Test Cases consisting of the inputs to the prompt, and the expected function call, then assert that there's a match. For more, check out our docs.
Function Call Prompts",
    "domain": "test.com",
    "hash": "#function-calling-in-prompt-evaluation",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "function-calling-in-prompt-evaluation",
        "title": "Function Calling in Prompt Evaluation",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-function-calling-in-prompt-evaluation-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Function Calling in Prompt Evaluation",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 7th, 2024
Test-driven development for your RAG-based LLM pipelines is now easier than ever within Vellum!
Three new Ragas Metrics ‚Äì Context Revelancy, Answer Relevance and Faithfulness ‚Äì are now available out-of-box in Vellum. These can be used within Workflow Evaluations to measure the quality of a RAG system.
For more info, check out our new help center article on Evaluating RAG Pipelines.
Ragas Metrics",
    "domain": "test.com",
    "hash": "#out-of-box-ragas-metrics",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "out-of-box-ragas-metrics",
        "title": "Out-of-Box Ragas Metrics",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-out-of-box-ragas-metrics-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Out-of-Box Ragas Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 7th, 2024
Subworkflow Nodes can now stream their output(s) to parent workflows.
This allows you to compose workflows using modular subworkflows without sacrificing the ability to delivery incremental results to your end user.
Note that only nodes immediately prior to Final Output Nodes can have their output(s) streamed.
Subworkflow Streaming",
    "domain": "test.com",
    "hash": "#subworkflow-node-streaming",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "subworkflow-node-streaming",
        "title": "Subworkflow Node Streaming",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-subworkflow-node-streaming-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Subworkflow Node Streaming",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-05",
    "content": "May 4th, 2024
You can now configure how many Test Cases should be run in parallel during an Evaluation. You might lower this value
if you're running into rate limits from the LLM provider, or might increase this value if your rate limits are high.
Test Case Concurrency",
    "domain": "test.com",
    "hash": "#default-test-case-concurrency-in-evaluations",
    "hierarchy": {
      "h0": {
        "title": "Changelog | May, 2024",
      },
      "h2": {
        "id": "default-test-case-concurrency-in-evaluations",
        "title": "Default Test Case Concurrency in Evaluations",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-05-default-test-case-concurrency-in-evaluations-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-05",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Default Test Case Concurrency in Evaluations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | April, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 30th, 2024
Gemini 1.5 Pro is now available in Vellum.
You can add it to your workspace through the models page.",
    "domain": "test.com",
    "hash": "#support-for-gemini-15-pro",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "support-for-gemini-15-pro",
        "title": "Support for Gemini 1.5 Pro",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-support-for-gemini-15-pro-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Gemini 1.5 Pro",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 30th, 2024
We've added new functionality to the monitoring tab on workflow deployments. It's now possible
to see a breakdown of executions by the release tag used, and further filter down based
on a specific release tag.
Release Tag Monitoring",
    "domain": "test.com",
    "hash": "#improved-monitoring-on-workflow-deployments",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "improved-monitoring-on-workflow-deployments",
        "title": "Improved Monitoring on Workflow Deployments",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-improved-monitoring-on-workflow-deployments-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Improved Monitoring on Workflow Deployments",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 30th, 2024
Introducing Reusable Metrics!
Metrics can now be shared across your Test Suites making it easier for you to consistently test and evaluate your Prompt / Workflow quality.
Define a suite of Custom Metrics tailored to your business logic and use-case to save time and ensure standardized evaluation criteria.
Metric Definition",
    "domain": "test.com",
    "hash": "#reusable-metrics",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "reusable-metrics",
        "title": "Reusable Metrics",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-reusable-metrics-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Reusable Metrics",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 30th, 2024
Prompts can now be be broken down into multiple sections and organized using "blocks." Prompt blocks can be reordered,
and toggled on or off.
Splitting your Prompt into multiple blocks can make it easier to navigate complex Prompts and help you focus
on iterating on specific sections. Check out the demo below to see how it works!",
    "domain": "test.com",
    "hash": "#prompt-blocks",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "prompt-blocks",
        "title": "Prompt Blocks",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-prompt-blocks-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Blocks",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 29th, 2024
It's now possible to filter workflow deployment executions by the release tag used when executing the workflow.
This can be very useful for monitoring differences between releases of a deployment. Are you still using an older
release in production? Are executions of your new release behaving as expected?
Execution Release Tags",
    "domain": "test.com",
    "hash": "#filtering-executions-on-release-tags",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "filtering-executions-on-release-tags",
        "title": "Filtering Executions on Release Tags",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-filtering-executions-on-release-tags-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Filtering Executions on Release Tags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 26th, 2024
The executions tab of the workflow deployments page now fetches historical executions much faster. This tab is a
great way to see how your customers are actually using your deployments.
In our test for deployments with over 200k executions, data now loads in under 4 seconds instead of the
previous 15+ seconds - a 4x speed improvement.",
    "domain": "test.com",
    "hash": "#faster-queries-on-workflow-deployment-executions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "faster-queries-on-workflow-deployment-executions",
        "title": "Faster Queries on Workflow Deployment Executions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-faster-queries-on-workflow-deployment-executions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Faster Queries on Workflow Deployment Executions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 25th, 2024
Vellum's Evaluation framework can now be used to test arbitrary functions defined in your codebase ‚Äì not just
Prompts and Workflows managed by Vellum.
For example, you might test a prompt chain that lives in your codebase and that's defined using another third party
library. This can be particularly useful if you want to incrementally migrate to Vellum Prompts/Workflows, but ensure
that the outputs remain consistent.
For a detailed example of how to use Vellum's evaluation framework to test external functions, see the
python example here",
    "domain": "test.com",
    "hash": "#support-for-evaluating-external-functions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "support-for-evaluating-external-functions",
        "title": "Support for Evaluating External Functions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-support-for-evaluating-external-functions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Evaluating External Functions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 24th, 2024
Vellum now supports models that you've fine-tuned on Fireworks AI. You can add your fine-tuned Fireworks model by navigating to the Models page and clicking on the featured model template at the top.
Fireworks Model Template
Note that only the Mistral family of models are supported currently. If there are other base models that you would like to see supported, please reach out to us!",
    "domain": "test.com",
    "hash": "#fireworks-finetuned-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "fireworks-finetuned-models",
        "title": "Fireworks Finetuned Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-fireworks-finetuned-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Fireworks Finetuned Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 23rd, 2024
We've updated the prompt editing UI throughout Vellum. You‚Äôll see the new look in the Prompt Editor, Comparison Mode, Chat Mode, Prompt Nodes in Workflows, and Deployment Overviews. This is the first in a series of exciting improvements to the prompt editing experience that will be rolling out over the coming weeks and months.
New Prompt Block UI",
    "domain": "test.com",
    "hash": "#updated-prompt-ui",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "updated-prompt-ui",
        "title": "Updated Prompt UI",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-updated-prompt-ui-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Updated Prompt UI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 23rd, 2024
The API for upserting a Prompt Sandbox Scenario now requests and responds with schemas that are more consistent with
other Vellum APIs, using discriminated unions for improved type safety. This API is available on version 0.4.0 of
our SDKs.
You can find the API documentation for it here.",
    "domain": "test.com",
    "hash": "#new-upsert-prompt-sandbox-scenario-api",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "new-upsert-prompt-sandbox-scenario-api",
        "title": "New Upsert Prompt Sandbox Scenario API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-new-upsert-prompt-sandbox-scenario-api-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New Upsert Prompt Sandbox Scenario API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 23rd, 2024
Workflows support Function Call values as a valid output type. Because these function calls often come from models, it is valuable to have evaluations on these workflows that ensure that the function call output is what we expect. Test suites in Vellum now support specifying test case input and evaluation values.
Test Case Function Call",
    "domain": "test.com",
    "hash": "#function-call-input-in-test-cases",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "function-call-input-in-test-cases",
        "title": "Function Call Input in Test Cases",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-function-call-input-in-test-cases-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Function Call Input in Test Cases",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 19th, 2024
The following models are now available in Vellum:
Llama-3-70B-Instruct

Llama-3-8B-Instruct

Mixtral-8x22B-Instruct-v0.1


They can be added to your workspace through the models page.",
    "domain": "test.com",
    "hash": "#support-for-additional-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "support-for-additional-models",
        "title": "Support for Additional Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-support-for-additional-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Additional Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 18th, 2024
If you've been using GPT models, you've likely relied on prompt engineering tips that worked well for those models.
But when you apply the same prompts to Claude 3 Opus, you might notice they don't perform as expected.
This happens because Claude 3 Opus is trained using different methods and data, so the way you prompt it differs
from how you would prompt GPT-4. We have some helpful tips in our guide,
but as of today, you can convert your prompts even faster...",
    "domain": "test.com",
    "hash": "#claude-3-opus-prompt-generators",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "claude-3-opus-prompt-generators",
        "title": "Claude 3 Opus Prompt Generators",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-claude-3-opus-prompt-generators-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Claude 3 Opus Prompt Generators",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "We've released a free tool for that allows you to paste your GPT-4 prompt and get an adapted Claude 3 Opus prompt
with suggestions for dynamic variables. You can try the tool here.
GPT-4 to Claude 3 Opus",
    "domain": "test.com",
    "hash": "#gpt-4-to-claude-3-opus-prompts",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "claude-3-opus-prompt-generators",
        "title": "Claude 3 Opus Prompt Generators",
      },
      "h3": {
        "id": "gpt-4-to-claude-3-opus-prompts",
        "title": "GPT-4 to Claude 3 Opus Prompts",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-gpt-4-to-claude-3-opus-prompts-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "GPT-4 to Claude 3 Opus Prompts",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "If you don't have a working GPT-4 prompt but need to create a prompt for Claude 3 Opus from scratch, you can use our
second new free tool ‚Äì "Claude Prompt Generator."
This generator lets you input your 'prompt objective' and creates a suitable prompt for Claude 3 Opus, with
suggestions for dynamic variables that you should include. You can try the tool here.",
    "domain": "test.com",
    "hash": "#claude-3-opus-prompt-generator",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "claude-3-opus-prompt-generators",
        "title": "Claude 3 Opus Prompt Generators",
      },
      "h3": {
        "id": "claude-3-opus-prompt-generator",
        "title": "Claude 3 Opus Prompt Generator",
      },
    },
    "level": "h3",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-claude-3-opus-prompt-generator-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Claude 3 Opus Prompt Generator",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 10th, 2024
When iterating on a Prompt in Vellum's Prompt Sandbox, you may find that its output stops mid-sentence. This is often
because the "Max Tokens" parameter is set too low, or the prompt itself is too long. To help you identify when this is
the case, we've added a warning that will appear when this max is hit.
Max Tokens Warning",
    "domain": "test.com",
    "hash": "#max-tokens-warning",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "max-tokens-warning",
        "title": "Max Tokens Warning",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-max-tokens-warning-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Max Tokens Warning",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 9th, 2024
OpenAI's newest GPT-4 Turbo model gpt-4-turbo-2024-04-09 is now available in Vellum!",
    "domain": "test.com",
    "hash": "#gpt-4-turbo-04092024-model",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "gpt-4-turbo-04092024-model",
        "title": "GPT-4 Turbo 04/09/2024 Model",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-gpt-4-turbo-04092024-model-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "GPT-4 Turbo 04/09/2024 Model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 9th, 2024
We have added the ability for you to track model host usage from the execute-prompt API. This API update is available on version 0.3.21 of our SDKs.
You can also now view model host usage in the Prompt Sandbox by enabling the "Track Usage" toggle in your Prompt Sandbox's settings.
Usage Tracking Sandbox",
    "domain": "test.com",
    "hash": "#usage-tracking-in-prompt-sandbox-and-prompt-api",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "usage-tracking-in-prompt-sandbox-and-prompt-api",
        "title": "Usage Tracking in Prompt Sandbox and Prompt API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-usage-tracking-in-prompt-sandbox-and-prompt-api-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Usage Tracking in Prompt Sandbox and Prompt API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 8th, 2024
We have a new API available in beta for listing the Test Cases belonging to a Test Suite at GET /v1/test-suites/{id}/test-cases.
This API is available on version 0.3.20 of our SDKs.",
    "domain": "test.com",
    "hash": "#new-api-for-listing-a-test-suites-test-cases",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "new-api-for-listing-a-test-suites-test-cases",
        "title": "New API for Listing a Test Suite's Test Cases",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-new-api-for-listing-a-test-suites-test-cases-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New API for Listing a Test Suite's Test Cases",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 5th, 2024
Prompt Sandboxes have an entirely new view mode: Prompt Editor. It's a dedicated space for iterating on a single Variant and Scenario. All of the features you need to work quickly are easily accessible, and collapsible sections make it simple to free up screen space. There are even more improved experiences and exciting coming down the pike for Prompt Editor, and many of those improvements will make their way into Comparison and Chat Modes, as well.",
    "domain": "test.com",
    "hash": "#prompt-editor",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "prompt-editor",
        "title": "Prompt Editor",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-prompt-editor-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Editor",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 5th, 2024
You can now copy logit bias parameters from one Prompt Variant and paste them into another Prompt. This works in both Prompt Sandboxes and Prompt Nodes within Workflows.
Logit Bias Copy",
    "domain": "test.com",
    "hash": "#copy-and-paste-logit-bias",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "copy-and-paste-logit-bias",
        "title": "Copy and Paste Logit Bias",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-copy-and-paste-logit-bias-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Copy and Paste Logit Bias",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 4th, 2024
We've made some changes to our Test Suite UX. Here's what's new:
Simplified Creation Process: We've broken down the test suite creation into clear, manageable steps, ensuring a more guided and less overwhelming setup.

In-Context Editing: You can now edit test suites directly from the Prompt or Workflow evaluations page via a new, sleek modal.

Enhanced Error Messaging: We've revamped our error messages to be clearer and more actionable. You'll now receive specific feedback that pinpoints exactly where things went wrong.


Test Suite Improvements",
    "domain": "test.com",
    "hash": "#test-suite-improvements",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "test-suite-improvements",
        "title": "Test Suite Improvements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-test-suite-improvements-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Test Suite Improvements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-04",
    "content": "April 3rd, 2024
We have two new APIs available in beta for accessing your Test Suite Runs:
A Retrieve endpoint to fetch metadata about the test suite run like it's current state at GET /v1/test_suite_runs/{id}

A List executions endpoint to fetch the results of the test suite run at GET /v1/test_suite_runs/{id}/executions


These APIs are available on version 0.3.15 of our SDKs.",
    "domain": "test.com",
    "hash": "#new-apis-for-accessing-test-suite-runs",
    "hierarchy": {
      "h0": {
        "title": "Changelog | April, 2024",
      },
      "h2": {
        "id": "new-apis-for-accessing-test-suite-runs",
        "title": "New APIs for Accessing Test Suite Runs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-04-new-apis-for-accessing-test-suite-runs-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-04",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "New APIs for Accessing Test Suite Runs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | March, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 26th, 2024
We've added the ability to configure the chunk size and the overlap between consecutive chunks for Document Indexes.
You can find it under the "Advanced" section when creating or cloning a Document Index.
Document Index Chunk Settings",
    "domain": "test.com",
    "hash": "#configurable-chunk-settings-for-document-indexes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "configurable-chunk-settings-for-document-indexes",
        "title": "Configurable Chunk Settings for Document Indexes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-configurable-chunk-settings-for-document-indexes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Configurable Chunk Settings for Document Indexes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 26th, 2024
There's a new debugging feature for iterating on Workflow Template Nodes. You can click the new "Test" button in the full-screen editor and test your template without having to run the whole Workflow. Then you can further iterate on your template by modifying your test data in the "Test Data" tab.
Workflow Template Debugger",
    "domain": "test.com",
    "hash": "#workflow-template-node-debugging",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "workflow-template-node-debugging",
        "title": "Workflow Template Node Debugging",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-workflow-template-node-debugging-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Template Node Debugging",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 26th, 2024
We have added a new Workflow node search feature to help you find your way in large and complex Workflows. Click the new search icon in the top right to quickly find the node you are looking for, or use the ‚åò + shift + F shortcut (ctrl + shift + F on windows).
Workflow Node Search",
    "domain": "test.com",
    "hash": "#workflow-node-search",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "workflow-node-search",
        "title": "Workflow Node Search",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-workflow-node-search-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Node Search",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 26th, 2024
While iteratively developing a Workflow in Vellum, you often want to focus on improving a specific branch or node. It can be cumbersome to re-run the entire Workflow just to test the part you're iterating on, especially if you already know what the upstream nodes are going to output.
To help address this, Vellum now supports node mocking. You can now mock out a Prompt or Subworkflow Node such that its execution is skipped and a hard-coded value is returned.
This can help you dramatically speed up your Workflow development since you no longer have to wait for early Prompt Nodes to complete. This has the added benefit of saving the expense of tokens with LLM providers!
For more information on Workflow Node Mocking, visit our new help center page.
Workflow Node Mocking",
    "domain": "test.com",
    "hash": "#workflow-node-mocking",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "workflow-node-mocking",
        "title": "Workflow Node Mocking",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-workflow-node-mocking-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Node Mocking",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 23rd, 2024
We now support both of the Claude 3 and both of the Mistral models on AWS Bedrock. Add these models to your workspace by heading to the models page and searching for the one you need from the search bar.
Bedrock Models",
    "domain": "test.com",
    "hash": "#claude-3-and-mistral-on-bedrock",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "claude-3-and-mistral-on-bedrock",
        "title": "Claude 3 and Mistral on Bedrock",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-claude-3-and-mistral-on-bedrock-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Claude 3 and Mistral on Bedrock",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 22nd, 2024
We've made some significant changes to Vellum's navigation UI.
The app sidebar has been reorganized with the goal of making it easier to navigate between a Prompt/Workflow's Sandbox,
Evaluations, and Deployments. You'll find that after you've clicked on a "Prompt" or "Workflow," there's an integrated
submenu within the navigation sidebar that shows "Sandbox," "Evaluations," and "Deployments."
Additionally, you'll find that some nav items, such as "Deployments", "Models," "API Keys," "Organization," and
"Profile" have been grouped into the new "More" and "Settings" nav items.
Navigation Updates",
    "domain": "test.com",
    "hash": "#navigation-updates",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "navigation-updates",
        "title": "Navigation Updates",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-navigation-updates-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Navigation Updates",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 22th, 2024
You can now see a read-only view of workflow diagrams for Workflow Deployment Executions, Workflow Test Case Executions, and Workflow Releases. You can access the diagram by clicking the "Graph View" icon tab on the top right.
This is particularly helpful if you want to visualize what your Workflow looked like at that time, as well as visualize the execution path your Workflow took.
Workflow Deployment Execution",
    "domain": "test.com",
    "hash": "#read-only-workflow-diagrams",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "read-only-workflow-diagrams",
        "title": "Read-only Workflow Diagrams",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-read-only-workflow-diagrams-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Read-only Workflow Diagrams",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 21th, 2024
The Test Cases table on the Test Suites page has been updated to use the same new styling and functionality as the Test Cases table that you'll find when viewing a Prompt/Workflow Evaluation Report. With this, adding, editing, and deleting Test Cases is generally more reliable. Additionally, special variables types, like Chat History, have an improved display are are no longer displayed as raw JSON.
Test Suite Table Updates",
    "domain": "test.com",
    "hash": "#test-suite-table-updates",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "test-suite-table-updates",
        "title": "Test Suite Table Updates",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-test-suite-table-updates-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Test Suite Table Updates",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 20th, 2024
Previously, API Nodes only accepted one configurable header, defined on the Authorization section on the node. You can now configure additional headers in the new advanced Settings section. Header values could be regular STRING values or Secrets, and any headers defined here would override the Authorization header.
API Node Headers",
    "domain": "test.com",
    "hash": "#additional-headers-on-api-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "additional-headers-on-api-nodes",
        "title": "Additional Headers on API Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-additional-headers-on-api-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Additional Headers on API Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 19th, 2024
You can now tell at a glance whether a given Prompt/Workflow Sandbox has been deployed. You can also hover over the tag to see when it was last deployed.
Sandbox Deployment Tag",
    "domain": "test.com",
    "hash": "#indicators-for-deployed-promptworkflow-sandboxes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "indicators-for-deployed-promptworkflow-sandboxes",
        "title": "Indicators for Deployed Prompt/Workflow Sandboxes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-indicators-for-deployed-promptworkflow-sandboxes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Indicators for Deployed Prompt/Workflow Sandboxes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 18th, 2024
You can now cancel running Workflow Deployment Executions. Simply click the cancel button on the Workflow Execution details page.
Cancellable Workflows",
    "domain": "test.com",
    "hash": "#cancellable-workflow-deployment-executions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "cancellable-workflow-deployment-executions",
        "title": "Cancellable Workflow Deployment Executions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-cancellable-workflow-deployment-executions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Cancellable Workflow Deployment Executions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 18th, 2024
There's a new debugging feature for iterating on custom Code Metrics. You can click the new "Test" button and test your code without having to run the whole test suite. You can update the example data that's passed into your Code Metric by going to the "Test Data" tab.
Workflow Code Execution Debugger",
    "domain": "test.com",
    "hash": "#code-execution-metric-debugging",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "code-execution-metric-debugging",
        "title": "Code Execution Metric Debugging",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-code-execution-metric-debugging-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Code Execution Metric Debugging",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 18th, 2024
You can now view Workflow Execution details from the Workflow Evaluations table! To view the details, click on the new "View Workflow Details" button located within a test case's value cell.
Workflow Executions2
Workflow Executions1",
    "domain": "test.com",
    "hash": "#workflow-details-for-workflow-evaluations",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "workflow-details-for-workflow-evaluations",
        "title": "Workflow Details for Workflow Evaluations",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-workflow-details-for-workflow-evaluations-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Details for Workflow Evaluations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 14th, 2024
Are your Workflows becoming giant and unwieldy? Wish you could define composable groups of nodes to be used across Workflows?
We're excited to introduce the latest node type in the Workflows node picker - Subworkflow Nodes! With Subworkflow Nodes, you can now link directly to deployed Workflows to reuse commonly grouped nodes and execution logic. Subworkflow Nodes also supports release tagging, giving users the flexibility to either pin to a specific version (say, production) or always automatically update with LATEST.
Subworkflow Nodes
For more details, check out our new help center doc.",
    "domain": "test.com",
    "hash": "#subworkflow-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "subworkflow-nodes",
        "title": "Subworkflow Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-subworkflow-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Subworkflow Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 13th, 2024
Image support is LIVE in the Vellum UI for OpenAI's GPT-4 Turbo with Vision! Vellum API's have had image support for a while and now you can add images in your Prompt and Workflow Sandbox scenarios!
Image Support in Vellum UI
For more details on supported image formats and working with OpenAI's vision models in Vellum, check out our new help center doc.",
    "domain": "test.com",
    "hash": "#image-support-in-the-ui",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "image-support-in-the-ui",
        "title": "Image Support in the UI",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-image-support-in-the-ui-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Image Support in the UI",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 11th, 2024
You can now view a Node's input values directly from the Workflow Editor! This makes it easier to understand what data is being passed into a Node and to debug issues.
Workflow Node Input Value Display",
    "domain": "test.com",
    "hash": "#workflow-node-input-value-display",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "workflow-node-input-value-display",
        "title": "Workflow Node Input Value Display",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-workflow-node-input-value-display-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Node Input Value Display",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 11th, 2024
You can now edit test cases directly from the "Evaluations" tab in Workflows and Prompts!
The new editing interface makes it easier than ever to make changes to test cases with long variable values, allows you to edit Chat History values with the same drag-and-drop editor you use elsewhere in the app, and adds support for formatted editing of JSON.
We're continuing to add support for more variable types and will soon be applying this new edit flow to other tables throughout the app.",
    "domain": "test.com",
    "hash": "#inline-editing-for-evaluations",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "inline-editing-for-evaluations",
        "title": "Inline Editing for Evaluations",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-inline-editing-for-evaluations-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Inline Editing for Evaluations",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 9th, 2024
Previously, if a Node in a Workflow errored, the Workflow would proceed to execute until another downstream Node tried to use the output of the Node that errored and would only then terminate. This made Workflows hard to debug and put the onus on you to implement error handling.
Going forward, by default, Workflows will immediately terminate if a Node errors. There are still cases in which you might want to continue despite a Node error (e.g. implementing your own error handling or retry logic). In this cases, you can disable the new "Reject on Error" toggle.
Workflow Node Reject on Error
Historical Workflow Nodes have this toggle disabled so that there's no change in behavior. New Nodes going forward will have this toggle enabled by default.",
    "domain": "test.com",
    "hash": "#workflow-node-reject-on-error-toggle",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "workflow-node-reject-on-error-toggle",
        "title": "Workflow Node 'Reject on Error' Toggle",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-workflow-node-reject-on-error-toggle-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Node 'Reject on Error' Toggle",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 8th, 2024
We have introduced a new debugging feature for workflow code execution nodes! You can click the new "Test" button in the full-screen editor and test your code without having to run the whole workflow! Then you can further iterate on your code by modifying your test data in the "Test Data" tab.
Workflow Code Execution Debugger",
    "domain": "test.com",
    "hash": "#workflow-code-execution-node-debugging",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "workflow-code-execution-node-debugging",
        "title": "Workflow Code Execution Node Debugging",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-workflow-code-execution-node-debugging-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Code Execution Node Debugging",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 7th, 2024
We've exposed a new API endpoint to list all the Document Indexes in a Workspace.
You can find the details of the API here.",
    "domain": "test.com",
    "hash": "#list-document-indexes-api",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "list-document-indexes-api",
        "title": "List Document Indexes API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-list-document-indexes-api-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "List Document Indexes API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 6th, 2024
You previously had to wait for a workflow to fully resolve before seeing it in the Workflow Executions table. We now start publishing executions as soon as Workflows are initiated! This allows users building complex Workflows to see any that are still in progress:
In Progress Workflow Executions Table
We also updated the Workflow Execution Details page to also reflect in progress workflows:
In Progress Workflow Execution Details",
    "domain": "test.com",
    "hash": "#in-progress-workflows-executions-now-available-in-monitoring",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "in-progress-workflows-executions-now-available-in-monitoring",
        "title": "In-Progress Workflows Executions Now Available in Monitoring",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-in-progress-workflows-executions-now-available-in-monitoring-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "In-Progress Workflows Executions Now Available in Monitoring",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 6th, 2024
Looking for more room to edit your scenarios in the prompt sandbox? We've just added an expand scenario modal! You can now easily make changes to scenarios with longer inputs.
Expand Scenario Modal",
    "domain": "test.com",
    "hash": "#expand-scenario-in-prompt-sandbox",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "expand-scenario-in-prompt-sandbox",
        "title": "Expand Scenario in Prompt Sandbox",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-expand-scenario-in-prompt-sandbox-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Expand Scenario in Prompt Sandbox",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 6th, 2024
You can now use print or console.log statements in code execution nodes and view the logs by looking at a node's result and clicking the logs tab.
Code Logs Exec Nodes
We've also added logs for Metrics. You can view them by enabling the logs column in the table columns settings.
Code Logs Eval",
    "domain": "test.com",
    "hash": "#code-execution-logs",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "code-execution-logs",
        "title": "Code Execution Logs",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-code-execution-logs-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Code Execution Logs",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 6th, 2024
We've made some huge improvements to code execution! You can now include custom packages for Code Execution Workflow nodes and Code Execution Metrics. On top of this, we have added support for TypeScript. You can select the programming language you want from the new "Runtime" dropdown.
We have also introduced a few smaller improvements:
The maximum size for code input values has been increased to 10mb, a significant leap from the previous cap of 128k characters

The layout of the workflow code execution node editor has been revamped with a new side by side layout

All Vellum input types are now supported for code execution node input variables

Line numbers in the code editor will no longer be squished together


Code Execution Improvements",
    "domain": "test.com",
    "hash": "#code-execution-improvements",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "code-execution-improvements",
        "title": "Code Execution Improvements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-code-execution-improvements-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Code Execution Improvements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 5th, 2024
Anthropic's two newest models, Claude 3 Opus and Claude 3 Sonnet, are now both available in Vellum! These models have been added to all workspaces so they should be selectable from prompt sandboxes upon refresh.
Claude 3",
    "domain": "test.com",
    "hash": "#claude-3",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "claude-3",
        "title": "Claude 3",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-claude-3-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Claude 3",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 5th, 2024
It used to be that the In-App Support Widget we showed in the bottom right corner of the screen would get in the way of other actions like Save buttons.
Now, that widget is hidden by default and you can open it by clicking the "Get Help" button in the side navigation. Once opened, we
also now display bookmarked links to useful resources like the Vellum Help Docs.
Get Help Button Opens Chat Widget",
    "domain": "test.com",
    "hash": "#in-app-support-now-accessed-via-get-help-button",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "in-app-support-now-accessed-via-get-help-button",
        "title": "In-App Support Now Accessed via "Get Help" Button",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-in-app-support-now-accessed-via-get-help-button-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "In-App Support Now Accessed via "Get Help" Button",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 4th, 2024
It's now possible to terminal a Workflow and raise an error through the use of Error Nodes. You can either re-raise an error from an upstream node, or construct and raise a custom error message.",
    "domain": "test.com",
    "hash": "#workflow-error-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "workflow-error-nodes",
        "title": "Workflow Error Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-workflow-error-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Error Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-03",
    "content": "March 1st, 2024
We've exposed a new API endpoint to retrieve details of a Workflow Deployment. This is useful if you want to do things like programmatically detect if a Workflow Deployment with a specific name exists, or has the inputs/outputs you expect.
You can find the details of the API here.",
    "domain": "test.com",
    "hash": "#retrieve-workflow-deployment-api",
    "hierarchy": {
      "h0": {
        "title": "Changelog | March, 2024",
      },
      "h2": {
        "id": "retrieve-workflow-deployment-api",
        "title": "Retrieve Workflow Deployment API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-03-retrieve-workflow-deployment-api-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-03",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Retrieve Workflow Deployment API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "description": "Discover the newest features and improvements in Vellum's product update for February.",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | February, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 29th, 2024
We've exposed a new API endpoint to add an existing entity to an existing folder. This is useful if you want to programmatically organize your entities in Vellum. You can find the new endpoint and details on how to invoke it in our API documentation.",
    "domain": "test.com",
    "hash": "#add-entity-to-folder-api",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "add-entity-to-folder-api",
        "title": "Add Entity to Folder API",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-add-entity-to-folder-api-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Add Entity to Folder API",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 28th, 2024
Vellum is now SOC 2 Type 2 compliant! This means that an independent auditor has verified that Vellum's information security practices, policies, and procedures meet the SOC 2 standards for security, availability, processing integrity, confidentiality, and privacy.
If you'd like to learn more about Vellum's security practices or request a copy of our SOC 2 report, please reach out to us at security@vellum.ai.",
    "domain": "test.com",
    "hash": "#vellum-is-soc-2-type-2-compliant",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "vellum-is-soc-2-type-2-compliant",
        "title": "Vellum is SOC 2 Type 2 Compliant",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-vellum-is-soc-2-type-2-compliant-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Vellum is SOC 2 Type 2 Compliant",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 23rd, 2024
Previously you were able to save your workflow execution to a test suite or sandbox scenario from the executions table. Now you can do the same from each execution's details page! Both the "Save As Test Case" and "Save As Scenario" buttons should now appear on the top right of the execution:
Save Workflow Execution Details",
    "domain": "test.com",
    "hash": "#save-workflow-execution-from-details-page",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "save-workflow-execution-from-details-page",
        "title": "Save Workflow Execution from Details Page",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-save-workflow-execution-from-details-page-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Save Workflow Execution from Details Page",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 21st, 2024
Have you ever wanted to pan around workflows using the W-A-S-D keys? Looking for more control over your screen real estate?
Good news! You can now adjust these settings and more in the new workflow UI settings! Access the settings by clicking the new gear icon in the top right of your workflow builder.
Workflow Builder Settings",
    "domain": "test.com",
    "hash": "#workflow-builder-ui-settings",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "workflow-builder-ui-settings",
        "title": "Workflow Builder UI Settings",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-workflow-builder-ui-settings-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Builder UI Settings",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 21st, 2024
You can now manage your Prompt and Workflow release process with greater flexibility and control using Custom Release Tags! Pin your Vellum API requests to tags you define for a given Prompt/Workflow Deployment. These tags can be easily re-assigned within the Vellum app so you can update your production, staging or other custom environment to point to a new version of a prompt or workflow ‚Äî all without making any code changes!
Going forward, new customers of Vellum will no longer see the legacy "Environment" tags in Vellum's UI. Custom Release Tags are the new, first-class mechanism for managing different releases of the same prompt/workflow in Vellum. We will slowly be deprecating and removing the legacy "Environment" tags for existing customers.
Learn more about Managing Releases in our Help Center article or watch the video walkthrough below:",
    "domain": "test.com",
    "hash": "#custom-release-tags",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "custom-release-tags",
        "title": "Custom Release Tags",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-custom-release-tags-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Custom Release Tags",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 15th, 2024
We've beautified the display of model function calls in both prompt sandboxes and workflow prompt nodes! Say goodbye to the hard to read and mundane JSON strings.
Fireworks Function Call Model",
    "domain": "test.com",
    "hash": "#better-function-call-display",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "better-function-call-display",
        "title": "Better Function Call Display",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-better-function-call-display-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Better Function Call Display",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 12th, 2024
Test Suite Runs have received a big upgrade, and now live in its own tab - Evaluations. You are now able to compare a Prompt or Workflow Variant against a Deployment, and view aggregate Metrics like Median or P90.
See a demo of the complete set of updates here:",
    "domain": "test.com",
    "hash": "#evaluation-reports",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "evaluation-reports",
        "title": "Evaluation Reports",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-evaluation-reports-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Evaluation Reports",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 5nd, 2024
OpenAI's GPT models have traditionally led the way in supporting structured data generation through function calling. But late last year Fireworks AI splashed in with their own function calling model! This model is now available in Vellum for those interested in an open source alternative to GPT.
Fireworks Function Call Model",
    "domain": "test.com",
    "hash": "#fireworks-function-calling-model",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "fireworks-function-calling-model",
        "title": "Fireworks Function Calling Model",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-fireworks-function-calling-model-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Fireworks Function Calling Model",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 2nd, 2024
When you hover over any node in your Workflow editor, you will see a new Duplicate Node icon. Clicking on this will create a new copy of a node! Never again will you need to start a node from scratch when you want to just tweak a field or two.
Clone Nodes",
    "domain": "test.com",
    "hash": "#cloning-workflow-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "cloning-workflow-nodes",
        "title": "Cloning Workflow Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-cloning-workflow-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Cloning Workflow Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-02",
    "content": "February 1st, 2024
You can now detect when a Prompt Node within a Workflow errors by using a Conditional Node. Using this, you can now build out retry logic around Prompt Nodes within your Workflow! This is useful if you want to catch retryable errors (like rate limit errors from an LLM provider) and try making the call to the LLM again.
See a demo of it in action here:",
    "domain": "test.com",
    "hash": "#prompt-node-retries",
    "hierarchy": {
      "h0": {
        "title": "Changelog | February, 2024",
      },
      "h2": {
        "id": "prompt-node-retries",
        "title": "Prompt Node Retries",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-02-prompt-node-retries-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-02",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Node Retries",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "description": "In January, 2024 Vellum released prompt usage tracking, single editor mode, and more!",
    "domain": "test.com",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Changelog | January, 2024",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 29th, 2024
Going forward, Vellum will now keep track of the token utilization of your Prompt Deployments. You can keep tabs on
input, output, and total tokens used per request.
Token Count Row Data
You can also view this data in aggregate in the Monitoring tab.
Token Count Time-Series Data
This is a precursor to more advanced usage and billing features coming down the road. If there's more you'd like to see
here, please share your feedback with us!",
    "domain": "test.com",
    "hash": "#prompt-deployment-usage-tracking",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "prompt-deployment-usage-tracking",
        "title": "Prompt Deployment Usage Tracking",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-prompt-deployment-usage-tracking-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Prompt Deployment Usage Tracking",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 29th, 2024
As the number of models available in Vellum grows, it's become harder to find the model you're looking for. To help with
this, we've added a search bar to the model selection dropdown in the Prompt and Workflow editors. This will make it
easier to find the model you're looking for, especially as we continue to add more models to the platform.
Model Search Bar",
    "domain": "test.com",
    "hash": "#model-search-bar",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "model-search-bar",
        "title": "Model Search Bar",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-model-search-bar-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Model Search Bar",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 25th, 2024
We've collaborated with our friends at velt.dev to deliver an all new "Single Editor Mode" in
Prompt and Workflow Sandboxes. With this, only one person can edit a Prompt/Workflow at a time and you can hand off
editing control to another collaborator. This is useful for avoiding conflicts when multiple people are
trying to edit the same Prompt/Workflow at the same time. Check out the video below to see it in action!",
    "domain": "test.com",
    "hash": "#single-editor-mode",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "single-editor-mode",
        "title": "Single Editor Mode",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-single-editor-mode-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Single Editor Mode",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 23rd, 2024
We've added a new API endpoint for executing a Workflow Deployment without streaming back its incremental results. This is useful
when you want to execute a Workflow and only care about its final result or if you're invoking your Workflow
via a service that doesn't support HTTP Streaming like Zapier.",
    "domain": "test.com",
    "hash": "#api-to-execute-workflow-wo-streaming",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "api-to-execute-workflow-wo-streaming",
        "title": "API to Execute Workflow w/o Streaming",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-api-to-execute-workflow-wo-streaming-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "API to Execute Workflow w/o Streaming",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 22nd, 2024
Now, when visiting the details page for a Workflow Deployment Execution, you'll find an improved loading state as well
as a simplified view for Conditional Nodes.",
    "domain": "test.com",
    "hash": "#workflow-deployment-execution-visualization-improvements",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "workflow-deployment-execution-visualization-improvements",
        "title": "Workflow Deployment Execution Visualization Improvements",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-workflow-deployment-execution-visualization-improvements-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Workflow Deployment Execution Visualization Improvements",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 18th, 2024
You can now easily import your existing function definition files (JSON or YAML) into Vellum function calling blocks
as well as export functions you've already defined in Vellum to pass along to engineers for implementation. Check it
out below!",
    "domain": "test.com",
    "hash": "#uploaddownload-of-function-definitions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "uploaddownload-of-function-definitions",
        "title": "Upload/Download of Function Definitions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-uploaddownload-of-function-definitions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Upload/Download of Function Definitions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "code_snippets": [
      {
        "code": "image_link = "https://storage.googleapis.com/vellum-public/help-docs/add_prompt_block_button.png"
response = client.execute_prompt(
    prompt_deployment_name="github-loom-demo",
    inputs=[
        PromptDeploymentInputRequest_ChatHistory(
            name="$chat_history",
            value=[
                ChatMessageRequest(
                    role=ChatMessageRole.USER,
                    content={
                        "type": "ARRAY",
                        "value": [
                            {"type": "STRING", "value": "What's in this image?"},
                            {"type": "IMAGE", "value": {"src": image_link}},
                        ],
                    },
                )
            ],
            type=VellumVariableType.CHAT_HISTORY,
        ),
    ],
)
print(response.outputs[0].value)",
        "lang": "python",
      },
    ],
    "content": "January 18th, 2024
Vellum now has API support for interacting with OpenAI's vision models, such as gpt-4-vision-preview.
You can learn more about OpenAI Vision models here. Note that there is
limited support for images in the Vellum UI at this time, but you can still use the API to interact with OpenAI Vision models.
UI support coming soon!
Here's a quick example on how to send an image to the model, using our python sdk:",
    "domain": "test.com",
    "hash": "#image-support-for-openai-vision-models",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "image-support-for-openai-vision-models",
        "title": "Image Support for OpenAI Vision Models",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-image-support-for-openai-vision-models-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Image Support for OpenAI Vision Models",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 12th, 2024
You can now organize entities in Vellum via folders! You can nest folders, share them by url, and move entities between folders.",
    "domain": "test.com",
    "hash": "#folders",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "folders",
        "title": "Folders",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-folders-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Folders",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 12th, 2024
There is now native support for setting the safetySetting parameters in Google Gemini prompts. You can learn more
about how these parameters are used by Google in their docs here.
Gemini Custom Parameters",
    "domain": "test.com",
    "hash": "#support-for-google-gemini-safety-settings",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "support-for-google-gemini-safety-settings",
        "title": "Support for Google Gemini Safety Settings",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-support-for-google-gemini-safety-settings-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for Google Gemini Safety Settings",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 11th, 2024
There is now native support for setting the user and seed parameters in OpenAI API requests, as well as specifying
that the response format be of type JSON. You can learn more about how these parameters are used by OpenAI in
their docs here.
OpenAI Custom Parameters",
    "domain": "test.com",
    "hash": "#support-for-openai-json-mode-user-id-and-seed-params",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "support-for-openai-json-mode-user-id-and-seed-params",
        "title": "Support for OpenAI JSON Mode, User ID, and Seed Params",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-support-for-openai-json-mode-user-id-and-seed-params-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Support for OpenAI JSON Mode, User ID, and Seed Params",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 9th, 2024
You can now clone a Workflow Scenario to create a new Scenario based on an existing one. This is useful when you want to
create a new Scenario that is similar to an existing one, but with some changes.
Clone Workflow Scenario",
    "domain": "test.com",
    "hash": "#cloning-workflow-scenarios",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "cloning-workflow-scenarios",
        "title": "Cloning Workflow Scenarios",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-cloning-workflow-scenarios-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Cloning Workflow Scenarios",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 9th, 2024
Now you can add and view metadata for your Vellum API keys. For example, you can see when an API key was created and by whom.
You can also assign a label to an API key to help you keep track of its purpose and an environment tag so that you know
where it's used.
API Key Metadata",
    "domain": "test.com",
    "hash": "#api-key-metadata",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "api-key-metadata",
        "title": "API Key Metadata",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-api-key-metadata-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "API Key Metadata",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 4th, 2024
You can now find the following actions at the top-level of the Workflow and Prompt Deployment Execution pages:
Save as Scenario: Useful for saving an edge case seen in production as a Scenario for qualitative eval.

Save as Test Case: Useful for saving an edge case seen in production to your bank of Test Cases for quantitative eval.

View Details: Drill in to see specifics about that specific Execution.


Top-Level Workflow Execution Actions",
    "domain": "test.com",
    "hash": "#top-level-workflow-execution-actions",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "top-level-workflow-execution-actions",
        "title": "Top-Level Workflow Execution Actions",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-top-level-workflow-execution-actions-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Top-Level Workflow Execution Actions",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "authed": false,
    "breadcrumb": [
      {
        "pathname": "/changelog/2024",
        "title": "2024",
      },
    ],
    "canonicalPathname": "/changelog/2024/2024-01",
    "content": "January 2nd, 2024
API Nodes and Code Nodes within Workflows now have improved error messages. When an error occurs, the error message
will now include the line number and column number where the error occurred. This will make it easier to debug errors
in your Workflows.
Improved Error Messages in Code & API Nodes",
    "domain": "test.com",
    "hash": "#improved-error-messages-in-code--api-nodes",
    "hierarchy": {
      "h0": {
        "title": "Changelog | January, 2024",
      },
      "h2": {
        "id": "improved-error-messages-in-code--api-nodes",
        "title": "Improved Error Messages in Code & API Nodes",
      },
    },
    "level": "h2",
    "objectID": "test:test.com:root.uv.changelog.changelog.2024.2024-01-improved-error-messages-in-code--api-nodes-0",
    "org_id": "test",
    "pathname": "/changelog/2024/2024-01",
    "tab": {
      "pathname": "/changelog",
      "title": "Changelog",
    },
    "title": "Improved Error Messages in Code & API Nodes",
    "type": "markdown",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_.execute-prompt",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/execute-prompt",
    "default_environment_id": "Production",
    "description": "Executes a deployed Prompt and returns the result.",
    "domain": "test.com",
    "endpoint_path": "/v1/execute-prompt",
    "endpoint_path_alternates": [
      "/v1/execute-prompt",
      "https://predict.vellum.ai/v1/execute-prompt",
      "https://predict.vellum.ai/v1/execute-prompt",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://predict.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.execute-prompt",
    "org_id": "test",
    "pathname": "/api-reference/prompts/execute-prompt",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Execute Prompt",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_.execute-prompt-stream",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/execute-prompt-stream",
    "default_environment_id": "Production",
    "description": "Executes a deployed Prompt and streams back the results.",
    "domain": "test.com",
    "endpoint_path": "/v1/execute-prompt-stream",
    "endpoint_path_alternates": [
      "/v1/execute-prompt-stream",
      "https://predict.vellum.ai/v1/execute-prompt-stream",
      "https://predict.vellum.ai/v1/execute-prompt-stream",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://predict.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "stream",
      "ExecutePromptEvent",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.execute-prompt-stream",
    "org_id": "test",
    "pathname": "/api-reference/prompts/execute-prompt-stream",
    "response_type": "stream",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Execute Prompt as Stream",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_.submit-completion-actuals",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/submit-completion-actuals",
    "default_environment_id": "Production",
    "description": "Used to submit feedback regarding the quality of previously generated completions.",
    "domain": "test.com",
    "endpoint_path": "/v1/submit-completion-actuals",
    "endpoint_path_alternates": [
      "/v1/submit-completion-actuals",
      "https://predict.vellum.ai/v1/submit-completion-actuals",
      "https://predict.vellum.ai/v1/submit-completion-actuals",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://predict.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "SubmitCompletionActualsErrorResponse",
      "SubmitCompletionActualsErrorResponse",
      "SubmitCompletionActualsErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.submit-completion-actuals",
    "org_id": "test",
    "pathname": "/api-reference/prompts/submit-completion-actuals",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Submit Prompt Execution Actuals",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_deployments.retrieve_provider_payload",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/retrieve-provider-payload",
    "default_environment_id": "Production",
    "description": "Given a set of input variable values, compile the exact payload that Vellum would send to the configured model provider
for execution if the execute-prompt endpoint had been invoked. Note that this endpoint does not actually execute the
prompt or make an API call to the model provider.
This endpoint is useful if you don't want to proxy LLM provider requests through Vellum and prefer to send them directly
to the provider yourself. Note that no guarantees are made on the format of this API's response schema, other than
that it will be a valid payload for the configured model provider. It's not recommended that you try to parse or
derive meaning from the response body and instead, should simply pass it directly to the model provider as is.
We encourage you to seek advise from Vellum Support before integrating with this API for production use.",
    "domain": "test.com",
    "endpoint_path": "/v1/deployments/provider-payload",
    "endpoint_path_alternates": [
      "/v1/deployments/provider-payload",
      "https://api.vellum.ai/v1/deployments/provider-payload",
      "https://api.vellum.ai/v1/deployments/provider-payload",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
      "ExecutePromptApiErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.retrieve_provider_payload",
    "org_id": "test",
    "pathname": "/api-reference/prompts/retrieve-provider-payload",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Provider Payload",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_sandboxes.deploy_prompt",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
      {
        "pathname": "/api-reference/prompts/sandboxes",
        "title": "Prompt Sandboxes",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/sandboxes/deploy-prompt",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/sandboxes/:id/prompts/:prompt_variant_id/deploy",
    "endpoint_path_alternates": [
      "/v1/sandboxes/{id}/prompts/{prompt_variant_id}/deploy",
      "https://api.vellum.ai/v1/sandboxes/:id/prompts/:prompt_variant_id/deploy",
      "https://api.vellum.ai/v1/sandboxes/%7Bid%7D/prompts/%7Bprompt_variant_id%7D/deploy",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.deploy_prompt",
    "org_id": "test",
    "pathname": "/api-reference/prompts/sandboxes/deploy-prompt",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Deploy Prompt Sandbox",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_sandboxes.upsert_sandbox_scenario",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
      {
        "pathname": "/api-reference/prompts/sandboxes",
        "title": "Prompt Sandboxes",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/sandboxes/upsert-scenario",
    "default_environment_id": "Production",
    "description": "Upserts a new scenario for a sandbox, keying off of the optionally provided scenario id.
If an id is provided and has a match, the scenario will be updated. If no id is provided or no match
is found, a new scenario will be appended to the end.
Note that a full replacement of the scenario is performed, so any fields not provided will be removed
or overwritten with default values.",
    "domain": "test.com",
    "endpoint_path": "/v1/sandboxes/:id/scenarios",
    "endpoint_path_alternates": [
      "/v1/sandboxes/{id}/scenarios",
      "https://api.vellum.ai/v1/sandboxes/:id/scenarios",
      "https://api.vellum.ai/v1/sandboxes/%7Bid%7D/scenarios",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.upsert_sandbox_scenario",
    "org_id": "test",
    "pathname": "/api-reference/prompts/sandboxes/upsert-scenario",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Upsert Prompt Sandbox Scenario",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_sandboxes.delete_sandbox_scenario",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
      {
        "pathname": "/api-reference/prompts/sandboxes",
        "title": "Prompt Sandboxes",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/sandboxes/delete-scenario",
    "default_environment_id": "Production",
    "description": "Deletes an existing scenario from a sandbox, keying off of the provided scenario id.",
    "domain": "test.com",
    "endpoint_path": "/v1/sandboxes/:id/scenarios/:scenario_id",
    "endpoint_path_alternates": [
      "/v1/sandboxes/{id}/scenarios/{scenario_id}",
      "https://api.vellum.ai/v1/sandboxes/:id/scenarios/:scenario_id",
      "https://api.vellum.ai/v1/sandboxes/%7Bid%7D/scenarios/%7Bscenario_id%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.delete_sandbox_scenario",
    "org_id": "test",
    "pathname": "/api-reference/prompts/sandboxes/delete-scenario",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Delete Prompt Sandbox Scenario",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_deployments.list",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
      {
        "pathname": "/api-reference/prompts/deployments",
        "title": "Prompt Deployments",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/deployments/list",
    "default_environment_id": "Production",
    "description": "Used to list all Prompt Deployments.",
    "domain": "test.com",
    "endpoint_path": "/v1/deployments",
    "endpoint_path_alternates": [
      "/v1/deployments",
      "https://api.vellum.ai/v1/deployments",
      "https://api.vellum.ai/v1/deployments",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.list",
    "org_id": "test",
    "pathname": "/api-reference/prompts/deployments/list",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "List Prompt Deployments",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_deployments.retrieve",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
      {
        "pathname": "/api-reference/prompts/deployments",
        "title": "Prompt Deployments",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/deployments/retrieve",
    "default_environment_id": "Production",
    "description": "Used to retrieve a Prompt Deployment given its ID or name.",
    "domain": "test.com",
    "endpoint_path": "/v1/deployments/:id",
    "endpoint_path_alternates": [
      "/v1/deployments/{id}",
      "https://api.vellum.ai/v1/deployments/:id",
      "https://api.vellum.ai/v1/deployments/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.retrieve",
    "org_id": "test",
    "pathname": "/api-reference/prompts/deployments/retrieve",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Prompt Deployment",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_deployments.retrieve_deployment_release_tag",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
      {
        "pathname": "/api-reference/prompts/deployments",
        "title": "Prompt Deployments",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/deployments/retrieve-release-tag",
    "default_environment_id": "Production",
    "description": "Retrieve a Deployment Release Tag by tag name, associated with a specified Deployment.",
    "domain": "test.com",
    "endpoint_path": "/v1/deployments/:id/release-tags/:name",
    "endpoint_path_alternates": [
      "/v1/deployments/{id}/release-tags/{name}",
      "https://api.vellum.ai/v1/deployments/:id/release-tags/:name",
      "https://api.vellum.ai/v1/deployments/%7Bid%7D/release-tags/%7Bname%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.retrieve_deployment_release_tag",
    "org_id": "test",
    "pathname": "/api-reference/prompts/deployments/retrieve-release-tag",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Prompt Deployment Release Tag",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_deployments.update_deployment_release_tag",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/prompts",
        "title": "Prompts",
      },
      {
        "pathname": "/api-reference/prompts/deployments",
        "title": "Prompt Deployments",
      },
    ],
    "canonicalPathname": "/api-reference/prompts/deployments/update-release-tag",
    "default_environment_id": "Production",
    "description": "Updates an existing Release Tag associated with the specified Deployment.",
    "domain": "test.com",
    "endpoint_path": "/v1/deployments/:id/release-tags/:name",
    "endpoint_path_alternates": [
      "/v1/deployments/{id}/release-tags/{name}",
      "https://api.vellum.ai/v1/deployments/:id/release-tags/:name",
      "https://api.vellum.ai/v1/deployments/%7Bid%7D/release-tags/%7Bname%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.update_deployment_release_tag",
    "org_id": "test",
    "pathname": "/api-reference/prompts/deployments/update-release-tag",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Update Prompt Deployment Release Tag",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_.execute-workflow",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/api-reference/workflows/execute-workflow",
    "default_environment_id": "Production",
    "description": "Executes a deployed Workflow and returns its outputs.",
    "domain": "test.com",
    "endpoint_path": "/v1/execute-workflow",
    "endpoint_path_alternates": [
      "/v1/execute-workflow",
      "https://predict.vellum.ai/v1/execute-workflow",
      "https://predict.vellum.ai/v1/execute-workflow",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://predict.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "ExecuteWorkflowErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.execute-workflow",
    "org_id": "test",
    "pathname": "/api-reference/workflows/execute-workflow",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Execute Workflow",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_.execute-workflow-stream",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/api-reference/workflows/execute-workflow-stream",
    "default_environment_id": "Production",
    "description": "Executes a deployed Workflow and streams back its results.",
    "domain": "test.com",
    "endpoint_path": "/v1/execute-workflow-stream",
    "endpoint_path_alternates": [
      "/v1/execute-workflow-stream",
      "https://predict.vellum.ai/v1/execute-workflow-stream",
      "https://predict.vellum.ai/v1/execute-workflow-stream",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://predict.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "stream",
      "WorkflowStreamEvent",
      "ExecuteWorkflowStreamErrorResponse",
      "ExecuteWorkflowStreamErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.execute-workflow-stream",
    "org_id": "test",
    "pathname": "/api-reference/workflows/execute-workflow-stream",
    "response_type": "stream",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Execute Workflow as Stream",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_.submit-workflow-execution-actuals",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/workflows",
        "title": "Workflows",
      },
    ],
    "canonicalPathname": "/api-reference/workflows/submit-workflow-execution-actuals",
    "default_environment_id": "Production",
    "description": "Used to submit feedback regarding the quality of previous workflow execution and its outputs.
Note: Uses a base url of https://predict.vellum.ai.",
    "domain": "test.com",
    "endpoint_path": "/v1/submit-workflow-execution-actuals",
    "endpoint_path_alternates": [
      "/v1/submit-workflow-execution-actuals",
      "https://predict.vellum.ai/v1/submit-workflow-execution-actuals",
      "https://predict.vellum.ai/v1/submit-workflow-execution-actuals",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://predict.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.submit-workflow-execution-actuals",
    "org_id": "test",
    "pathname": "/api-reference/workflows/submit-workflow-execution-actuals",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Submit Workflow Execution Actuals",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_workflowSandboxes.deploy_workflow",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/workflows",
        "title": "Workflows",
      },
      {
        "pathname": "/api-reference/workflows/sandboxes",
        "title": "Workflow Sandboxes",
      },
    ],
    "canonicalPathname": "/api-reference/workflows/sandboxes/deploy-workflow",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/workflow-sandboxes/:id/workflows/:workflow_id/deploy",
    "endpoint_path_alternates": [
      "/v1/workflow-sandboxes/{id}/workflows/{workflow_id}/deploy",
      "https://api.vellum.ai/v1/workflow-sandboxes/:id/workflows/:workflow_id/deploy",
      "https://api.vellum.ai/v1/workflow-sandboxes/%7Bid%7D/workflows/%7Bworkflow_id%7D/deploy",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742:endpoint_workflowSandboxes.deploy_workflow",
    "org_id": "test",
    "pathname": "/api-reference/workflows/sandboxes/deploy-workflow",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Deploy Workflow",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_workflowDeployments.list",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/workflows",
        "title": "Workflows",
      },
      {
        "pathname": "/api-reference/workflows/deployments",
        "title": "Workflow Deployments",
      },
    ],
    "canonicalPathname": "/api-reference/workflows/deployments/list",
    "default_environment_id": "Production",
    "description": "Used to list all Workflow Deployments.",
    "domain": "test.com",
    "endpoint_path": "/v1/workflow-deployments",
    "endpoint_path_alternates": [
      "/v1/workflow-deployments",
      "https://api.vellum.ai/v1/workflow-deployments",
      "https://api.vellum.ai/v1/workflow-deployments",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_workflowDeployments.list",
    "org_id": "test",
    "pathname": "/api-reference/workflows/deployments/list",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "List Workflow Deployments",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_workflowDeployments.retrieve",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/workflows",
        "title": "Workflows",
      },
      {
        "pathname": "/api-reference/workflows/deployments",
        "title": "Workflow Deployments",
      },
    ],
    "canonicalPathname": "/api-reference/workflows/deployments/retrieve",
    "default_environment_id": "Production",
    "description": "Used to retrieve a workflow deployment given its ID or name.",
    "domain": "test.com",
    "endpoint_path": "/v1/workflow-deployments/:id",
    "endpoint_path_alternates": [
      "/v1/workflow-deployments/{id}",
      "https://api.vellum.ai/v1/workflow-deployments/:id",
      "https://api.vellum.ai/v1/workflow-deployments/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_workflowDeployments.retrieve",
    "org_id": "test",
    "pathname": "/api-reference/workflows/deployments/retrieve",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Workflow Deployment",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_workflowDeployments.retrieve_workflow_release_tag",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/workflows",
        "title": "Workflows",
      },
      {
        "pathname": "/api-reference/workflows/deployments",
        "title": "Workflow Deployments",
      },
    ],
    "canonicalPathname": "/api-reference/workflows/deployments/retrieve-release-tag",
    "default_environment_id": "Production",
    "description": "Retrieve a Workflow Release Tag by tag name, associated with a specified Workflow Deployment.",
    "domain": "test.com",
    "endpoint_path": "/v1/workflow-deployments/:id/release-tags/:name",
    "endpoint_path_alternates": [
      "/v1/workflow-deployments/{id}/release-tags/{name}",
      "https://api.vellum.ai/v1/workflow-deployments/:id/release-tags/:name",
      "https://api.vellum.ai/v1/workflow-deployments/%7Bid%7D/release-tags/%7Bname%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_workflowDeployments.retrieve_workflow_release_tag",
    "org_id": "test",
    "pathname": "/api-reference/workflows/deployments/retrieve-release-tag",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Workflow Deployment Release Tag",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_workflowDeployments.update_workflow_release_tag",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/workflows",
        "title": "Workflows",
      },
      {
        "pathname": "/api-reference/workflows/deployments",
        "title": "Workflow Deployments",
      },
    ],
    "canonicalPathname": "/api-reference/workflows/deployments/update-release-tag",
    "default_environment_id": "Production",
    "description": "Updates an existing Release Tag associated with the specified Workflow Deployment.",
    "domain": "test.com",
    "endpoint_path": "/v1/workflow-deployments/:id/release-tags/:name",
    "endpoint_path_alternates": [
      "/v1/workflow-deployments/{id}/release-tags/{name}",
      "https://api.vellum.ai/v1/workflow-deployments/:id/release-tags/:name",
      "https://api.vellum.ai/v1/workflow-deployments/%7Bid%7D/release-tags/%7Bname%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_workflowDeployments.update_workflow_release_tag",
    "org_id": "test",
    "pathname": "/api-reference/workflows/deployments/update-release-tag",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Update Workflow Deployment Release Tag",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_.search",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/search",
    "default_environment_id": "Production",
    "description": "Perform a search against a document index.",
    "domain": "test.com",
    "endpoint_path": "/v1/search",
    "endpoint_path_alternates": [
      "/v1/search",
      "https://predict.vellum.ai/v1/search",
      "https://predict.vellum.ai/v1/search",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://predict.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "SearchErrorResponse",
      "SearchErrorResponse",
      "SearchErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documentIndexes.search",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/search",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Search",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documentIndexes.add_document",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/add-document",
    "default_environment_id": "Production",
    "description": "Adds a previously uploaded Document to the specified Document Index.",
    "domain": "test.com",
    "endpoint_path": "/v1/document-indexes/:id/documents/:document_id",
    "endpoint_path_alternates": [
      "/v1/document-indexes/{id}/documents/{document_id}",
      "https://api.vellum.ai/v1/document-indexes/:id/documents/:document_id",
      "https://api.vellum.ai/v1/document-indexes/%7Bid%7D/documents/%7Bdocument_id%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documentIndexes.add_document",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/add-document",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Add Document",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documentIndexes.create",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/create",
    "default_environment_id": "Production",
    "description": "Creates a new document index.",
    "domain": "test.com",
    "endpoint_path": "/v1/document-indexes",
    "endpoint_path_alternates": [
      "/v1/document-indexes",
      "https://api.vellum.ai/v1/document-indexes",
      "https://api.vellum.ai/v1/document-indexes",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documentIndexes.create",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/create",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Create Document Index",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documentIndexes.retrieve",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/retrieve",
    "default_environment_id": "Production",
    "description": "Used to retrieve a Document Index given its ID or name.",
    "domain": "test.com",
    "endpoint_path": "/v1/document-indexes/:id",
    "endpoint_path_alternates": [
      "/v1/document-indexes/{id}",
      "https://api.vellum.ai/v1/document-indexes/:id",
      "https://api.vellum.ai/v1/document-indexes/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documentIndexes.retrieve",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/retrieve",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Document Index",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documentIndexes.list",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/list",
    "default_environment_id": "Production",
    "description": "Used to retrieve a list of Document Indexes.",
    "domain": "test.com",
    "endpoint_path": "/v1/document-indexes",
    "endpoint_path_alternates": [
      "/v1/document-indexes",
      "https://api.vellum.ai/v1/document-indexes",
      "https://api.vellum.ai/v1/document-indexes",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documentIndexes.list",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/list",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "List Document Indexes",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documentIndexes.partialUpdate",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/partial-update",
    "default_environment_id": "Production",
    "description": "Used to partial update a Document Index given its ID or name.",
    "domain": "test.com",
    "endpoint_path": "/v1/document-indexes/:id",
    "endpoint_path_alternates": [
      "/v1/document-indexes/{id}",
      "https://api.vellum.ai/v1/document-indexes/:id",
      "https://api.vellum.ai/v1/document-indexes/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documentIndexes.partialUpdate",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/partial-update",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Update Document Index",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documentIndexes.update",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/update",
    "default_environment_id": "Production",
    "description": "Used to fully update a Document Index given its ID or name.",
    "domain": "test.com",
    "endpoint_path": "/v1/document-indexes/:id",
    "endpoint_path_alternates": [
      "/v1/document-indexes/{id}",
      "https://api.vellum.ai/v1/document-indexes/:id",
      "https://api.vellum.ai/v1/document-indexes/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PUT",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documentIndexes.update",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/update",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Replace Document Index",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documentIndexes.destroy",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/destroy",
    "default_environment_id": "Production",
    "description": "Used to delete a Document Index given its ID or name.",
    "domain": "test.com",
    "endpoint_path": "/v1/document-indexes/:id",
    "endpoint_path_alternates": [
      "/v1/document-indexes/{id}",
      "https://documents.vellum.ai/v1/document-indexes/:id",
      "https://documents.vellum.ai/v1/document-indexes/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://documents.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742:endpoint_documentIndexes.destroy",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/destroy",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Destroy",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documentIndexes.remove_document",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/document-indexes",
        "title": "Document Indexes",
      },
    ],
    "canonicalPathname": "/api-reference/document-indexes/remove-document",
    "default_environment_id": "Production",
    "description": "Removes a Document from a Document Index without deleting the Document itself.",
    "domain": "test.com",
    "endpoint_path": "/v1/document-indexes/:id/documents/:document_id",
    "endpoint_path_alternates": [
      "/v1/document-indexes/{id}/documents/{document_id}",
      "https://documents.vellum.ai/v1/document-indexes/:id/documents/:document_id",
      "https://documents.vellum.ai/v1/document-indexes/%7Bid%7D/documents/%7Bdocument_id%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://documents.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742:endpoint_documentIndexes.remove_document",
    "org_id": "test",
    "pathname": "/api-reference/document-indexes/remove-document",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Remove Document",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documents.upload",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/api-reference/documents/upload",
    "default_environment_id": "Production",
    "description": "Upload a document to be indexed and used for search.
Note: Uses a base url of https://documents.vellum.ai.
This is a multipart/form-data request. The contents field should be a file upload. It also expects a JSON body with the following fields:
add_to_index_names: list[str] - Optionally include the names of all indexes that you'd like this document to be included in

external_id: str | None - Optionally include an external ID for this document. This is useful if you want to re-upload the same document later when its contents change and would like it to be re-indexed.

label: str - A human-friendly name for this document. Typically the filename.

keywords: list[str] | None - Optionally include a list of keywords that'll be associated with this document. Used when performing keyword searches.

metadata: dict[str, Any] - A stringified JSON object containing any metadata associated with the document that you'd like to filter upon later.",
    "domain": "test.com",
    "endpoint_path": "/v1/upload-document",
    "endpoint_path_alternates": [
      "/v1/upload-document",
      "https://documents.vellum.ai/v1/upload-document",
      "https://documents.vellum.ai/v1/upload-document",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://documents.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
      "UploadDocumentErrorResponse",
      "UploadDocumentErrorResponse",
      "UploadDocumentErrorResponse",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documents.upload",
    "org_id": "test",
    "pathname": "/api-reference/documents/upload",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Upload Document",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documents.retrieve",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/api-reference/documents/retrieve",
    "default_environment_id": "Production",
    "description": "Retrieve a Document, keying off of either its Vellum-generated ID or its external ID.",
    "domain": "test.com",
    "endpoint_path": "/v1/documents/:id",
    "endpoint_path_alternates": [
      "/v1/documents/{id}",
      "https://api.vellum.ai/v1/documents/:id",
      "https://api.vellum.ai/v1/documents/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documents.retrieve",
    "org_id": "test",
    "pathname": "/api-reference/documents/retrieve",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Document",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documents.list",
    "api_type": "http",
    "authed": false,
    "availability": "GenerallyAvailable",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/api-reference/documents/list",
    "default_environment_id": "Production",
    "description": "Used to list documents. Optionally filter on supported fields.",
    "domain": "test.com",
    "endpoint_path": "/v1/documents",
    "endpoint_path_alternates": [
      "/v1/documents",
      "https://api.vellum.ai/v1/documents",
      "https://api.vellum.ai/v1/documents",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documents.list",
    "org_id": "test",
    "pathname": "/api-reference/documents/list",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "List Documents",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documents.partialUpdate",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/api-reference/documents/partial-update",
    "default_environment_id": "Production",
    "description": "Update a Document, keying off of either its Vellum-generated ID or its external ID. Particularly useful for updating its metadata.",
    "domain": "test.com",
    "endpoint_path": "/v1/documents/:id",
    "endpoint_path_alternates": [
      "/v1/documents/{id}",
      "https://api.vellum.ai/v1/documents/:id",
      "https://api.vellum.ai/v1/documents/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_documents.partialUpdate",
    "org_id": "test",
    "pathname": "/api-reference/documents/partial-update",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Update Document",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_documents.destroy",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/documents",
        "title": "Documents",
      },
    ],
    "canonicalPathname": "/api-reference/documents/destroy",
    "default_environment_id": "Production",
    "description": "Delete a Document, keying off of either its Vellum-generated ID or its external ID.",
    "domain": "test.com",
    "endpoint_path": "/v1/documents/:id",
    "endpoint_path_alternates": [
      "/v1/documents/{id}",
      "https://documents.vellum.ai/v1/documents/:id",
      "https://documents.vellum.ai/v1/documents/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://documents.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742:endpoint_documents.destroy",
    "org_id": "test",
    "pathname": "/api-reference/documents/destroy",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Destroy",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_testSuites.list_test_suite_test_cases",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/test-suites",
        "title": "Test Suites",
      },
      {
        "pathname": "/api-reference/test-suites/test-cases",
        "title": "Test Cases",
      },
    ],
    "canonicalPathname": "/api-reference/test-suites/test-cases/list",
    "default_environment_id": "Production",
    "description": "List the Test Cases associated with a Test Suite",
    "domain": "test.com",
    "endpoint_path": "/v1/test-suites/:id/test-cases",
    "endpoint_path_alternates": [
      "/v1/test-suites/{id}/test-cases",
      "https://api.vellum.ai/v1/test-suites/:id/test-cases",
      "https://api.vellum.ai/v1/test-suites/%7Bid%7D/test-cases",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.list_test_suite_test_cases",
    "org_id": "test",
    "pathname": "/api-reference/test-suites/test-cases/list",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "List Test Cases",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_testSuites.upsert_test_suite_test_case",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/test-suites",
        "title": "Test Suites",
      },
      {
        "pathname": "/api-reference/test-suites/test-cases",
        "title": "Test Cases",
      },
    ],
    "canonicalPathname": "/api-reference/test-suites/test-cases/upsert",
    "default_environment_id": "Production",
    "description": "Upserts a new test case for a test suite, keying off of the optionally provided test case id.
If an id is provided and has a match, the test case will be updated. If no id is provided or no match
is found, a new test case will be appended to the end.
Note that a full replacement of the test case is performed, so any fields not provided will be removed
or overwritten with default values.",
    "domain": "test.com",
    "endpoint_path": "/v1/test-suites/:id/test-cases",
    "endpoint_path_alternates": [
      "/v1/test-suites/{id}/test-cases",
      "https://api.vellum.ai/v1/test-suites/:id/test-cases",
      "https://api.vellum.ai/v1/test-suites/%7Bid%7D/test-cases",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.upsert_test_suite_test_case",
    "org_id": "test",
    "pathname": "/api-reference/test-suites/test-cases/upsert",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Upsert Test Cases",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_testSuites.test_suite_test_cases_bulk",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/test-suites",
        "title": "Test Suites",
      },
      {
        "pathname": "/api-reference/test-suites/test-cases",
        "title": "Test Cases",
      },
    ],
    "canonicalPathname": "/api-reference/test-suites/test-cases/bulk-update",
    "default_environment_id": "Production",
    "description": "Created, replace, and delete Test Cases within the specified Test Suite in bulk",
    "domain": "test.com",
    "endpoint_path": "/v1/test-suites/:id/test-cases-bulk",
    "endpoint_path_alternates": [
      "/v1/test-suites/{id}/test-cases-bulk",
      "https://api.vellum.ai/v1/test-suites/:id/test-cases-bulk",
      "https://api.vellum.ai/v1/test-suites/%7Bid%7D/test-cases-bulk",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "stream",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.test_suite_test_cases_bulk",
    "org_id": "test",
    "pathname": "/api-reference/test-suites/test-cases/bulk-update",
    "response_type": "stream",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Bulk Update Test Cases",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_testSuites.delete_test_suite_test_case",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/test-suites",
        "title": "Test Suites",
      },
      {
        "pathname": "/api-reference/test-suites/test-cases",
        "title": "Test Cases",
      },
    ],
    "canonicalPathname": "/api-reference/test-suites/test-cases/delete",
    "default_environment_id": "Production",
    "description": "Deletes an existing test case for a test suite, keying off of the test case id.",
    "domain": "test.com",
    "endpoint_path": "/v1/test-suites/:id/test-cases/:test_case_id",
    "endpoint_path_alternates": [
      "/v1/test-suites/{id}/test-cases/{test_case_id}",
      "https://api.vellum.ai/v1/test-suites/:id/test-cases/:test_case_id",
      "https://api.vellum.ai/v1/test-suites/%7Bid%7D/test-cases/%7Btest_case_id%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
    ],
    "method": "DELETE",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.delete_test_suite_test_case",
    "org_id": "test",
    "pathname": "/api-reference/test-suites/test-cases/delete",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Delete Test Case",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_testSuiteRuns.create",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/test-suites",
        "title": "Test Suites",
      },
      {
        "pathname": "/api-reference/test-suites/runs",
        "title": "Test Suite Runs",
      },
    ],
    "canonicalPathname": "/api-reference/test-suites/runs/create",
    "default_environment_id": "Production",
    "description": "Trigger a Test Suite and create a new Test Suite Run",
    "domain": "test.com",
    "endpoint_path": "/v1/test-suite-runs",
    "endpoint_path_alternates": [
      "/v1/test-suite-runs",
      "https://api.vellum.ai/v1/test-suite-runs",
      "https://api.vellum.ai/v1/test-suite-runs",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_testSuiteRuns.create",
    "org_id": "test",
    "pathname": "/api-reference/test-suites/runs/create",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Create Test Suite Run",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_testSuiteRuns.retrieve",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/test-suites",
        "title": "Test Suites",
      },
      {
        "pathname": "/api-reference/test-suites/runs",
        "title": "Test Suite Runs",
      },
    ],
    "canonicalPathname": "/api-reference/test-suites/runs/retrieve",
    "default_environment_id": "Production",
    "description": "Retrieve a specific Test Suite Run by ID",
    "domain": "test.com",
    "endpoint_path": "/v1/test-suite-runs/:id",
    "endpoint_path_alternates": [
      "/v1/test-suite-runs/{id}",
      "https://api.vellum.ai/v1/test-suite-runs/:id",
      "https://api.vellum.ai/v1/test-suite-runs/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_testSuiteRuns.retrieve",
    "org_id": "test",
    "pathname": "/api-reference/test-suites/runs/retrieve",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Test Suite Run",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_testSuiteRuns.listExecutions",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/test-suites",
        "title": "Test Suites",
      },
      {
        "pathname": "/api-reference/test-suites/runs",
        "title": "Test Suite Runs",
      },
    ],
    "canonicalPathname": "/api-reference/test-suites/runs/list-executions",
    "default_environment_id": "Production",
    "domain": "test.com",
    "endpoint_path": "/v1/test-suite-runs/:id/executions",
    "endpoint_path_alternates": [
      "/v1/test-suite-runs/{id}/executions",
      "https://api.vellum.ai/v1/test-suite-runs/:id/executions",
      "https://api.vellum.ai/v1/test-suite-runs/%7Bid%7D/executions",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.subpackage_testSuiteRuns.listExecutions",
    "org_id": "test",
    "pathname": "/api-reference/test-suites/runs/list-executions",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "List Test Suite Executions",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_folderEntities.add_entity_to_folder",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/folders",
        "title": "Folders",
      },
    ],
    "canonicalPathname": "/api-reference/folders/add-entity-to-folder",
    "default_environment_id": "Production",
    "description": "Add an entity to a specific folder or root directory.
Adding an entity to a folder will remove it from any other folders it might have been a member of.",
    "domain": "test.com",
    "endpoint_path": "/v1/folders/:folder_id/add-entity",
    "endpoint_path_alternates": [
      "/v1/folders/{folder_id}/add-entity",
      "https://api.vellum.ai/v1/folders/:folder_id/add-entity",
      "https://api.vellum.ai/v1/folders/%7Bfolder_id%7D/add-entity",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
    ],
    "method": "POST",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.add_entity_to_folder",
    "org_id": "test",
    "pathname": "/api-reference/folders/add-entity-to-folder",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Add Entity to Folder",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_folderEntities.list",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/folders",
        "title": "Folders",
      },
    ],
    "canonicalPathname": "/api-reference/folders/list-folder-entities",
    "default_environment_id": "Production",
    "description": "List all folder entities within a specified folder.",
    "domain": "test.com",
    "endpoint_path": "/v1/folder-entities",
    "endpoint_path_alternates": [
      "/v1/folder-entities",
      "https://api.vellum.ai/v1/folder-entities",
      "https://api.vellum.ai/v1/folder-entities",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.list-1",
    "org_id": "test",
    "pathname": "/api-reference/folders/list-folder-entities",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "List Folder Entities",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_workspaceSecrets.retrieve",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/secrets",
        "title": "Secrets",
      },
    ],
    "canonicalPathname": "/api-reference/secrets/retrieve-workspace-secret",
    "default_environment_id": "Production",
    "description": "Used to retrieve a Workspace Secret given its ID or name.",
    "domain": "test.com",
    "endpoint_path": "/v1/workspace-secrets/:id",
    "endpoint_path_alternates": [
      "/v1/workspace-secrets/{id}",
      "https://api.vellum.ai/v1/workspace-secrets/:id",
      "https://api.vellum.ai/v1/workspace-secrets/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "GET",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.retrieve-1",
    "org_id": "test",
    "pathname": "/api-reference/secrets/retrieve-workspace-secret",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Retrieve Workspace Secret",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
  {
    "api_definition_id": "83d12b33-dab6-466a-9d55-0dbc8135e742",
    "api_endpoint_id": "endpoint_workspaceSecrets.partialUpdate",
    "api_type": "http",
    "authed": false,
    "availability": "Beta",
    "breadcrumb": [
      {
        "pathname": "/api-reference",
        "title": "API Reference",
      },
      {
        "pathname": "/api-reference/secrets",
        "title": "Secrets",
      },
    ],
    "canonicalPathname": "/api-reference/secrets/update-workspace-secret",
    "default_environment_id": "Production",
    "description": "Used to update a Workspace Secret given its ID or name.",
    "domain": "test.com",
    "endpoint_path": "/v1/workspace-secrets/:id",
    "endpoint_path_alternates": [
      "/v1/workspace-secrets/{id}",
      "https://api.vellum.ai/v1/workspace-secrets/:id",
      "https://api.vellum.ai/v1/workspace-secrets/%7Bid%7D",
    ],
    "environments": [
      {
        "id": "Production",
        "url": "https://api.vellum.ai",
      },
    ],
    "keywords": [
      "endpoint",
      "api",
      "http",
      "rest",
      "openapi",
      "json",
    ],
    "method": "PATCH",
    "objectID": "test:test.com:83d12b33-dab6-466a-9d55-0dbc8135e742.partialUpdate",
    "org_id": "test",
    "pathname": "/api-reference/secrets/update-workspace-secret",
    "response_type": "json",
    "tab": {
      "pathname": "/api-reference",
      "title": "API Reference",
    },
    "title": "Update Workspace Secret",
    "type": "api-reference",
    "visible_by": [
      "role/everyone",
    ],
  },
]