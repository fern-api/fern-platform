{
  "baseUrl": {
    "domain": "boundary.docs.buildwithfern.com"
  },
  "definition": {
    "config": {
      "navigation": {
        "tabs": [
          {
            "type": "group",
            "title": "Home",
            "icon": "fa-solid fa-house",
            "items": [
              {
                "type": "page",
                "id": "pages/welcome.mdx",
                "title": "Welcome",
                "urlSlug": "welcome",
                "fullSlug": ["home"],
                "hidden": false
              }
            ],
            "urlSlugOverride": "home",
            "urlSlug": "home",
            "skipUrlSlug": false
          },
          {
            "type": "group",
            "title": "Guide",
            "icon": "fa-solid fa-book",
            "items": [
              {
                "type": "section",
                "title": "Introduction",
                "urlSlug": "introduction",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "01-guide/what-are-function-definitions.mdx",
                    "title": "What is BAML?",
                    "icon": "fa-regular fa-question-circle",
                    "urlSlug": "what-is-baml",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/what-is-baml_src.mdx",
                    "title": "What's the baml_src folder",
                    "icon": "fa-regular fa-folder",
                    "urlSlug": "baml_src",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/what-is-baml_client.mdx",
                    "title": "What's baml_client",
                    "icon": "fa-regular fa-folder-gear",
                    "urlSlug": "baml_client",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Installation: Editors",
                "urlSlug": "installation-editors",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "01-guide/01-editors/vscode.mdx",
                    "title": "VSCode Extension",
                    "icon": "fa-brands fa-microsoft",
                    "urlSlug": "vs-code-extension",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/01-editors/cursor.mdx",
                    "title": "Cursor Extension",
                    "icon": "fa-brands fa-microsoft",
                    "urlSlug": "cursor-extension",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/01-editors/others.mdx",
                    "title": "Others",
                    "icon": "fa-brands fa-microsoft",
                    "urlSlug": "others",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Installation: Language",
                "urlSlug": "installation-language",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "01-guide/02-languages/python.mdx",
                    "title": "Python",
                    "icon": "fa-brands fa-python",
                    "urlSlug": "python",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/02-languages/typescript.mdx",
                    "title": "Typescript",
                    "icon": "fa-brands fa-js",
                    "urlSlug": "typescript",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/02-languages/ruby.mdx",
                    "title": "Ruby",
                    "icon": "fa-regular fa-gem",
                    "urlSlug": "ruby",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/02-languages/rest.mdx",
                    "title": "REST API (other languages)",
                    "icon": "fa-regular fa-network-wired",
                    "urlSlug": "rest-api-other-languages",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/08-integrations/nextjs.mdx",
                    "title": "NextJS",
                    "icon": "fa-brands fa-react",
                    "urlSlug": "next-js",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Development",
                "urlSlug": "development",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "01-guide/03-development/environment-variables.mdx",
                    "title": "Environment Variables",
                    "icon": "fa-regular fa-cogs",
                    "urlSlug": "environment-variables",
                    "fullSlug": ["", "guide", "development", "environment-variables"],
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/03-development/terminal-logs.mdx",
                    "title": "Terminal Logs",
                    "icon": "fa-regular fa-file-lines",
                    "urlSlug": "terminal-logs",
                    "fullSlug": ["", "guide", "development", "terminal-logs"],
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/03-development/upgrade-baml-versions.mdx",
                    "title": "Upgrade BAML versions",
                    "icon": "fa-regular fa-circle-arrow-up",
                    "urlSlug": "upgrade-baml-versions",
                    "fullSlug": ["", "guide", "development", "upgrade-baml-versions"],
                    "hidden": false
                  },
                  {
                    "type": "section",
                    "title": "Deploying",
                    "icon": "fa-regular fa-rocket",
                    "urlSlug": "deploying",
                    "collapsed": false,
                    "hidden": false,
                    "items": [
                      {
                        "type": "page",
                        "id": "01-guide/03-development/deploying/aws.mdx",
                        "title": "AWS",
                        "icon": "fa-brands fa-aws",
                        "urlSlug": "aws",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "01-guide/03-development/deploying/nextjs.mdx",
                        "title": "NextJs",
                        "icon": "fa-brands fa-react",
                        "urlSlug": "next-js",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "01-guide/03-development/deploying/docker.mdx",
                        "title": "Docker",
                        "icon": "fa-brands fa-docker",
                        "urlSlug": "docker",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "01-guide/03-development/deploying/openapi.mdx",
                        "title": "Docker (REST API)",
                        "icon": "fa-brands fa-docker",
                        "urlSlug": "docker-rest-api",
                        "hidden": false
                      }
                    ],
                    "skipUrlSlug": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "BAML Basics",
                "urlSlug": "baml-basics",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "01-guide/04-baml-basics/my-first-function.mdx",
                    "title": "Prompting with BAML",
                    "icon": "fa-solid fa-terminal",
                    "urlSlug": "prompting-with-baml",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/04-baml-basics/switching-llms.mdx",
                    "title": "Switching LLMs",
                    "icon": "fa-regular fa-random",
                    "urlSlug": "switching-ll-ms",
                    "fullSlug": ["guide", "baml-basics", "switching-llms"],
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/04-baml-basics/testing-functions.mdx",
                    "title": "Testing functions",
                    "icon": "fa-regular fa-vial",
                    "urlSlug": "testing-functions",
                    "fullSlug": ["", "guide", "baml-basics", "testing-functions"],
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/04-baml-basics/streaming.mdx",
                    "title": "Streaming",
                    "icon": "fa-regular fa-faucet",
                    "urlSlug": "streaming",
                    "fullSlug": ["", "guide", "baml-basics", "streaming"],
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/04-baml-basics/multi-modal.mdx",
                    "title": "Multi-Modal (Images / Audio)",
                    "icon": "fa-regular fa-image",
                    "urlSlug": "multi-modal-images-audio",
                    "fullSlug": ["", "guide", "baml-basics", "multi-modal"],
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/04-baml-basics/error-handling.mdx",
                    "title": "Error Handling",
                    "icon": "fa-regular fa-triangle-exclamation",
                    "urlSlug": "error-handling",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/04-baml-basics/concurrent-calls.mdx",
                    "title": "Concurrent Calls",
                    "icon": "fa-regular fa-clock-rotate-left",
                    "urlSlug": "concurrent-calls",
                    "fullSlug": ["", "guide", "baml-basics", "concurrent-calls"],
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "BAML Advanced",
                "urlSlug": "baml-advanced",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "01-guide/05-baml-advanced/client-registry.mdx",
                    "title": "LLM Client Registry",
                    "icon": "fa-regular fa-gears",
                    "urlSlug": "llm-client-registry",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/05-baml-advanced/dynamic-types.mdx",
                    "title": "Dynamic / Runtime Types",
                    "icon": "fa-solid fa-person-running",
                    "urlSlug": "dynamic-runtime-types",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/05-baml-advanced/reusing-prompt-snippets.mdx",
                    "title": "Reusing Prompt Snippets",
                    "icon": "fa-regular fa-repeat",
                    "urlSlug": "reusing-prompt-snippets",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/05-baml-advanced/prompt-caching.mdx",
                    "title": "Prompt Caching / Message Role Metadata",
                    "icon": "fa-regular fa-database",
                    "urlSlug": "prompt-caching-message-role-metadata",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/05-baml-advanced/validations.mdx",
                    "title": "Validations",
                    "icon": "fa-regular fa-check-circle",
                    "urlSlug": "validations",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Boundary Cloud",
                "urlSlug": "boundary-cloud",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "section",
                    "title": "Functions",
                    "icon": "fa-cloud",
                    "urlSlug": "functions",
                    "collapsed": false,
                    "hidden": false,
                    "items": [
                      {
                        "type": "page",
                        "id": "01-guide/functions/get-started.mdx",
                        "title": "Get Started",
                        "urlSlug": "get-started",
                        "fullSlug": ["guide", "cloud", "functions", "get-started"],
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "01-guide/functions/using-openapi.mdx",
                        "title": "Using OpenAPI",
                        "urlSlug": "using-open-api",
                        "fullSlug": ["guide", "cloud", "functions", "using-openapi"],
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "01-guide/functions/environment-variables.mdx",
                        "title": "Environment Variables",
                        "urlSlug": "environment-variables",
                        "fullSlug": ["guide", "cloud", "functions", "environment-variables"],
                        "hidden": false
                      }
                    ],
                    "skipUrlSlug": false
                  },
                  {
                    "type": "section",
                    "title": "Observability",
                    "icon": "fa-chart-simple",
                    "urlSlug": "observability",
                    "collapsed": false,
                    "hidden": false,
                    "items": [
                      {
                        "type": "page",
                        "id": "01-guide/07-observability/studio.mdx",
                        "title": "Tracking Usage",
                        "icon": "fa-regular fa-bar-chart",
                        "urlSlug": "tracking-usage",
                        "hidden": false
                      }
                    ],
                    "skipUrlSlug": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Comparisons",
                "urlSlug": "comparisons",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "01-guide/09-comparisons/marvin.mdx",
                    "title": "BAML vs Marvin",
                    "icon": "fa-solid fa-magnifying-glass",
                    "urlSlug": "baml-vs-marvin",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/09-comparisons/pydantic.mdx",
                    "title": "BAML vs Pydantic",
                    "icon": "fa-solid fa-magnifying-glass",
                    "urlSlug": "baml-vs-pydantic",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "page",
                "id": "01-guide/contact.mdx",
                "title": "Contact",
                "icon": "fa-regular fa-envelope",
                "urlSlug": "contact",
                "hidden": false
              }
            ],
            "urlSlugOverride": "guide",
            "urlSlug": "guide",
            "skipUrlSlug": false
          },
          {
            "type": "group",
            "title": "Examples",
            "icon": "fa-solid fa-grid-2",
            "items": [
              {
                "type": "page",
                "id": "02-examples/interactive-examples.mdx",
                "title": "Interactive Examples",
                "icon": "fa-solid fa-play",
                "urlSlug": "interactive-examples",
                "hidden": false
              },
              {
                "type": "section",
                "title": "Prompt Engineering",
                "urlSlug": "prompt-engineering",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "01-guide/06-prompt-engineering/hallucinations.mdx",
                    "title": "Reducing Hallucinations",
                    "icon": "fa-regular fa-person-fairy",
                    "urlSlug": "reducing-hallucinations",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/06-prompt-engineering/chat-history.mdx",
                    "title": "Chat",
                    "icon": "fa-regular fa-comments",
                    "urlSlug": "chat",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/06-prompt-engineering/tools.mdx",
                    "title": "Tools / Function Calling",
                    "icon": "fa-regular fa-wrench",
                    "urlSlug": "tools-function-calling",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/06-prompt-engineering/chain-of-thought.mdx",
                    "title": "Chain of Thought",
                    "icon": "fa-solid fa-brain",
                    "urlSlug": "chain-of-thought",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/06-prompt-engineering/symbol-tuning.mdx",
                    "title": "Symbol Tuning",
                    "icon": "fa-regular fa-adjust",
                    "urlSlug": "symbol-tuning",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/introduction.mdx",
                    "title": "Retrieval Augmented Generation",
                    "icon": "fa-regular fa-database",
                    "urlSlug": "retrieval-augmented-generation",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              }
            ],
            "urlSlug": "examples",
            "skipUrlSlug": false
          },
          {
            "type": "group",
            "title": "BAML Reference",
            "icon": "fa-solid fa-code",
            "items": [
              {
                "type": "page",
                "id": "03-reference/overview.mdx",
                "title": "Overview",
                "urlSlug": "overview",
                "hidden": false
              },
              {
                "type": "section",
                "title": "baml-cli",
                "urlSlug": "baml-cli",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "03-reference/baml-cli/init.mdx",
                    "title": "init",
                    "urlSlug": "init",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml-cli/generate.mdx",
                    "title": "generate",
                    "urlSlug": "generate",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml-cli/serve.mdx",
                    "title": "serve",
                    "urlSlug": "serve",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml-cli/dev.mdx",
                    "title": "dev",
                    "urlSlug": "dev",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Language Reference",
                "urlSlug": "baml",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "section",
                    "title": "General BAML Syntax",
                    "urlSlug": "general-baml-syntax",
                    "collapsed": false,
                    "hidden": false,
                    "items": [
                      {
                        "type": "page",
                        "id": "03-reference/baml/comments.mdx",
                        "title": "comments",
                        "urlSlug": "comments",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "03-reference/baml/env-vars.mdx",
                        "title": "Enviornment Variables",
                        "urlSlug": "enviornment-variables",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "03-reference/baml/string.mdx",
                        "title": "string",
                        "urlSlug": "string",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "03-reference/baml/int-float.mdx",
                        "title": "int / float",
                        "urlSlug": "int-float",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "03-reference/baml/bool.mdx",
                        "title": "bool",
                        "urlSlug": "bool",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "03-reference/baml/array.mdx",
                        "title": "array (list)",
                        "urlSlug": "array-list",
                        "hidden": false
                      },
                      {
                        "type": "page",
                        "id": "03-reference/baml/map.mdx",
                        "title": "map (dictionary)",
                        "urlSlug": "map-dictionary",
                        "hidden": false
                      }
                    ],
                    "skipUrlSlug": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/types.mdx",
                    "title": "Types",
                    "urlSlug": "types",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/function.mdx",
                    "title": "function",
                    "urlSlug": "function",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/test.mdx",
                    "title": "test",
                    "urlSlug": "test",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/template_string.mdx",
                    "title": "template_string",
                    "urlSlug": "template-string",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/client-llm.mdx",
                    "title": "client<llm>",
                    "urlSlug": "client-llm",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/class.mdx",
                    "title": "class",
                    "urlSlug": "class",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/enum.mdx",
                    "title": "enum",
                    "urlSlug": "enum",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/generator.mdx",
                    "title": "generator",
                    "urlSlug": "generator",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Attributes",
                "urlSlug": "attributes",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "03-reference/baml/attributes/attributes-overview.mdx",
                    "title": "What are attributes?",
                    "urlSlug": "what-are-attributes",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/attributes/alias.mdx",
                    "title": "@alias / @@alias",
                    "urlSlug": "alias-alias",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/attributes/description.mdx",
                    "title": "@description / @@description",
                    "urlSlug": "description-description",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/attributes/skip.mdx",
                    "title": "@skip",
                    "urlSlug": "skip",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/attributes/assert.mdx",
                    "title": "@assert",
                    "urlSlug": "assert",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/attributes/check.mdx",
                    "title": "@check",
                    "urlSlug": "check",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/attributes/dynamic.mdx",
                    "title": "@@dynamic",
                    "urlSlug": "dynamic",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "LLM Client Providers",
                "urlSlug": "llm-client-providers",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/aws-bedrock.mdx",
                    "title": "AWS Bedrock",
                    "urlSlug": "aws-bedrock",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/anthropic.mdx",
                    "title": "Anthropic",
                    "urlSlug": "anthropic",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/google-ai.mdx",
                    "title": "Google AI: Gemini",
                    "urlSlug": "google-ai-gemini",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/vertex.mdx",
                    "title": "Google: Vertex",
                    "urlSlug": "google-vertex",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/openai.mdx",
                    "title": "OpenAI",
                    "urlSlug": "open-ai",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/azure.mdx",
                    "title": "OpenAI from Azure",
                    "urlSlug": "open-ai-from-azure",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/openai-generic.mdx",
                    "title": "openai-generic",
                    "urlSlug": "openai-generic",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/groq.mdx",
                    "title": "openai-generic: Groq",
                    "urlSlug": "openai-generic-groq",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/huggingface.mdx",
                    "title": "openai-generic: Hugging Face",
                    "urlSlug": "openai-generic-hugging-face",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/keywordsai.mdx",
                    "title": "openai-generic: Keywords AI",
                    "urlSlug": "openai-generic-keywords-ai",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/lmstudio.mdx",
                    "title": "openai-generic: LM Studio",
                    "urlSlug": "openai-generic-lm-studio",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/ollama.mdx",
                    "title": "openai-generic: Ollama",
                    "urlSlug": "openai-generic-ollama",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/openrouter.mdx",
                    "title": "openai-generic: OpenRouter",
                    "urlSlug": "openai-generic-open-router",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/together.mdx",
                    "title": "openai-generic: TogetherAI",
                    "urlSlug": "openai-generic-together-ai",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/unify.mdx",
                    "title": "openai-generic: Unify AI",
                    "urlSlug": "openai-generic-unify-ai",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/providers/vllm.mdx",
                    "title": "openai-generic: vLLM",
                    "urlSlug": "openai-generic-v-llm",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "LLM Client Strategies",
                "urlSlug": "llm-client-strategies",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/strategy/retry.mdx",
                    "title": "Retry Policy",
                    "urlSlug": "retry-policy",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/strategy/fallback.mdx",
                    "title": "Fallback",
                    "urlSlug": "fallback",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/clients/strategy/round-robin.mdx",
                    "title": "Round Robin",
                    "urlSlug": "round-robin",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "baml_client",
                "urlSlug": "baml-client",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "03-reference/baml_client/typebuilder.mdx",
                    "title": "TypeBuilder",
                    "urlSlug": "type-builder",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "01-guide/05-baml-advanced/client-registry.mdx",
                    "title": "ClientRegistry",
                    "urlSlug": "client-registry",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Prompt Syntax",
                "urlSlug": "prompt-syntax",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "03-reference/baml/prompt-syntax/what-is-jinja.mdx",
                    "title": "What is jinja?",
                    "urlSlug": "what-is-jinja",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/prompt-syntax/output-format.mdx",
                    "title": "ctx.output_format",
                    "urlSlug": "ctx-output-format",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/prompt-syntax/ctx.mdx",
                    "title": "ctx.client",
                    "urlSlug": "ctx-client",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/prompt-syntax/role.mdx",
                    "title": "_.role",
                    "urlSlug": "role",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/prompt-syntax/variables.mdx",
                    "title": "Variables",
                    "urlSlug": "variables",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/prompt-syntax/conditionals.mdx",
                    "title": "Conditionals",
                    "urlSlug": "conditionals",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/baml/prompt-syntax/loops.mdx",
                    "title": "Loops",
                    "urlSlug": "loops",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Editor Extension Settings",
                "urlSlug": "editor-extension-settings",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "03-reference/vscode-ext/clipath.mdx",
                    "title": "baml.cliPath",
                    "urlSlug": "baml-cli-path",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/vscode-ext/generateCodeOnSave.mdx",
                    "title": "baml.generateCodeOnSave",
                    "urlSlug": "baml-generate-code-on-save",
                    "hidden": false
                  },
                  {
                    "type": "page",
                    "id": "03-reference/vscode-ext/restartTSServerOnSave.mdx",
                    "title": "baml.restartTSServerOnSave",
                    "urlSlug": "baml-restart-ts-server-on-save",
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Boundary Extraction API",
                "urlSlug": "boundary-extraction-api",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "apiV2",
                    "node": {
                      "id": "eaa1074a-b676-4a6b-93df-43c52a64d62f",
                      "type": "apiReference",
                      "title": "API Reference",
                      "apiDefinitionId": "eaa1074a-b676-4a6b-93df-43c52a64d62f",
                      "overviewPageId": "03-reference/extract/summary.mdx",
                      "paginated": false,
                      "slug": "ref/boundary-extraction-api/extract",
                      "hideTitle": false,
                      "showErrors": true,
                      "children": [
                        {
                          "id": "eaa1074a-b676-4a6b-93df-43c52a64d62f.extractData",
                          "type": "endpoint",
                          "method": "POST",
                          "endpointId": "endpoint_.extractData",
                          "apiDefinitionId": "eaa1074a-b676-4a6b-93df-43c52a64d62f",
                          "isResponseStream": false,
                          "title": "Extract",
                          "slug": "ref/boundary-extraction-api/extract/extract-data"
                        },
                        {
                          "id": "eaa1074a-b676-4a6b-93df-43c52a64d62f.page:03-reference/extract/examples.mdx",
                          "type": "page",
                          "pageId": "03-reference/extract/examples.mdx",
                          "title": "Extraction Examples",
                          "slug": "ref/boundary-extraction-api/extract/extraction-examples"
                        }
                      ],
                      "pointsTo": "ref/boundary-extraction-api/extract/extract-data"
                    }
                  }
                ],
                "skipUrlSlug": false
              },
              {
                "type": "section",
                "title": "Boundary Cloud API",
                "urlSlug": "boundary-cloud-api",
                "collapsed": false,
                "hidden": false,
                "items": [
                  {
                    "type": "page",
                    "id": "03-reference/cloud/limits.mdx",
                    "title": "Limits",
                    "urlSlug": "limits",
                    "fullSlug": ["ref", "cloud", "limits"],
                    "hidden": false
                  }
                ],
                "skipUrlSlug": false
              }
            ],
            "urlSlugOverride": "ref",
            "urlSlug": "ref",
            "skipUrlSlug": false
          },
          {
            "type": "link",
            "title": "Playground",
            "icon": "fa-solid fa-play",
            "url": "https://promptfiddle.com"
          },
          {
            "type": "group",
            "title": "Changelog",
            "icon": "fa-regular fa-history",
            "items": [
              {
                "type": "page",
                "id": "pages/changelog.mdx",
                "title": "Changelog",
                "icon": "fa-regular fa-history",
                "urlSlug": "changelog",
                "hidden": false
              }
            ],
            "urlSlugOverride": "changelog",
            "urlSlug": "changelog",
            "skipUrlSlug": false
          }
        ]
      },
      "logoHeight": 40,
      "colorsV3": {
        "type": "light",
        "accentPrimary": {
          "r": 167,
          "g": 139,
          "b": 250,
          "a": 1
        },
        "logo": "e6cede84-0939-4dc8-a33e-3f0293e31ec5",
        "background": {
          "type": "solid",
          "r": 255,
          "g": 255,
          "b": 255,
          "a": 1
        },
        "sidebarBackground": {
          "r": 254,
          "g": 254,
          "b": 254,
          "a": 1
        }
      },
      "navbarLinks": [
        {
          "type": "github",
          "url": "https://github.com/boundaryml/baml"
        },
        {
          "type": "filled",
          "text": "Help on Discord",
          "url": "https://discord.gg/BTNBeXGuaS"
        }
      ],
      "title": "Boundary Documentation",
      "favicon": "e6cede84-0939-4dc8-a33e-3f0293e31ec5",
      "typographyV2": {
        "headingsFont": {
          "type": "custom",
          "name": "OpenSans",
          "variants": [
            {
              "fontFile": "735fbf2b-9cb4-42f9-a586-4ddbed3f764e",
              "weight": ["600"]
            }
          ]
        },
        "bodyFont": {
          "type": "custom",
          "name": "OpenSans",
          "variants": [
            {
              "fontFile": "e0fb9eac-cbc9-41c7-8f84-dcb0231b7167",
              "weight": ["400"]
            },
            {
              "fontFile": "52f14847-0fa6-4168-8581-c354550d3d52",
              "style": ["italic"]
            }
          ]
        }
      },
      "layout": {
        "pageWidth": {
          "type": "full"
        },
        "headerHeight": {
          "type": "px",
          "value": 54
        },
        "searchbarPlacement": "HEADER",
        "tabsPlacement": "HEADER",
        "contentAlignment": "CENTER",
        "headerPosition": "FIXED",
        "disableHeader": false
      },
      "css": {
        "inline": [
          ".fern-sidebar-link-content {\n  padding-top: 0.4rem;\n  padding-bottom: 0.4rem;\n}\n\n.fern-sidebar-link-container {\n  min-height: 26px;\n}\n\n.fern-prose code {\n  background-color: #f8f8f8;\n  /* font-weight: 600; */\n}\n\n.prose code {\n  background-color: #f8f8f8;\n}\n\n.fern-sidebar-link-container[data-state=\"active\"] .fern-sidebar-link {\n  font-weight: 600;\n}\n.fern-header {\n  border: 0px;\n}\n.fern-header-tabs-list {\n  height: 28px;\n  font-size: 12px;\n}\n.fern-header-tabs {\n  height: 32px;\n  max-height: 36px;\n}\n\n.fern-header-tab-button > * {\n  font-size: 12px;\n  /* color */\n  color: #333333;\n}\n\n/* .fern-sidebar-container {\n  color: #000000;\n} */\n\nbody .p {\n  font-size: 1rem !important;\n}\n\np {\n  color: #333333;\n}\n\nfern-docs {\n  color: #333333;\n}\n\nul {\n  color: #333333;\n}\n\nfern-button-text {\n  color: #ffffff;\n}\n\nh1,\nh2,\nh3,\nh4,\nh5,\nh6 {\n  color: #333333;\n}\n\n.fern-sidebar-link-container:not([data-state=\"active\"]) .fern-sidebar-link {\n  color: #333333;\n}\n\nspan:not(a span) {\n  color: #333333;\n}\n\n:root {\n  --accent-aaa: 139, 92, 246;\n}\n\n.fern-mdx-link {\n  text-decoration-color: rgba(167, 139, 250, 1);\n}\n\n/* .fern-sidebar-group li {\n    margin-top: 0px !important;\n} */\n\n/* .group/sidebar {\n  z-index: 1000;\n  padding-top: 20px;\n} */\n\n/* .fern-search-bar {\n  margin-bottom: 10px;\n} */\n"
        ]
      },
      "js": {
        "files": []
      },
      "redirects": [
        {
          "source": "/docs/get-started/what-is-baml",
          "destination": "/guide/introduction/what-is-baml"
        },
        {
          "source": "/docs/get-started/interactive-demos",
          "destination": "/examples/interactive-examples"
        },
        {
          "source": "/docs/get-started/quickstart/python",
          "destination": "/guide/installation-language/python"
        },
        {
          "source": "/docs/get-started/quickstart/typescript",
          "destination": "/guide/installation-language/typescript"
        },
        {
          "source": "/docs/get-started/quickstart/ruby",
          "destination": "/guide/installation-language/ruby"
        },
        {
          "source": "/docs/get-started/quickstart/openapi",
          "destination": "/guide/installation-language/rest-api-other-languages"
        },
        {
          "source": "/docs/get-started/quickstart/editors-vscode",
          "destination": "/guide/installation-editors/vs-code-extension"
        },
        {
          "source": "/docs/get-started/quickstart/editors-other",
          "destination": "/guide/installation-editors/others"
        },
        {
          "source": "/docs/get-started/debugging/vscode-playground",
          "destination": "/guide/development/terminal-logs"
        },
        {
          "source": "/docs/get-started/debugging/enable-logging",
          "destination": "/guide/development/terminal-logs"
        },
        {
          "source": "/baml/get-started/debugging/exception-handling",
          "destination": "/guide/baml-basics/error-handling"
        },
        {
          "source": "/docs/get-started/deploying/docker",
          "destination": "/guide/development/deploying/docker"
        },
        {
          "source": "/docs/get-started/deploying/nextjs",
          "destination": "/guide/development/deploying/nextjs"
        },
        {
          "source": "/docs/get-started/deploying/aws",
          "destination": "/guide/development/deploying/aws"
        },
        {
          "source": "/docs/get-started/deploying/openapi",
          "destination": "/guide/development/deploying/openapi"
        },
        {
          "source": "/docs/snippets/syntax/comments",
          "destination": "/ref/baml/general-baml-syntax/comments"
        },
        {
          "source": "/docs/snippets/syntax/strings",
          "destination": "/ref/baml/general-baml-syntax/string"
        },
        {
          "source": "/docs/snippets/syntax/lists",
          "destination": "/ref/baml/general-baml-syntax/array-list"
        },
        {
          "source": "/docs/snippets/syntax/dictionaries",
          "destination": "/ref/baml/general-baml-syntax/map-dictionary"
        },
        {
          "source": "/docs/snippets/supported-types",
          "destination": "/ref/baml/types"
        },
        {
          "source": "/docs/snippets/clients/overview",
          "destination": "/ref/baml/client-llm"
        },
        {
          "source": "/docs/snippets/clients/providers/anthropic",
          "destination": "/ref/llm-client-providers/anthropic"
        },
        {
          "source": "/docs/snippets/clients/providers/aws-bedrock",
          "destination": "/ref/llm-client-providers/aws-bedrock"
        },
        {
          "source": "/docs/snippets/clients/providers/azure",
          "destination": "/ref/llm-client-providers/open-ai-from-azure"
        },
        {
          "source": "/docs/snippets/clients/providers/gemini",
          "destination": "/ref/llm-client-providers/google-ai-studio"
        },
        {
          "source": "/docs/snippets/clients/providers/groq",
          "destination": "/ref/llm-client-providers/openai-generic-groq"
        },
        {
          "source": "/docs/snippets/clients/providers/huggingface",
          "destination": "/ref/llm-client-providers/openai-generic-hugging-face"
        },
        {
          "source": "/docs/snippets/clients/providers/ollama",
          "destination": "/ref/llm-client-providers/openai-generic-ollama"
        },
        {
          "source": "/docs/snippets/clients/providers/openai",
          "destination": "/ref/llm-client-providers/open-ai"
        },
        {
          "source": "/docs/snippets/clients/providers/openai-generic",
          "destination": "/ref/llm-client-providers/openai-generic"
        },
        {
          "source": "/docs/snippets/clients/providers/openrouter",
          "destination": "/ref/llm-client-providers/openai-generic-open-router"
        },
        {
          "source": "/docs/snippets/clients/providers/together",
          "destination": "/ref/llm-client-providers/openai-generic-together-ai"
        },
        {
          "source": "/docs/snippets/clients/providers/vertex",
          "destination": "/ref/llm-client-providers/vertex"
        },
        {
          "source": "/docs/snippets/clients/providers/vllm",
          "destination": "/ref/llm-client-providers/openai-generic-v-llm"
        },
        {
          "source": "/docs/snippets/clients/providers/lmstudio",
          "destination": "/ref/llm-client-providers/openai-generic-lm-studio"
        },
        {
          "source": "/docs/snippets/clients/providers/keywordsai",
          "destination": "/ref/llm-client-providers/openai-generic-keywords-ai"
        },
        {
          "source": "/docs/snippets/clients/fallback",
          "destination": "/ref/llm-client-strategies/fallback"
        },
        {
          "source": "/docs/snippets/clients/round-robin",
          "destination": "/ref/llm-client-strategies/round-robin"
        },
        {
          "source": "/docs/snippets/clients/retry",
          "destination": "/ref/llm-client-strategies/retry-policy"
        },
        {
          "source": "/docs/snippets/functions/overview",
          "destination": "/ref/baml/function"
        },
        {
          "source": "/docs/snippets/functions/classification",
          "destination": "/guide/baml-basics/prompting-with-baml"
        },
        {
          "source": "/docs/snippets/functions/extraction",
          "destination": "/guide/baml-basics/prompting-with-baml"
        },
        {
          "source": "/docs/snippets/functions/function-calling",
          "destination": "/examples/prompt-engineering/tools-function-calling"
        },
        {
          "source": "/docs/snippets/class",
          "destination": "/ref/baml/class"
        },
        {
          "source": "/docs/snippets/enum",
          "destination": "/ref/baml/enum"
        },
        {
          "source": "/docs/snippets/prompt-syntax/what-is-jinja",
          "destination": "/ref/prompt-syntax/what-is-jinja"
        },
        {
          "source": "/docs/snippets/prompt-syntax/output-format",
          "destination": "/ref/prompt-syntax/ctx-output-format"
        },
        {
          "source": "/docs/snippets/prompt-syntax/roles",
          "destination": "/ref/prompt-syntax/role"
        },
        {
          "source": "/docs/snippets/prompt-syntax/variables",
          "destination": "/ref/prompt-syntax/variables"
        },
        {
          "source": "/docs/snippets/prompt-syntax/conditionals",
          "destination": "/ref/prompt-syntax/conditionals"
        },
        {
          "source": "/docs/snippets/prompt-syntax/loops",
          "destination": "/ref/prompt-syntax/loops"
        },
        {
          "source": "/docs/snippets/prompt-syntax/comments",
          "destination": "/ref/baml/general-baml-syntax/comments"
        },
        {
          "source": "/docs/snippets/prompt-syntax/ctx",
          "destination": "/ref/prompt-syntax/ctx-client"
        },
        {
          "source": "/docs/snippets/template-string",
          "destination": "/ref/baml/template-string"
        },
        {
          "source": "/docs/snippets/test-cases",
          "destination": "/ref/baml/test"
        },
        {
          "source": "/docs/calling-baml/dynamic-types",
          "destination": "/guide/baml-advanced/dynamic-runtime-types"
        },
        {
          "source": "/docs/calling-baml/client-registry",
          "destination": "/guide/baml-advanced/llm-client-registry"
        },
        {
          "source": "/docs/calling-baml/checks-and-asserts",
          "destination": "/guide/baml-basics/error-handling"
        },
        {
          "source": "/docs/calling-baml/generate-baml-client",
          "destination": "/ref/baml-client/type-builder"
        },
        {
          "source": "/docs/calling-baml/set-env-vars",
          "destination": "/docs/guide/development/environment-variables"
        },
        {
          "source": "/docs/calling-baml/calling-functions",
          "destination": "/guide/baml-basics/prompting-and-calling-ll-ms-with-baml"
        },
        {
          "source": "/docs/calling-baml/streaming",
          "destination": "/guide/baml-basics/streaming"
        },
        {
          "source": "/docs/calling-baml/concurrent-calls",
          "destination": "/guide/baml-basics/concurrent-calls"
        },
        {
          "source": "/docs/calling-baml/multi-modal",
          "destination": "/guide/baml-basics/multi-modal"
        },
        {
          "source": "/docs/baml-nextjs/baml-nextjs",
          "destination": "/guides/installation-language/next-js"
        },
        {
          "source": "/docs/observability/overview",
          "destination": "/guide/observability/tracking-usage"
        },
        {
          "source": "/docs/observability/tracing-tagging",
          "destination": "/guide/observability/tracking-usage"
        },
        {
          "source": "/docs/comparisons/marvin",
          "destination": "/guide/comparisons/baml-vs-marvin"
        },
        {
          "source": "/docs/comparisons/pydantic",
          "destination": "/guide/comparisons/baml-vs-pydantic"
        },
        {
          "source": "/contact",
          "destination": "/guide/contact"
        },
        {
          "source": "/docs/reference/env-vars",
          "destination": "/docs/guide/development/environment-variables"
        },
        {
          "source": "/docs/incidents/2024-07-10-ssrf-issue-in-fiddle-proxy",
          "destination": "/changelog/changelog"
        },
        {
          "source": "/document-extraction-api/overview/docs/api",
          "destination": "/ref/boundary-extraction-api/extract"
        },
        {
          "source": "/document-extraction-api/overview/docs/api/extract-data",
          "destination": "/ref/boundary-extraction-api/extract"
        },
        {
          "source": "/document-extraction-api/overview/docs/api/extraction-examples",
          "destination": "/ref/boundary-extraction-api/extract/examples"
        }
      ]
    },
    "apis": {
      "eaa1074a-b676-4a6b-93df-43c52a64d62f": {
        "id": "eaa1074a-b676-4a6b-93df-43c52a64d62f",
        "rootPackage": {
          "endpoints": [
            {
              "environments": [
                {
                  "id": "Default",
                  "baseUrl": "https://api2.boundaryml.com/v3"
                }
              ],
              "defaultEnvironment": "Default",
              "urlSlug": "extract-data",
              "migratedFromUrlSlugs": ["extract"],
              "method": "POST",
              "id": "extractData",
              "originalEndpointId": "endpoint_.extractData",
              "name": "Extract",
              "path": {
                "pathParameters": [],
                "parts": [
                  {
                    "type": "literal",
                    "value": ""
                  },
                  {
                    "type": "literal",
                    "value": "/extract"
                  }
                ]
              },
              "queryParameters": [],
              "headers": [],
              "request": {
                "contentType": "multipart/form-data",
                "type": {
                  "type": "fileUpload",
                  "value": {
                    "name": "ExtractDataRequest",
                    "properties": [
                      {
                        "type": "file",
                        "value": {
                          "type": "fileArray",
                          "key": "files",
                          "isOptional": false
                        }
                      },
                      {
                        "type": "bodyProperty",
                        "key": "prompt",
                        "valueType": {
                          "type": "optional",
                          "itemType": {
                            "type": "primitive",
                            "value": {
                              "type": "string"
                            }
                          }
                        },
                        "description": "Instruction for data extraction. Like \"focus on the colors of the images in this document\" or \"only focus on extracting addresses\""
                      }
                    ]
                  }
                }
              },
              "response": {
                "type": {
                  "type": "reference",
                  "value": {
                    "type": "id",
                    "value": "type_:ExtractResponse"
                  }
                },
                "description": "Successful Response"
              },
              "errors": [],
              "errorsV2": [
                {
                  "type": {
                    "type": "alias",
                    "value": {
                      "type": "id",
                      "value": "type_:ErrorResponse"
                    }
                  },
                  "statusCode": 400,
                  "description": "Invalid Request Parameters",
                  "examples": [
                    {
                      "responseBody": {
                        "type": "json",
                        "value": {
                          "error": "error"
                        }
                      }
                    }
                  ]
                },
                {
                  "type": {
                    "type": "alias",
                    "value": {
                      "type": "id",
                      "value": "type_:ErrorResponse"
                    }
                  },
                  "statusCode": 415,
                  "description": "Unsupported Media Type",
                  "examples": [
                    {
                      "responseBody": {
                        "type": "json",
                        "value": {
                          "error": "error"
                        }
                      }
                    }
                  ]
                },
                {
                  "type": {
                    "type": "alias",
                    "value": {
                      "type": "id",
                      "value": "type_:HttpValidationError"
                    }
                  },
                  "statusCode": 422,
                  "description": "Validation Error",
                  "examples": [
                    {
                      "responseBody": {
                        "type": "json",
                        "value": {}
                      }
                    }
                  ]
                },
                {
                  "type": {
                    "type": "alias",
                    "value": {
                      "type": "id",
                      "value": "type_:ErrorResponse"
                    }
                  },
                  "statusCode": 500,
                  "description": "Internal Server Error",
                  "examples": [
                    {
                      "responseBody": {
                        "type": "json",
                        "value": {
                          "error": "error"
                        }
                      }
                    }
                  ]
                }
              ],
              "examples": [
                {
                  "path": "/extract",
                  "pathParameters": {},
                  "queryParameters": {},
                  "headers": {},
                  "requestBody": {},
                  "responseStatusCode": 200,
                  "responseBody": {
                    "extractions": [
                      {
                        "source": {
                          "type": "type"
                        },
                        "output": {
                          "key": "value"
                        }
                      }
                    ],
                    "usage": {
                      "consumed_chars": 1,
                      "produced_chars": 1,
                      "consumed_megapixels": 1.1
                    },
                    "request_id": "request_id"
                  },
                  "codeExamples": {
                    "nodeAxios": ""
                  },
                  "requestBodyV3": {
                    "type": "form",
                    "value": {
                      "files": {
                        "type": "filenames",
                        "value": []
                      },
                      "prompt": {
                        "type": "json"
                      }
                    }
                  },
                  "responseBodyV3": {
                    "type": "json",
                    "value": {
                      "extractions": [
                        {
                          "source": {
                            "type": "type"
                          },
                          "output": {
                            "key": "value"
                          }
                        }
                      ],
                      "usage": {
                        "consumed_chars": 1,
                        "produced_chars": 1,
                        "consumed_megapixels": 1.1
                      },
                      "request_id": "request_id"
                    }
                  },
                  "codeSamples": []
                },
                {
                  "path": "/extract",
                  "pathParameters": {},
                  "queryParameters": {},
                  "headers": {},
                  "responseStatusCode": 400,
                  "codeExamples": {
                    "nodeAxios": ""
                  },
                  "requestBodyV3": {
                    "type": "form",
                    "value": {
                      "files": {
                        "type": "filenames",
                        "value": ["<filename1>", "<filename2>"]
                      },
                      "prompt": {
                        "type": "json"
                      }
                    }
                  },
                  "responseBodyV3": {
                    "type": "json",
                    "value": {
                      "error": "string"
                    }
                  },
                  "codeSamples": []
                },
                {
                  "path": "/extract",
                  "pathParameters": {},
                  "queryParameters": {},
                  "headers": {},
                  "responseStatusCode": 415,
                  "codeExamples": {
                    "nodeAxios": ""
                  },
                  "requestBodyV3": {
                    "type": "form",
                    "value": {
                      "files": {
                        "type": "filenames",
                        "value": ["<filename1>", "<filename2>"]
                      },
                      "prompt": {
                        "type": "json"
                      }
                    }
                  },
                  "responseBodyV3": {
                    "type": "json",
                    "value": {
                      "error": "string"
                    }
                  },
                  "codeSamples": []
                },
                {
                  "path": "/extract",
                  "pathParameters": {},
                  "queryParameters": {},
                  "headers": {},
                  "responseStatusCode": 422,
                  "codeExamples": {
                    "nodeAxios": ""
                  },
                  "requestBodyV3": {
                    "type": "form",
                    "value": {
                      "files": {
                        "type": "filenames",
                        "value": ["<filename1>", "<filename2>"]
                      },
                      "prompt": {
                        "type": "json"
                      }
                    }
                  },
                  "responseBodyV3": {
                    "type": "json",
                    "value": {
                      "detail": [
                        {
                          "loc": ["string"],
                          "msg": "string",
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "codeSamples": []
                },
                {
                  "path": "/extract",
                  "pathParameters": {},
                  "queryParameters": {},
                  "headers": {},
                  "responseStatusCode": 500,
                  "codeExamples": {
                    "nodeAxios": ""
                  },
                  "requestBodyV3": {
                    "type": "form",
                    "value": {
                      "files": {
                        "type": "filenames",
                        "value": ["<filename1>", "<filename2>"]
                      },
                      "prompt": {
                        "type": "json"
                      }
                    }
                  },
                  "responseBodyV3": {
                    "type": "json",
                    "value": {
                      "error": "string"
                    }
                  },
                  "codeSamples": []
                }
              ],
              "description": "Upload one or more files along with a prompt to extract data. The API processes the files based on the prompt and returns the extracted information.\n\nA PDF may generate an array of many extracted JSON blobs, 1 per page for example.",
              "authed": true
            }
          ],
          "subpackages": [],
          "types": [
            "type_:ExtractResponse",
            "type_:Extraction",
            "type_:Source",
            "type_:Usage",
            "type_:ErrorResponse",
            "type_:HttpValidationError",
            "type_:ValidationErrorLocItem",
            "type_:ValidationError"
          ],
          "webhooks": [],
          "websockets": []
        },
        "types": {
          "type_:ExtractResponse": {
            "name": "ExtractResponse",
            "shape": {
              "type": "object",
              "extends": [],
              "properties": [
                {
                  "key": "extractions",
                  "valueType": {
                    "type": "list",
                    "itemType": {
                      "type": "id",
                      "value": "type_:Extraction"
                    }
                  }
                },
                {
                  "key": "usage",
                  "valueType": {
                    "type": "id",
                    "value": "type_:Usage"
                  }
                },
                {
                  "description": "Unique identifier for the request.",
                  "key": "request_id",
                  "valueType": {
                    "type": "primitive",
                    "value": {
                      "type": "string"
                    }
                  }
                }
              ]
            }
          },
          "type_:Extraction": {
            "name": "Extraction",
            "shape": {
              "type": "object",
              "extends": [],
              "properties": [
                {
                  "key": "source",
                  "valueType": {
                    "type": "id",
                    "value": "type_:Source"
                  }
                },
                {
                  "description": "Extracted data from the file, in JSON format.",
                  "key": "output",
                  "valueType": {
                    "type": "map",
                    "keyType": {
                      "type": "primitive",
                      "value": {
                        "type": "string"
                      }
                    },
                    "valueType": {
                      "type": "unknown"
                    }
                  }
                }
              ]
            }
          },
          "type_:Source": {
            "name": "Source",
            "shape": {
              "type": "object",
              "extends": [],
              "properties": [
                {
                  "description": "Media type of the file.",
                  "key": "type",
                  "valueType": {
                    "type": "primitive",
                    "value": {
                      "type": "string"
                    }
                  }
                },
                {
                  "description": "Name of the file.",
                  "key": "name",
                  "valueType": {
                    "type": "optional",
                    "itemType": {
                      "type": "primitive",
                      "value": {
                        "type": "string"
                      }
                    }
                  }
                },
                {
                  "description": "Page number if applicable.",
                  "key": "page",
                  "valueType": {
                    "type": "optional",
                    "itemType": {
                      "type": "primitive",
                      "value": {
                        "type": "integer"
                      }
                    }
                  }
                }
              ]
            }
          },
          "type_:Usage": {
            "description": "Usage statistics for the request. A request goes through the BoundaryML pipeline, where documents can be converted into images. In the process, the number of characters consumed, produced, and the number of megapixels consumed are tracked.",
            "name": "Usage",
            "shape": {
              "type": "object",
              "extends": [],
              "properties": [
                {
                  "description": "Number of characters processed.",
                  "key": "consumed_chars",
                  "valueType": {
                    "type": "primitive",
                    "value": {
                      "type": "integer"
                    }
                  }
                },
                {
                  "description": "Number of characters produced.",
                  "key": "produced_chars",
                  "valueType": {
                    "type": "primitive",
                    "value": {
                      "type": "integer"
                    }
                  }
                },
                {
                  "description": "Number of megapixels processed.",
                  "key": "consumed_megapixels",
                  "valueType": {
                    "type": "primitive",
                    "value": {
                      "type": "double"
                    }
                  }
                }
              ]
            }
          },
          "type_:ErrorResponse": {
            "name": "ErrorResponse",
            "shape": {
              "type": "object",
              "extends": [],
              "properties": [
                {
                  "description": "Error message detailing the issue.",
                  "key": "error",
                  "valueType": {
                    "type": "primitive",
                    "value": {
                      "type": "string"
                    }
                  }
                }
              ]
            }
          },
          "type_:HttpValidationError": {
            "name": "HttpValidationError",
            "shape": {
              "type": "object",
              "extends": [],
              "properties": [
                {
                  "key": "detail",
                  "valueType": {
                    "type": "optional",
                    "itemType": {
                      "type": "list",
                      "itemType": {
                        "type": "id",
                        "value": "type_:ValidationError"
                      }
                    }
                  }
                }
              ]
            }
          },
          "type_:ValidationErrorLocItem": {
            "name": "ValidationErrorLocItem",
            "shape": {
              "type": "undiscriminatedUnion",
              "variants": [
                {
                  "type": {
                    "type": "primitive",
                    "value": {
                      "type": "string"
                    }
                  }
                },
                {
                  "type": {
                    "type": "primitive",
                    "value": {
                      "type": "integer"
                    }
                  }
                }
              ]
            }
          },
          "type_:ValidationError": {
            "name": "ValidationError",
            "shape": {
              "type": "object",
              "extends": [],
              "properties": [
                {
                  "key": "loc",
                  "valueType": {
                    "type": "list",
                    "itemType": {
                      "type": "id",
                      "value": "type_:ValidationErrorLocItem"
                    }
                  }
                },
                {
                  "key": "msg",
                  "valueType": {
                    "type": "primitive",
                    "value": {
                      "type": "string"
                    }
                  }
                },
                {
                  "key": "type",
                  "valueType": {
                    "type": "primitive",
                    "value": {
                      "type": "string"
                    }
                  }
                }
              ]
            }
          }
        },
        "subpackages": {},
        "auth": {
          "type": "bearerAuth",
          "tokenName": "token"
        },
        "hasMultipleBaseUrls": false,
        "globalHeaders": []
      }
    },
    "files": {
      "e0fb9eac-cbc9-41c7-8f84-dcb0231b7167": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/open-sans-v17-all-charsets-regular.woff2",
      "52f14847-0fa6-4168-8581-c354550d3d52": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/open-sans-v17-all-charsets-italic.woff2",
      "735fbf2b-9cb4-42f9-a586-4ddbed3f764e": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/open-sans-v17-all-charsets-700.woff2",
      "3f232775-f97b-449b-a76d-293b065c6307": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/cloud/boundary-cloud-diagram.drawio.svg",
      "e6cede84-0939-4dc8-a33e-3f0293e31ec5": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/favicon.ico",
      "eac0fc8d-8eb6-46bd-8c46-ae7a229b6571": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/baml-client.png",
      "d4e9528c-697a-4aa9-ac02-f92823252a6b": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/code-lens.png",
      "ca2d243f-56cf-4f0f-9ed0-19a76c9783b4": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/open-playground.png",
      "31b47e5b-eaae-4210-8bad-2a1726ab8509": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/playground-preview.png",
      "611a9c17-6270-4503-82de-9bf30804f3f2": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/test-cases.png",
      "60b8f7d0-6d4d-4c8f-a940-0395a20022c3": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/test-case-buttons.png",
      "362a1df5-e3ab-4ef1-9327-82ddc7addaba": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/languages/baml-to-py.png",
      "793e29f1-ff38-4e00-b009-36b0685a8945": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/languages/baml-to-ts.png",
      "465b1613-60ca-43e7-bcb3-9daa124d4ca4": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/languages/baml-to-rb.png",
      "6b35bd0f-00fa-4c89-a93e-ddaf360396d5": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/languages/baml-to-rest.png",
      "a410f5dc-aff8-430e-bce9-3fec22eca527": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/terminal-logs/log_message.png",
      "7340ce64-d200-46e9-8089-eb49f9338588": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/extract-resume-prompt-preview.png",
      "88e2dcb3-ce22-49e4-a43f-084b9d5f27fe": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/curl-preview.png",
      "0ad3106d-8c49-4263-920c-ee1d3b8a2165": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/cloud/deploy-screenshot.png",
      "0bcebfbb-1507-45ee-aa10-05bbe8964f40": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/cloud/local-function-call.png",
      "e6c19acc-fae1-40bb-9c5a-0901494bf5cd": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/cloud/api-keys.png",
      "e2159ce8-f03f-4fda-9d91-92cd7cc867cf": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/cloud/set-env-vars.png",
      "b598692d-ded9-4b4b-b3a2-e61c2f98c230": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/studio/dashboard-test-pic.png",
      "a9cd259f-3f20-4faa-b421-b3858d948384": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/comparisons/prompt_view.gif",
      "2c00e872-0f76-48d3-b852-d2d6c988643a": "https://fdr-prod-docs-files.s3.us-east-1.amazonaws.com/https%3A//boundary.docs.buildwithfern.com/2024-11-04T22%3A22%3A19.879Z/assets/vscode/bedrock-playground.png"
    },
    "jsFiles": {},
    "filesV2": {
      "e0fb9eac-cbc9-41c7-8f84-dcb0231b7167": {
        "type": "url",
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/open-sans-v17-all-charsets-regular.woff2"
      },
      "52f14847-0fa6-4168-8581-c354550d3d52": {
        "type": "url",
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/open-sans-v17-all-charsets-italic.woff2"
      },
      "735fbf2b-9cb4-42f9-a586-4ddbed3f764e": {
        "type": "url",
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/open-sans-v17-all-charsets-700.woff2"
      },
      "3f232775-f97b-449b-a76d-293b065c6307": {
        "type": "url",
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/cloud/boundary-cloud-diagram.drawio.svg"
      },
      "e6cede84-0939-4dc8-a33e-3f0293e31ec5": {
        "type": "image",
        "width": 16,
        "height": 16,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/favicon.ico"
      },
      "eac0fc8d-8eb6-46bd-8c46-ae7a229b6571": {
        "type": "image",
        "width": 840,
        "height": 608,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/baml-client.png"
      },
      "d4e9528c-697a-4aa9-ac02-f92823252a6b": {
        "type": "image",
        "width": 978,
        "height": 166,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/code-lens.png"
      },
      "ca2d243f-56cf-4f0f-9ed0-19a76c9783b4": {
        "type": "image",
        "width": 1036,
        "height": 248,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/open-playground.png"
      },
      "31b47e5b-eaae-4210-8bad-2a1726ab8509": {
        "type": "image",
        "width": 1848,
        "height": 386,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/playground-preview.png"
      },
      "611a9c17-6270-4503-82de-9bf30804f3f2": {
        "type": "image",
        "width": 1876,
        "height": 860,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/test-cases.png"
      },
      "60b8f7d0-6d4d-4c8f-a940-0395a20022c3": {
        "type": "image",
        "width": 1876,
        "height": 860,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/test-case-buttons.png"
      },
      "362a1df5-e3ab-4ef1-9327-82ddc7addaba": {
        "type": "image",
        "width": 1419,
        "height": 484,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/languages/baml-to-py.png"
      },
      "793e29f1-ff38-4e00-b009-36b0685a8945": {
        "type": "image",
        "width": 1419,
        "height": 484,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/languages/baml-to-ts.png"
      },
      "465b1613-60ca-43e7-bcb3-9daa124d4ca4": {
        "type": "image",
        "width": 1419,
        "height": 484,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/languages/baml-to-rb.png"
      },
      "6b35bd0f-00fa-4c89-a93e-ddaf360396d5": {
        "type": "image",
        "width": 2655,
        "height": 488,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/languages/baml-to-rest.png"
      },
      "a410f5dc-aff8-430e-bce9-3fec22eca527": {
        "type": "image",
        "width": 1390,
        "height": 686,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/terminal-logs/log_message.png"
      },
      "7340ce64-d200-46e9-8089-eb49f9338588": {
        "type": "image",
        "width": 3054,
        "height": 1504,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/extract-resume-prompt-preview.png"
      },
      "88e2dcb3-ce22-49e4-a43f-084b9d5f27fe": {
        "type": "image",
        "width": 1538,
        "height": 1300,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/curl-preview.png"
      },
      "0ad3106d-8c49-4263-920c-ee1d3b8a2165": {
        "type": "image",
        "width": 714,
        "height": 690,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/cloud/deploy-screenshot.png"
      },
      "0bcebfbb-1507-45ee-aa10-05bbe8964f40": {
        "type": "image",
        "width": 634,
        "height": 658,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/cloud/local-function-call.png"
      },
      "e6c19acc-fae1-40bb-9c5a-0901494bf5cd": {
        "type": "image",
        "width": 858,
        "height": 411,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/cloud/api-keys.png"
      },
      "e2159ce8-f03f-4fda-9d91-92cd7cc867cf": {
        "type": "image",
        "width": 707,
        "height": 271,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/cloud/set-env-vars.png"
      },
      "b598692d-ded9-4b4b-b3a2-e61c2f98c230": {
        "type": "image",
        "width": 2622,
        "height": 1574,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/studio/dashboard-test-pic.png"
      },
      "a9cd259f-3f20-4faa-b421-b3858d948384": {
        "type": "image",
        "width": 1952,
        "height": 1080,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/comparisons/prompt_view.gif"
      },
      "2c00e872-0f76-48d3-b852-d2d6c988643a": {
        "type": "image",
        "width": 938,
        "height": 330,
        "url": "https://fdr-prod-docs-files-public.s3.amazonaws.com/https://boundary.docs.buildwithfern.com/2024-11-04T22:22:19.879Z/assets/vscode/bedrock-playground.png"
      }
    },
    "pages": {
      "pages/welcome.mdx": {
        "markdown": "---\ntitle: \"\\U0001F3E0 Welcome\"\ndescription: The easiest way to use LLMs\nslug: home\nlayout: overview\nhide-toc: false\n---\n\n**BAML is a domain-specific language to generate structured outputs from LLMs -- with the best developer experience.**\n\nWith BAML you can build reliable Agents, Chatbots with RAG, extract data from PDFs, and more.\n\n### A small sample of features:\n1. **An amazingly fast developer experience** for prompting in the BAML VSCode playground\n2. **Fully type-safe outputs**, even when streaming structured data (that means autocomplete!)\n3. **Flexibility** -- it works with **any LLM**, **any language**, and **any schema**.\n4. **State-of-the-art structured outputs** that even [outperform OpenAI with their own models](https://www.boundaryml.com/blog/sota-function-calling?q=0) -- plus it works with OpenSource models.\n\n\n## Products\n\n<Cards cols={2}>\n  <Card\n    title=\"Guide\"\n    icon=\"fa-regular fa-pen\"\n    href=\"/guide/introduction/what-is-baml\"\n  >\n    Everything you need to know about how to get started with BAML. From installation to prompt engineering techniques.\n  </Card>\n  <Card\n    title=\"Playground\"\n    icon=\"fa-regular fa-browser\"\n    href=\"https://promptfiddle.com\"\n  >\n    An online interactive playground to playaround with BAML without any installations.\n  </Card>\n  <Card\n    title=\"Examples\"\n    icon=\"fa-regular fa-grid-2\"\n    href=\"/examples\"\n  >\n    Examples of prompts, projects, and more.\n  </Card>\n  <Card\n    title=\"Reference\"\n    icon=\"fa-regular fa-code\"\n    href=\"/ref\"\n  >\n    Language docs on all BAML syntax. Quickly learn syntax with simple examples and code snippets.\n  </Card>\n</Cards>\n\n## Motivation\n\nPrompts are more than just f-strings; they're actual functions with logic that can quickly become complex to organize, maintain, and test.\n\nCurrently, developers craft LLM prompts as if they're writing raw HTML and CSS in text files, lacking:\n- Type safety\n- Hot-reloading or previews\n- Linting\n\nThe situation worsens when dealing with structured outputs. Since most prompts rely on Python and Pydantic, developers must _execute_ their code and set up an entire Python environment just to test a minor prompt adjustment, or they have to setup a whole Python microservice just to call an LLM.\n\nBAML allows you to view and run prompts directly within your editor, similar to how Markdown Preview function -- no additional setup necessary, that interoperates with all your favorite languages and frameworks.\n\nJust as TSX/JSX provided the ideal abstraction for web development, BAML offers the perfect abstraction for prompt engineering. Watch our [demo video](/guide/introduction/what-is-baml#demo-video) to see it in action.\n\n## Comparisons\n\nHere's our in-depth comparison with a couple of popular frameworks:\n- [BAML vs Pydantic](/guide/comparisons/baml-vs-pydantic)\n- [BAML vs Marvin](/guide/comparisons/baml-vs-marvin)\n\n{/* \n<div className=\"motivation\">\n  Insert something powerful here.\n</div>\n\n<ButtonGroup>\n<Button href=\"https://calendly.com/boundary-founders/connect-45\" intent=\"primary\" rightIcon=\"arrow-right\" large>\n  Schedule a demo with our team!\n</Button>\n\n<Button href=\"https://buildwithfern.com/showcase\" minimal large>\n  View our showcase\n</Button>\n</ButtonGroup> */}\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/pages/welcome.mdx"
      },
      "01-guide/what-are-function-definitions.mdx": {
        "markdown": "---\ntitle: What is BAML?\n---\n\nThe best way to understand BAML and its developer experience is to see it live in a demo (see below).\n\n### Demo video\nHere we write a BAML function definition, and then call it from a Python script.\n\n<iframe src=\"https://fast.wistia.net/embed/iframe/5fxpquglde?seo=false&videoFoam=false\" title=\"BAML Demo Video\" allow=\"autoplay; fullscreen\" allowtransparency=\"true\" frameborder=\"0\" scrolling=\"no\" class=\"wistia_embed\" name=\"wistia_embed\" msallowfullscreen width=\"640\" height=\"352\"></iframe>\n<script src=\"https://fast.wistia.net/assets/external/E-v1.js\" async></script>\n\n### Examples\n- [Interactive NextJS app with streaming](https://baml-examples.vercel.app/examples/stream-object)\n- [Starter boilerplates for Python, Typescript, Ruby, etc.](https://github.com/boundaryml/baml-examples)\n\n### High-level Developer Flow\n\n<Steps>\n### Write a BAML function definition\n```baml main.baml\nclass WeatherAPI {\n  city string @description(\"the user's city\")\n  timeOfDay string @description(\"As an ISO8601 timestamp\")\n}\n\nfunction UseTool(user_message: string) -> WeatherAPI {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Extract.... {# we will explain the rest in the guides #}\n  \"#\n}\n```\nHere you can run tests in the VSCode Playground.\n\n### Generate `baml_client` from those .baml files.\nThis is auto-generated code with all boilerplate to call the LLM endpoint, parse the output, fix broken JSON, and handle errors.\n<img src=\"file:eac0fc8d-8eb6-46bd-8c46-ae7a229b6571\" width=\"300px\"/>\n\n\n### Call your function in any language\nwith type-safety, autocomplete, retry-logic, robust JSON parsing, etc..\n<CodeGroup>\n```python Python\nimport asyncio\nfrom baml_client import b\nfrom baml_client.types import WeatherAPI\n\ndef main():\n    weather_info = b.UseTool(\"What's the weather like in San Francisco?\")\n    print(weather_info)\n    assert isinstance(weather_info, WeatherAPI)\n    print(f\"City: {weather_info.city}\")\n    print(f\"Time of Day: {weather_info.timeOfDay}\")\n\nif __name__ == '__main__':\n    main()\n```\n\n```typescript TypeScript\nimport { b } from './baml_client'\nimport { WeatherAPI } from './baml_client/types'\nimport assert from 'assert'\n\nconst main = async () => {\n  const weatherInfo = await b.UseTool(\"What's the weather like in San Francisco?\")\n  console.log(weatherInfo)\n  assert(weatherInfo instanceof WeatherAPI)\n  console.log(`City: ${weatherInfo.city}`)\n  console.log(`Time of Day: ${weatherInfo.timeOfDay}`)\n}\n```\n\n```ruby Ruby\nrequire_relative \"baml_client/client\"\n\n$b = Baml.Client\n\ndef main\n  weather_info = $b.UseTool(user_message: \"What's the weather like in San Francisco?\")\n  puts weather_info\n  raise unless weather_info.is_a?(Baml::Types::WeatherAPI)\n  puts \"City: #{weather_info.city}\"\n  puts \"Time of Day: #{weather_info.timeOfDay}\"\nend\n```\n\n```python Other Languages\n# read the installation guide for other languages!\n```\n</CodeGroup>\n</Steps>\n\nContinue on to the [Installation Guides](/guide/installation-language) for your language to setup BAML in a few minutes! \n\nYou don't need to migrate 100% of your LLM code to BAML in one go! It works along-side any existing LLM framework.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/what-are-function-definitions.mdx"
      },
      "01-guide/what-is-baml_src.mdx": {
        "markdown": "---\ntitle: What is baml_src?\n---\n\n**baml_src** is where you keep all your BAML files, and where all the prompt-related code lives. It must be named `baml_src` for our tooling to pick it up, but it can live wherever you want.\n\nIt helps keep your project organized, and makes it easy to separate prompt engineering from the rest of your code.\n\n<img src=\"file:eac0fc8d-8eb6-46bd-8c46-ae7a229b6571\" width=\"300px\"/>\n\n\nSome things to note:\n1. All declarations within this directory are accessible across all files contained in the `baml_src` folder.\n2. You can have multiple files, and even nest subdirectories.\n\nYou don't need to worry about including this directory when deploying your code. See: [Deploying](/guide/development/deploying/aws)\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/what-is-baml_src.mdx"
      },
      "01-guide/what-is-baml_client.mdx": {
        "markdown": "---\ntitle: What is baml_client?\n---\n\n**baml_client** is the code that gets generated from your BAML files that transforms your BAML prompts into the same equivalent function in your language, with validated type-safe outputs.\n<img src=\"file:eac0fc8d-8eb6-46bd-8c46-ae7a229b6571\" width=\"300px\"/>\n\n```python Python\nfrom baml_client import b\nresume_info = b.ExtractResume(\"....some text...\")\n```\n\nThis has all the boilerplate to:\n1. call the LLM endpoint with the right parameters, \n2. parse the output, \n3. fix broken JSON (if any)\n4. return the result in a nice typed object.\n5. handle errors\n\nIn Python, your BAML types get converted to Pydantic models. In Typescript, they get converted to TypeScript types, and so on. **BAML acts like a universal type system that can be used in any language**.\n\n\n\n### Generating baml_client\n\n Refer to the **[Installation](/guide/installation-language/python)** guides for how to set this up for your language, and how to generate it.\n\n But at a high-level, you just include a [generator block](/ref/baml/generator) in any of your BAML files.\n\n<CodeBlocks>\n\n```baml Python\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"python/pydantic\"\n    \n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    \n    // What interface you prefer to use for the generated code (sync/async)\n    // Both are generated regardless of the choice, just modifies what is exported\n    // at the top level\n    default_client_mode \"sync\"\n    \n    // Version of runtime to generate code for (should match installed baml-py version)\n    version \"0.54.0\"\n}\n```\n\n```baml TypeScript\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"typescript\"\n    \n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    \n    // What interface you prefer to use for the generated code (sync/async)\n    // Both are generated regardless of the choice, just modifies what is exported\n    // at the top level\n    default_client_mode \"async\"\n    \n    // Version of runtime to generate code for (should match the package @boundaryml/baml version)\n    version \"0.54.0\"\n}\n```\n\n```baml Ruby (beta)\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"ruby/sorbet\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    \n    // Version of runtime to generate code for (should match installed `baml` package version)\n    version \"0.54.0\"\n}\n```\n\n```baml OpenAPI\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"rest/openapi\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    \n    // Version of runtime to generate code for (should match installed `baml` package version)\n    version \"0.54.0\"\n\n    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client\n    // This command will be run from within $output_dir\n    on_generate \"npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g OPENAPI_CLIENT_TYPE -o .\"\n}\n```\n</CodeBlocks>\n\nThe `baml_client` transforms a BAML function into the same equivalent function in your language, \n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/what-is-baml_client.mdx"
      },
      "01-guide/01-editors/vscode.mdx": {
        "markdown": "We provide a BAML VSCode extension:     https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension\n\n\n\n| Feature | Supported |\n|---------|-----------|\n| Syntax highlighting for BAML files | ✅ |\n| Code snippets for BAML | ✅ |\n| LLM playground for testing BAML functions | ✅ |\n| Jump to definition for BAML files | ✅ |\n| Jump to definition between Python/TS files and BAML files | ✅ |\n| Auto generate `baml_client` on save | ✅ |\n| BAML formatter | ❌ |\n\n## Opening BAML Playground\n \nOnce you open a `.baml` file, in VSCode, you should see a small button over every BAML function: `Open Playground`.\n\n<img src=\"file:d4e9528c-697a-4aa9-ac02-f92823252a6b\" />\n\nOr type `BAML Playground` in the VSCode Command Bar (`CMD + Shift + P` or `CTRL + Shift + P`) to open the playground.\n\n<img src=\"file:ca2d243f-56cf-4f0f-9ed0-19a76c9783b4\" />\n\n## Setting Env Variables\n\nClick on the `Settings` button in top right of the playground and set the environment variables.\n\nIt should have an indicator saying how many unset variables are there.\n\n<img src=\"file:31b47e5b-eaae-4210-8bad-2a1726ab8509\" />\n\nThe playground should persist the environment variables between closing and opening VSCode.\n\n<Tip>\n  You can set environment variables lazily. If anything is unset you'll get an error when you run the function.\n</Tip>\n\n<Info>\n  Environment Variables are stored in VSCode's local storage! We don't save any additional data to disk, or send them across the network.\n</Info>\n\n\n## Running Tests\n\n- Click on the `Run All Tests` button in the playground.\n\n- Press the `▶️` button next to an individual test case to run that just that test case.\n\n\n## Switching Functions\n\nThe playground will automatically switch to the function you're currently editing.\n\nTo manually change it, click on the current function name in the playground (next to the dropdown) and search for your desired function.\n\n## Switching Test Cases\n\nThe test case with the highlighted background is the currently rendered test case. Clicking on a different test case will render that test case.\n\n<img src=\"file:611a9c17-6270-4503-82de-9bf30804f3f2\" />\n\nYou can toggle between seeing the results of all test cases or all test cases for the current function.\n\n<img src=\"file:60b8f7d0-6d4d-4c8f-a940-0395a20022c3\" />\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/01-editors/vscode.mdx"
      },
      "01-guide/01-editors/cursor.mdx": {
        "markdown": "---\ntitle: Cursor\n---\nRefer to the [Cursor Extension Installation Guide](https://www.cursor.com/how-to-install-extension) to install the extension in Cursor.\n\n<Warning>\nYou may need to update BAML extension manually using the process above. Auto-update does not seem to be working well for many extensions in Cursor.\n</Warning>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/01-editors/cursor.mdx"
      },
      "01-guide/01-editors/others.mdx": {
        "markdown": "We don't currently have any tier support for any other editors.\n\n* JetBrains IDEs\n* Helix\n* Zed\n* Vim\n* Emacs\n* Sublime Text\n* Atom\n\n\nSince the extension is a language server, we can technically pull out the language server and syntax highlighter and support any editor supporting the language server protocol.\nIf you're interested in contributing to the project and supporting another editor, [please reach out](/contact).\n\nAn alternative is to edit your files in our [Playground](https://www.promptfiddle.com/), and copy the code into your editor, but we recommend using VSCode to edit BAML files for now.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/01-editors/others.mdx"
      },
      "01-guide/02-languages/python.mdx": {
        "markdown": "<Note>You can check out this repo: \nhttps://github.com/BoundaryML/baml-examples/tree/main/python-fastapi-starter</Note>\n\nTo set up BAML with Python do the following:\n\n<Steps>\n  ### Install BAML VSCode/Cursor Extension\n      https://marketplace.visualstudio.com/items?itemName=boundary.baml-extension\n\n      - syntax highlighting\n      - testing playground\n      - prompt previews\n\n      <Tip>\n      In your VSCode User Settings, highly recommend adding this to get better autocomplete for python in general, not just BAML.\n\n      ```json\n      {\n        \"python.analysis.typeCheckingMode\": \"basic\"\n      }\n      ```\n      </Tip>\n  \n  ### Install BAML\n      <CodeBlocks>\n        ```bash pip\n        pip install baml-py\n        ```\n\n        ```bash poetry\n        poetry add baml-py\n        ```\n\n         ```bash uv\n        uv add baml-py\n        ```\n        </CodeBlocks>\n  \n  ### Add BAML to your existing project\n      This will give you some starter BAML code in a `baml_src` directory.\n\n      <CodeBlocks>\n      ```bash pip\n      baml-cli init\n      ```\n\n      ```bash poetry\n      poetry run baml-cli init\n      ```\n\n      ```bash uv\n      uv run baml-cli init\n      ```\n      </CodeBlocks>\n  \n  ### Generate the `baml_client` python module from `.baml` files\n\n  One of the files in your `baml_src` directory will have a [generator block](/ref/baml/generator). The next commmand will auto-generate the `baml_client` directory, which will have auto-generated python code to call your BAML functions. \n  \n  Any types defined in .baml files will be converted into Pydantic models in the `baml_client` directory.\n\n\n    <CodeBlocks>\n    ```bash pip\n    baml-cli generate\n    ```\n\n\n    ```bash poetry\n    poetry run baml-cli generate\n    ```\n\n    ```bash uv\n    uv run baml-cli generate\n    ```\n    </CodeBlocks>\n\n    See [What is baml_client](/guide/introduction/baml_client) to learn more about how this works.\n    <img src=\"file:362a1df5-e3ab-4ef1-9327-82ddc7addaba\" />\n\n    <Tip>\n      If you set up the [VSCode extension](https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension), it will automatically run `baml-cli generate` on saving a BAML file.\n    </Tip>\n\n  \n  ### Use a BAML function in Python!\n    <Error>If `baml_client` doesn't exist, make sure to run the previous step! </Error>\n\n    <CodeBlocks>\n    ```python main.py \n    from baml_client.sync_client import b\n    from baml_client.types import Resume\n\n    def example(raw_resume: str) -> Resume: \n      # BAML's internal parser guarantees ExtractResume\n      # to be always return a Resume type\n      response = b.ExtractResume(raw_resume)\n      return response\n\n    def example_stream(raw_resume: str) -> Resume:\n      stream = b.stream.ExtractResume(raw_resume)\n      for msg in stream:\n        print(msg) # This will be a PartialResume type\n      \n      # This will be a Resume type\n      final = stream.get_final_response()\n\n      return final\n    ```\n\n    ```python async_main.py\n    from baml_client.async_client import b\n    from baml_client.types import Resume\n\n    async def example(raw_resume: str) -> Resume: \n      # BAML's internal parser guarantees ExtractResume\n      # to be always return a Resume type\n      response = await b.ExtractResume(raw_resume)\n      return response\n\n    async def example_stream(raw_resume: str) -> Resume:\n      stream = b.stream.ExtractResume(raw_resume)\n      async for msg in stream:\n        print(msg) # This will be a PartialResume type\n      \n      # This will be a Resume type\n      final = stream.get_final_response()\n\n      return final\n    ```\n    </CodeBlocks>\n  \n</Steps>\n\n\n## BAML with Jupyter Notebooks\n\nYou can use the baml_client in a Jupyter notebook. \n\nOne of the common problems is making sure your code changes are picked up by the notebook without having to restart the whole kernel (and re-run all the cells)\n\n**To make sure your changes in .baml files are reflected in your notebook you must do these steps:**\n\n<Steps>\n### Setup the autoreload extension\n\n```python cell0\n%load_ext autoreload\n%autoreload 2\n```\nThis will make sure to reload imports, such as baml_client's \"b\" object before every cell runs.\n\n### Import baml_client module in your notebook\n\nNote it's different from how we import in python.\n```python cell1\n# Assuming your baml_client is inside a dir called app/\nimport app.baml_client as client # you can name this \"llm\" or \"baml\" or whatever you want\n```\n\nUsually we import things as \n`from baml_client import b`, and we can call our functions using `b`, but the `%autoreload` notebook extension does not work well with `from...import` statements.\n\n\n### Call BAML functions using the module name as a prefix\n\n```python cell2\nraw_resume = \"Here's some resume text\"\nclient.b.ExtractResume(raw_resume)\n```\nNow your changes in .baml files are reflected in your notebook automatically, without needing to restart the Jupyter kernel.\n\n<Note>\nIf you want to keep using the `from baml_client import b` style, you'll just need to re-import it everytime you regenerate the baml_client.\n</Note>\n\n<Warning>\nPylance will complain about any schema changes you make in .baml files. You can ignore these errors. If you want it to pick up your new types, you'll need to restart the kernel.\nThis auto-reload approach works best if you're only making changes to the prompts.\n</Warning>\n\n</Steps>\n\nYou're all set! Continue on to the [Deployment Guides](/docs/get-started/deploying) for your language to learn how to deploy your BAML code or check out the [Interactive Examples](https://baml-examples.vercel.app/) to see more examples.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/02-languages/python.mdx"
      },
      "01-guide/02-languages/typescript.mdx": {
        "markdown": "<Note>You can check out this repo: https://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter</Note>\n\nTo set up BAML with Typescript do the following:\n\n<Steps>\n  ### Install BAML VSCode/Cursor Extension\n      https://marketplace.visualstudio.com/items?itemName=boundary.baml-extension\n\n      - syntax highlighting\n      - testing playground\n      - prompt previews\n  \n  ### Install BAML\n      <CodeBlocks>\n        ```bash npm\n        npm install @boundaryml/baml\n        ```\n        \n        ```bash pnpm\n        pnpm add @boundaryml/baml\n        ```\n\n        ```bash yarn\n        yarn add @boundaryml/baml\n        ```\n\n        ```bash deno\n        deno install npm:@boundaryml/baml\n        ```\n    </CodeBlocks>\n  \n  ### Add BAML to your existing project\n      This will give you some starter BAML code in a `baml_src` directory.\n      \n      <CodeBlocks>\n        ```bash npm\n        npx baml-cli init\n        ```\n        \n        ```bash pnpm\n        pnpx baml-cli init\n        ```\n\n        ```bash yarn\n        yarn baml-cli init\n        ```\n\n        ```bash deno\n        dpx baml-cli init\n        ```\n    </CodeBlocks>\n  \n  ### Generate the `baml_client` typescript package from `.baml` files\n\n    One of the files in your `baml_src` directory will have a [generator block](/ref/baml/generator). This tells BAML how to generate the `baml_client` directory, which will have auto-generated typescript code to call your BAML functions.\n\n    ```bash\n    npx baml-cli generate\n    ```\n\n    You can modify your `package.json` so you have a helper prefix in front of your build command.\n\n    ```json package.json\n    {\n      \"scripts\": {\n        // Add a new command\n        \"baml-generate\": \"baml-cli generate\",\n        // Always call baml-generate on every build.\n        \"build\": \"npm run baml-generate && tsc --build\",\n      }\n    }\n    ```\n    \n    See [What is baml_src](/guide/introduction/baml_src) to learn more about how this works.\n    <img src=\"file:793e29f1-ff38-4e00-b009-36b0685a8945\" />\n\n   \n    <Tip>\n      If you set up the [VSCode extension](https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension), it will automatically run `baml-cli generate` on saving a BAML file.\n    </Tip>\n  \n  ### Use a BAML function in Typescript!\n    <Error>If `baml_client` doesn't exist, make sure to run the previous step! </Error>\n\n    <CodeBlocks>\n    ```typescript index.ts\n    import {b} from \"baml_client\"\n    import type {Resume} from \"baml_client/types\"\n\n    async function Example(raw_resume: string): Resume {\n      // BAML's internal parser guarantees ExtractResume\n      // to be always return a Resume type\n      const response = await b.ExtractResume(raw_resume);\n      return response;\n    }\n\n    async function ExampleStream(raw_resume: string): Resume {\n      const stream = b.stream.ExtractResume(raw_resume);\n      for await (const msg of stream) {\n        console.log(msg) // This will be a Partial<Resume> type\n      }\n\n      // This is guaranteed to be a Resume type.\n      return await stream.get_final_response();\n    }\n    ```\n\n    ```typescript sync_example.ts\n    import {b} from \"baml_client/sync_client\"\n    import type {Resume} from \"baml_client/types\"\n\n    function Example(raw_resume: string): Resume {\n      // BAML's internal parser guarantees ExtractResume\n      // to be always return a Resume type\n      const response = b.ExtractResume(raw_resume);\n      return response;\n    }\n\n    // Streaming is not available in the sync_client.\n\n    ```\n    </CodeBlocks>\n</Steps>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/02-languages/typescript.mdx"
      },
      "01-guide/02-languages/ruby.mdx": {
        "markdown": "<Note>You can check out this repo: https://github.com/BoundaryML/baml-examples/tree/main/ruby-example</Note>\n\nTo set up BAML with Ruby do the following:\n\n<Steps>\n  ### Install BAML VSCode Extension\n      https://marketplace.visualstudio.com/items?itemName=boundary.baml-extension\n\n      - syntax highlighting\n      - testing playground\n      - prompt previews\n  \n  ### Install BAML\n      ```bash bundle\n      bundle add baml sorbet-runtime\n      ```\n  \n  ### Add BAML to your existing project\n      This will give you some starter BAML code in a `baml_src` directory.\n\n      ```bash\n      bundle exec baml-cli init\n      ```\n  \n  ### Generate Ruby code from `.baml` files\n\n    ```bash\n    bundle exec baml-cli generate\n    ```\n    `\n    See [What is baml_src](/guide/introduction/baml_src) to learn more about how this works.\n    <img src=\"file:465b1613-60ca-43e7-bcb3-9daa124d4ca4\" />\n    \n    As fun as writing BAML is, we want you be able to leverage BAML with existing ruby modules. This command gives you a ruby module that is a type-safe interface to every BAML function.\n\n    <Tip>\n      Our [VSCode extension](https://marketplace.visualstudio.com/items?itemName=Boundary.baml-extension) automatically runs this command when you save a BAML file.\n    </Tip>\n  \n  ### Use a BAML function in Ruby!\n    <Error>If `baml_client` doesn't exist, make sure to run the previous step!</Error>\n\n    <CodeBlocks>\n    ```ruby main.rb\n    require_relative \"baml_client/client\"\n\n    def example(raw_resume)\n        # r is an instance of Baml::Types::Resume, defined in baml_client/types\n        r = Baml.Client.ExtractResume(resume: raw_resume)\n\n        puts \"ExtractResume response:\"\n        puts r.inspect\n    end\n\n    def example_stream(raw_resume)\n        stream = Baml.Client.stream.ExtractResume(resume: raw_resume)\n\n        stream.each do |msg|\n            # msg is an instance of Baml::PartialTypes::Resume\n            # defined in baml_client/partial_types\n            puts msg.inspect\n        end\n\n        stream.get_final_response\n    end\n\n    example 'Grace Hopper created COBOL'\n    example_stream 'Grace Hopper created COBOL'\n    ```\n    </CodeBlocks>\n</Steps>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/02-languages/ruby.mdx"
      },
      "01-guide/02-languages/rest.mdx": {
        "markdown": "<Info>\n  Requires BAML version >=0.55\n</Info>\n\n<Warning>\n  This feature is a preview feature and may change. Please provide feedback either\n  in [Discord][discord] or on [GitHub][openapi-feedback-github-issue] so that\n  we can stabilize the feature and keep you updated!\n</Warning>\n\nBAML allows you to expose your BAML functions as RESTful APIs:\n\n<img src=\"file:6b35bd0f-00fa-4c89-a93e-ddaf360396d5\" />\n\nWe integrate with [OpenAPI](openapi) (universal API definitions), so you can get typesafe client libraries for free!\n\n<Steps>\n  ### Install BAML VSCode Extension\n      https://marketplace.visualstudio.com/items?itemName=boundary.baml-extension\n\n      - syntax highlighting\n      - testing playground\n      - prompt previews\n\n  ### Install NPX + OpenAPI\n\n     <Tabs>\n        <Tab title=\"macOS (brew)\">\n          ```bash\n          brew install npm openapi-generator\n          # 'npm' will install npx\n          # 'openapi-generator' will install both Java and openapi-generator-cli\n          ```\n        </Tab>\n\n        <Tab title=\"Linux (apt)\">\n          OpenAPI requires `default-jdk`\n\n          ```bash\n          apt install npm default-jdk -y\n          # 'npm' will install npx; 'default-jdk' will install java\n          ```\n        </Tab>\n\n        <Tab title=\"Linux (yum/dnf)\">\n          OpenAPI requires Java\n\n          ```bash\n          dnf install npm java-21-openjdk -y\n          # dnf is the successor to yum\n          ```\n\n          Amazon Linux 2023:\n          ```bash\n          dnf install npm java-21-amazon-corretto -y\n          # 'npm' will install npx\n          # 'java-21-amazon-corretto' will install java\n          ```\n\n          Amazon Linux 2:\n          ```bash\n          curl -sL https://rpm.nodesource.com/setup_16.x | bash -\n          yum install nodejs -y\n          # 'nodejs' will install npx\n          amazon-linux-extras install java-openjdk11 -y\n          # 'java-openjdk11' will install java\n          ```\n        </Tab>\n\n        <Tab title=\"Windows\">\n          To install `npx` and `java` (for OpenAPI):\n\n            1. Use the [Node.js installer](https://nodejs.org/en/download/prebuilt-installer) to install `npx` (default installer settings are fine).\n            2. Run `npm install -g npm@latest` to update `npx` (there is currently an [issue][npx-windows-issue] with the default install of `npx` on Windows where it doesn't work out of the box).\n            3. Run the [Adoptium OpenJDK `.msi` installer](https://adoptium.net/temurin/releases/?os=windows) (install the JDK; default installer settings are fine).\n\n          You can verify that `npx` and `java` are installed by running:\n          \n          ```powershell\n          npx -version\n          java -version\n          ```\n        </Tab>\n\n        <Tab title=\"Other\">\n          To install `npx`, use the [Node.js installer](https://nodejs.org/en/download/prebuilt-installer).\n\n          To install `java` (for OpenAPI), use the [Adoptium OpenJDK packages](https://adoptium.net/installation/linux/).\n        </Tab>\n      </Tabs>\n\n  ### Add BAML to your existing project\n      This will give you some starter BAML code in a `baml_src` directory.\n    <Tabs>\n\n      <Tab title=\"C#\">\n      ```bash\n      npx @boundaryml/baml init \\\n        --client-type rest/openapi --openapi-client-type csharp\n      ```\n      </Tab>\n\n      <Tab title=\"C++\">\n\n      <Tip>OpenAPI supports [5 different C++ client types][openapi-client-types];\n      any of them will work with BAML.</Tip>\n\n      ```bash\n      npx @boundaryml/baml init \\\n        --client-type rest/openapi --openapi-client-type cpp-restsdk\n      ```\n      </Tab>\n\n      <Tab title=\"Go\">\n      ```bash\n      npx @boundaryml/baml init \\\n        --client-type rest/openapi --openapi-client-type go\n      ```\n      </Tab>\n\n      <Tab title=\"Java\">\n      \n      ```bash\n      npx @boundaryml/baml init \\\n        --client-type rest/openapi --openapi-client-type java\n      ```\n\n      Notice that `on_generate` has been initialized for you to:\n\n      - run the OpenAPI generator to generate a Java client library, and _also_\n      - run `mvn clean install` to install the generated client library to your\n        local Maven repository\n\n      <Warning>\n        If you only use Maven through an IDE (e.g. IntelliJ IDEA), you should\n        remove `&& mvn clean install` from the generated `on_generate` command.\n      </Warning>\n\n      </Tab>\n\n      <Tab title=\"PHP\">\n      ```bash\n      npx @boundaryml/baml init \\\n        --client-type rest/openapi --openapi-client-type php\n      ```\n      </Tab>\n\n      <Tab title=\"Ruby\">\n      ```bash\n      npx @boundaryml/baml init \\\n        --client-type rest/openapi --openapi-client-type ruby\n      ```\n      </Tab>\n\n      <Tab title=\"Rust\">\n      ```bash\n      npx @boundaryml/baml init \\\n        --client-type rest/openapi --openapi-client-type rust\n      ```\n      </Tab>\n\n      <Tab title=\"Other\">\n\n      As long as there's an OpenAPI client generator that works with your stack,\n      you can use it with BAML. Check out the [full list in the OpenAPI docs][openapi-client-types].\n\n      ```bash\n      npx @boundaryml/baml init \\\n        --client-type rest/openapi --openapi-client-type $OPENAPI_CLIENT_TYPE\n      ```\n      </Tab>\n\n    </Tabs>\n\n  ### Start the BAML development server\n\n    ```bash\n    npx @boundaryml/baml dev --preview\n    ```\n    \n    This will do four things:\n\n    - serve your BAML functions over a RESTful interface on `localhost:2024`\n    - generate an OpenAPI schema in `baml_client/openapi.yaml`\n    - run `openapi-generator -g $OPENAPI_CLIENT_TYPE` in `baml_client` directory to\n      generate an OpenAPI client for you to use\n    - re-run the above steps whenever you modify any `.baml` files\n\n  <Note>\n    BAML-over-REST is currently a preview feature. Please provide feedback\n    either in [Discord][discord] or on [GitHub][openapi-feedback-github-issue]\n    so that we can stabilize the feature and keep you updated!\n  </Note>\n\n  ### Use a BAML function in any language!\n  \n  `openapi-generator` will generate a `README` with instructions for installing\n  and using your client; we've included snippets for some of the most popular\n  languages below. Check out\n  [`baml-examples`](https://github.com/BoundaryML/baml-examples) for example\n  projects with instructions for running them.\n\n  <Note>\n    We've tested the below listed OpenAPI clients, but not all of them. If you run\n    into issues with any of the OpenAPI clients, please let us know, either in\n    [Discord][discord] or by commenting on\n    [GitHub][openapi-feedback-github-issue] so that we can either help you out\n    or fix it!\n  </Note>\n\n<Tabs>\n\n<Tab title=\"Go\">\n\nRun this with `go run main.go`:\n\n```go main.go\npackage main\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n  baml \"my-golang-app/baml_client\"\n)\n\nfunc main() {\n\tcfg := baml.NewConfiguration()\n\tb := baml.NewAPIClient(cfg).DefaultAPI\n\textractResumeRequest := baml.ExtractResumeRequest{\n\t\tResume: \"Ada Lovelace (@gmail.com) was an English mathematician and writer\",\n\t}\n\tresp, r, err := b.ExtractResume(context.Background()).ExtractResumeRequest(extractResumeRequest).Execute()\n\tif err != nil {\n\t\tfmt.Printf(\"Error when calling b.ExtractResume: %v\\n\", err)\n\t\tfmt.Printf(\"Full HTTP response: %v\\n\", r)\n\t\treturn\n\t}\n\tlog.Printf(\"Response from server: %v\\n\", resp)\n}\n```\n</Tab>\n\n<Tab title=\"Java\">\nFirst, add the OpenAPI-generated client to your project.\n\n<AccordionGroup>\n\n<Accordion title=\"If you have 'mvn' in your PATH\">\n\nYou can use the default `on_generate` command, which will tell `baml dev` to\ninstall the OpenAPI-generated client into your local Maven repository by running\n`mvn clean install` every time you save a change to a BAML file.\n\nTo depend on the client in your local Maven repo, you can use these configs:\n\n<CodeGroup>\n```xml pom.xml\n<dependency>\n  <groupId>org.openapitools</groupId>\n  <artifactId>openapi-java-client</artifactId>\n  <version>0.1.0</version>\n  <scope>compile</scope>\n</dependency>\n```\n\n```kotlin settings.gradle.kts\nrepositories {\n    mavenCentral()\n    mavenLocal()\n}\n\ndependencies {\n    implementation(\"org.openapitools:openapi-java-client:0.1.0\")\n}\n```\n</CodeGroup>\n\n</Accordion>\n\n<Accordion title=\"If you don't have 'mvn' in your PATH\">\n\nYou'll probably want to comment out `on_generate` and instead use either the [OpenAPI Maven plugin] or [OpenAPI Gradle plugin] to build your OpenAPI client.\n\n[OpenAPI Maven plugin]: https://github.com/OpenAPITools/openapi-generator/tree/master/modules/openapi-generator-maven-plugin\n[OpenAPI Gradle plugin]: https://github.com/OpenAPITools/openapi-generator/tree/master/modules/openapi-generator-gradle-plugin\n\n<CodeGroup>\n```xml pom.xml\n<build>\n    <plugins>\n        <plugin>\n            <groupId>org.openapitools</groupId>\n            <artifactId>openapi-generator-maven-plugin</artifactId>\n            <version>7.8.0</version> <!-- Use the latest stable version -->\n            <executions>\n                <execution>\n                    <goals>\n                        <goal>generate</goal>\n                    </goals>\n                    <configuration>\n                        <inputSpec>${project.basedir}/baml_client/openapi.yaml</inputSpec>\n                        <generatorName>baml</generatorName> <!-- or another generator name, e.g. 'kotlin' or 'spring' -->\n                        <output>${project.build.directory}/generated-sources/openapi</output>\n                        <apiPackage>com.boundaryml.baml_client.api</apiPackage>\n                        <modelPackage>com.boundaryml.baml_client.model</modelPackage>\n                        <invokerPackage>com.boundaryml.baml_client</invokerPackage>\n                        <java8>true</java8>\n                    </configuration>\n                </execution>\n            </executions>\n        </plugin>\n    </plugins>\n</build>\n```\n\n```kotlin settings.gradle.kts\nplugins {\n    id(\"org.openapi.generator\") version \"7.8.0\"\n}\n\nopenApiGenerate {\n    generatorName.set(\"java\") // Change to 'kotlin', 'spring', etc. if needed\n    inputSpec.set(\"${projectDir}/baml_client/openapi.yaml\")\n    outputDir.set(\"$buildDir/generated-sources/openapi\")\n    apiPackage.set(\"com.boundaryml.baml_client.api\")\n    modelPackage.set(\"com.boundaryml.baml_client.model\")\n    invokerPackage.set(\"com.boundaryml.baml_client\")\n    additionalProperties.set(mapOf(\"java8\" to \"true\"))\n}\n\nsourceSets[\"main\"].java {\n    srcDir(\"$buildDir/generated-sources/openapi/src/main/java\")\n}\n\ntasks.named(\"compileJava\") {\n    dependsOn(\"openApiGenerate\")\n}\n```\n</CodeGroup>\n\n</Accordion>\n</AccordionGroup>\n\nThen, copy this code into wherever your `main` function is:\n\n```Java\nimport com.boundaryml.baml_client.ApiClient;\nimport com.boundaryml.baml_client.ApiException;\nimport com.boundaryml.baml_client.Configuration;\n// NOTE: baml_client/README.md will suggest importing from models.* - that is wrong.\n// See https://github.com/OpenAPITools/openapi-generator/issues/19431 for more details.\nimport com.boundaryml.baml_client.model.*;\nimport com.boundaryml.baml_client.api.DefaultApi;\n\npublic class Example {\n  public static void main(String[] args) {\n    ApiClient defaultClient = Configuration.getDefaultApiClient();\n    DefaultApi apiInstance = new DefaultApi(defaultClient);\n    ExtractResumeRequest extractResumeRequest = new ExtractResumeRequest(); // ExtractResumeRequest | \n    try {\n      Resume result = apiInstance.extractResume(extractResumeRequest);\n      System.out.println(result);\n    } catch (ApiException e) {\n      System.err.println(\"Exception when calling DefaultApi#extractResume\");\n      System.err.println(\"Status code: \" + e.getCode());\n      System.err.println(\"Reason: \" + e.getResponseBody());\n      System.err.println(\"Response headers: \" + e.getResponseHeaders());\n      e.printStackTrace();\n    }\n  }\n}\n\n```\n</Tab>\n\n<Tab title=\"PHP\">\n\n<Warning>\n  The PHP OpenAPI generator doesn't support OpenAPI's `oneOf` type, which is\n  what we map BAML union types to. Please let us know if this is an issue for\n  you, and you need help working around it.\n</Warning>\n\nFirst, add the OpenAPI-generated client to your project:\n\n```json composer.json\n    \"repositories\": [\n        {\n            \"type\": \"path\",\n            \"url\": \"baml_client\"\n        }\n    ],\n    \"require\": {\n        \"boundaryml/baml-client\": \"*@dev\"\n    }\n```\n\nYou can now use this code to call a BAML function:\n\n```PHP\n<?php\nrequire_once(__DIR__ . '/vendor/autoload.php');\n\n$apiInstance = new BamlClient\\Api\\DefaultApi(\n    new GuzzleHttp\\Client()\n);\n$extract_resume_request = new BamlClient\\Model\\ExtractResumeRequest();\n$extract_resume_request->setResume(\"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\");\n\ntry {\n    $result = $apiInstance->extractResume($extract_resume_request);\n    print_r($result);\n} catch (Exception $e) {\n    echo 'Exception when calling DefaultApi->extractResume: ', $e->getMessage(), PHP_EOL;\n}\n```\n</Tab>\n\n<Tab title=\"Ruby\">\n\nUse `ruby -Ilib/baml_client app.rb` to run this:\n\n```ruby app.rb\nrequire 'baml_client'\nrequire 'pp'\n\napi_client = BamlClient::ApiClient.new\nb = BamlClient::DefaultApi.new(api_client)\n\nextract_resume_request = BamlClient::ExtractResumeRequest.new(\n  resume: <<~RESUME\n    John Doe\n\n    Education\n    - University of California, Berkeley\n    - B.S. in Computer Science\n    - graduated 2020\n\n    Skills\n    - Python\n    - Java\n    - C++\n  RESUME\n)\n\nbegin\n  result = b.extract_resume(extract_resume_request)\n  pp result\n\n  edu0 = result.education[0]\n  puts \"Education: #{edu0.school}, #{edu0.degree}, #{edu0.year}\"\nrescue BamlClient::ApiError => e\n  puts \"Error when calling DefaultApi#extract_resume\"\n  pp e\nend\n```\n</Tab>\n\n<Tab title=\"Rust\">\n\n<Tip>\n  If you're using `cargo watch -- cargo build` and seeing build failures because it can't find\n  the generated `baml_client`, try increasing the delay on `cargo watch` to 1 second like so:\n\n  ```bash\n  cargo watch --delay 1 -- cargo build\n  ```\n</Tip>\n\nFirst, add the OpenAPI-generated client to your project:\n\n```toml Cargo.toml\n[dependencies]\nbaml-client = { path = \"./baml_client\" }\n```\n\nYou can now use `cargo run`:\n\n```rust\nuse baml_client::models::ExtractResumeRequest;\nuse baml_client::apis::default_api as b;\n\n#[tokio::main]\nasync fn main() {\n    let config = baml_client::apis::configuration::Configuration::default();\n\n    let resp = b::extract_resume(&config, ExtractResumeRequest {\n        resume: \"Tony Hoare is a British computer scientist who has made foundational contributions to programming languages, algorithms, operating systems, formal verification, and concurrent computing.\".to_string(),\n    }).await.unwrap();\n\n    println!(\"{:#?}\", resp);\n}\n```\n</Tab>\n\n</Tabs>\n\n</Steps>\n\n[discord]: https://discord.gg/BTNBeXGuaS\n[openapi-feedback-github-issue]: https://github.com/BoundaryML/baml/issues/892\n[npx-windows-issue]: https://github.com/nodejs/node/issues/53538\n[openapi-client-types]: https://github.com/OpenAPITools/openapi-generator#overview\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/02-languages/rest.mdx"
      },
      "01-guide/08-integrations/nextjs.mdx": {
        "markdown": "---\ntitle: Next.js Integration\n---\n\nBAML can be used with Vercel's AI SDK to stream BAML functions to your UI.\n\nThe latest example code is found in our [NextJS starter](https://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter), but this tutorial will guide you on how to add BAML step-by-step.\n\nSee the [live demo](https://baml-examples.vercel.app/)\n\n<Note>\nYou will need to use Server Actions, from the App Router, for this tutorial. You can still stream BAML functions from Route Handlers however.\n</Note>\n\n\n<Steps>\n### Install BAML, and Generate a BAML client for TypeScript\n- Follow [the TS installation guide](/docs/get-started/quickstart/typescript)\n- Install the VSCode extension and Save a baml file to generate the client (or use `npx baml-cli generate`).\n\n\n### Update next.config.mjs\n\n@boundaryml/baml uses a native node addon to run the BAML functions. You need to tell NextJS to use the loader for these files.\n```JS\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  experimental: {\n    serverComponentsExternalPackages: [\"@boundaryml/baml\"],\n  },\n  webpack: (config, { dev, isServer, webpack, nextRuntime }) => {\n    config.module.rules.push({\n      test: /\\.node$/,\n      use: [\n        {\n          loader: \"nextjs-node-loader\",\n          options: {\n            outputPath: config.output.path,\n          },\n        },\n      ],\n    });\n\n    return config;\n  },\n};\n\nexport default nextConfig;\n```\n\n\n### Create some helper utilities to stream BAML functions\nLet's add some helpers to export our baml functions as streamable server actions. See the last line in this file, where we export the `extractResume` function.\n\nIn `app/utils/streamableObject.tsx` add the following code:\n```typescript\nimport { createStreamableValue, StreamableValue as BaseStreamableValue } from \"ai/rsc\";\nimport { BamlStream } from \"@boundaryml/baml\";\nimport { b } from \"@/baml_client\"; // You can change the path of this to wherever your baml_client is located.\n\n\n// ------------------------------\n// Helper functions\n// ------------------------------\n\n/**\n * Type alias for defining a StreamableValue based on a BamlStream.\n * It captures either a partial or final result depending on the stream state.\n */\ntype StreamableValue<T extends BamlStream<any, any>> =\n  | { partial: T extends BamlStream<infer StreamRet, any> ? StreamRet : never }\n  | { final: T extends BamlStream<any, infer Ret> ? Ret : never };\n\n/**\n * Helper function to manage and handle a BamlStream.\n * It consumes the stream, updates the streamable value for each partial event,\n * and finalizes the stream when complete.\n *\n * @param bamlStream - The BamlStream to be processed.\n * @returns A promise that resolves with an object containing the BaseStreamableValue.\n */\nexport async function streamHelper<T extends BamlStream<any, any>>(\n  bamlStream: T,\n): Promise<{\n  object: BaseStreamableValue<StreamableValue<T>>;\n}> {\n  const stream = createStreamableValue<StreamableValue<T>>();\n\n  // Asynchronous function to process the BamlStream events\n  (async () => {\n    try {\n      // Iterate through the stream and update the stream value with partial data\n      for await (const event of bamlStream) {\n        stream.update({ partial: event });\n      }\n\n      // Obtain the final response once all events are processed\n      const response = await bamlStream.getFinalResponse();\n      stream.done({ final: response });\n    } catch (err) {\n      // Handle any errors during stream processing\n      stream.error(err);\n    }\n  })();\n\n  return { object: stream.value };\n}\n\n/**\n * Utility function to create a streamable function from a BamlStream-producing function.\n * This function returns an asynchronous function that manages the streaming process.\n *\n * @param func - A function that produces a BamlStream when called.\n * @returns An asynchronous function that returns a BaseStreamableValue for the stream.\n */\nexport function makeStreamable<\n  BamlStreamFunc extends (...args: any) => BamlStream<any, any>,\n>(\n  func: BamlStreamFunc\n): (...args: Parameters<BamlStreamFunc>) => Promise<{\n  object: BaseStreamableValue<StreamableValue<ReturnType<BamlStreamFunc>>>;\n}> {\n  return async (...args) => {\n    const boundFunc = func.bind(b.stream);\n    const stream = boundFunc(...args);\n    return streamHelper(stream);\n  };\n}\n\n```\n\n### Export your BAML functions to streamable server actions\n\nIn `app/actions/extract.tsx` add the following code:\n```typescript\nimport { makeStreamable } from \"../_baml_utils/streamableObjects\";\n\n\nexport const extractResume = makeStreamable(b.stream.ExtractResume);\n```\n\n### Create a hook to use the streamable functions in React Components\nThis hook will work like [react-query](https://react-query.tanstack.com/), but for BAML functions.\nIt will give you partial data, the loading status, and whether the stream was completed.\n\nIn `app/_hooks/useStream.ts` add:\n```typescript\nimport { useState, useEffect } from \"react\";\nimport { readStreamableValue, StreamableValue } from \"ai/rsc\";\n\n/**\n * A hook that streams data from a server action. The server action must return a StreamableValue.\n * See the example actiimport { useState, useEffect } from \"react\";\nimport { readStreamableValue, StreamableValue } from \"ai/rsc\";\n\n/**\n * A hook that streams data from a server action. The server action must return a StreamableValue.\n * See the example action in app/actions/streamable_objects.tsx\n *  **/\nexport function useStream<PartialRet, Ret, P extends any[]>(\n  serverAction: (...args: P) => Promise<{ object: StreamableValue<{ partial: PartialRet } | { final: Ret }, any> }>\n) {\n  const [isLoading, setIsLoading] = useState(false);\n  const [isComplete, setIsComplete] = useState(false);\n  const [isError, setIsError] = useState(false);\n  const [error, setError] = useState<Error | null>(null);\n  const [partialData, setPartialData] = useState<PartialRet | undefined>(undefined); // Initialize data state\n  const [streamResult, setData] = useState<Ret  | undefined>(undefined); // full non-partial data\n\n  const mutate = async (\n    ...params: Parameters<typeof serverAction>\n  ): Promise<Ret | undefined> => {\n    console.log(\"mutate\", params);\n    setIsLoading(true);\n    setIsError(false);\n    setError(null);\n\n    try {\n      const { object } = await serverAction(...params);\n      const asyncIterable = readStreamableValue(object);\n\n      for await (const value of asyncIterable) {\n        if (value !== undefined) {\n\n          // could also add a callback here.\n          // if (options?.onData) {\n          //   options.onData(value as T);\n          // }\n          console.log(\"value\", value);\n          if (\"partial\" in value) {\n            setPartialData(value.partial); // Update data state with the latest value\n          } else if (\"final\" in value) {\n            setData(value.final); // Update data state with the latest value\n            setIsComplete(true);\n            return value.final;\n          }\n        }\n      }\n\n      // // If it completes, it means it's the full data.\n      // return streamedData;\n    } catch (err) {\n      console.log(\"error\", err);\n\n      setIsError(true);\n      setError(new Error(JSON.stringify(err) ?? \"An error occurred\"));\n      return undefined;\n    } finally {\n      setIsLoading(false);\n    }\n  };\n\n  // If you use the \"data\" property, your component will re-render when the data gets updated.\n  return { data: streamResult, partialData, isLoading, isComplete, isError, error, mutate };\n}\n\n```\n\n\n\n### Stream your BAML function in a component\nIn `app/page.tsx` you can use the hook to stream the BAML function and render the result in real-time.\n\n```tsx\n\"use client\";\nimport {\n  extractResume,\n  extractUnstructuredResume,\n} from \"../../actions/streamable_objects\";\n// import types from baml files like this:\nimport { Resume } from \"@/baml_client\";\n\nexport default function Home() {\n  // you can also rename these fields by using \":\", like how we renamed partialData to \"partialResume\"\n  // `mutate` is a function that will start the stream. It takes in the same arguments as the BAML function.\n  const { data: completedData, partialData: partialResume, isLoading, isError, error, mutate } = useStream(extractResume);\n\n  return (\n    <div>\n      <h1>BoundaryML Next.js Example</h1>\n      \n      <button onClick={() => mutate(\"Some resume text\")}>Stream BAML</button>\n      {isLoading && <p>Loading...</p>}\n      {isError && <p>Error: {error?.message}</p>}\n      {partialData && <pre>{JSON.stringify(partialData, null, 2)}</pre>}\n      {data && <pre>{JSON.stringify(data, null, 2)}</pre>}\n    </div>\n  );\n}\n```\n\n</Steps>\n\n\nAnd now you're all set!\n\nIf you have issues with your environment variables not loading, you may want to use [dotenv-cli](https://www.npmjs.com/package/dotenv-cli) to load your env vars before the nextjs process starts:\n\n`dotenv -- npm run dev`\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/08-integrations/nextjs.mdx"
      },
      "01-guide/03-development/environment-variables.mdx": {
        "markdown": "---\ntitle: Set Environment Variables\nslug: /guide/development/environment-variables\n---\n\n\n\n## Environment Variables in BAML\n\nSometimes you'll see environment variables used in BAML, like in clients:\n\n```baml\n\nclient<llm> GPT4o {\n  provider baml-openai-chat\n  options {\n    model gpt-4o\n    api_key env.OPENAI_API_KEY\n  }\n}\n```\n\nTo set environment variables:\n\n<AccordionGroup>\n  <Accordion title=\"In the VSCode Playground\">\n\n   \nOnce you open a `.baml` file, in VSCode, you should see a small button over every BAML function: `Open Playground`.\n\nThen you should be able to set environment variables in the settings tab.\n\n<img src=\"file:d4e9528c-697a-4aa9-ac02-f92823252a6b\" />\n\nOr type `BAML Playground` in the VSCode Command Bar (`CMD + Shift + P` or `CTRL + Shift + P`) to open the playground.\n\n  </Accordion>\n\n  <Accordion title=\"For your app (default)\">\n    BAML will expect these to be set already in your program **before** you import the baml_client in Python/ TS / etc.\n\n    Any of the following strategies for setting env vars are compatible with BAML:\n    - setting them in your shell before running your program\n    - in your `Dockerfile`\n    - in your `next.config.js`\n    - in your Kubernetes manifest\n    - from secrets-store.csi.k8s.io\n    - from a secrets provider such as [Infisical](https://infisical.com/) / [Doppler](https://www.doppler.com/)\n    - from a `.env` file (using `dotenv` cli)\n    - using account credentials for ephemeral token generation (e.g. Vertex AI Auth Tokens)\n\n    ```bash\n    export MY_SUPER_SECRET_API_KEY=\"...\"\n    python my_program_using_baml.py\n    ```\n  </Accordion>\n  \n  <Accordion title=\"For your app (manually)\">\n    <Info>\n    Requires BAML Version 0.57+\n    </Info>\n\n    If you don't want BAML to try to auto-load your env vars, you can call manually `reset_baml_env_vars`\nwith the current environment variables.\n    <CodeBlocks>\n\n    ```python Python\n\n    from baml_client import b\n    from baml_client import reset_baml_env_vars\n    import os\n    import dotenv\n\n    dotenv.load_dotenv()\n    reset_baml_env_vars(dict(os.environ))\n    ```\n\n    ```typescript TypeScript\n    import dotenv from 'dotenv'\n    // Wait to import the BAML client until after loading environment variables\n    import { b, resetBamlEnvVars } from 'baml-client'\n\n    dotenv.config()\n    resetBamlEnvVars(process.env)\n    ```\n\n    ```ruby Ruby (beta)\n    require 'dotenv/load'\n\n    # Wait to import the BAML client until after loading environment variables\n    # reset_baml_env_vars is not yet implemented in the Ruby client\n    require 'baml_client'\n    ```\n\n\n    </CodeBlocks>\n  </Accordion>\n</AccordionGroup>\n\n\n## Dynamically setting LLM API Keys\nYou can set the API key for an LLM dynamically by passing in the key as a header or as a parameter (depending on the provider), using the [ClientRegistry](/guide/baml-advanced/llm-client-registry).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/03-development/environment-variables.mdx"
      },
      "01-guide/03-development/terminal-logs.mdx": {
        "markdown": "---\nslug: /guide/development/terminal-logs\n---\nYou can add logging to determine what the BAML runtime is doing when it calls LLM endpoints and parses responses.\n\nTo enable logging, set the `BAML_LOG` environment variable:\n```sh\n# default is warn\nBAML_LOG=info\n```\n\n| Level | Description |\n|-------|-------------|\n| `error` | Fatal errors by BAML |\n| `warn` | Logs any time a function fails (includes LLM calling failures, parsing failures) |\n| `info` | Logs every call to a function (including prompt, raw response, and parsed response) |\n| `debug` | Requests and detailed parsing errors (warning: may be a lot of logs) |\n| `trace` | Everything and more |\n| `off` | No logging |\n\n\nExample log:\n<img src=\"file:a410f5dc-aff8-430e-bce9-3fec22eca527\" />\n\n---\n\nSince `>0.54.0`:\n\nTo truncate each log entry to a certain length, set the `BOUNDARY_MAX_LOG_CHUNK_CHARS` environment variable:\n\n```sh\nBOUNDARY_MAX_LOG_CHUNK_CHARS=3000\n```\n\nThis will truncate each part in a log entry to 3000 characters.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/03-development/terminal-logs.mdx"
      },
      "01-guide/03-development/upgrade-baml-versions.mdx": {
        "markdown": "---\nslug: /guide/development/upgrade-baml-versions\ntitle: Upgrading BAML / Fixing Version Mismatches\n---\n\nRemember that the generated `baml_client` code is generated by your `baml_py` / `@boundaryml/baml` package dependency (using `baml-cli generate`), but can also be generated by the VSCode extension when you save a BAML file.\n\n**To upgrade BAML versions:**\n1. Update the `generator` clause in your `generators.baml` file (or wherever you have it defined) to the new version. If you ran `baml-cli init`, one has already been generated for you!\n```baml generators.baml\ngenerator TypescriptGenerator {\n    output_type \"typescript\"\n    ....\n    // Version of runtime to generate code for (should match the package @boundaryml/baml version)\n    version \"0.62.0\"\n}\n```\n\n2. Update your `baml_py`  / `@boundaryml/baml` package dependency to the same version.\n\n\n<CodeBlock>\n```sh pip\npip install --upgrade baml-py\n```\n```sh npm\nnpm install @boundaryml/baml@latest\n```\n\n```sh ruby\ngem install baml\n```\n</CodeBlock>\n\n3. Update VSCode BAML extension to point to the same version. Read here for how to keep VSCode in sync with your `baml_py` / `@boundaryml/baml` package dependency: [VSCode BAML Extension reference](/ref/editor-extension-settings/baml-cli-path)\n\nYou only need to do this for minor version upgrades (e.g., 0.54.0 -> 0.62.0), not patch versions (e.g., 0.62.0 -> 0.62.1).\n\n\n\n## Troubleshooting\n\nSee the [VSCode BAML Extension reference](/guide/reference/vscode-ext/clipath) for more information on how to prevent version mismatches.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/03-development/upgrade-baml-versions.mdx"
      },
      "01-guide/03-development/deploying/aws.mdx": {
        "markdown": "---\ntitle: AWS\n---\n\nYou can use [SST](https://sst.dev/) to define the Lambda configuration and deploy it.\n\nThe example below builds the BAML x86_64 rust binaries into a Lambda layer and uses the layer in the Lambda function.\n\n[Example Node + SST Project](https://github.com/BoundaryML/baml-examples/tree/main/node-aws-lambda-sst)\n\nLet us know if you want to deploy a python BAML project on AWS. Our example project is coming soon.\n\n### Current limitations\nThe BAML binaries only support the NodeJS 20.x runtime (or a runtime using Amazon Linux 2023). Let us know if you need a different runtime version.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/03-development/deploying/aws.mdx"
      },
      "01-guide/03-development/deploying/nextjs.mdx": {
        "markdown": "---\ntitle: NextJS\n---\n\nTo deploy a NextJS with BAML, take a look at the starter template:\nhttps://github.com/BoundaryML/baml-examples/tree/main/nextjs-starter\n\nAll you need is to modify the `nextjs.config.mjs` to allow BAML to run properly:\n```JS\n/** @type {import('next').NextConfig} */\nconst nextConfig = {\n  experimental: {\n    serverComponentsExternalPackages: [\"@boundaryml/baml\"],\n  },\n  webpack: (config, { dev, isServer, webpack, nextRuntime }) => {\n    config.module.rules.push({\n      test: /\\.node$/,\n      use: [\n        {\n          loader: \"nextjs-node-loader\",\n          options: {\n            outputPath: config.output.path,\n          },\n        },\n      ],\n    });\n\n    return config;\n  },\n};\n\nexport default nextConfig;\n```\n\nand change your `package.json` to build the baml client automatically (and enable logging in dev mode if you want):\n\n```json\n \"scripts\": {\n    \"dev\": \"BAML_LOG=info next dev\",\n    \"build\": \"pnpm generate && next build\",\n    \"start\": \"next start\",\n    \"lint\": \"next lint\",\n    \"generate\": \"baml-cli generate --from ./baml_src\"\n  },\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/03-development/deploying/nextjs.mdx"
      },
      "01-guide/03-development/deploying/docker.mdx": {
        "markdown": "---\ntitle: Docker\n---\n\n\nWhen you develop with BAML, the BAML VScode extension generates a `baml_client` directory (on every save) with all the generated code you need to use your AI functions in your application.\n\nWe recommend you add `baml_client` to your `.gitignore` file to avoid committing generated code to your repository, and re-generate the client code when you build and deploy your application.\n\nYou _could_ commit the generated code if you're starting out to not deal with this, just make sure the VSCode extension version matches your baml package dependency version (e.g. `baml-py` for python and `@boundaryml/baml` for TS) so there are no compatibility issues.\n\nTo build your client you can use the following command. See also [baml-cli generate](/ref/baml-cli/generate):\n  \n<CodeBlocks>\n\n```dockerfile python Dockerfile\nRUN baml-cli generate --from path-to-baml_src\n```\n\n```dockerfile TypeScript Dockerfile\n# Do this early on in the dockerfile script before transpiling to JS\nRUN npx baml-cli generate --from path-to-baml_src\n```\n\n```dockerfile Ruby Dockerfile\nRUN bundle add baml\nRUN bundle exec baml-cli generate --from path/to/baml_src\n```\n</CodeBlocks>\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/03-development/deploying/docker.mdx"
      },
      "01-guide/03-development/deploying/openapi.mdx": {
        "markdown": "---\ntitle: OpenAPI\n---\n\n<Info>\n  This feature was added in: v0.55.0.\n</Info>\n\n<Info>\n  This page assumes you've gone through the [OpenAPI quickstart].\n</Info>\n\n[OpenAPI quickstart]: /docs/get-started/quickstart/openapi\n\nTo deploy BAML as a RESTful API, you'll need to do three things:\n\n- host your BAML functions in a Docker container\n- update your app to call it\n- run BAML and your app side-by-side using `docker-compose`\n\nRead on to learn how to do this with `docker-compose`.\n\n<Tip>\n  You can also run `baml-cli` in a subprocess from your app directly, and we\n  may recommend this approach in the future. Please let us know if you'd\n  like to see instructions for doing so, and in what language, by asking in\n  [Discord][discord] or [on the GitHub issue][openapi-feedback-github-issue].\n</Tip>\n\n## Host your BAML functions in a Docker container\n\nIn the directory containing your `baml_src/` directory, create a\n`baml.Dockerfile` to host your BAML functions in a Docker container:\n\n<Note>\n  BAML-over-HTTP is currently a preview feature. Please provide feedback either\n  in [Discord][discord] or on [GitHub][openapi-feedback-github-issue] so that\n  we can stabilize the feature and keep you updated!\n</Note>\n\n```docker title=\"baml.Dockerfile\"\nFROM node:20\n\nWORKDIR /app\nCOPY baml_src/ .\n\n# If you want to pin to a specific version (which we recommend):\n# RUN npm install -g @boundaryml/baml@VERSION\nRUN npm install -g @boundaryml/baml\n\nCMD baml-cli serve --preview --port 2024\n```\n\n<Tabs>\n\n<Tab title=\"Using docker-compose\">\n\nAssuming you intend to run your own application in a container, we recommend\nusing `docker-compose` to run your app and BAML-over-HTTP side-by-side:\n\n```bash\ndocker compose up --build --force-recreate\n```\n\n```yaml title=\"docker-compose.yaml\"\nservices:\n  baml-over-http:\n    build:\n      # This will build baml.Dockerfile when you run docker-compose up\n      context: .\n      dockerfile: baml.Dockerfile\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:2024/_debug/ping\" ]\n      interval: 1s\n      timeout: 100ms\n      retries: 3\n    # This allows you to 'curl localhost:2024/_debug/ping' from your machine,\n    # i.e. the Docker host\n    ports:\n      - \"2024:2024\"\n\n  debug-container:\n    image: amazonlinux:latest\n    depends_on:\n      # Wait until the baml-over-http healthcheck passes to start this container\n      baml-over-http:\n        condition: service_healthy\n    command: \"curl -v http://baml-over-http:2024/_debug/ping\"\n```\n\n<Note>\n  To call the BAML server from your laptop (i.e. the host machine), you must use\n  `localhost:2024`. You may only reach it as `baml-over-http:2024` from within\n  another Docker container.\n</Note>\n\n</Tab>\n\n<Tab title=\"Using docker\">\n\nIf you don't care about using `docker-compose`, you can just run:\n\n```bash\ndocker build -t baml-over-http -f baml.Dockerfile .\ndocker run -p 2024:2024 baml-over-http\n```\n</Tab>\n\n</Tabs>\n\nTo verify for yourself that BAML-over-HTTP is up and running, you can run:\n\n```bash\ncurl http://localhost:2024/_debug/ping\n```\n\n## Update your app to call it\n\nUpdate your code to use `BOUNDARY_ENDPOINT`, if set, as the endpoint for your BAML functions. \n\nIf you plan to use [Boundary Cloud](/ref/cloud/functions/get-started) to host your BAML functions, you should also update it to use `BOUNDARY_API_KEY`.\n\n<Tabs>\n\n<Tab title=\"Go\">\n\n```go\nimport (\n    \"os\"\n    baml \"my-golang-app/baml_client\"\n)\n\nfunc main() {\n    cfg := baml.NewConfiguration()\n    if boundaryEndpoint := os.Getenv(\"BOUNDARY_ENDPOINT\"); boundaryEndpoint != \"\" {\n        cfg.BasePath = boundaryEndpoint\n    }\n    if boundaryApiKey := os.Getenv(\"BOUNDARY_API_KEY\"); boundaryApiKey != \"\" {\n        cfg.DefaultHeader[\"Authorization\"] = \"Bearer \" + boundaryApiKey\n    }\n    b := baml.NewAPIClient(cfg).DefaultAPI\n    // Use `b` to make API calls\n}\n```\n\n</Tab>\n\n<Tab title=\"Java\">\n```java\nimport com.boundaryml.baml_client.ApiClient;\nimport com.boundaryml.baml_client.ApiException;\nimport com.boundaryml.baml_client.Configuration;\nimport com.boundaryml.baml_client.api.DefaultApi;\nimport com.boundaryml.baml_client.auth.*;\n\npublic class ApiExample {\n    public static void main(String[] args) {\n        ApiClient apiClient = Configuration.getDefaultApiClient();\n\n        String boundaryEndpoint = System.getenv(\"BOUNDARY_ENDPOINT\");\n        if (boundaryEndpoint != null && !boundaryEndpoint.isEmpty()) {\n            apiClient.setBasePath(boundaryEndpoint);\n        }\n\n        String boundaryApiKey = System.getenv(\"BOUNDARY_API_KEY\");\n        if (boundaryApiKey != null && !boundaryApiKey.isEmpty()) {\n            apiClient.addDefaultHeader(\"Authorization\", \"Bearer \" + boundaryApiKey);\n        }\n\n        DefaultApi apiInstance = new DefaultApi(apiClient);\n        // Use `apiInstance` to make API calls\n    }\n}\n```\n\n</Tab>\n\n<Tab title=\"PHP\">\n\n```php\nrequire_once(__DIR__ . '/vendor/autoload.php');\n\n$config = BamlClient\\Configuration::getDefaultConfiguration();\n\n$boundaryEndpoint = getenv('BOUNDARY_ENDPOINT');\n$boundaryApiKey = getenv('BOUNDARY_API_KEY');\n\nif ($boundaryEndpoint) {\n    $config->setHost($boundaryEndpoint);\n}\n\nif ($boundaryApiKey) {\n    $config->setAccessToken($boundaryApiKey);\n}\n\n$apiInstance = new OpenAPI\\Client\\Api\\DefaultApi(\n    new GuzzleHttp\\Client(),\n    $config\n);\n\n// Use `$apiInstance` to make API calls\n```\n\n</Tab>\n\n<Tab title=\"Ruby\">\n```ruby\nrequire 'baml_client'\n\napi_client = BamlClient::ApiClient.new\n\nboundary_endpoint = ENV['BOUNDARY_ENDPOINT']\nif boundary_endpoint\n  api_client.host = boundary_endpoint\nend\n\nboundary_api_key = ENV['BOUNDARY_API_KEY']\nif boundary_api_key\n  api_client.default_headers['Authorization'] = \"Bearer #{boundary_api_key}\"\nend\nb = BamlClient::DefaultApi.new(api_client)\n# Use `b` to make API calls\n```\n</Tab>\n\n<Tab title=\"Rust\">\n```rust\nlet mut config = baml_client::apis::configuration::Configuration::default();\nif let Some(base_path) = std::env::var(\"BOUNDARY_ENDPOINT\").ok() {\n    config.base_path = base_path;\n}\nif let Some(api_key) = std::env::var(\"BOUNDARY_API_KEY\").ok() {\n    config.bearer_access_token = Some(api_key);\n}\n// Use `config` to make API calls\n```\n</Tab>\n\n</Tabs>\n\n\n## Run your app with docker-compose\n\nReplace `debug-container` with the Dockerfile for your app in the\n`docker-compose.yaml` file:\n\n```yaml\nservices:\n  baml-over-http:\n    build:\n      context: .\n      dockerfile: baml.Dockerfile\n    networks:\n      - my-app-network\n    healthcheck:\n      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:2024/_debug/ping\" ]\n      interval: 1s\n      timeout: 100ms\n      retries: 3\n    ports:\n      - \"2024:2024\"\n\n  my-app:\n    build:\n      context: .\n      dockerfile: my-app.Dockerfile\n    depends_on:\n      baml-over-http:\n        condition: service_healthy\n    environment:\n      - BAML_ENDPOINT=http://baml-over-http:2024\n\n  debug-container:\n    image: amazonlinux:latest\n    depends_on:\n      baml-over-http:\n        condition: service_healthy\n    command: sh -c 'curl -v \"$${BAML_ENDPOINT}/_debug/ping\"'\n    environment:\n      - BAML_ENDPOINT=http://baml-over-http:2024\n```\n\nAdditionally, you'll want to make sure that you generate the BAML client at\nimage build time, because `baml_client/` should not be checked into your repo.\n\nThis means that in the CI workflow you use to push your Docker images, you'll\nwant to do something like this:\n\n```yaml .github/workflows/build-image.yaml\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Build the BAML client\n        run: |\n          set -eux\n          npx @boundaryml/baml generate\n          docker build -t my-app .\n```\n\n## (Optional) Secure your BAML functions\n\nTo secure your BAML server, you can also set a password on it using the\n`BAML_PASSWORD` environment variable:\n\n<Tabs>\n\n<Tab title=\"bash\">\n\n```bash\nBAML_PASSWORD=sk-baml-your-secret-password \\\n  baml-cli serve --preview --port 2024\n```\n</Tab>\n\n<Tab title=\"Dockerfile\">\n\n```docker\nFROM node:20\n\nWORKDIR /app\nRUN npm install -g @boundaryml/baml\nCOPY baml_src/ .\n\nENV BAML_PASSWORD=sk-baml-your-secret-password\nCMD baml-cli serve --preview --port 2024\n```\n</Tab>\n\n</Tabs>\n\nThis will require incoming requests to attach your specified password as\nauthorization metadata. You can verify this by confirming that this returns `403\nForbidden`:\n\n```bash\ncurl -v \"http://localhost:2024/_debug/status\"\n```\n\nIf you attach your password to the request, you'll see that it now returns `200 OK`:\n\n<Tabs>\n\n<Tab title=\"Using HTTP basic auth\">\n```bash\nexport BAML_PASSWORD=sk-baml-your-secret-password\ncurl \"http://baml:${BAML_PASSWORD}@localhost:2024/_debug/status\"\n```\n</Tab>\n\n<Tab title=\"Using X-BAML-API-KEY\">\n```bash\nexport BAML_PASSWORD=sk-baml-your-secret-password\ncurl \"http://localhost:2024/_debug/status\" -H \"X-BAML-API-KEY: ${BAML_PASSWORD}\"\n```\n</Tab>\n\n</Tabs>\n\n<Note>\n  `BAML_PASSWORD` will secure all endpoints _except_ `/_debug/ping`, so that you\n  can always debug the reachability of your BAML server.\n</Note>\n\n[discord]: https://discord.gg/BTNBeXGuaS\n[openapi-feedback-github-issue]: https://github.com/BoundaryML/baml/issues/892\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/03-development/deploying/openapi.mdx"
      },
      "01-guide/04-baml-basics/my-first-function.mdx": {
        "markdown": "---\ntitle: Prompting in BAML\n---\n\n<Note>\nWe recommend reading the [installation](/guide/installation-language/python) instructions first\n</Note>\n\nBAML functions are special definitions that get converted into real code (Python, TS, etc) that calls LLMs. Think of them as a way to define AI-powered functions that are type-safe and easy to use in your application.\n\n### What BAML Functions Actually Do\nWhen you write a BAML function like this:\n\n```rust BAML\nfunction ExtractResume(resume_text: string) -> Resume {\n  client \"openai/gpt-4o\"\n  // The prompt uses Jinja syntax.. more on this soon.\n  prompt #\"\n     Extract info from this text.\n\n    {# special macro to print the output schema + instructions #}\n    {{ ctx.output_format }}\n\n    Resume:\n    ---\n    {{ resume_text }}\n    ---\n  \"#\n}\n```\n\nBAML converts it into code that:\n\n1. Takes your input (`resume_text`)\n2. Sends a request to OpenAI's GPT-4 API with your prompt.\n3. Parses the JSON response into your `Resume` type\n4. Returns a type-safe object you can use in your code\n\n### Prompt Preview + seeing the CURL request\nFor maximum transparency, you can see the API request BAML makes to the LLM provider using the VSCode extension.\nBelow you can see the **Prompt Preview**, where you see the full rendered prompt (once you add a test case):\n\n<img src=\"file:7340ce64-d200-46e9-8089-eb49f9338588\" alt=\"Prompt preview\" />\n\nNote how the `{{ ctx.output_format }}` macro is replaced with the output schema instructions.\n\nThe Playground will also show you the **Raw CURL request** (if you click on the \"curl\" checkbox):\n\n<img src=\"file:88e2dcb3-ce22-49e4-a43f-084b9d5f27fe\" alt=\"Raw CURL request\" />\n\n<Warning>\nAlways include the `{{ ctx.output_format }}` macro in your prompt. This injects your output schema into the prompt, which helps the LLM output the right thing. You can also [customize what it prints](/reference/prompt-syntax/ctx-output-format).\n\nOne of our design philosophies is to never hide the prompt from you. You control and can always see the entire prompt.\n</Warning>\n\n## Calling the function\nRecall that BAML will generate a `baml_client` directory in the language of your choice using the parameters in your [`generator`](/ref/baml/generator) config. This contains the function and types you defined.\n\nNow we can call the function, which will make a request to the LLM and return the `Resume` object:\n<CodeBlocks>\n```python python\n# Import the baml client (We call it `b` for short)\nfrom baml_client import b\n# Import the Resume type, which is now a Pydantic model!\nfrom baml_client.types import Resume \n\ndef main():\nresume_text = \"\"\"Jason Doe\\nPython, Rust\\nUniversity of California, Berkeley, B.S.\\nin Computer Science, 2020\\nAlso an expert in Tableau, SQL, and C++\\n\"\"\"\n\n    # this function comes from the autogenerated \"baml_client\".\n    # It calls the LLM you specified and handles the parsing.\n    resume = b.ExtractResume(resume_text)\n\n    # Fully type-checked and validated!\n    assert isinstance(resume, Resume)\n\n```\n\n```typescript typescript\nimport b from 'baml_client'\nimport { Resume } from 'baml_client/types'\n\nasync function main() {\n  const resume_text = `Jason Doe\\nPython, Rust\\nUniversity of California, Berkeley, B.S.\\nin Computer Science, 2020\\nAlso an expert in Tableau, SQL, and C++`\n\n  // this function comes from the autogenerated \"baml_client\".\n  // It calls the LLM you specified and handles the parsing.\n  const resume = await b.ExtractResume(resume_text)\n\n  // Fully type-checked and validated!\n  resume.name === 'Jason Doe'\n  if (resume instanceof Resume) {\n    console.log('resume is a Resume')\n  }\n}\n```\n\n```ruby ruby\n\nrequire_relative \"baml_client/client\"\nb = Baml.Client\n\n# Note this is not async\nres = b.TestFnNamedArgsSingleClass(\n    myArg: Baml::Types::Resume.new(\n        key: \"key\",\n        key_two: true,\n        key_three: 52,\n    )\n)\n```\n\n</CodeBlocks>\n\n\n<Warning>\nDo not modify any code inside `baml_client`, as it's autogenerated.\n</Warning>\n\n## Next steps\n\nCheckout [PromptFiddle](https://promptfiddle.com) to see various interactive BAML function examples or view the [example prompts](/examples)\n\nRead the next guide to learn more about choosing different LLM providers and running tests in the VSCode extension.\n\n<CardGroup cols={2}>\n\n  <Card title=\"Switching LLMs\" icon=\"fa-solid fa-gears\" href=\"/guide/baml-basics/switching-llms\">\n\n    Use any provider or open-source model\n\n  </Card>\n\n  <Card title=\"Testing Functions\" icon=\"fa-solid fa-vial\" href=\"/guide/baml-basics/testing-functions\">\n\n    Test your functions in the VSCode extension\n\n  </Card>\n\n  <Card title=\"Chat Roles\" icon=\"fa-solid fa-comments\" href=\"/examples/prompt-engineering/chat\">\n\n    Define user or assistant roles in your prompts\n\n  </Card>\n\n  <Card title=\"Function Calling / Tools\" icon=\"fa-solid fa-toolbox\" href=\"/examples/prompt-engineering/tools-function-calling\">\n\n    Use function calling or tools in your prompts\n\n  </Card>\n\n</CardGroup>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/04-baml-basics/my-first-function.mdx"
      },
      "01-guide/04-baml-basics/switching-llms.mdx": {
        "markdown": "---\ntitle: Switching LLMs\nslug: guide/baml-basics/switching-llms\n---\n\nBAML Supports getting structured output from **all** major providers as well as all OpenAI-API compatible open-source models. See [LLM Providers Reference](/ref/llm-client-providers/open-ai) for how to set each one up.\n<Tip>\nBAML can help you get structured output from **any Open-Source model**, with better performance than other techniques, even when it's not officially supported via a Tool-Use API (like o1-preview) or fine-tuned for it! [Read more about how BAML does this](https://www.boundaryml.com/blog/schema-aligned-parsing).\n</Tip>\n\n### Using `client \"<provider>/<model>\"`\n\nUsing `openai/model-name` or `anthropic/model-name` will assume you have the ANTHROPIC_API_KEY or OPENAI_API_KEY environment variables set.\n\n```rust BAML\nfunction MakeHaiku(topic: string) -> string {\n  client \"openai/gpt-4o\" // or anthropic/claude-3-5-sonnet-20240620\n  prompt #\"\n    Write a haiku about {{ topic }}.\n  \"#\n}\n```\n\n### Using a named client\n<Note>Use this if you are using open-source models or need customization</Note>\nThe longer form uses a named client, and supports adding any parameters supported by the provider or changing the temperature, top_p, etc.\n\n```rust BAML\nclient<llm> MyClient {\n  provider \"openai\"\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n    // other params like temperature, top_p, etc.\n    temperature 0.0\n    base_url \"https://my-custom-endpoint.com/v1\"\n    // add headers\n    headers {\n      \"anthropic-beta\" \"prompt-caching-2024-07-31\"\n    }\n  }\n\n}\n\nfunction MakeHaiku(topic: string) -> string {\n  client MyClient\n  prompt #\"\n    Write a haiku about {{ topic }}.\n  \"#\n}\n```\n\nConsult the [provider documentation](#fields) for a list of supported providers\nand models, the default options, and setting [retry policies](/docs/reference/retry-policy).\n\n<Tip>\nIf you want to specify which client to use at runtime, in your Python/TS/Ruby code,\nyou can use the [client registry](/guide/baml-advanced/llm-client-registry) to do so.\n\nThis can come in handy if you're trying to, say, send 10% of your requests to a\ndifferent model.\n</Tip>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/04-baml-basics/switching-llms.mdx"
      },
      "01-guide/04-baml-basics/testing-functions.mdx": {
        "markdown": "---\nslug: /guide/baml-basics/testing-functions\n---\n\n\nYou can test your BAML functions in the VSCode Playground by adding a `test` snippet into a BAML file:\n\n```baml\nenum Category {\n    Refund\n    CancelOrder\n    TechnicalSupport\n    AccountIssue\n    Question\n}\n\nfunction ClassifyMessage(input: string) -> Category {\n  client GPT4Turbo\n  prompt #\"\n    ... truncated ...\n  \"#\n}\n\ntest Test1 {\n  functions [ClassifyMessage]\n  args {\n    // input is the first argument of ClassifyMessage\n    input \"Can't access my account using my usual login credentials, and each attempt results in an error message stating 'Invalid username or password.' I have tried resetting my password using the 'Forgot Password' link, but I haven't received the promised password reset email.\"\n  }\n}\n```\nSee the [interactive examples](https://promptfiddle.com)\n\nThe BAML playground will give you a starting snippet to copy that will match your function signature.\n\n<Warning>\nBAML doesn't use colons `:` between key-value pairs except in function parameters.\n</Warning>\n\n<hr />\n## Complex object inputs\n\nObjects are injected as dictionaries\n```rust\nclass Message {\n  user string\n  content string\n}\n\nfunction ClassifyMessage(messages: Messages[]) -> Category {\n...\n}\n\ntest Test1 {\n  functions [ClassifyMessage]\n  args {\n    messages [\n      {\n        user \"hey there\"\n        // multi-line string using the #\"...\"# syntax\n        content #\"\n          You can also add a multi-line\n          string with the hashtags\n          Instead of ugly json with \\n\n        \"#\n      }\n    ]\n  }\n}\n```\n<hr />\n## Test Image Inputs in the Playground\n\nFor a function that takes an image as input, like so:\n\n```baml\nfunction MyFunction(myImage: image) -> string {\n  client GPT4o\n  prompt #\"\n    Describe this image: {{myImage}}\n  \"#\n}\n```\n\nYou can define test cases using image files, URLs, or base64 strings.\n\n<Tabs>\n\n<Tab title=\"File\">\n\n<Warning>\n  Committing a lot of images into your repository can make it slow to clone and\n  pull your repository. If you expect to commit >500MiB of images, please read\n  [GitHub's size limit documentation][github-large-files] and consider setting\n  up [large file storage][github-lfs].\n</Warning>\n\n[github-large-files]: https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github\n[github-lfs]: https://docs.github.com/en/repositories/working-with-files/managing-large-files/configuring-git-large-file-storage\n\n```baml\ntest Test1 {\n  functions [MyFunction]\n  args {\n    myImage {\n      file \"../path/to/image.png\"\n    }\n  }\n}\n```\n\n<ParamField path=\"file\" type=\"string\" required=\"true\">\n  The path to the image file, relative to the directory containing the current BAML file.\n\n  Image files must be somewhere in `baml_src/`.\n</ParamField>\n\n<ParamField path=\"media_type\" type=\"string\">\n  The mime-type of the image. If not set, and the provider expects a mime-type\n  to be provided, BAML will try to infer it based on first, the file extension,\n  and second, the contents of the file.\n</ParamField>\n\n</Tab>\n\n<Tab title=\"URL\">\n```baml\ntest Test1 {\n  functions [MyFunction]\n  args {\n    myImage {\n      url \"https....\"\n    }\n  }\n}\n```\n\n<ParamField path=\"url\" type=\"string\" required=\"true\">\n  The publicly accessible URL from which the image may be downloaded.\n</ParamField>\n\n<ParamField path=\"media_type\" type=\"string\">\n  The mime-type of the image. If not set, and the provider expects a mime-type\n  to be provided, BAML will try to infer it based on the contents of the file.\n</ParamField>\n\n</Tab>\n\n<Tab title=\"Base64\">\n```baml\ntest Test1 {\n  args {\n    myImage {\n      base64 \"base64string\"\n      media_type \"image/png\"\n    }\n  }\n}\n```\n\n<ParamField path=\"base64\" type=\"string\" required=\"true\">\n  The base64-encoded image data.\n</ParamField>\n\n<ParamField path=\"media_type\" type=\"string\">\n  The mime-type of the image. If not set, and the provider expects a mime-type\n  to be provided, BAML will try to infer it based on the contents of the file.\n\n  If `base64` is a data URL, this field will be ignored.\n</ParamField>\n\n</Tab>\n</Tabs>\n\n<br />\n## Test Audio Inputs in the Playground\n\nFor a function that takes audio as input, like so:\n\n```baml\nfunction MyFunction(myAudio: audio) -> string {\n  client GPT4o\n  prompt #\"\n    Describe this audio: {{myAudio}}\n  \"#\n}\n```\n\nYou can define test cases using audio files, URLs, or base64 strings.\n\n<Tabs>\n\n<Tab title=\"File\">\n\n<Warning>\n  Committing a lot of audio files into your repository can make it slow to clone and\n  pull your repository. If you expect to commit >500MiB of audio, please read\n  [GitHub's size limit documentation][github-large-files] and consider setting\n  up [large file storage][github-lfs].\n</Warning>\n\n```baml\ntest Test1 {\n  functions [MyFunction]\n  args {\n    myAudio {\n      file \"../path/to/audio.mp3\"\n    }\n  }\n}\n```\n\n<ParamField path=\"file\" type=\"string\" required=\"true\">\n  The path to the audio file, relative to the directory containing the current BAML file.\n\n  audio files must be somewhere in `baml_src/`.\n</ParamField>\n\n<ParamField path=\"media_type\" type=\"string\">\n  The mime-type of the audio. If not set, and the provider expects a mime-type\n  to be provided, BAML will try to infer it based on first, the file extension,\n  and second, the contents of the file.\n</ParamField>\n\n</Tab>\n\n<Tab title=\"URL\">\n```baml\ntest Test1 {\n  functions [MyFunction]\n  args {\n    myAudio {\n      url \"https....\"\n    }\n  }\n}\n```\n\n<ParamField path=\"url\" type=\"string\" required=\"true\">\n  The publicly accessible URL from which the audio may be downloaded.\n</ParamField>\n\n<ParamField path=\"media_type\" type=\"string\">\n  The mime-type of the audio. If not set, and the provider expects a mime-type\n  to be provided, BAML will try to infer it based on the contents of the file.\n</ParamField>\n\n</Tab>\n\n<Tab title=\"Base64\">\n```baml\ntest Test1 {\n  args {\n    myAudio {\n      base64 \"base64string\"\n      media_type \"audio/mp3\"\n    }\n  }\n}\n```\n\n<ParamField path=\"base64\" type=\"string\" required=\"true\">\n  The base64-encoded audio data.\n</ParamField>\n\n<ParamField path=\"media_type\" type=\"string\">\n  The mime-type of the audio. If not set, and the provider expects a mime-type\n  to be provided, BAML will try to infer it based on the contents of the file.\n\n  If `base64` is a data URL, this field will be ignored.\n</ParamField>\n</Tab>\n</Tabs>\n\n## Assertions\n\nThis is coming soon! We'll be supporting assertions in test cases. For now -- when you run a test you'll only see errors parsing the output into the right schema, or LLM-provider errors.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/04-baml-basics/testing-functions.mdx"
      },
      "01-guide/04-baml-basics/streaming.mdx": {
        "markdown": "---\nslug: /guide/baml-basics/streaming\n---\n\nBAML lets you stream in structured JSON output from LLMs as it comes in.\n\nIf you tried streaming in a JSON output from an LLM you'd see something like:\n```\n{\"items\": [{\"name\": \"Appl\n{\"items\": [{\"name\": \"Apple\", \"quantity\": 2, \"price\": 1.\n{\"items\": [{\"name\": \"Apple\", \"quantity\": 2, \"price\": 1.50}], \"total_cost\":\n{\"items\": [{\"name\": \"Apple\", \"quantity\": 2, \"price\": 1.50}], \"total_cost\": 3.00} # Completed\n```\n\nBAML automatically fixes this partial JSON, and transforms all your types into `Partial` types with all `Optional` fields only during the stream.\n\n<Tip>You can check out more examples (including streaming in FastAPI and NextJS) in the [BAML Examples] repo.</Tip>\n\n[call BAML functions]: /docs/calling-baml/calling-functions\n[BAML Examples]: https://github.com/BoundaryML/baml-examples/tree/main\n\nLets stream the output of this function `function ExtractReceiptInfo(email: string) -> ReceiptInfo` for our example:\n\n<Accordion title=\"extract-receipt-info.baml\">\n\n```rust\nclass ReceiptItem {\n  name string\n  description string?\n  quantity int\n  price float\n}\n\nclass ReceiptInfo {\n    items ReceiptItem[]\n    total_cost float?\n}\n\nfunction ExtractReceiptInfo(email: string) -> ReceiptInfo {\n  client GPT4o\n  prompt #\"\n    Given the receipt below:\n\n    {{ email }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n```\n</Accordion>\n\n<Tabs>\n\n<Tab title=\"Python\">\nBAML will generate `b.stream.ExtractReceiptInfo()` for you, which you can use like so:\n\n```python main.py\nimport asyncio\nfrom baml_client import b, partial_types, types\n\n# Using a stream:\ndef example1(receipt: str):\n    stream = b.stream.ExtractReceiptInfo(receipt)\n\n    # partial is a Partial type with all Optional fields\n    for partial in stream:\n        print(f\"partial: parsed {len(partial.items)} items (object: {partial})\")\n\n    # final is the full, original, validated ReceiptInfo type\n    final = stream.get_final_response()\n    print(f\"final: {len(final.items)} items (object: {final})\")\n\n# Using only get_final_response() of a stream\n#\n# In this case, you should just use b.ExtractReceiptInfo(receipt) instead,\n# which is slightly faster and more efficient.\ndef example2(receipt: str):\n    final = b.stream.ExtractReceiptInfo(receipt).get_final_response()\n    print(f\"final: {len(final.items)} items (object: {final})\")\n\n# Using the async client:\nasync def example3(receipt: str):\n    # Note the import of the async client\n    from baml_client.async_client import b\n    stream = b.stream.ExtractReceiptInfo(receipt)\n    async for partial in stream:\n        print(f\"partial: parsed {len(partial.items)} items (object: {partial})\")\n\n    final = await stream.get_final_response()\n    print(f\"final: {len(final.items)} items (object: {final})\")\n\nreceipt = \"\"\"\n04/14/2024 1:05 pm\n\nTicket: 220000082489\nRegister: Shop Counter\nEmployee: Connor\nCustomer: Sam\nItem\t#\tPrice\nGuide leash (1 Pair) uni UNI\n1\t$34.95\nThe Index Town Walls\n1\t$35.00\nBoot Punch\n3\t$60.00\nSubtotal\t$129.95\nTax ($129.95 @ 9%)\t$11.70\nTotal Tax\t$11.70\nTotal\t$141.65\n\"\"\"\n\nif __name__ == '__main__':\n    asyncio.run(example1(receipt))\n    asyncio.run(example2(receipt))\n    asyncio.run(example3(receipt))\n```\n</Tab>\n\n<Tab title=\"TypeScript\">\nBAML will generate `b.stream.ExtractReceiptInfo()` for you, which you can use like so:\n\n```ts main.ts\nimport { b } from './baml_client'\n\n// Using both async iteration and getFinalResponse() from a stream\nconst example1 = async (receipt: string) => {\n  const stream = b.stream.ExtractReceiptInfo(receipt)\n\n  // partial is a Partial type with all Optional fields\n  for await (const partial of stream) {\n    console.log(`partial: ${partial.items?.length} items (object: ${partial})`)\n  }\n\n  // final is the full, original, validated ReceiptInfo type\n  const final = await stream.getFinalResponse()\n  console.log(`final: ${final.items.length} items (object: ${final})`)\n}\n\n// Using only async iteration of a stream\nconst example2 = async (receipt: string) => {\n  for await (const partial of b.stream.ExtractReceiptInfo(receipt)) {\n    console.log(`partial: ${partial.items?.length} items (object: ${partial})`)\n  }\n}\n\n// Using only getFinalResponse() of a stream\n//\n// In this case, you should just use b.ExtractReceiptInfo(receipt) instead,\n// which is faster and more efficient.\nconst example3 = async (receipt: string) => {\n  const final = await b.stream.ExtractReceiptInfo(receipt).getFinalResponse()\n  console.log(`final: ${final.items.length} items (object: ${final})`)\n}\n\nconst receipt = `\n04/14/2024 1:05 pm\n\nTicket: 220000082489\nRegister: Shop Counter\nEmployee: Connor\nCustomer: Sam\nItem\t#\tPrice\nGuide leash (1 Pair) uni UNI\n1\t$34.95\nThe Index Town Walls\n1\t$35.00\nBoot Punch\n3\t$60.00\nSubtotal\t$129.95\nTax ($129.95 @ 9%)\t$11.70\nTotal Tax\t$11.70\nTotal\t$141.65\n`\n\nif (require.main === module) {\n  example1(receipt)\n  example2(receipt)\n  example3(receipt)\n}\n```\n</Tab>\n\n<Tab title=\"Ruby (beta)\">\nBAML will generate `Baml.Client.stream.ExtractReceiptInfo()` for you,\nwhich you can use like so:\n\n```ruby main.rb\nrequire_relative \"baml_client/client\"\n\n$b = Baml.Client\n\n# Using both iteration and get_final_response() from a stream\ndef example1(receipt)\n  stream = $b.stream.ExtractReceiptInfo(receipt)\n\n  stream.each do |partial|\n    puts \"partial: #{partial.items&.length} items\"\n  end\n\n  final = stream.get_final_response\n  puts \"final: #{final.items.length} items\"\nend\n\n# Using only iteration of a stream\ndef example2(receipt)\n  $b.stream.ExtractReceiptInfo(receipt).each do |partial|\n    puts \"partial: #{partial.items&.length} items\"\n  end\nend\n\n# Using only get_final_response() of a stream\n#\n# In this case, you should just use BamlClient.ExtractReceiptInfo(receipt) instead,\n# which is faster and more efficient.\ndef example3(receipt)\n  final = $b.stream.ExtractReceiptInfo(receipt).get_final_response\n  puts \"final: #{final.items.length} items\"\nend\n\nreceipt = <<~RECEIPT\n  04/14/2024 1:05 pm\n\n  Ticket: 220000082489\n  Register: Shop Counter\n  Employee: Connor\n  Customer: Sam\n  Item  #  Price\n  Guide leash (1 Pair) uni UNI\n  1 $34.95\n  The Index Town Walls\n  1 $35.00\n  Boot Punch\n  3 $60.00\n  Subtotal $129.95\n  Tax ($129.95 @ 9%) $11.70\n  Total Tax $11.70\n  Total $141.65\nRECEIPT\n\nif __FILE__ == $0\n  example1(receipt)\n  example2(receipt)\n  example3(receipt)\nend\n```\n\n</Tab>\n<Tab title=\"OpenAPI\">\n\nStreaming is not yet supported via OpenAPI, but it will be coming soon!\n\n</Tab>\n</Tabs>\n\n<Note>\nNumber fields are always streamed in only when the LLM completes them. E.g. if the final number is 129.95, you'll only see null or 129.95 instead of partial numbers like 1, 12, 129.9, etc.\n</Note>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/04-baml-basics/streaming.mdx"
      },
      "01-guide/04-baml-basics/multi-modal.mdx": {
        "markdown": "---\nslug: /guide/baml-basics/multi-modal\n---\n\n## Multi-modal input\n\nYou can use `audio` or `image` input types in BAML prompts. Just create an input argument of that type and render it in the prompt.\n\nCheck the \"raw curl\" checkbox in the playground to see how BAML translates multi-modal input into the LLM Request body.\n\n```baml\n// \"image\" is a reserved keyword so we name the arg \"img\"\nfunction DescribeMedia(img: image) -> string {\n  client openai/gpt-4o\n  // Most LLM providers require images or audio to be sent as \"user\" messages.\n  prompt #\"\n    {{_.role(\"user\")}}\n    Describe this image: {{ img }}\n  \"#\n}\n\n// See the \"testing functions\" Guide for more on testing Multimodal functions\ntest Test {\n  args {\n    img {\n      url \"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\"\n    }\n  }\n}\n```\nSee how to [test images in the playground](/guide/baml-basics/testing-functions#images).\n\n## Calling Multimodal BAML Functions\n\n#### Images\nCalling a BAML function with an `image` input argument type (see [image types](/ref/baml/types#image))\n\nThe `from_url` and `from_base64` methods create an `Image` object based on input type.\n<CodeBlocks>\n```python Python\nfrom baml_py import Image\nfrom baml_client import b\n\nasync def test_image_input():\n  # from URL\n  res = await b.TestImageInput(\n      img=Image.from_url(\n          \"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\"\n      )\n  )\n\n  # Base64 image\n  image_b64 = \"iVBORw0K....\"\n  res = await b.TestImageInput(\n    img=Image.from_base64(\"image/png\", image_b64)\n  )\n```\n\n```typescript TypeScript\nimport { b } from '../baml_client'\nimport { Image } from \"@boundaryml/baml\"\n...\n\n  // URL\n  let res = await b.TestImageInput(\n    Image.fromUrl('https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png'),\n  )\n\n  // Base64\n  const image_b64 = \"iVB0R...\"\n  let res = await b.TestImageInput(\n    Image.fromBase64('image/png', image_b64),\n  )\n  \n```\n\n```ruby Ruby (beta)\nwe're working on it!\n```\n\n</CodeBlocks>\n \n### Audio\nCalling functions that have `audio` types. See [audio types](/ref/baml/types#audio)\n\n<CodeBlocks>\n```python Python\nfrom baml_py import Audio\nfrom baml_client import b\n\nasync def run():\n  # from URL\n  res = await b.TestAudioInput(\n      img=Audio.from_url(\n          \"https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg\"\n      )\n  )\n\n  # Base64\n  b64 = \"iVBORw0K....\"\n  res = await b.TestAudioInput(\n    audio=Audio.from_base64(\"audio/ogg\", b64)\n  )\n```\n\n```typescript TypeScript\nimport { b } from '../baml_client'\nimport { Audio } from \"@boundaryml/baml\"\n...\n\n  // URL\n  let res = await b.TestAudioInput(\n    Audio.fromUrl('https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg'),\n  )\n\n  // Base64\n  const audio_base64 = \"..\"\n  let res = await b.TestAudioInput(\n    Audio.fromBase64('audio/ogg', audio_base64),\n  )\n  \n```\n\n```ruby Ruby (beta)\nwe're working on it!\n```\n</CodeBlocks>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/04-baml-basics/multi-modal.mdx"
      },
      "01-guide/04-baml-basics/error-handling.mdx": {
        "markdown": "\nWhen BAML raises an exception, it will be an instance of a subclass of `BamlError`. This allows you to catch all BAML-specific exceptions with a single `except` block.\n\n## Example\n<CodeGroup>\n```python Python\nfrom baml_client import b\nfrom baml_py.errors import BamlError, BamlInvalidArgumentError, BamlClientError, BamlClientHttpError, BamlValidationError\n\ntry:\n  b.CallFunctionThatRaisesError()\nexcept BamlError as e:\n  print(e)\n\n\ntry:\n  b.CallFunctionThatRaisesError()\nexcept BamlValidationError as e:\n  # The original prompt sent to the LLM\n  print(e.prompt)\n  # The LLM response string\n  print(e.raw_output)\n  # A human-readable error message\n  print(e.message)\n```\n\n\n```typescript TypeScript\nimport { b } from './baml_client'\n// For catching parsing errors, you can import this\nimport { BamlValidationError } from '@boundaryml/baml'\n// The rest of the BAML errors contain a string that is prefixed with:\n// \"BamlError:\"\n// Subclasses are sequentially appended to the string.\n// For example, BamlInvalidArgumentError is returned as:\n// \"BamlError: BamlInvalidArgumentError:\"\n// Or, BamlClientHttpError is returned as:\n// \"BamlError: BamlClientError: BamlClientHttpError:\"\n\n\nasync function example() {\n  try {\n    await b.CallFunctionThatRaisesError()\n  } catch (e) {\n    if (e instanceof BamlValidationError) {\n      // You should be lenient to these fields missing.\n      // The original prompt sent to the LLM\n      console.log(e.prompt)\n      // The LLM response string\n      console.log(e.raw_output)\n      // A human-readable error message\n      console.log(e.message)\n    } else {\n      // Handle other BAML errors\n      console.log(e)\n    }\n  }\n}\n\n```\n\n```ruby Ruby\n# Example coming soon\n```  \n</CodeGroup>\n\n\n## BamlError\n\nBase class for all BAML exceptions.  \n\n<ParamField\n  path=\"message\"\n  type=\"string\"\n>\n  A human-readable error message.\n</ParamField>\n\n### BamlInvalidArgumentError\n\nSubclass of `BamlError`.\n\nRaised when one or multiple arguments to a function are invalid.\n\n### BamlClientError\n\nSubclass of `BamlError`.\n\nRaised when a client fails to return a valid response.\n\n<Warning>\nIn the case of aggregate clients like `fallback` or those with `retry_policy`, only the last client's error is raised.  \n</Warning>\n\n#### BamlClientHttpError\n\nSubclass of `BamlClientError`.\n\nRaised when the HTTP request made by a client fails with a non-200 status code.\n\n<ParamField\n  path=\"status_code\"\n  type=\"int\"\n>\n  The status code of the response.\n\nCommon status codes are:\n\n- 1: Other\n- 2: Other\n- 400: Bad Request\n- 401: Unauthorized\n- 403: Forbidden\n- 404: Not Found\n- 429: Too Many Requests\n- 500: Internal Server Error\n</ParamField>\n\n### BamlValidationError\n\nSubclass of `BamlError`.\n\nRaised when BAML fails to parse a string from the LLM into the specified object.\n\n<ParamField\n  path=\"raw_output\"\n  type=\"string\"\n>\n  The raw text from the LLM that failed to parse into the expected return type of a function.\n</ParamField>\n\n<ParamField\n  path=\"message\"\n  type=\"string\"\n>\n  The parsing-related error message.\n</ParamField>\n\n<ParamField\n  path=\"prompt\"\n  type=\"string\"\n>\n  The original prompt that was sent to the LLM, formatted as a plain string. Images sent as base64-encoded strings are not serialized into this field.\n</ParamField>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/04-baml-basics/error-handling.mdx"
      },
      "01-guide/04-baml-basics/concurrent-calls.mdx": {
        "markdown": "---\ntitle: Concurrent function calls\nslug: /guide/baml-basics/concurrent-calls\n---\n\n\nWe’ll use `function ClassifyMessage(input: string) -> Category` for our example:\n\n<Accordion title=\"classify-message.baml\">\n```baml\nenum Category {\n    Refund\n    CancelOrder\n    TechnicalSupport\n    AccountIssue\n    Question\n}\n\nfunction ClassifyMessage(input: string) -> Category {\n  client GPT4o\n  prompt #\"\n    Classify the following INPUT into ONE\n    of the following categories:\n\n    INPUT: {{ input }}\n\n    {{ ctx.output_format }}\n\n    Response:\n  \"#\n}\n```\n</Accordion>\n\n<Tabs>\n<Tab title=\"Python\">\n\nYou can make concurrent `b.ClassifyMessage()` calls like so:\n\n```python main.py\nimport asyncio\n\nfrom baml_client import b\nfrom baml_client.types import Category\n\nasync def main():\n    await asyncio.gather(\n        b.ClassifyMessage(\"I want to cancel my order\"),\n        b.ClassifyMessage(\"I want a refund\")\n    )\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n</Tab>\n\n<Tab title=\"TypeScript\">\n\nYou can make concurrent `b.ClassifyMessage()` calls like so:\n\n```ts main.ts\nimport { b } from './baml_client'\nimport { Category } from './baml_client/types'\nimport assert from 'assert'\n\nconst main = async () => {\n  const category = await Promise.all(\n    b.ClassifyMessage('I want to cancel my order'),\n    b.ClassifyMessage('I want a refund'),\n  )\n}\n\nif (require.main === module) {\n  main()\n}\n\n```\n</Tab>\n\n<Tab title=\"Ruby (beta)\">\n\nBAML Ruby (beta) does not currently support async/concurrent calls.\n\nPlease [contact us](/contact) if this is something you need.\n\n</Tab>\n</Tabs>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/04-baml-basics/concurrent-calls.mdx"
      },
      "01-guide/05-baml-advanced/client-registry.mdx": {
        "markdown": "---\ntitle: Client Registry\n---\n\nIf you need to modify the model / parameters for an LLM client at runtime, you can modify the `ClientRegistry` for any specified function.\n\n<Tabs>\n\n<Tab title=\"Python\">\n\n```python\nimport os\nfrom baml_py import ClientRegistry\n\nasync def run():\n    cr = ClientRegistry()\n    # Creates a new client\n    cr.add_llm_client(name='MyAmazingClient', provider='openai', options={\n        \"model\": \"gpt-4o\",\n        \"temperature\": 0.7,\n        \"api_key\": os.environ.get('OPENAI_API_KEY')\n    })\n    # Sets MyAmazingClient as the primary client\n    cr.set_primary('MyAmazingClient')\n\n    # ExtractResume will now use MyAmazingClient as the calling client\n    res = await b.ExtractResume(\"...\", { \"client_registry\": cr })\n```\n\n</Tab>\n\n<Tab title=\"TypeScript\">\n```typescript\nimport { ClientRegistry } from '@boundaryml/baml'\n\nasync function run() {\n    const cr = new ClientRegistry()\n    // Creates a new client\n    cr.addLlmClient({ name: 'MyAmazingClient', provider: 'openai', options: {\n        model: \"gpt-4o\",\n        temperature: 0.7,\n        api_key: process.env.OPENAI_API_KEY\n    }})\n    // Sets MyAmazingClient as the primary client\n    cr.setPrimary('MyAmazingClient')\n\n    // ExtractResume will now use MyAmazingClient as the calling client\n    const res = await b.ExtractResume(\"...\", { clientRegistry: cr })\n}\n```\n</Tab>\n\n<Tab title=\"Ruby\">\n\n```ruby\nrequire_relative \"baml_client/client\"\n\ndef run\n  cr = Baml::ClientRegistry.new\n\n  # Creates a new client\n  cr.add_llm_client(\n    name: 'MyAmazingClient',\n    provider: 'openai',\n    options: {\n      model: 'gpt-4o',\n      temperature: 0.7,\n      api_key: ENV['OPENAI_API_KEY']\n    }\n  )\n\n  # Sets MyAmazingClient as the primary client\n  cr.set_primary('MyAmazingClient')\n\n  # ExtractResume will now use MyAmazingClient as the calling client\n  res = Baml.Client.extract_resume(input: '...', baml_options: { client_registry: cr })\nend\n\n# Call the asynchronous function\nrun\n```\n</Tab>\n\n<Tab title=\"OpenAPI\">\n\nThe API supports passing client registry as a field on `__baml_options__` in the request body.\n\nExample request body:\n\n```json\n{\n    \"resume\": \"Vaibhav Gupta\",\n    \"__baml_options__\": {\n        \"client_registry\": {\n            \"clients\": [\n                {\n                    \"name\": \"OpenAI\",\n                    \"provider\": \"openai\",\n                    \"retry_policy\": null,\n                    \"options\": {\n                        \"model\": \"gpt-4o-mini\",\n                        \"api_key\": \"sk-...\"\n                    }\n                }\n            ],\n            \"primary\": \"OpenAI\"\n        }\n    }\n}\n```\n\n```sh\ncurl -X POST http://localhost:2024/call/ExtractResume \\\n    -H 'Content-Type: application/json' -d @body.json\n```\n\n</Tab>\n\n</Tabs>\n\n## ClientRegistry Interface\n\n<Tip>\n    Note: `ClientRegistry` is imported from `baml_py` in Python and `@boundaryml/baml` in TypeScript, not `baml_client`.\n\n    As we mature `ClientRegistry`, we will add a more type-safe and ergonomic interface directly in `baml_client`. See [Github issue #766](https://github.com/BoundaryML/baml/issues/766).\n</Tip>\n\nMethods use `snake_case` in Python and `camelCase` in TypeScript.\n\n### add_llm_client / addLlmClient\nA function to add an LLM client to the registry.\n\n<ParamField\n    path=\"name\"\n    type=\"string\"\n    required\n>\n    The name of the client.\n\n    <Warning>\n    Using the exact same name as a client also defined in .baml files overwrites the existing client whenever the ClientRegistry is used.\n    </Warning>\n</ParamField>\n\n<ParamField path=\"provider\" type=\"string\" required>\nThis configures which provider to use. The provider is responsible for handling the actual API calls to the LLM service. The provider is a required field.\n\nThe configuration modifies the URL request BAML runtime makes.\n\n| Provider Name    | Docs                                                                | Notes                                                      |\n| ---------------- | ------------------------------------------------------------------- | ---------------------------------------------------------- |\n| `anthropic`      | [Anthropic](/docs/snippets/clients/providers/anthropic)             |                                                            |\n| `aws-bedrock`    | [AWS Bedrock](/docs/snippets/clients/providers/aws-bedrock)         |                                                            |\n| `azure-openai`   | [Azure OpenAI](/docs/snippets/clients/providers/azure)              |                                                            |\n| `google-ai`      | [Google AI](/docs/snippets/clients/providers/gemini)                |                                                            |\n| `openai`         | [OpenAI](/docs/snippets/clients/providers/openai)                   |                                                            |\n| `openai-generic` | [OpenAI (generic)](/docs/snippets/clients/providers/openai-generic) | Any model provider that supports an OpenAI-compatible API  |\n| `vertex-ai`      | [Vertex AI](/docs/snippets/clients/providers/vertex)                |                                                            |\n\nWe also have some special providers that allow composing clients together:\n| Provider Name  | Docs                             | Notes                                                      |\n| -------------- | -------------------------------- | ---------------------------------------------------------- |\n| `fallback`     | [Fallback](/docs/snippets/clients/fallback)             | Used to chain models conditional on failures               |\n| `round-robin`  | [Round Robin](/docs/snippets/clients/round-robin)       | Used to load balance                                       |\n\n</ParamField>\n\n<ParamField path=\"options\" type=\"dict[str, Any]\" required>\nThese vary per provider. Please see provider specific documentation for more\ninformation. Generally they are pass through options to the POST request made\nto the LLM.\n</ParamField>\n\n\n\n<ParamField path=\"retry_policy\" type=\"string\">\nThe name of a retry policy that is already defined in a .baml file. See [Retry Policies](/docs/snippets/clients/retry).\n</ParamField>\n\n### set_primary / setPrimary\nThis sets the client for the function to use. (i.e. replaces the `client` property in a function)\n\n<ParamField\n    path=\"name\"\n    type=\"string\"\n    required\n>\n    The name of the client to use.\n\n    This can be a new client that was added with `add_llm_client` or an existing client that is already in a .baml file.\n</ParamField>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/05-baml-advanced/client-registry.mdx"
      },
      "01-guide/05-baml-advanced/dynamic-types.mdx": {
        "markdown": "---\ntitle: Dynamic Types - TypeBuilder\n---\n\nSometimes you have **output schemas that change at runtime** -- for example if\nyou have a list of Categories that you need to classify that come from a\ndatabase, or your schema is user-provided.\n\n`TypeBuilder` is used to create or modify dynamic types at runtime to achieve this.\n\n\n### Dynamic BAML Enums\n\nImagine we want to make a categorizer prompt, but the list of categories to output come from a database.\n1. Add `@@dynamic` to the class or enum definition to mark it as dynamic in BAML.\n\n```rust baml\nenum Category {\n  VALUE1 // normal static enum values that don't change\n  VALUE2\n  @@dynamic // this enum can have more values added at runtime\n} \n\n// The Category enum can now be modified at runtime!\nfunction DynamicCategorizer(input: string) -> Category {\n  client GPT4\n  prompt #\"\n    Given a string, classify it into a category\n    {{ input }}\n     \n    {{ ctx.output_format }}\n  \"#\n}\n\n```\n\n2. Import the `TypeBuilder` from baml_client in your runtime code and modify `Category`. All dynamic types you\ndefine in BAML will be available as properties of `TypeBuilder`. Think of the\ntypebuilder as a registry of modified runtime types that the baml function will\nread from when building the output schema in the prompt.\n\n<Tabs>\n\n<Tab title=\"Python\">\n```python\nfrom baml_client.type_builder import TypeBuilder\nfrom baml_client import b\n\nasync def run():\n  tb = TypeBuilder()\n  tb.Category.add_value('VALUE3')\n  tb.Category.add_value('VALUE4')\n  # Pass the typebuilder in the baml_options argument -- the last argument of the function.\n  res = await b.DynamicCategorizer(\"some input\", { \"tb\": tb })\n  # Now res can be VALUE1, VALUE2, VALUE3, or VALUE4\n  print(res)\n\n```\n</Tab>\n\n<Tab title=\"TypeScript\">\n```typescript\nimport TypeBuilder from '../baml_client/type_builder'\nimport {\n  b\n} from '../baml_client'\n\nasync function run() {\n  const tb = new TypeBuilder()\n  tb.Category.addValue('VALUE3')\n  tb.Category.addValue('VALUE4')\n  const res = await b.DynamicCategorizer(\"some input\", { tb: tb })\n  // Now res can be VALUE1, VALUE2, VALUE3, or VALUE4\n  console.log(res)\n}\n```\n</Tab>\n\n<Tab title=\"Ruby\">\n```ruby\nrequire_relative '../baml_client'\n\ndef run\n  tb = Baml::TypeBuilder.new\n  tb.Category.add_value('VALUE3')\n  tb.Category.add_value('VALUE4')\n  res = Baml.Client.dynamic_categorizer(input: \"some input\", baml_options: {tb: tb})\n  # Now res can be VALUE1, VALUE2, VALUE3, or VALUE4\n  puts res\nend\n```\n</Tab>\n\n<Tab title=\"OpenAPI\">\nDynamic types are not yet supported when used via OpenAPI.\n\nPlease let us know if you want this feature, either via [Discord] or [GitHub][openapi-feedback-github-issue].\n\n[Discord]: https://discord.gg/BTNBeXGuaS\n[openapi-feedback-github-issue]: https://github.com/BoundaryML/baml/issues/892\n</Tab>\n\n</Tabs>\n\n\n\n### Dynamic BAML Classes\nNow we'll add some properties to a `User` class at runtime using @@dynamic.\n\n\n```rust BAML\nclass User {\n  name string\n  age int\n  @@dynamic\n}\n\nfunction DynamicUserCreator(user_info: string) -> User {\n  client GPT4\n  prompt #\"\n    Extract the information from this chunk of text:\n    \"{{ user_info }}\"\n     \n    {{ ctx.output_format }}\n  \"#\n}\n```\n\nWe can then modify the `User` schema at runtime. Since we marked `User` with `@@dynamic`, it'll be available as a property of `TypeBuilder`.\n\n<CodeBlocks>\n\n```python Python\nfrom baml_client.type_builder import TypeBuilder\nfrom baml_client import b\n\nasync def run():\n  tb = TypeBuilder()\n  tb.User.add_property('email', tb.string())\n  tb.User.add_property('address', tb.string()).description(\"The user's address\")\n  res = await b.DynamicUserCreator(\"some user info\", { \"tb\": tb })\n  # Now res can have email and address fields\n  print(res)\n\n```\n\n```typescript TypeScript\nimport TypeBuilder from '../baml_client/type_builder'\nimport {\n  b\n} from '../baml_client'\n\nasync function run() {\n  const tb = new TypeBuilder()\n  tb.User.add_property('email', tb.string())\n  tb.User.add_property('address', tb.string()).description(\"The user's address\")\n  const res = await b.DynamicUserCreator(\"some user info\", { tb: tb })\n  // Now res can have email and address fields\n  console.log(res)\n}\n```\n\n```ruby Ruby\nrequire_relative 'baml_client/client'\n\ndef run\n  tb = Baml::TypeBuilder.new\n  tb.User.add_property('email', tb.string)\n  tb.User.add_property('address', tb.string).description(\"The user's address\")\n  \n  res = Baml.Client.dynamic_user_creator(input: \"some user info\", baml_options: {tb: tb})\n  # Now res can have email and address fields\n  puts res\nend\n```\n</CodeBlocks>\n\n### Creating new dynamic classes or enums not in BAML\nThe previous examples showed how to modify existing types. Here we create a new `Hobbies` enum, and a new class called `Address` without having them defined in BAML.\n\nNote that you must attach the new types to the existing Return Type of your BAML function(in this case it's `User`).\n\n<CodeBlocks>\n\n```python Python\nfrom baml_client.type_builder import TypeBuilder\nfrom baml_client.async_client import b\n\nasync def run():\n  tb = TypeBuilder()\n  hobbies_enum = tb.add_enum(\"Hobbies\")\n  hobbies_enum.add_value(\"Soccer\")\n  hobbies_enum.add_value(\"Reading\")\n\n  address_class = tb.add_class(\"Address\")\n  address_class.add_property(\"street\", tb.string()).description(\"The user's street address\")\n\n  tb.User.add_property(\"hobby\", hobbies_enum.type().optional())\n  tb.User.add_property(\"address\", address_class.type().optional())\n  res = await b.DynamicUserCreator(\"some user info\", {\"tb\": tb})\n  # Now res might have the hobby property, which can be Soccer or Reading\n  print(res)\n\n```\n\n```typescript TypeScript\nimport TypeBuilder from '../baml_client/type_builder'\nimport { b } from '../baml_client'\n\nasync function run() {\n  const tb = new TypeBuilder()\n  const hobbiesEnum = tb.addEnum('Hobbies')\n  hobbiesEnum.addValue('Soccer')\n  hobbiesEnum.addValue('Reading')\n\n  const addressClass = tb.addClass('Address')\n  addressClass.addProperty('street', tb.string()).description(\"The user's street address\")\n\n\n  tb.User.addProperty('hobby', hobbiesEnum.type().optional())\n  tb.User.addProperty('address', addressClass.type())\n  const res = await b.DynamicUserCreator(\"some user info\", { tb: tb })\n  // Now res might have the hobby property, which can be Soccer or Reading\n  console.log(res)\n}\n```\n\n```ruby Ruby\nrequire_relative 'baml_client/client'\n\ndef run\n  tb = Baml::TypeBuilder.new\n  hobbies_enum = tb.add_enum('Hobbies')\n  hobbies_enum.add_value('Soccer')\n  hobbies_enum.add_value('Reading')\n\n  address_class = tb.add_class('Address')\n  address_class.add_property('street', tb.string)\n\n  tb.User.add_property('hobby', hobbies_enum.type.optional)\n  tb.User.add_property('address', address_class.type.optional)\n  \n  res = Baml::Client.dynamic_user_creator(input: \"some user info\", baml_options: { tb: tb })\n  # Now res might have the hobby property, which can be Soccer or Reading\n  puts res\nend\n```\n</CodeBlocks>\n\n\nTypeBuilder provides methods for building different kinds of types:\n\n| Method | Description | Example |\n|--------|-------------|---------|\n| `string()` | Creates a string type | `tb.string()` |\n| `int()` | Creates an integer type | `tb.int()` |\n| `float()` | Creates a float type | `tb.float()` |\n| `bool()` | Creates a boolean type | `tb.bool()` |\n| `list()` | Makes a type into a list | `tb.string().list()` |\n| `optional()` | Makes a type optional | `tb.string().optional()` |\n\n### Adding descriptions to dynamic types\n\n<CodeBlocks>\n\n```python Python\ntb = TypeBuilder()\ntb.User.add_property(\"email\", tb.string()).description(\"The user's email\")\n```\n\n```typescript TypeScript\nconst tb = new TypeBuilder()\ntb.User.addProperty(\"email\", tb.string()).description(\"The user's email\")\n```\n\n```ruby Ruby\ntb = Baml::TypeBuilder.new\ntb.User.add_property(\"email\", tb.string).description(\"The user's email\")\n```\n\n</CodeBlocks>\n\n### Building dynamic types from JSON schema\n\nWe have a working implementation of this, but are waiting for a concrete use case to merge it.\nPlease chime in on [the GitHub issue](https://github.com/BoundaryML/baml/issues/771) if this is\nsomething you'd like to use.\n\n<CodeBlocks>\n\n```python Python\nimport pydantic\nfrom baml_client import b\n\nclass Person(pydantic.BaseModel):\n    last_name: list[str]\n    height: Optional[float] = pydantic.Field(description=\"Height in meters\")\n\ntb = TypeBuilder()\ntb.unstable_features.add_json_schema(Person.model_json_schema())\n\nres = await b.ExtractPeople(\n    \"My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.\",\n    {\"tb\": tb},\n)\n```\n\n```typescript TypeScript\nimport 'z' from zod\nimport 'zodToJsonSchema' from zod-to-json-schema\nimport { b } from '../baml_client'\n\nconst personSchema = z.object({\n  animalLiked: z.object({\n    animal: z.string().describe('The animal mentioned, in singular form.'),\n  }),\n  hobbies: z.enum(['chess', 'sports', 'music', 'reading']).array(),\n  height: z.union([z.string(), z.number().int()]).describe('Height in meters'),\n})\n\nlet tb = new TypeBuilder()\ntb.unstableFeatures.addJsonSchema(zodToJsonSchema(personSchema, 'Person'))\n\nconst res = await b.ExtractPeople(\n  \"My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.\",\n  { tb },\n)\n```\n\n```ruby Ruby\ntb = Baml::TypeBuilder.new\ntb.unstable_features.add_json_schema(...)\n\nres = Baml::Client.extract_people(\n  input: \"My name is Harrison. My hair is black and I'm 6 feet tall. I'm pretty good around the hoop. I like giraffes.\",\n  baml_options: { tb: tb }\n)\n\nputs res\n```\n</CodeBlocks>\n\n### Testing dynamic types in BAML\nThis feature is coming soon! Let us know if you're interested in testing it out!\n\nYou can still write tests in Python, TypeScript, Ruby, etc in the meantime.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/05-baml-advanced/dynamic-types.mdx"
      },
      "01-guide/05-baml-advanced/reusing-prompt-snippets.mdx": {
        "markdown": "---\ntitle: Reusing Prompt Snippets\n---\n\n\nWriting prompts requires a lot of string manipulation. BAML has a `template_string` to let you combine different string templates together. Under-the-hood they use [jinja](/ref/prompt-syntax/what-is-jinja) to evaluate the string and its inputs.\n\n**Template Strings are functions that always return a string.** They can be used to define reusable parts of a prompt, or to make the prompt more readable by breaking it into smaller parts.\n\nExample\n```baml BAML\n// Inject a list of \"system\" or \"user\" messages into the prompt.\n// Note the syntax -- there are no curlies. Just a string block.\ntemplate_string PrintMessages(messages: Message[]) #\"\n  {% for m in messages %}\n    {{ _.role(m.role) }}\n    {{ m.message }}\n  {% endfor %}\n\"#\n\nfunction ClassifyConversation(messages: Message[]) -> Category[] {\n  client GPT4Turbo\n  prompt #\"\n    Classify this conversation:\n    {{ PrintMessages(messages) }}\n\n    Use the following categories:\n    {{ ctx.output_format}}\n  \"#\n}\n```\n\nIn this example we can call the template_string `PrintMessages` to subdivide the prompt into \"user\" or \"system\" messages using `_.role()` (see [message roles](/ref/prompt-syntax/role)). This allows us to reuse the logic for printing messages in multiple prompts. \n\nYou can nest as many template strings inside each other and call them however many times you want.\n\n<Warning>\n  The BAML linter may give you a warning when you use template strings due to a static analysis limitation. You can ignore this warning. If it renders in the playground, you're good!\n</Warning>\nUse the playground preview to ensure your template string is being evaluated correctly!\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/05-baml-advanced/reusing-prompt-snippets.mdx"
      },
      "01-guide/05-baml-advanced/prompt-caching.mdx": {
        "markdown": "---\ntitle: Prompt Caching / Message Role Metadata\n---\n\nRecall that an LLM request usually looks like this, where it sometimes has metadata in each `message`. In this case, Anthropic has a `cache_control` key.\n\n```curl {3,11} Anthropic Request\ncurl https://api.anthropic.com/v1/messages \\\n  -H \"content-type: application/json\" \\\n  -H \"anthropic-beta: prompt-caching-2024-07-31\" \\\n  -d '{\n    \"model\": \"claude-3-5-sonnet-20241022\",\n    \"max_tokens\": 1024,\n    \"messages\": [\n       {\n        \"type\": \"text\", \n        \"text\": \"<the entire contents of Pride and Prejudice>\",\n        \"cache_control\": {\"type\": \"ephemeral\"}\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Analyze the major themes in Pride and Prejudice.\"\n      }\n    ]\n  }'\n```\n\n\nThis is nearly the same as this BAML code, minus the `cache_control` metadata:\n\n\nLet's add the `cache-control` metadata to each of our messges in BAML now.\nThere's just 2 steps:\n\n<Steps>\n### Allow role metadata and header in the client definition\n```baml {5-8} main.baml\nclient<llm> AnthropicClient {\n  provider \"anthropic\"\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    allowed_role_metadata [\"cache_control\"]\n    headers {\n      \"anthropic-beta\" \"prompt-caching-2024-07-31\"\n    }\n  }\n}\n```\n\n### Add the metadata to the messages\n```baml {2,6} main.baml\nfunction AnalyzeBook(book: string) -> string {\n  client<llm> AnthropicClient\n  prompt #\"\n    {{ _.role(\"user\") }}\n    {{ book }}\n    {{ _.role(\"user\", cache_control={\"type\": \"ephemeral\"}) }}\n    Analyze the major themes in Pride and Prejudice.\n  \"#\n}\n```\n\n</Steps>\n\nWe have the \"allowed_role_metadata\" so that if you swap to other LLM clients, we don't accidentally forward the wrong metadata to the new provider API.\n\n\n<Tip>\nRemember to check the \"raw curl\" checkbox in the VSCode Playground to see the exact request being sent!\n</Tip>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/05-baml-advanced/prompt-caching.mdx"
      },
      "01-guide/05-baml-advanced/validations.mdx": {
        "markdown": "\nWith custom type validations, you can set specific rules to ensure your data's\nvalue falls within an acceptable range.\n\nBAML provides two types of validations:\n- **`@assert`** for strict validations. If a type fails an `@assert` validation, it\n  will not be returned in the response. If the failing assertion was part of the\n  top-level type, it will raise an exception. If it's part of a container, it\n  will be removed from the container.\n- **`@check`** for non-exception-raising validations. Whether a `@check` passes or\n  fails, the data will be returned. You can access the results of invidividual\n  checks in the response data.\n\n## Assertions\n\nAssertions are used to guarantee properties about a type or its components in a response.\nThey can be written directly as inline attributes next to the field\ndefinition or on the line following the field definition, or on a top-level type used\nin a function declaration.\n\n### Using `@assert`\n\nBAML will raise an exception if a function returns a `Foo` where `Foo.bar`\nis not between 0 and 10.\n\nIf the function `NextInt8` returns `128`, BAML will raise an exception.\n\n```baml BAML\nclass Foo {\n  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }}) //this = Foo.bar value\n}\n\nfunction NextInt8(a: int) -> int @assert(ok_int8, {{ this >= -128 and this < 127 }}) {\n  client GPT4\n  prompt #\"Return the number after {{ a }}\"#\n}\n```\n\nAsserts may be applied to a whole class via `@@assert`.\n\n```baml BAML\nclass Bar {\n  baz int\n  quux string\n  @@assert(length_limit, {{ this.quux|length < this.baz }})\n}\n```\n\n### Using `@assert` with `Union` Types\n\nNote that when using [`Unions`](/ref/baml/types#union-), it is\ncrucial to specify where the `@assert` attribute is applied within the union\ntype, as it is not known until runtime which type the value will be.\n\n```baml BAML\nclass Foo {\n  bar (int @assert(positive, {{ this > 0 }}) | bool @assert(is_true, {{ this }}))\n}\n```\n\nIn the above example, the `@assert` attribute is applied specifically to the\n`int` and `string` instances of the `Union`, rather than to the `Foo.bar` field\nas a whole.\n\nLikewise, the keyword `this` refers to the value of the type instance it is\ndirectly associated with (e.g., `int` or `string`).\n\n## Chaining Assertions\nYou can have multiple assertions on a single field by chaining multiple `@assert` attributes.\n\nIn this example, the asserts on `bar` and `baz` are equivalent.\n```baml BAML\nclass Foo {\n  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }})\n  baz int @assert(positive, {{ this > 0 }}) @assert(less_than_10, {{ this < 10 }})\n}\n```\n\nChained asserts are evaluated in order from left to right. If the first assert\nfails, the second assert will not be evaluated.\n\n## Writing Assertions\n\nAssertions are represented as Jinja expressions and can be used to validate\nvarious types of data. Possible constraints include checking the length of a\nstring, comparing two values, or verifying the presence of a substring with\nregular expressions.\n\nIn the future, we plan to support shorthand syntax for common assertions to make\nwriting them easier.\n\nFor now, see our [Jinja cookbook / guide](/ref/prompt-syntax/what-is-jinja)\nor the [Minijinja filters docs](https://docs.rs/minijinja/latest/minijinja/filters/index.html#functions)\nfor more information on writing expressions.\n\n\n\n### Expression keywords\n\n- `this` refers to the value of the current field being validated.\n\n\n`this.field` is used to refer to a specific field within the context of `this`.\nAccess nested fields of a data type by chaining the field names together with a `.` as shown below.\n```baml BAML\nclass Resume {\n  name string\n  experience string[]\n\n}\n\nclass Person {\n  resume Resume @assert({{ this.experience|length > 0 }}, \"Nonzero experience\")\n  person_name name\n}\n```\n\n## Assertion Errors\n\nWhen validations fail, your BAML function will raise a `BamlValidationError`\nexception, same as when parsing fails. You can catch this exception and handle\nit as you see fit.\n\nYou can define custom names for each assertion, which will be included\nin the exception for that failure case. If you don't define a custom name,\nBAML will display the body of the assert expression.\n\nIn this example, if the `quote` field is empty, BAML raises a\n`BamlValidationError` with the message **\"exact_citation_not_found\"**. If the\n`website_link` field does not contain **\"https://\",** it raises a\n`BamlValidationError` with the message **invalid_link**.\n\n```baml BAML\nclass Citation {\n  //@assert(<name>, <expr>)\n  quote string @assert(exact_citation_found,\n\t  {{ this|length > 0 }}\n  )\n\n  website_link string @assert(valid_link,\n    {{ this|regex_match(\"https://\") }}\n  )\n}\n```\n\n<CodeBlocks>\n\n```python Python\nfrom baml_client import b\nfrom baml_client.types import Citation\n\ndef main():\n    try:\n        citation: Citation = b.GetCitation(\"SpaceX, is an American spacecraft manufacturer, launch service provider...\")\n\n        # Access the value of the quote field\n        quote = citation.quote\n        website_link = citation.website_link\n        print(f\"Quote: {quote} from {website_link}\")\n        \n    except BamlValidationError as e:\n        print(f\"Validation error: {str(e)}\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")\n\n```\n\n```typescript Typescript\nimport { b, BamlValidationError } from './baml_client';\nimport { Citation } from './baml_client/types';\n\nconst main = () => {\n    try {\n        const citation = b.GetCitation(\"SpaceX, is an American spacecraft manufacturer, launch service provider...\");\n        \n        const quote = citation.quote.value;\n        console.log(`Quote: ${quote}`);\n\n        const checks = citation.quote.checks;\n        console.log(`Check exact_citation_found: ${checks.exact_citation_found.status}`);\n        for (const check of get_checks(checks)) {\n            console.log(`Check ${check.name}: ${check.status}`);\n        }\n\n        const author = citation.author;\n        console.log(`Author: ${author}`);\n    } catch (e) {\n        if (e instanceof BamlValidationError) {\n            console.log(`Validation error: ${e}`);\n        } else {\n            console.error(e);\n        }\n    }\n};\n```\n\n</CodeBlocks>\n\n\n## Checks\n\n`@check` attributes add validations without raising exceptions if they fail.\nTypes with `@check` attributes allow the validations to be inspected at\nruntime.\n\n\n```baml BAML\n( bar int @check(less_than_zero, {{ this < 0 }}) )[]\n```\n\n<CodeBlocks>\n```python Python\nList[Checked[int, Dict[Literal[\"less_than_zero\"]]]]\n```\n\n```typescript Typescript\nChecked<int,\"less_than_zero\">[]\n```\n</CodeBlocks>\n\n\nThe following example uses both `@check` and `@assert`. If `line_number` fails its\n`@assert`, no `Citation` will be returned by `GetCitation()`. However,\n`exact_citation_not_found` can fail without interrupting the result. Because it\nwas a `@check`, client code can inspect the result of the check.\n\n\n```baml BAML\nclass Citation {\n  quote string @check(\n      exact_citation_match,\n\t  {{ this|length > 0 }}\n  )\n  line_number string @assert(\n    has_line_number\n    {{ this|length >= 0 }}\n  )\n}\n\nfunction GetCitation(full_text: string) -> Citation {\n  client GPT4 \n  prompt #\"\n    Generate a citation of the text below in MLA format:\n    {{full_text}}\n\n    {{ctx.output_format}}\n  \"#\n}\n\n```\n\n<CodeBlocks>\n```python Python\nfrom baml_client import b\nfrom baml_client.types import Citation, get_checks\n\ndef main():\n    citation = b.GetCitation(\"SpaceX, is an American spacecraft manufacturer, launch service provider...\")\n\n    # Access the value of the quote field\n    quote = citation.quote.value \n    print(f\"Quote: {quote}\")\n\n    # Access a particular check.\n    quote_match_check = citation.quote.checks['exact_citation_match'].status\n    print(f\"Citation match status: {quote_match_check})\")\n\n    # Access each check and its status.\n    for check in get_checks(citation.quote.checks):\n        print(f\"Check {check.name}: {check.status}\")\n```\n\n```typescript Typescript\nimport { b, get_checks } from './baml_client'\nimport { Citation } from './baml_client/types'\n\nconst main = () => {\n    const citation = b.GetCitation(\"SpaceX, is an American spacecraft manufacturer, launch service provider...\")\n\n    // Access the value of the quote field\n    const quote = citation.quote.value\n    console.log(`Quote: ${quote}`)\n\n    // Access a particular check.\n    const quote_match_check = citation.quote.checks.exact_citation_match.status;\n    console.log(`Exact citation status: ${quote_match_check}`);\n\n    // Access each check and its status.\n    for (const check of get_checks(citation.quote.checks)) {\n        console.log(`Check: ${check.name}, Status: ${check.status}`)\n    }\n}\n```\n\n\n</CodeBlocks>\n\nYou can also chain multiple `@check` and `@assert` attributes on a single field.\n\n```baml BAML\nclass Foo {\n  bar string @check(bar_nonempty, {{ this|length > 0 }})\n  @assert(bar_no_foo, {{ this|contains(\"foo\") }})\n  @check(bar_no_fizzle, {{ this|contains(\"fizzle\") }})\n  @assert(bar_no_baz, {{ this|contains(\"baz\") }})\n}\n```\n\n<Tip> When using `@check`, all checks on the response data are evaluated even if\none fails. In contrast, with `@assert`, a failure will stop the parsing process\nand immediately raise an exception. </Tip>\n\n\n## Advanced Example\n\nThe following example shows more complex minijinja expressions, see the\n[Minijinja filters docs](https://docs.rs/minijinja/latest/minijinja/filters/index.html#functions)\nfor more information on available operators to use in your assertions.\n\n--------\n\nThe `Book` and `Library` classes below demonstrate how to validate a book's\ntitle, author, ISBN, publication year, genres, and a library's name and books.\nThe block-level assertion in the `Library` class ensures that all books have\nunique ISBNs.\n\n```baml BAML\nclass Book {\n    title string @assert(this|length > 0)\n    author string @assert(this|length > 0)\n    isbn string @assert(\n        {{ this|regex_match(\"^(97(8|9))?\\d{9}(\\d|X)$\") }},\n        \"Invalid ISBN format\"\n    )\n    publication_year int @assert(valid_pub_year, {{ 1000 <= this <= 2100 }})\n    genres string[] @assert(valid_length, {{ 1 <= this|length <= 10 }})\n}\n\nclass Library {\n    name string\n    books Book[] @assert(nonempty_books, {{ this|length > 0 }})\n                 @assert(unique_isbn, {{ this|map(attribute='isbn')|unique()|length == this|length }} )\n}\n```\n\nIn this example, we use a block-level `@@assert` to check a dependency across\na pair of fields.\n\n```baml BAML\nclass Person {\n    name string @assert(valid_name, {{ this|length >= 2 }})\n    age int @assert(valid_age, {{ this >= 0 }})\n    address Address\n\n    @@assert(not_usa_minor, {{\n        this.age >= 18 or this.address.country != \"USA\",\n    }})\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/05-baml-advanced/validations.mdx"
      },
      "01-guide/functions/get-started.mdx": {
        "markdown": "---\nslug: guide/cloud/functions/get-started\n---\n\n_Learn how to host your BAML code on Boundary Functions and call it over HTTP._\n\n<Info>\n  This is a preview feature, available starting with `baml-cli v0.66.0`.\n</Info>\n\n<Note>\n  The BAML language, compiler, and runtime will always be 100% free and\n  open-source: we will always allow you to run BAML functions directly in your\n  own backends.\n  \n  Boundary Functions' goal is to make it even easier to host and run BAML\n  functions, by adding support for features like rate limits, telemetry, and\n  end-user feedback.\n</Note>\n\nBoundary Functions allows you to host your BAML functions on our infrastructure, exposing\none REST API endpoint per BAML function.\n\n<div class=\"flex flex-col items-center\">\n  <img src=\"file:3f232775-f97b-449b-a76d-293b065c6307\" alt=\"OpenAPI diagram\" />\n</div>\n\nThis guide will walk you through:\n\n  - creating a Boundary Cloud account,\n  - deploying your BAML code to Boundary Functions,\n  - setting your API keys, and\n  - calling your BAML functions.\n\nOnce you've deployed your BAML functions, you can use the [OpenAPI client] to\ncall them.\n\n[OpenAPI client]: /guide/installation-language/rest-api-other-languages\n\n## Get Started\n\nFirst, create your account and organization at https://dashboard.boundaryml.com.\n\nThen, log in from your terminal:\n\n```bash\nbaml-cli login\n```\n\nand run this command in your `baml_src/` directory:\n\n```bash\nbaml-cli deploy\n```\n\nThis will prompt you to create a new Boundary project, deploy your BAML code to it,\nand then point you to the dashboard, where you can set environment variables and\ncreate API keys to use to call your BAML functions.\n\n<div class=\"flex flex-col items-center\">\n  <img class=\"rounded-md px-4 py-2 m-0\" src=\"file:0ad3106d-8c49-4263-920c-ee1d3b8a2165\" alt=\"Boundary Functions deploy output\" />\n</div>\n\nOnce you've set the environment variables you need (probably `ANTHROPIC_API_KEY`\nand/or `OPENAI_API_KEY`), you can call your BAML functions!\n\nIf you still have the `ExtractResume` function that your BAML project was created with,\nyou can use this command to test it out:\n\n```bash\ncurl https://api2.boundaryml.com/v3/functions/prod/call/ExtractResume \\\n  -H \"Authorization: Bearer $BOUNDARY_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @- << EOF\n{\n  \"resume\": \"\n    Grace Hopper\n    grace.hopper@example.com\n\n    Experience:\n    - Rear Admiral, US Navy\n    - Senior Programmer, Eckert-Mauchly Computer Corporation\n    - Associate Professor, Vassar College\n\n    Skills:\n    - COBOL\n    - Compiler development\n  \"\n}\nEOF\n```\n\nCongratulations! You've gotten your first BAML functions working on Boundary Functions.\n\n## Local development and testing\n\nTo test your BAML functions locally, you can use `baml-cli dev`:\n\n```bash\nANTHROPIC_API_KEY=... OPENAI_API_KEY=... baml-cli dev\n```\n\nwhich will allow you to call your functions at `http://localhost:2024/call/<function_name>` instead of\n`https://api2.boundaryml.com/v3/functions/prod/call/<function_name>` using the exact same `curl` command:\n\n```bash\ncurl http://localhost:2024/functions/prod/call/ExtractResume \\\n  -H \"Authorization: Bearer $BOUNDARY_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @- << EOF\n{\n  \"resume\": \"\n    Grace Hopper\n    grace.hopper@example.com\n\n    Experience:\n    - Rear Admiral, US Navy\n    - Senior Programmer, Eckert-Mauchly Computer Corporation\n    - Associate Professor, Vassar College\n\n    Skills:\n    - COBOL\n    - Compiler development\n  \"\n}\nEOF\n```\n\n<div class=\"flex flex-col items-center\">\n  <img class=\"rounded-md px-4 py-2 m-0\" src=\"file:0bcebfbb-1507-45ee-aa10-05bbe8964f40\" alt=\"Boundary Functions local output\" />\n</div>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/functions/get-started.mdx"
      },
      "01-guide/functions/using-openapi.mdx": {
        "markdown": "---\nslug: guide/cloud/functions/using-openapi\n---\n\n_Learn how to use your OpenAPI client to call your functions in Boundary Functions._\n\n<Info>\n  This page assumes you've already deployed your BAML code to Boundary Functions. If\n  you haven't done that yet, check out the guide for [getting started](/guide/cloud/functions/get-started).\n</Info>\n\n<Info>\n  This page assumes you're using an OpenAPI-generated BAML client. If you\n  haven't done that yet, check out the [OpenAPI quickstart](/docs/get-started/quickstart/openapi).\n</Info>\n\n## Create an API key\n\nYou can create API keys in the [Boundary\nDashboard](https://dashboard.boundaryml.com/) by going to the left sidebar and\nclicking on the key icon.\n\n<div class=\"flex flex-col items-center\">\n  <img src=\"file:e6c19acc-fae1-40bb-9c5a-0901494bf5cd\" alt=\"Boundary Functions API keys\" />\n</div>\n\nOnce you've created a new key, update your application code to use it as `BOUNDARY_API_KEY`.\n\n## Update your application code\n\nYou also need to update your application code to use `BOUNDARY_ENDPOINT` and\n`BOUNDARY_API_KEY`, if set, when constructing the OpenAPI client.\n\n<Tabs>\n\n<Tab title=\"Go\">\n\n```go\nimport (\n    \"os\"\n    baml \"my-golang-app/baml_client\"\n)\n\nfunc main() {\n    cfg := baml.NewConfiguration()\n    if boundaryEndpoint := os.Getenv(\"BOUNDARY_ENDPOINT\"); boundaryEndpoint != \"\" {\n        cfg.BasePath = boundaryEndpoint\n    }\n    if boundaryApiKey := os.Getenv(\"BOUNDARY_API_KEY\"); boundaryApiKey != \"\" {\n        cfg.DefaultHeader[\"Authorization\"] = \"Bearer \" + boundaryApiKey\n    }\n    b := baml.NewAPIClient(cfg).DefaultAPI\n    // Use `b` to make API calls\n}\n```\n\n</Tab>\n\n<Tab title=\"Java\">\n```java\nimport com.boundaryml.baml_client.ApiClient;\nimport com.boundaryml.baml_client.ApiException;\nimport com.boundaryml.baml_client.Configuration;\nimport com.boundaryml.baml_client.api.DefaultApi;\nimport com.boundaryml.baml_client.auth.*;\n\npublic class ApiExample {\n    public static void main(String[] args) {\n        ApiClient apiClient = Configuration.getDefaultApiClient();\n\n        String boundaryEndpoint = System.getenv(\"BOUNDARY_ENDPOINT\");\n        if (boundaryEndpoint != null && !boundaryEndpoint.isEmpty()) {\n            apiClient.setBasePath(boundaryEndpoint);\n        }\n\n        String boundaryApiKey = System.getenv(\"BOUNDARY_API_KEY\");\n        if (boundaryApiKey != null && !boundaryApiKey.isEmpty()) {\n            apiClient.addDefaultHeader(\"Authorization\", \"Bearer \" + boundaryApiKey);\n        }\n\n        DefaultApi apiInstance = new DefaultApi(apiClient);\n        // Use `apiInstance` to make API calls\n    }\n}\n```\n\n</Tab>\n\n<Tab title=\"PHP\">\n\n```php\nrequire_once(__DIR__ . '/vendor/autoload.php');\n\n$config = BamlClient\\Configuration::getDefaultConfiguration();\n\n$boundaryEndpoint = getenv('BOUNDARY_ENDPOINT');\n$boundaryApiKey = getenv('BOUNDARY_API_KEY');\n\nif ($boundaryEndpoint) {\n    $config->setHost($boundaryEndpoint);\n}\n\nif ($boundaryApiKey) {\n    $config->setAccessToken($boundaryApiKey);\n}\n\n$apiInstance = new OpenAPI\\Client\\Api\\DefaultApi(\n    new GuzzleHttp\\Client(),\n    $config\n);\n\n// Use `$apiInstance` to make API calls\n```\n\n</Tab>\n\n<Tab title=\"Ruby\">\n```ruby\nrequire 'baml_client'\n\napi_client = BamlClient::ApiClient.new\n\nboundary_endpoint = ENV['BOUNDARY_ENDPOINT']\nif boundary_endpoint\n  api_client.host = boundary_endpoint\nend\n\nboundary_api_key = ENV['BOUNDARY_API_KEY']\nif boundary_api_key\n  api_client.default_headers['Authorization'] = \"Bearer #{boundary_api_key}\"\nend\nb = BamlClient::DefaultApi.new(api_client)\n# Use `b` to make API calls\n```\n</Tab>\n\n<Tab title=\"Rust\">\n```rust\nlet mut config = baml_client::apis::configuration::Configuration::default();\nif let Some(base_path) = std::env::var(\"BOUNDARY_ENDPOINT\").ok() {\n    config.base_path = base_path;\n}\nif let Some(api_key) = std::env::var(\"BOUNDARY_API_KEY\").ok() {\n    config.bearer_access_token = Some(api_key);\n}\n// Use `config` to make API calls\n```\n</Tab>\n\n</Tabs>\n\n\n## Set your environment variables\n\nYou can now set the following environment variables in your application:\n\n```bash\nBOUNDARY_API_KEY=...\nBOUNDARY_ENDPOINT=https://api2.boundaryml.com/v3/functions/prod/\n```\n\n## Call your functions\n\nYou should now be able to call your deployed BAML functions using your OpenAPI client!\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/functions/using-openapi.mdx"
      },
      "01-guide/functions/environment-variables.mdx": {
        "markdown": "---\nslug: guide/cloud/functions/environment-variables\n---\n\n_Learn how to use Boundary Functions Environment Variables, which are key-value pairs\nconfigured outside your source code._\n\nEnvironment variables are key-value pairs, configured outside your source code,\nand used to provide secrets for your deployed BAML functions, such as\n`ANTHROPIC_API_KEY` and `OPENAI_API_KEY`.\n\nYou can set environment variables in the [Boundary\nDashboard](https://dashboard.boundaryml.com/) by going to the left sidebar and\nclicking on the cloud icon.\n\n<div class=\"flex flex-col items-center\">\n  <img src=\"file:e2159ce8-f03f-4fda-9d91-92cd7cc867cf\" alt=\"Boundary Cloud secrets\" />\n</div>\n\nChanges to environment variables will take effect immediately.\n\n## Limits\n\nSee [Limits](/ref/cloud/limits) for more information.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/functions/environment-variables.mdx"
      },
      "01-guide/07-observability/studio.mdx": {
        "markdown": "---\ntitle: Boundary Studio\n---\n\n<Tip>\nFor the remaining of 2024, Boundary Studio is free for new accounts!\n\nBoundary Studio 2 will be released in 2025 with a new pricing model.\n</Tip>\n\nTo enable observability with BAML, you'll first need to sign up for a [Boundary Studio](https://app.boundaryml.com) account. \n\nOnce you've signed up, you'll be able to create a new project and get your project token.\n\nThen simply add the following environment variables prior to running your application:\n\n```bash\nexport BOUNDARY_PROJECT_ID=project_uuid\nexport BOUNDARY_SECRET=your_token\n```\n\nThere you'll be able to see all the metrics and logs from your application including:\n\n- Cost\n- Function calls\n- Execution time\n- Token Usage\n- Prompt Logs\n- and more...\n\n## Tracing Custom Events\n\n\nBAML allows you to trace any function with the **@trace** decorator.\nThis will make the function's input and output show up in the Boundary dashboard. This works for any python function you define yourself. BAML LLM functions (or any other function declared in a .baml file) are already traced by default. Logs are only sent to the Dashboard if you setup your environment variables correctly.\n\n### Example\n\nIn the example below, we trace each of the two functions `pre_process_text` and `full_analysis`:\n\n<CodeGroup>\n```python Python\nfrom baml_client import baml\nfrom baml_client.types import Book, AuthorInfo\nfrom baml_client.tracing import trace\n\n# You can also add a custom name with trace(name=\"my_custom_name\")\n# By default, we use the function's name.\n@trace\ndef pre_process_text(text):\n    return text.replace(\"\\n\", \" \")\n\n\n@trace\nasync def full_analysis(book: Book):\n    sentiment = await baml.ClassifySentiment(\n        pre_process_text(book.content)\n    )\n    book_analysis = await baml.AnalyzeBook(book)\n    return book_analysis\n\n\n@trace\nasync def test_book1():\n    content = \"\"\"Before I could reply that he [Gatsby] was my neighbor...\n    \"\"\"\n    processed_content = pre_process_text(content)\n    return await full_analysis(\n        Book(\n            title=\"The Great Gatsby\",\n            author=AuthorInfo(firstName=\"F. Scott\", lastName=\"Fitzgerald\"),\n            content=processed_content,\n        ),\n    )\n```\n\n```typescript TypeScript\nimport { baml } from 'baml_client';\nimport { Book, AuthorInfo } from 'baml_client/types';\nimport { traceSync, traceAsync } from 'baml_client/tracing';\n\nconst preProcessText = traceSync(function(text: string): Promise<string> {\n    return text.replace(/\\n/g, \" \");\n});\n\nconst fullAnalysis = traceAsync(async function(book: Book): Promise<any> {\n    const sentiment = await baml.ClassifySentiment(\n        preProcessText(book.content)\n    );\n    const bookAnalysis = await baml.AnalyzeBook(book);\n    return bookAnalysis;\n});\n\nconst testBook1 = traceAsync(async function(): Promise<any> {\n    const content = `Before I could reply that he [Gatsby] was my neighbor...`;\n    const processedContent = preProcessText(content);\n    return await fullAnalysis(\n        new Book(\n            \"The Great Gatsby\",\n            new AuthorInfo(\"F. Scott\", \"Fitzgerald\"),\n            processedContent\n        )\n    );\n});\n```\n\n```text Ruby\nTracing non-baml functions is not yet supported in Ruby.\n```\n\n\n```text REST (OpenAPI)\nTracing non-baml functions is not yet supported in REST (OpenAPI).\n```\n</CodeGroup>\n\n\nThis allows us to see each function invocation, as well as all its children in the dashboard:\n\n<img src=\"file:b598692d-ded9-4b4b-b3a2-e61c2f98c230\" width=\"auto\" />\n\nSee [running tests](/running-tests) for more information on how to run this test.\n\n### Adding custom tags\n\nThe dashboard view allows you to see custom tags for each of the function calls. This is useful for adding metadata to your traces and allow you to query your generated logs more easily.\n\nTo add a custom tag, you can import **set_tags(..)** as below:\n\n```python\nfrom baml_client.tracing import set_tags, trace\nimport typing\n\n@trace\nasync def pre_process_text(text):\n    set_tags(userId=\"1234\")\n\n    # You can also create a dictionary and pass it in\n    tags_dict: typing.Dict[str, str] = {\"userId\": \"1234\"}\n    set_tags(**tags_dict) # \"**\" unpacks the dictionary\n    return text.replace(\"\\n\", \" \")\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/07-observability/studio.mdx"
      },
      "01-guide/09-comparisons/marvin.mdx": {
        "markdown": "---\ntitle: Comparing Marvin\n---\n\n\n[Marvin](https://github.com/PrefectHQ/marvin) lets developers do extraction or classification tasks in Python as shown below (TypeScript is not supported):\n\n\n```python\nimport pydantic\n\nclass Location(pydantic.BaseModel):\n    city: str\n    state: str\n\nmarvin.extract(\"I moved from NY to CHI\", target=Location)\n```\n\nYou can also provide instructions:\n```python\nmarvin.extract(\n    \"I paid $10 for 3 tacos and got a dollar and 25 cents back.\",\n    target=float,\n    instructions=\"Only extract money\"\n)\n\n#  [10.0, 1.25]\n```\nor using enums to classify\n```python\nfrom enum import Enum\nimport marvin\n\nclass RequestType(Enum):\n    SUPPORT = \"support request\"\n    ACCOUNT = \"account issue\"\n    INQUIRY = \"general inquiry\"\n\nrequest = marvin.classify(\"Reset my password\", RequestType)\nassert request == RequestType.ACCOUNT\n```\n\n\nFor enum classification, you can add more instructions to each enum, but then you don't get fully typed outputs, nor can reuse the enum in your own code. You're back to working with raw strings.\n\n```python\n# Classifying a task based on project specifications\nproject_specs = {\n    \"Frontend\": \"Tasks involving UI design, CSS, and JavaScript.\",\n    \"Backend\": \"Tasks related to server, database, and application logic.\",\n    \"DevOps\": \"Tasks involving deployment, CI/CD, and server maintenance.\"\n}\n\ntask_description = \"Set up the server for the new application.\"\n\ntask_category = marvin.classify(\n    task_description,\n    labels=list(project_specs.keys()),\n    instructions=\"Match the task to the project category based on the provided specifications.\"\n)\nassert task_category == \"Backend\"\n```\n\nMarvin has some inherent limitations for example:\n1. How to use a different model?\n2. What is the full prompt? Where does it live? What if I want to change it because it doesn't work well for my use-case? How many tokens is it?\n3. How do I test this function?\n4. How do I visualize results over time in production?\n\n\n### Using BAML\nHere is the BAML equivalent of this classification task based off the prompt Marvin uses under-the-hood. Note how the prompt becomes transparent to you using BAML. You can easily make it more complex or simpler depending on the model.\n\n```baml\nenum RequestType {\n  SUPPORT @alias(\"support request\")\n  ACCOUNT @alias(\"account issue\") @description(\"A detailed description\")\n  INQUIRY @alias(\"general inquiry\")\n}\n\nfunction ClassifyRequest(input: string) -> RequestType {\n  client GPT4 // choose even open source models\n  prompt #\"\n    You are an expert classifier that always maintains as much semantic meaning\n    as possible when labeling text. Classify the provided data,\n    text, or information as one of the provided labels:\n\n    TEXT:\n    ---\n    {{ input }}\n    ---\n\n    {{ ctx.output_format }}\n\n    The best label for the text is:\n  \"#\n}\n```\nAnd you can call this function in your code\n```python\nfrom baml_client import baml as b\n\n...\nrequestType = await b.ClassifyRequest(\"Reset my password\")\n# fully typed output\nassert requestType == RequestType.ACCOUNT\n```\n\nThe prompt string may be more wordy, but with BAML you now have\n1. Fully typed responses, guaranteed\n1. Full transparency and flexibility of the prompt string\n1. Full freedom for what model to use\n1. Helper functions to manipulate types in prompts (print_enum)\n1. Testing capabilities using the VSCode playground\n1. Analytics in the Boundary Dashboard\n1. Support for TypeScript\n1. A better understanding of how prompt engineering works\n\n\nMarvin was a big source of inspiration for us -- their approach is simple and elegant. We recommend checking out Marvin if you're just starting out with prompt engineering or want to do a one-off simple task in Python. But if you'd like a whole added set of features, we'd love for you to give BAML a try and let us know what you think.\n\n### Limitations of BAML\n\nBAML does have some limitations we are continuously working on. Here are a few of them:\n1. It is a new language. However, it is fully open source and getting started takes less than 10 minutes. We are on-call 24/7 to help with any issues (and even provide prompt engineering tips)\n1. Developing requires VSCode. You _could_ use vim and we have workarounds but we don't recommend it.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/09-comparisons/marvin.mdx"
      },
      "01-guide/09-comparisons/pydantic.mdx": {
        "markdown": "---\ntitle: Comparing Pydantic\n---\n\nPydantic is a popular library for data validation in Python used by most -- if not all -- LLM frameworks, like [instructor](https://github.com/jxnl/instructor/tree/main).\n\nBAML also uses Pydantic. The BAML Rust compiler can generate Pydantic models from your `.baml` files. But that's not all the compiler does -- it also takes care of fixing common LLM parsing issues, supports more data types, handles retries, and reduces the amount of boilerplate code you have to write.\n\nLet's dive into how Pydantic is used and its limitations.\n\n### Why working with LLMs requires more than just Pydantic\n\nPydantic can help you get structured output from an LLM easily at first glance:\n```python\nclass Resume(BaseModel):\n    name: str\n    skills: List[str]\n\ndef create_prompt(input_text: str) -> str:\n    PROMPT_TEMPLATE = f\"\"\"Parse the following resume and return a structured representation of the data in the schema below.\nResume:\n---\n{input_text}\n---\n\nSchema:\n{Resume.model_json_schema()['properties']}\n\nOutput JSON:\n\"\"\"\n    return PROMPT_TEMPLATE\n\ndef extract_resume(input_text: str) -> Union[Resume, None]:\n    prompt = create_prompt(input_text)\n    chat_completion = client.chat.completions.create(\n        model=\"gpt-4\", messages=[{\"role\": \"system\", \"content\": prompt}]\n    )\n    try:\n        output = chat_completion.choices[0].message.content\n        if output:\n            return Resume.model_validate_json(output)\n        return None\n    except Exception as e:\n        raise e\n```\n\nThat's pretty good, but now we want to add an `Education` model to the `Resume` model. We add the following code:\n\n```diff\n...\n+class Education(BaseModel):\n+    school: str\n+    degree: str\n+    year: int\n\nclass Resume(BaseModel):\n    name: str\n    skills: List[str]\n+   education: List[Education]\n\ndef create_prompt(input_text: str) -> str:\n    additional_models = \"\"\n+    if \"$defs\" in Resume.model_json_schema():\n+        additional_models += f\"\\nUse these other schema definitions as +well:\\n{Resume.model_json_schema()['$defs']}\"\n    PROMPT_TEMPLATE = f\"\"\"Parse the following resume and return a structured representation of the data in the schema below.\nResume:\n---\n{input_text}\n---\n\nSchema:\n{Resume.model_json_schema()['properties']}\n\n+ {additional_models}\n\nOutput JSON:\n\"\"\".strip()\n    return PROMPT_TEMPLATE\n...\n```\nA little ugly, but still readable... But managing all these prompt strings can make your codebase disorganized very quickly.\n\nThen you realize the LLM sometimes outputs some text before giving you the json, like this:\n\n```diff\n+ The output is:\n{\n  \"name\": \"John Doe\",\n  ... // truncated for brevity\n}\n```\n\nSo you add a regex to address that that extracts everything in `{}`:\n\n```diff\ndef extract_resume(input_text: str) -> Union[Resume, None]:\n    prompt = create_prompt(input_text)\n    print(prompt)\n    chat_completion = client.chat.completions.create(\n        model=\"gpt-4\", messages=[{\"role\": \"system\", \"content\": prompt}]\n    )\n    try:\n        output = chat_completion.choices[0].message.content\n        print(output)\n        if output:\n+            # Extract JSON block using regex\n+            json_match = re.search(r\"\\{.*?\\}\", output, re.DOTALL)\n+            if json_match:\n+                json_output = json_match.group(0)\n                return Resume.model_validate_json(output)\n        return None\n    except Exception as e:\n        raise e\n```\n\nNext you realize you actually want an array of `Resumes`, but you can't really use `List[Resume]` because Pydantic and Python don't work this way, so you have to add another wrapper:\n\n```diff\n+class ResumeArray(BaseModel):\n+    resumes: List[Resume]\n```\nNow you need to change the rest of your code to handle different models. That's good longterm, but it is now more boilerplate you have to write, test and maintain. \n\nNext, you notice the LLM sometimes outputs a single resume `{...}`, and sometimes an array `[{...}]`...\nYou must now change your parser to handle both cases:\n\n```diff\n+def extract_resume(input_text: str) -> Union[List[Resume], None]:\n+    prompt = create_prompt(input_text) # Also requires changes\n    chat_completion = client.chat.completions.create(\n        model=\"gpt-4\", messages=[{\"role\": \"system\", \"content\": prompt}]\n    )\n    try:\n        output = chat_completion.choices[0].message.content\n        if output:\n            # Extract JSON block using regex\n            json_match = re.search(r\"\\{.*?\\}\", output, re.DOTALL)\n            if json_match:\n                json_output = json_match.group(0)\n                try:\n+                  parsed = json.loads(json_output)\n+                  if isinstance(parsed, list):\n+                      return list(map(Resume.model_validate_json, parsed))\n+                  else:\n+                      return [ResumeArray(**parsed)]\n        return None\n    except Exception as e:\n        raise e\n```\nYou could retry the call against the LLM to fix the issue, but that will cost you precious seconds and tokens, so handling this corner case manually is the only solution.\n\n\n\n\n\n--- \n## A small tangent -- JSON schemas vs type definitions\nSidenote: At this point your prompt looks like this:\n\n```\nJSON Schema:\n{'name': {'title': 'Name', 'type': 'string'}, 'skills': {'items': {'type': 'string'}, 'title': 'Skills', 'type': 'array'}, 'education': {'anyOf': [{'$ref': '#/$defs/Education'}, {'type': 'null'}]}}\n\n\nUse these other JSON schema definitions as well:\n{'Education': {'properties': {'degree': {'title': 'Degree', 'type': 'string'}, 'major': {'title': 'Major', 'type': 'string'}, 'school': {'title': 'School', 'type': 'string'}, 'year': {'title': 'Year', 'type': 'integer'}}, 'required': ['degree', 'major', 'school', 'year'], 'title': 'Education', 'type': 'object'}}\n```\n\nand sometimes even GPT-4 outputs incorrect stuff like this, even though it's technically correct JSON (OpenAI's \"JSON mode\" will still break you)\n```\n{\n  \"name\": \n  {\n    \"title\": \"Name\", \n    \"type\": \"string\", \n    \"value\": \"John Doe\"\n  }, \n  \"skills\": \n  {\n    \"items\": \n    {\n      \"type\": \"string\", \n      \"values\": \n      [\n        \"Python\", \n        \"JavaScript\", \n        \"React\"\n      ]\n    ... // truncated for brevity\n```\n(this is an actual result from GPT-4 before some more prompt engineering)\n\nwhen all you really want is a prompt that looks like the one below -- with way less tokens (and less likelihood of confusion). :\n```diff\nParse the following resume and return a structured representation of the data in the schema below.\nResume:\n---\nJohn Doe\nPython, Rust\nUniversity of California, Berkeley, B.S. in Computer Science, 2020\n---\n\n+JSON Schema:\n+{\n+  \"name\": string,\n+  \"skills\": string[]\n+  \"education\": {\n+    \"school\": string,\n+    \"degree\": string,\n+    \"year\": integer\n+  }[]\n+}\n\nOutput JSON:\n```\nAhh, much better. **That's 80% less tokens** with a simpler prompt, for the same results. (See also Microsoft's [TypeChat](https://microsoft.github.io/TypeChat/docs/introduction/) which uses a similar schema format using typescript types)\n\n---  \nBut we digress, let's get back to the point. You can see how this can get out of hand quickly, and how Pydantic wasn't really made with LLMs in mind.  We haven't gotten around to adding resilience like **retries, or falling back to a different model in the event of an outage**. There's still a lot of wrapper code to write.\n\n### Pydantic and Enums\nThere are other core limitations.\nSay you want to do a classification task using Pydantic. An Enum is a great fit for modelling this.\n\nAssume this is our prompt:\n```text\nClassify the company described in this text into the best\nof the following categories:\n\nText:\n---\n{some_text}\n---\n\nCategories:\n- Technology: Companies involved in the development and production of technology products or services\n- Healthcare: Includes companies in pharmaceuticals, biotechnology, medical devices.\n- Real estate: Includes real estate investment trusts (REITs) and companies involved in real estate development.\n\nThe best category is:\n```\n\nSince we have descriptions, we need to generate a custom enum we can use to build the prompt:\n\n```python\nclass FinancialCategory(Enum):\n    technology = (\n        \"Technology\",\n        \"Companies involved in the development and production of technology products or services.\",\n    )\n    ...\n    real_estate = (\n        \"Real Estate\",\n        \"Includes real estate investment trusts (REITs) and companies involved in real estate development.\",\n    )\n\n    def __init__(self, category, description):\n        self._category = category\n        self._description = description\n\n    @property\n    def category(self):\n        return self._category\n\n    @property\n    def description(self):\n        return self._description\n\n```\nWe add a class method to load the right enum from the LLM output string:\n```python\n    @classmethod\n    def from_string(cls, category: str) -> \"FinancialCategory\":\n        for c in cls:\n            if c.category == category:\n                return c\n        raise ValueError(f\"Invalid category: {category}\")\n```\nUpdate the prompt to use the enum descriptions:\n```python\ndef print_categories_and_descriptions():\n    for category in FinancialCategory:\n        print(f\"{category.category}: {category.description}\")\n\ndef create_prompt(text: str) -> str:\n    additional_models = \"\"\n    print_categories_and_descriptions()\n    PROMPT_TEMPLATE = f\"\"\"Classify the company described in this text into the best\nof the following categories:\n\nText:\n---\n{text}\n---\n\nCategories:\n{print_categories_and_descriptions()}\n\nThe best category is:\n\"\"\"\n    return PROMPT_TEMPLATE\n```\nAnd then we use it in our AI function:\n```python\ndef classify_company(text: str) -> FinancialCategory:\n    prompt = create_prompt(text)\n    chat_completion = client.chat.completions.create(\n        model=\"gpt-4\", messages=[{\"role\": \"system\", \"content\": prompt}]\n    )\n    try:\n        output = chat_completion.choices[0].message.content\n        if output:\n            # Use our helper function!\n            return FinancialCategory.from_string(output)\n        return None\n    except Exception as e:\n        raise e\n```\n\nWhat gets hairy is if you want to change your types. \n- What if you want the LLM to return an object instead? You have to change your enum, your prompt, AND your parser.\n- What if you want to handle cases where the LLM outputs \"Real Estate\" or \"real estate\"? \n- What if you want to save the enum information in a database? `str(category)` will save `FinancialCategory.healthcare` into your DB, but your parser only recognizes \"Healthcare\", so you'll need more boilerplate if you ever want to programmatically analyze your data.\n\n\n### Alternatives\nThere are libraries like [instructor](https://github.com/jxnl/instructor/tree/main) do provide a great amount of boilerplate but you're still:\n\n1. Using prompts that you cannot control. E.g. [a commit may change your results underneath you](https://github.com/jxnl/instructor/commit/1b6d8253c0f7dfdaa6cb1dbdbd37684d192ddecf). \n1. Using more tokens than you may need to to declare schemas (higher costs and latencies)\n1. **There are no included testing capabilities.**. Developers have to copy-paste JSON blobs everywhere, potentially between their IDEs and other websites. Existing LLM Playgrounds were not made with structured data in mind.\n1. Lack of observability. No automatic tracing of requests.\n\n## Enter BAML\nThe Boundary toolkit helps you iterate seamlessly compared to Pydantic.\n\nHere's all the BAML code you need to solve the Extract Resume problem from earlier (VSCode prompt preview is shown on the right):\n\n<img src=\"file:7340ce64-d200-46e9-8089-eb49f9338588\" />\n\n<Note>\nHere we use a \"GPT4\" client, but you can use any model. See [client docs](/docs/syntax/client/client)\n</Note>\n{/* \n```baml\n\n\nclass Education {\n  school string\n  degree string\n  year int\n}\n\nclass Resume {\n  name string\n  skills string[]\n  education Education[]\n}\n\nfunction ExtractResume(resume_text: string) -> Resume {\n  client GPT4\n  prompt #\"\n    Parse the following resume and return a structured representation of the data in the schema below.\n\n    Resume:\n    ---\n    {{ input.resume_text }}\n    ---\n\n    Output in this JSON format:\n    {{ ctx.output_format }}\n\n    Output JSON:\n  \"#\n}\n``` */}\nThe BAML compiler generates a python client that imports and calls the function:\n```python\nfrom baml_client import baml as b\n\nasync def main():\n  resume = await b.ExtractResume(resume_text=\"\"\"John Doe\nPython, Rust\nUniversity of California, Berkeley, B.S. in Computer Science, 2020\"\"\")\n\n  assert resume.name == \"John Doe\"\n```\nThat's it! No need to write any more code. Since the compiler knows what your function signature is we literally generate a custom deserializer for your own unique usecase that _just works_.\n\n\n\n\nConverting the `Resume` into an array of resumes requires a single line change in BAML (vs having to create array wrapper classes and parsing logic). \n\nIn this image we change the types and BAML automatically updates the prompt, parser, and the Python types you get back.\n\n<img src=\"file:a9cd259f-3f20-4faa-b421-b3858d948384\" />\n\n\n\nAdding retries or resilience requires just [a couple of modifications](/docs/syntax/client/retry). And best of all, **you can test things instantly, without leaving your VSCode**.\n\n### Conclusion\nWe built BAML because writing a Python library was just not powerful enough to do everything we envisioned, as we have just explored.\n\nCheck out the [Hello World](/docs/guides/hello_world/level0) tutorial to get started.\n\nOur mission is to make the best DX for AI engineers working with LLMs. Contact us at founders@boundaryml.com or [Join us on Discord](https://discord.gg/BTNBeXGuaS) to stay in touch with the community and influence the roadmap.\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/09-comparisons/pydantic.mdx"
      },
      "01-guide/contact.mdx": {
        "markdown": "\nWe have seen many different prompts for many use-cases. We'd love to hear about your prompt and how you use BAML.\n\nContact Us at [contact@boundaryml.com](mailto:contact@boundaryml.com)\n\nor join our [Discord](https://discord.gg/BTNBeXGuaS)\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/contact.mdx"
      },
      "02-examples/interactive-examples.mdx": {
        "markdown": "---\ntitle: Interactive Examples\n---\n\nCheck out the [live examples](https://baml-examples.vercel.app/) that use NextJS, and the [source code on Github](https://github.com/boundaryml/baml-examples).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/02-examples/interactive-examples.mdx"
      },
      "01-guide/06-prompt-engineering/hallucinations.mdx": {
        "markdown": "---\ntitle: Reduce Hallucinations\n---\n\nWe recommend these simple ways to reduce hallucinations:\n\n\n### 1. Set temperature to 0.0 (especially if extracting data verbatim)\nThis will make the model less creative and more likely to just extract the data that you want verbatim.\n```baml clients.baml\nclient<llm> MyClient {\n  provider openai\n  options {\n    temperature 0.0\n  }\n}\n```\n\n### 2. Reduce the number of input tokens\nReduce the amount of data you're giving the model to process to reduce confusion.\n\nPrune as much data as possible, or split your prompt into multiple prompts analyzing subsets of the data.\n\nIf you're processing `images`, try cropping the parts of the image that you don't need. LLMs can only handle images of certain sizes, so every pixel counts. Make sure you resize images to the model's input size (even if the provider does the resizing for you), so you can gauge how clear the image is at the model's resolution. You'll notice the blurrier the image is, the higher the hallucination rate.\n\nLet us know if you want more tips for processing images, we have some helper prompts we can share with you, or help debug your prompt.\n\n\n\n### 2. Use reasoning or reflection prompting\nRead our [chain-of-thought guide](/examples/prompt-engineering/chain-of-thought) for more.\n\n### 3. Watch out for contradictions and word associations\n\nEach word you add into the prompt will cause it to associate it with something it saw before in its training data. This is why we have techniques like [symbol tuning](/examples/prompt-engineering/symbol-tuning) to help control this bias.\n\n\nLet's say you have a prompt that says:\n```\nAnswer in this JSON schema:\n\n\n\nBut when you answer, add some comments in the JSON indicating your reasoning for the field like this:\n\nExample:\n---\n{\n  // I used the name \"John\" because it's the name of the person who wrote the prompt\n  \"name\": \"John\"\n}\n\nJSON:\n```\n\nThe LLM may not write the `// comment` inline, because it's been trained to associate JSON with actual \"valid\" JSON.\n\nYou can get around this with some more coaxing like:\n```text {12,13}\nAnswer in this JSON schema:\n\n\n\nBut when you answer, add some comments in the JSON indicating your reasoning for the field like this:\n---\n{\n  // I used the name \"John\" because it's the name of the person who wrote the prompt\n  \"name\": \"John\"\n}\n\nIt's ok if this isn't fully valid JSON, \nwe will fix it afterwards and remove the comments.\n\nJSON:\n```\n\nThe LLM made an assumption that you want \"JSON\" -- which doesn't use comments -- and our instructions were not explicit enough to override that bias originally.\n\nKeep on reading for more tips and tricks! Or reach out in our Discord \n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/06-prompt-engineering/hallucinations.mdx"
      },
      "01-guide/06-prompt-engineering/chat-history.mdx": {
        "markdown": "---\ntitle: Chat\n---\n\nIn this guide we'll build a small chatbot that takes in user messages and generates responses.\n\n\n```baml chat-history.baml\nclass MyUserMessage {\n  role \"user\" | \"assistant\"\n  content string\n}\n\nfunction ChatWithLLM(messages: MyUserMessage[]) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Answer the user's questions based on the chat history:\n    {% for message in messages %}\n      {{ _.role(message.role) }} \n      {{ message.content }}\n    {% endfor %}\n\n    Answer:\n  \"#\n}\n\ntest TestName {\n  functions [ChatWithLLM]\n  args {\n    messages [\n      {\n        role \"user\"\n        content \"Hello!\"\n      }\n      {\n        role \"assistant\"\n        content \"Hi!\"\n      }\n    ]\n  }\n}\n\n```\n\n#### Code\n<CodeGroup>\n```python Python\nfrom baml_client import b\nfrom baml_client.types import MyUserMessage\n\ndef main():\n    messages: list[MyUserMessage] = []\n    \n    while True:\n        content = input(\"Enter your message (or 'quit' to exit): \")\n        if content.lower() == 'quit':\n            break\n        \n        messages.append(MyUserMessage(role=\"user\", content=content))\n        \n        agent_response = b.ChatWithLLM(messages=messages)\n        print(f\"AI: {agent_response}\")\n        print()\n        \n        # Add the agent's response to the chat history\n        messages.append(MyUserMessage(role=\"assistant\", content=agent_response))\n\nif __name__ == \"__main__\":\n    main()\n```\n```typescript Typescript\nimport { b, MyUserMessage } from 'baml_client';\nimport * as readline from 'readline';\n\nconst rl = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout\n});\n\nconst messages: MyUserMessage[] = [];\n\nfunction askQuestion(query: string): Promise<string> {\n  return new Promise((resolve) => {\n    rl.question(query, resolve);\n  });\n}\n\nasync function main() {\n\n  while (true) {\n    const content = await askQuestion(\"Enter your message (or 'quit' to exit): \");\n    if (content.toLowerCase() === 'quit') {\n      break;\n    }\n\n    messages.push({ role: \"user\", content });\n\n    const agentResponse = await b.ChatWithLLM({ messages });\n    console.log(`AI: ${agentResponse}`);\n    console.log();\n\n    // Add the agent's response to the chat history\n    messages.push({ role: \"assistant\", content: agentResponse });\n  }\n\n  rl.close();\n}\n\nmain();\n```\n</CodeGroup>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/06-prompt-engineering/chat-history.mdx"
      },
      "01-guide/06-prompt-engineering/tools.mdx": {
        "markdown": "---\ntitle: Tools / Function Calling\n---\n\n\n\"Function calling\" is a technique for getting an LLM to choose a function to call for you.\n\nThe way it works is:\n1. You define a task with certain function(s)\n2. Ask the LLM to **choose which function to call**\n3. **Get the function parameters from the LLM** for the appropriate function it choose\n4. **Call the functions** in your code with those parameters\n\nIn BAML, you can get represent a `tool` or a `function` you want to call as a BAML `class`, and make the function output be that class definition.\n\n```baml BAML\nclass WeatherAPI {\n  city string @description(\"the user's city\")\n  timeOfDay string @description(\"As an ISO8601 timestamp\")\n}\n\nfunction UseTool(user_message: string) -> WeatherAPI {\n  client GPT4Turbo\n  prompt #\"\n    Extract the info from this message\n    ---\n    {{ user_message }}\n    ---\n\n    {# special macro to print the output schema. #}\n    {{ ctx.output_format }}\n\n    JSON:\n  \"#\n}\n```\nCall the function like this:\n\n<CodeGroup>\n```python Python\nimport asyncio\nfrom baml_client import b\nfrom baml_client.types import WeatherAPI\n\ndef main():\n    weather_info = b.UseTool(\"What's the weather like in San Francisco?\")\n    print(weather_info)\n    assert isinstance(weather_info, WeatherAPI)\n    print(f\"City: {weather_info.city}\")\n    print(f\"Time of Day: {weather_info.timeOfDay}\")\n\nif __name__ == '__main__':\n    main()\n```\n\n```typescript TypeScript\nimport { b } from './baml_client'\nimport { WeatherAPI } from './baml_client/types'\nimport assert from 'assert'\n\nconst main = async () => {\n  const weatherInfo = await b.UseTool(\"What's the weather like in San Francisco?\")\n  console.log(weatherInfo)\n  assert(weatherInfo instanceof WeatherAPI)\n  console.log(`City: ${weatherInfo.city}`)\n  console.log(`Time of Day: ${weatherInfo.timeOfDay}`)\n}\n```\n\n```ruby Ruby\nrequire_relative \"baml_client/client\"\n\n$b = Baml.Client\n\ndef main\n  weather_info = $b.UseTool(user_message: \"What's the weather like in San Francisco?\")\n  puts weather_info\n  raise unless weather_info.is_a?(Baml::Types::WeatherAPI)\n  puts \"City: #{weather_info.city}\"\n  puts \"Time of Day: #{weather_info.timeOfDay}\"\nend\n```\n</CodeGroup>\n\n## Choosing multiple Tools\n\nTo choose ONE tool out of many, you can use a union:\n```baml BAML\nfunction UseTool(user_message: string) -> WeatherAPI | MyOtherAPI {\n  .... // same thing\n}\n```\n\n<Tip>If you use [VSCode Playground](/guides/installation-editors/vs-code-extension), you can see what we inject into the prompt, with full transparency.</Tip>\n\nCall the function like this:\n\n<CodeGroup>\n```python Python\nimport asyncio\nfrom baml_client import b\nfrom baml_client.types import WeatherAPI, MyOtherAPI\n\nasync def main():\n    tool = b.UseTool(\"What's the weather like in San Francisco?\")\n    print(tool)\n    \n    if isinstance(tool, WeatherAPI):\n        print(f\"Weather API called:\")\n        print(f\"City: {tool.city}\")\n        print(f\"Time of Day: {tool.timeOfDay}\")\n    elif isinstance(tool, MyOtherAPI):\n        print(f\"MyOtherAPI called:\")\n        # Handle MyOtherAPI specific attributes here\n\nif __name__ == '__main__':\n    main()\n```\n\n```typescript TypeScript\nimport { b } from './baml_client'\nimport { WeatherAPI, MyOtherAPI } from './baml_client/types'\n\nconst main = async () => {\n  const tool = await b.UseTool(\"What's the weather like in San Francisco?\")\n  console.log(tool)\n  \n  if (tool instanceof WeatherAPI) {\n    console.log(\"Weather API called:\")\n    console.log(`City: ${tool.city}`)\n    console.log(`Time of Day: ${tool.timeOfDay}`)\n  } else if (tool instanceof MyOtherAPI) {\n    console.log(\"MyOtherAPI called:\")\n    // Handle MyOtherAPI specific attributes here\n  }\n}\n\nmain()\n```\n\n```ruby Ruby\nrequire_relative \"baml_client/client\"\n\n$b = Baml.Client\n\ndef main\n  tool = $b.UseTool(user_message: \"What's the weather like in San Francisco?\")\n  puts tool\n  \n  case tool\n  when Baml::Types::WeatherAPI\n    puts \"Weather API called:\"\n    puts \"City: #{tool.city}\"\n    puts \"Time of Day: #{tool.timeOfDay}\"\n  when Baml::Types::MyOtherAPI\n    puts \"MyOtherAPI called:\"\n    # Handle MyOtherAPI specific attributes here\n  end\nend\n\nmain\n```\n</CodeGroup>\n\n## Choosing N Tools\nTo choose many tools, you can use a union of a list:\n```baml BAML\nfunction UseTool(user_message: string) -> (WeatherAPI | MyOtherAPI)[] {\n  .... // same thing\n}\n```\n\nCall the function like this:\n\n<CodeGroup>\n```python Python\nimport asyncio\nfrom baml_client import b\nfrom baml_client.types import WeatherAPI, MyOtherAPI\n\nasync def main():\n    tools = b.UseTool(\"What's the weather like in San Francisco and New York?\")\n    print(tools)  \n    \n    for tool in tools:\n        if isinstance(tool, WeatherAPI):\n            print(f\"Weather API called:\")\n            print(f\"City: {tool.city}\")\n            print(f\"Time of Day: {tool.timeOfDay}\")\n        elif isinstance(tool, MyOtherAPI):\n            print(f\"MyOtherAPI called:\")\n            # Handle MyOtherAPI specific attributes here\n\nif __name__ == '__main__':\n    main()\n```\n\n```typescript TypeScript\nimport { b } from './baml_client'\nimport { WeatherAPI, MyOtherAPI } from './baml_client/types'\n\nconst main = async () => {\n  const tools = await b.UseTool(\"What's the weather like in San Francisco and New York?\")\n  console.log(tools)\n  \n  tools.forEach(tool => {\n    if (tool instanceof WeatherAPI) {\n      console.log(\"Weather API called:\")\n      console.log(`City: ${tool.city}`)\n      console.log(`Time of Day: ${tool.timeOfDay}`)\n    } else if (tool instanceof MyOtherAPI) {\n      console.log(\"MyOtherAPI called:\")\n      // Handle MyOtherAPI specific attributes here\n    }\n  })\n}\n\nmain()\n```\n\n```ruby Ruby\nrequire_relative \"baml_client/client\"\n\n$b = Baml.Client\n\ndef main\n  tools = $b.UseTool(user_message: \"What's the weather like in San Francisco and New York?\")\n  puts tools\n  \n  tools.each do |tool|\n    case tool\n    when Baml::Types::WeatherAPI\n      puts \"Weather API called:\"\n      puts \"City: #{tool.city}\"\n      puts \"Time of Day: #{tool.timeOfDay}\"\n    when Baml::Types::MyOtherAPI\n      puts \"MyOtherAPI called:\"\n      # Handle MyOtherAPI specific attributes here\n    end\n  end\nend\n\nmain\n```\n</CodeGroup>\n\n## Function-calling APIs vs Prompting\nInjecting your function schemas into the prompt, as BAML does, outperforms function-calling across all benchmarks for major providers ([see our Berkeley FC Benchmark results with BAML](https://www.boundaryml.com/blog/sota-function-calling?q=0)).\n\nAmongst other limitations, function-calling APIs will at times:\n1. Return a schema when you don't want any (you want an error)\n2. Not work for tools with more than 100 parameters.\n3. Use [many more tokens than prompting](https://www.boundaryml.com/blog/type-definition-prompting-baml).\n\nKeep in mind that \"JSON mode\" is nearly the same thing as \"prompting\", but it enforces the LLM response is ONLY a JSON blob.\nBAML does not use JSON mode since it allows developers to use better prompting techniques like chain-of-thought, to allow the LLM to express its reasoning before printing out the actual schema. BAML's parser can find the json schema(s) out of free-form text for you. Read more about different approaches to structured generation [here](https://www.boundaryml.com/blog/schema-aligned-parsing)\n\nBAML will still support native function-calling APIs in the future (please let us know more about your use-case so we can prioritize accordingly)\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/06-prompt-engineering/tools.mdx"
      },
      "01-guide/06-prompt-engineering/chain-of-thought.mdx": {
        "markdown": "---\ntitle: Chain of Thought Prompting\n---\n\nChain of thought prompting is a technique that encourages the language model to think step by step, reasoning through the problem before providing an answer. This can improve the quality of the response and make it easier to understand.\n\n\nIn the below example, we use chain of thought prompting to extract information from an email.\n\nBAML will still parse the response as an `OrderInfo` object, even though there is additional text in the response.\n```baml\nclass Email {\n    subject string\n    body string\n    from_address string\n}\n\n\nclass OrderInfo {\n    order_status \"ORDERED\" | \"SHIPPED\" | \"DELIVERED\" | \"CANCELLED\"\n    tracking_number string?\n    estimated_arrival_date string?\n}\n\nfunction GetOrderInfo(email: Email) -> OrderInfo {\n  client GPT4o\n  prompt #\"\n    Extract the info from this email in the INPUT:\n\n    INPUT:\n    -------\n    from: {{email.from_address}}\n    Email Subject: {{email.subject}}\n    Email Body: {{email.body}}\n    -------\n\n    {{ ctx.output_format }}\n\n    Before you output the JSON, please explain your\n    reasoning step-by-step. Here is an example on how to do this:\n    'If we think step by step we can see that ...\n     therefore the output JSON is:\n    {\n      ... the json schema ...\n    }'\n  \"#\n}\n\ntest Test1 {\n  functions [GetOrderInfo]\n  args {\n    email {\n      from_address \"hello@amazon.com\"\n      subject \"Your Amazon.com order of 'Wood Dowel Rods...' has shipped!\"\n      body #\"\n        Hi Sam, your package will arrive:\n        Thurs, April 4\n        Track your package:\n        www.amazon.com/gp/your-account/ship-track?ie=23&orderId123\n\n        On the way:\n        Wood Dowel Rods...\n        Order #113-7540940\n        Ship to:\n            Sam\n            SEATTLE, WA\n\n        Shipment total:\n        $0.00\n    \"#\n\n    }\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/06-prompt-engineering/chain-of-thought.mdx"
      },
      "01-guide/06-prompt-engineering/symbol-tuning.mdx": {
        "markdown": "---\ntitle: Creating a Classification Function with Symbol Tuning\n---\n\nAliasing field names to abstract symbols like \"k1\", \"k2\", etc. can improve classification results. This technique, known as symbol tuning, helps the LLM focus on your descriptions rather than being biased by the enum or property names themselves.\n\nSee the paper [Symbol Tuning Improves In-Context Learning in Language Models](https://arxiv.org/abs/2305.08298) for more details.\n\n```baml\nenum MyClass {\n    Refund @alias(\"k1\")\n    @description(\"Customer wants to refund a product\")\n\n    CancelOrder @alias(\"k2\")\n    @description(\"Customer wants to cancel an order\")\n\n    TechnicalSupport @alias(\"k3\")\n    @description(\"Customer needs help with a technical issue unrelated to account creation or login\")\n\n    AccountIssue @alias(\"k4\")\n    @description(\"Specifically relates to account-login or account-creation\")\n\n    Question @alias(\"k5\")\n    @description(\"Customer has a question\")\n}\n\nfunction ClassifyMessageWithSymbol(input: string) -> MyClass {\n  client GPT4o\n\n  prompt #\"\n    Classify the following INPUT into ONE\n    of the following categories:\n\n    INPUT: {{ input }}\n\n    {{ ctx.output_format }}\n\n    Response:\n  \"#\n}\n\ntest Test1 {\n  functions [ClassifyMessageWithSymbol]\n  args {\n    input \"I can't access my account using my login credentials. I havent received the promised reset password email. Please help.\"\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/06-prompt-engineering/symbol-tuning.mdx"
      },
      "01-guide/introduction.mdx": {
        "markdown": "\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/01-guide/introduction.mdx"
      },
      "03-reference/overview.mdx": {
        "markdown": "---\ntitle: BAML Reference\n---\n\nWelcome to the BAML reference guide!\n\nHere you can learn about every BAML keyword, feature, and setting.\n\nFor more in-depth explanations, we recommend reading the [Guides](/guide) first.\n\n<Cards>\n  <Card title=\"BAML Language\" icon=\"fa-solid fa-language\" href=\"/ref/baml\">\n    Learn everything about BAML's language features.\n  </Card>\n\n  <Card title=\"Prompt (Jinja) Syntax\" icon=\"fa-solid fa-code\" href=\"/ref/prompt-syntax\">\n    Learn about BAML's Jinja prompt syntax.\n  </Card>\n\n  <Card title=\"BAML CLI\" icon=\"fa-solid fa-terminal\" href=\"/ref/baml-cli\">\n    BAML CLI commands and flags.\n  </Card>\n\n  <Card title=\"VSCode Settings\" icon=\"fa-solid fa-gears\" href=\"/ref/editor-extension-settings\">\n    VSCode BAML Extension settings\n  </Card>\n\n  <Card title=\"LLM Clients\" icon=\"fa-solid fa-brain\" href=\"/ref/llm-client-providers\">\n    LLM clients and how to configure them.\n  </Card>\n\n  <Card title=\"baml_client\" icon=\"fa-solid fa-running\" href=\"/ref/baml-client\">\n    API Reference for the `baml_client` object.\n  </Card>\n\n</Cards>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/overview.mdx"
      },
      "03-reference/baml-cli/init.mdx": {
        "markdown": "\nThe `init` command is used to initialize a project with BAML. It sets up the necessary directory structure and configuration files to get you started with BAML.\n\n## Usage\n\n```\nbaml-cli init [OPTIONS]\n```\n\n## Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--dest <PATH>` | Specifies where to initialize the BAML project | Current directory (`.`) |\n| `--client-type <TYPE>` | Type of BAML client to generate | Guesses based on where the CLI was installed from (`python/pydantic` for pip, `typescript` for npm, etc.) |\n| `--openapi-client-type <TYPE>` | The OpenAPI client generator to run, if `--client-type=openapi` | None |\n\n## Description\n\nThe `init` command performs the following actions:\n\n1. Creates a new BAML project structure in `${DEST}/baml_src`.\n2. Creates a `generators.baml` file in the `baml_src` directory with initial configuration.\n3. Includes some additional examples files in `baml_src` to get you started.\n\n## Client Types\n\nThe `--client-type` option allows you to specify the type of BAML client to generate. Available options include:\n\n- `python/pydantic`: For Python clients using Pydantic\n- `typescript`: For TypeScript clients\n- `ruby/sorbet`: For Ruby clients using Sorbet\n- `rest/openapi`: For REST clients using OpenAPI\n\nIf not specified, it uses the default from the runtime CLI configuration.\n\n## OpenAPI Client Types\n\nWhen using `--client-type=rest/openai`, you can specify the OpenAPI client generator using the `--openapi-client-type` option. Some examples include:\n\n- `go`\n- `java`\n- `php`\n- `ruby`\n- `rust`\n- `csharp`\n\nFor a full list of supported OpenAPI client types, refer to the [OpenAPI Generator documentation](https://github.com/OpenAPITools/openapi-generator#overview).\n\n## Examples\n\n1. Initialize a BAML project in the current directory with default settings:\n   ```\n   baml init\n   ```\n\n2. Initialize a BAML project in a specific directory:\n   ```\n   baml init --dest /path/to/my/project\n   ```\n\n3. Initialize a BAML project for Python with Pydantic:\n   ```\n   baml init --client-type python/pydantic\n   ```\n\n4. Initialize a BAML project for OpenAPI with a Go client:\n   ```\n   baml init --client-type openapi --openapi-client-type go\n   ```\n\n## Notes\n\n- If the destination directory already contains a `baml_src` directory, the command will fail to prevent overwriting existing projects.\n- The command attempts to infer the OpenAPI generator command based on what's available in your system PATH. It checks for `openapi-generator`, `openapi-generator-cli`, or falls back to using `npx @openapitools/openapi-generator-cli`.\n- After initialization, follow the instructions provided in the console output for language-specific setup steps.\n\nFor more information on getting started with BAML, visit the [BAML documentation](https://docs.boundaryml.com/docs/get-started/quickstart).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml-cli/init.mdx"
      },
      "03-reference/baml-cli/generate.mdx": {
        "markdown": "The `generate` command is used to generate BAML clients based on your BAML source files. It processes the BAML configurations and creates the necessary client code for your specified output type.\n\n## Usage\n\n```\nbaml-cli generate [OPTIONS]\n```\n\n## Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--from <PATH>` | Path to the `baml_src` directory | `./baml_src` |\n| `--no-version-check` | Generate `baml_client` without checking for version mismatch | `false` |\n\n## Description\n\nThe `generate` command performs the following actions:\n\n1. Finds all generators in the BAML project (usualy in `generators.baml`).\n2. Ensure all generators match the CLI version.\n3. Generate each `baml_client` based on the generator configurations.\n\n## Examples\n\n1. Generate clients using default settings:\n   ```\n   baml-cli generate\n   ```\n\n2. Generate clients from a specific directory:\n   ```\n   baml-cli generate --from /path/to/my/baml_src\n   ```\n\n3. Generate clients without version check:\n   ```\n   baml-cli generate --no-version-check\n   ```\n\n## Output\n\nThe command provides informative output about the generation process:\n\n- If no clients were generated, it will suggest a configuration to add to your BAML files.\n- If clients were generated, it will report the number of clients generated and their locations.\n\n\n## Notes\n\n- If no generator configurations are found in the BAML files, the command will generate a default client based on the CLI defaults and provide instructions on how to add a generator configuration to your BAML files.\n- If generator configurations are found, the command will generate clients according to those configurations.\n- If one of the generators fails, the command will stop at that point and report the error.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml-cli/generate.mdx"
      },
      "03-reference/baml-cli/serve.mdx": {
        "markdown": "The `serve` command starts a BAML-over-HTTP API server that exposes your BAML functions via HTTP endpoints. This feature allows you to interact with your BAML functions through a RESTful API interface.\n\n<Warning>\n  **Warning: Preview Feature**\n  \n  1. You must include the `--preview` flag when running the `dev` command.\n  2. Be aware that this feature is still being stabilized and may change in future releases.\n</Warning>\n\n## Usage\n\n```\nbaml-cli serve [OPTIONS] --preview\n```\n\n<Tip>\nIf you're actively developing, you can use the `dev` command to include hotreload functionality:\n```\nbaml-cli dev [OPTIONS] --preview\n```\n\n[See more](./dev)\n</Tip>\n\n## Options\n\n| Option | Description | Default |\n|--------|-------------|---------|\n| `--from <PATH>` | Path to the `baml_src` directory | `./baml_src` |\n| `--port <PORT>` | Port to expose BAML on | `2024` |\n| `--no-version-check` | Generate `baml_client` without checking for version mismatch | `false` |\n| `--preview` | Enable the preview feature | |\n\n## Description\n\nThe `serve` command performs the following actions:\n\n1. Exposes BAML functions as HTTP endpoints on the specified port.\n2. Provides authentication middleware for secure access.\n\n## Endpoints\n\n\n- `POST /call/:function_name`: Call a BAML function\n\n**Debugging**\n- `GET /docs`: Interactive API documentation (Swagger UI)\n- `GET /openapi.json`: OpenAPI specification for the BAML functions\n- `GET /_debug/ping`: Health check endpoint\n- `GET /_debug/status`: Server status and authentication check\n\n## Authentication\n\nWe support the header: `x-baml-api-key`\n\nSet the `BAML_PASSWORD` environment variable to enable authentication.\n\n## Examples\n\n1. Start the server with default settings:\n   ```\n   baml-cli serve --preview\n   ```\n\n2. Start the server with a custom source directory and port:\n   ```\n   baml-cli serve --from /path/to/my/baml_src --port 3000 --preview\n   ```\n\n## Testing\n\nTo test the server, you can use the following `curl` commands:\n\n1. Check if the server is running:\n   ```bash\n   curl http://localhost:2024/_debug/ping\n   ```\n\n2. Call a function:\n   ```bash\n   curl -X POST http://localhost:2024/call/MyFunctionName -d '{\"arg1\": \"value1\", \"arg2\": \"value2\"}'\n   ```\n\n   ```bash API Key\n    curl -X POST http://localhost:2024/call/MyFunctionName -H \"x-baml-api-key: ${BAML_PASSWORD}\" -d '{\"arg1\": \"value1\", \"arg2\": \"value2\"}'\n    ```\n\n3. Access the API documentation:\n   Open `http://localhost:2024/docs` in your web browser.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml-cli/serve.mdx"
      },
      "03-reference/baml-cli/dev.mdx": {
        "markdown": "The `dev` command starts a development server that watches your BAML source files for changes and automatically reloads the BAML runtime. This feature is designed to streamline the development process by providing real-time updates as you modify your BAML configurations.\n\n\n<Warning>\n  **Warning: Preview Feature**\n  \n  1. You must include the `--preview` flag when running the `dev` command.\n  2. Be aware that this feature is still being stabilized and may change in future releases.\n</Warning>\n\n## Usage\n\n```\nbaml-cli dev [OPTIONS] --preview\n```\n\n## Details\n\nSee the [serve](./serve) command for more information on the arguments.\n\nThe dev command performs the exact same functionality, but it additionally:\n\n1. Watches the BAML source files for changes.\n2. Automatically reloads the server when changes are detected.\n3. Automatically runs any generators when changes are detected.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml-cli/dev.mdx"
      },
      "03-reference/baml/comments.mdx": {
        "markdown": "## Single line / trailing comments\n\nDenoted by `//`.\n\n```baml\n// hello there!\nfoo // this is a trailing comment\n```\n\n## Docstrings\n\nWe have no special syntax for docstrings. Instead, we use comments.\nEventually, we'll support a `///` syntax for docstrings which will\nalso be used for generating documentation in `baml_client`.\n\n{/* ## Docstrings\n\nTo add a docstring to any block, use `///`.\n\n```baml\n/// This is a docstring for a class\nclass Foo {\n    /// This is a docstring for a property\n    property1 string\n}\n``` */}\n\n{/* ## Multiline comments\n\nMultiline comments are denoted via `{//` and `//}`.\n\n```baml\n{//\n    this is a multiline comment\n    foo\n    bar\n//}\n``` */}\n \n## Comments in block strings\n\nSee [Block Strings](./values/string#block-strings) for more information.\n\n```jinja\n#\"\n    My string. {#\n        This is a comment\n    #}\n    hi!\n\"#\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/comments.mdx"
      },
      "03-reference/baml/env-vars.mdx": {
        "markdown": "To set a value to an environment variable, use the following syntax:\n\n```baml\nenv.YOUR_VARIABLE_NAME\n```\n\n<Warning>Environment variables with spaces in their names are not supported.</Warning>\n\n### Example\n\nUsing an environment variable for API key:\n\n```baml\nclient<llm> MyCustomClient {\n    provider \"openai\"\n    options {\n        model \"gpt-4o-mini\"\n        // Set the API key using an environment variable\n        api_key env.MY_SUPER_SECRET_API_KEY\n    }\n}\n```\n\n## Setting Environment Variables\nTo set environment variables:\n\n<AccordionGroup>\n  <Accordion title=\"In the VSCode Playground\">\n\n   \nOnce you open a `.baml` file, in VSCode, you should see a small button over every BAML function: `Open Playground`.\n\nThen you should be able to set environment variables in the settings tab.\n\n<img src=\"file:d4e9528c-697a-4aa9-ac02-f92823252a6b\" />\n\nOr type `BAML Playground` in the VSCode Command Bar (`CMD + Shift + P` or `CTRL + Shift + P`) to open the playground.\n\n  </Accordion>\n\n  <Accordion title=\"For your app (default)\">\n    BAML will expect these to be set already in your program **before** you import the baml_client in Python/ TS / etc.\n\n    Any of the following strategies for setting env vars are compatible with BAML:\n    - setting them in your shell before running your program\n    - in your `Dockerfile`\n    - in your `next.config.js`\n    - in your Kubernetes manifest\n    - from secrets-store.csi.k8s.io\n    - from a secrets provider such as [Infisical](https://infisical.com/) / [Doppler](https://www.doppler.com/)\n    - from a `.env` file (using `dotenv` cli)\n    - using account credentials for ephemeral token generation (e.g. Vertex AI Auth Tokens)\n\n    ```bash\n    export MY_SUPER_SECRET_API_KEY=\"...\"\n    python my_program_using_baml.py\n    ```\n  </Accordion>\n  \n  <Accordion title=\"For your app (manually)\">\n    <Info>\n    Requires BAML Version 0.57+\n    </Info>\n\n    If you don't want BAML to try to auto-load your env vars, you can call manually `reset_baml_env_vars`\nwith the current environment variables.\n    <CodeBlocks>\n\n    ```python Python\n\n    from baml_client import b\n    from baml_client import reset_baml_env_vars\n    import os\n    import dotenv\n\n    dotenv.load_dotenv()\n    reset_baml_env_vars(dict(os.environ))\n    ```\n\n    ```typescript TypeScript\n    import dotenv from 'dotenv'\n    // Wait to import the BAML client until after loading environment variables\n    import { b, resetBamlEnvVars } from 'baml-client'\n\n    dotenv.config()\n    resetBamlEnvVars(process.env)\n    ```\n\n    ```ruby Ruby (beta)\n    require 'dotenv/load'\n\n    # Wait to import the BAML client until after loading environment variables\n    # reset_baml_env_vars is not yet implemented in the Ruby client\n    require 'baml_client'\n    ```\n\n\n    </CodeBlocks>\n  </Accordion>\n</AccordionGroup>\n\n\n## Error Handling\nErrors for unset environment variables are only thrown when the variable is accessed. If your BAML project has 15 environment variables and 1 is used for the function you are calling, only that one environment variable will be checked for existence.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/env-vars.mdx"
      },
      "03-reference/baml/string.mdx": {
        "markdown": "BAML treats templatized strings as first-class citizens.\n\n## Quoted Strings\n\nThese is a valid **inline string**, which is surrounded by double quotes. They behave like regular strings in most programming languages, and can be escaped with a backslash.\n\n<Tip>These cannot have template variables or expressions inside them. Use a block string for that.</Tip>\n\n\n```rust\n\"Hello World\"\n\n\"\\n\"\n```\n\n## Unquoted Strings\n\nBAML also supports simple **unquoted in-line** strings. The string below is valid! These are useful for simple strings such as configuration options.\n\n```rust\nHello World\n```\n\nUnquoted strings **may not** have any of the following since they are reserved characters (note this may change in the future):\n\n- Quotes \"double\" or 'single'\n- At-signs @\n- Curlies {}\n- hashtags #\n- Parentheses ()\n- Brackets []\n- commas ,\n- newlines\n\nWhen in doubt, use a quoted string or a block string, but the VSCode extension will warn you if there is a parsing issue.\n\n## Block Strings\n\nIf a string is on multiple lines, it must be surrounded by #\" and \"#. This is called a **block string**.\n\n```rust\n#\"\nHello\nWorld\n\"#\n```\n\nBlock strings are automatically dedented and stripped of the first and last newline. This means that the following will render the same thing as above\n\n```rust\n#\"\n    Hello\n    World\n\"#\n```\n\nWhen used for templating, block strings can contain expressions and variables using [Jinja](https://jinja.palletsprojects.com/en/3.0.x/templates/) syntax.\n\n```rust\ntemplate_string Greeting(name: string) #\"\n  Hello {{ name }}!\n\"#\n```\n\n### Escape Characters\n\nEscaped characters are injected as is into the string.\n\n```rust\n#\"\\n\"#\n```\n\nThis will render as `\\\\n` in the output.\n\n### Adding a `\"#`\nTo include a `\"#` in a block string, you can prefix it with a different count of `#`.\n\n\n```baml\n###\"\n  #\"Hello\"#\n\"###\n```\n\nThis will render as `#\"Hello\"#`.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/string.mdx"
      },
      "03-reference/baml/int-float.mdx": {
        "markdown": "\nNumerical values as denoted more specifically in BAML.\n\n| Value | Description |\n| --- | --- |\n| `int` | Integer |\n| `float` | Floating point number |\n\n\nWe support implicit casting of int -> float, but if you need something to explicitly be a float, use `0.0` instead of `0`.\n\n\n## Usage\n\n\n```baml\nfunction DescribeCircle(radius: int | float, pi: float?) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Describe a circle with a radius of {{ radius }} units.\n        Include the area of the circle using pi as {{ pi or 3.14159 }}.\n        \n        What are some properties of the circle?\n    \"#\n}\n\ntest CircleDescription {\n    functions [DescribeCircle]\n    // will be cast to int\n    args { radius 5 }\n}\n\ntest CircleDescription2 {\n    functions [DescribeCircle]\n    // will be cast to float\n    args { \n        radius 5.0 \n        pi 3.14\n    }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/int-float.mdx"
      },
      "03-reference/baml/bool.mdx": {
        "markdown": "`true` or `false`\n\n## Usage\n\n```baml\nfunction CreateStory(long: bool) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Write a story that is {{ \"10 paragraphs\" if long else \"1 paragraph\" }} long.\n    \"#\n}\n\ntest LongStory {\n    functions [CreateStory]\n    args { long true }\n}\n\ntest ShortStory {\n    functions [CreateStory]\n    args { long false }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/bool.mdx"
      },
      "03-reference/baml/array.mdx": {
        "markdown": "Allow you to store and manipulate collections of data. They can be declared in a concise and readable manner, supporting both single-line and multi-line formats.\n\n## Syntax\n\nTo declare an array in a BAML file, you can use the following syntax:\n\n```baml\n{\n  key1 [value1, value2, value3],\n  key2 [\n    value1,\n    value2,\n    value3\n  ],\n  key3 [\n    {\n      subkey1 \"valueA\",\n      subkey2 \"valueB\"\n    },\n    {\n      subkey1 \"valueC\",\n      subkey2 \"valueD\"\n    }\n  ]\n}\n```\n\n### Key Points:\n- **Commas**: Optional for multi-line arrays, but recommended for clarity.\n- **Nested Arrays**: Supported, allowing complex data structures.\n- **Key-Value Pairs**: Arrays can contain objects with key-value pairs.\n\n## Usage Examples\n\n### Example 1: Simple Array\n\n```baml\nfunction DescriptionGame(items: string[]) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        What 3 words best describe all of these: {{ items }}.\n    \"#\n}\n\ntest FruitList {\n    functions [DescriptionGame]\n    args { items [\"apple\", \"banana\", \"cherry\"] }\n}\n```\n\n### Example 2: Multi-line Array\n\n```baml\ntest CityDescription {\n    functions [DescriptionGame]\n    args { items [\n            \"New York\",\n            \"Los Angeles\",\n            \"Chicago\"\n        ]\n    }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/array.mdx"
      },
      "03-reference/baml/map.mdx": {
        "markdown": "Map values (AKA Dictionaries) allow you to store key-value pairs.\n\n<Tip>Most of BAML (clients, tests, classes, etc) is represented as a map.</Tip>\n\n## Syntax\n\nTo declare a map in a BAML file, you can use the following syntax:\n\n```baml\n{\n  key1 value1,\n  key2 {\n    nestedKey1 nestedValue1,\n    nestedKey2 nestedValue2\n  }\n}\n```\n\n### Key Points:\n- **Colons**: Not used in BAML maps; keys and values are separated by spaces.\n- **Value Types**: Maps can contain unquoted or quoted strings, booleans, numbers, and nested maps as values.\n- **Classes**: Classes in BAML are represented as maps with keys and values.\n\n## Usage Examples\n\n### Example 1: Simple Map\n\n```baml\n\nclass Person {\n    name string\n    age int\n    isEmployed bool\n}\n\nfunction DescribePerson(person: Person) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Describe the person with the following details: {{ person }}.\n    \"#\n}\n\ntest PersonDescription {\n    functions [DescribePerson]\n    args { \n        person {\n            name \"John Doe\",\n            age 30,\n            isEmployed true\n        }\n    }\n}\n```\n\n### Example 2: Nested Map\n\n```baml\n\nclass Company {\n    name string\n    location map<string, string>\n    employeeCount int\n}\n\nfunction DescribeCompany(company: Company) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Describe the company with the following details: {{ company }}.\n    \"#\n}\n\ntest CompanyDescription {\n    functions [DescribeCompany]\n    args { \n        company {\n            name \"TechCorp\",\n            location {\n                city \"San Francisco\",\n                state \"California\"\n            },\n            employeeCount 500\n        }\n    }\n}\n```\n\n### Example 3: Map with Multiline String\n\n```baml\nclass Project {\n    title string\n    description string\n}\n\nfunction DescribeProject(project: Project) -> string {\n    client \"openai/gpt-4o-mini\"\n    prompt #\"\n        Describe the project with the following details: {{ project }}.\n    \"#\n}\n\ntest ProjectDescription {\n    functions [DescribeProject]\n    args { \n        project {\n            title \"AI Research\",\n            description #\"\n                This project focuses on developing\n                advanced AI algorithms to improve\n                machine learning capabilities.\n            \"#\n        }\n    }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/map.mdx"
      },
      "03-reference/baml/types.mdx": {
        "markdown": "\n\n\nHere's a list of all the types that can be represented in BAML:\n\n## Primitive Types\n* `bool`\n* `int`\n* `float`\n* `string`\n* `null`\n\n## Literal Types\n<Info>\n  This feature was added in: v0.61.0.\n</Info>\n\nThe primitive types `string`, `int` and `bool` can be constrained to a specific value.\nFor example, you can use literal values as return types:\n\n```rust\nfunction ClassifyIssue(issue_description: string) -> \"bug\" | \"enhancement\" {\n  client GPT4Turbo\n  prompt #\"\n    Classify the issue based on the following description:\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\")}}\n    {{ issue_description }}\n  \"#\n}\n```\n\nSee [Union(|)](#union-) for more details.\n\n\n## Multimodal Types\nSee [calling a function with multimodal types](/docs/snippets/calling-baml/multi-modal)\nand [testing image inputs](/docs/snippets/test-cases#images)\n\n<Accordion title=\"Implementation details: runtime and security considerations\">\n  BAML's multimodal types are designed for ease of use: we have deliberately made it\n  easy for you to construct a `image` or `audio` instance from a URL. Under the\n  hood, depending on the model you're using, BAML may need to download the image\n  and transcode it (usually as base64) for the model to consume.\n\n  This ease-of-use does come with some tradeoffs; namely, if you construct\n  an `image` or `audio` instance using untrusted user input, you may be exposing\n  yourself to [server-side request forgery (SSRF) attacks][ssrf]. Attackers may be\n  able to fetch files on your internal network, on external networks using your\n  application's identity, or simply excessively drive up your cloud network\n  bandwidth bill.\n\n  To prevent this, we recommend only using URLs from trusted sources/users or\n  validating them using allowlists or denylists.\n\n[ssrf]: https://portswigger.net/web-security/ssrf\n</Accordion>\n\n### `image`\n\nYou can use an image like this for models that support them:\n\n```rust\nfunction DescribeImage(myImg: image) -> string {\n  client GPT4Turbo\n  prompt #\"\n    {{ _.role(\"user\")}}\n    Describe the image in four words:\n    {{ myImg }}\n  \"#\n}\n```\n\nYou cannot name a variable `image` at the moment as it is a reserved keyword.\n\nCalling a function with an image type:\n\n<CodeBlocks>\n```python Python\nfrom baml_py import Image\nfrom baml_client import b\n\nasync def test_image_input():\n  # from URL\n  res = await b.TestImageInput(\n    img=Image.from_url(\"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\")\n  )\n\n  # Base64 image\n  image_b64 = \"iVBORw0K....\"\n  res = await b.TestImageInput(\n    img=Image.from_base64(\"image/png\", image_b64)\n  )\n```\n\n```typescript TypeScript\nimport { b } from '../baml_client'\nimport { Image } from \"@boundaryml/baml\"\n...\n\n  // URL\n  let res = await b.TestImageInput(\n    Image.fromUrl('https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png'),\n  )\n\n  // Base64\n  let res = await b.TestImageInput(\n    Image.fromBase64('image/png', image_b64),\n  )\n```\n\n```ruby Ruby\nrequire_relative \"baml_client/client\"\n\nb = Baml.Client\nImage = Baml::Image\n\ndef test_image_input\n  # from URL\n  res = b.TestImageInput(\n    img: Image.from_url(\"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\")\n  )\n\n  # Base64 image\n  image_b64 = \"iVBORw0K....\"\n  res = b.TestImageInput(\n    img: Image.from_base64(\"image/png\", image_b64)\n  )\nend\n```\n</CodeBlocks>\n\n<Accordion title=\"Pydantic compatibility\">\nIf using Pydantic, the following are valid ways to construct the `Image` type.\n\n```json\n{\n  \"url\": \"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\"\n}\n```\n\n```json\n{\n  \"url\": \"https://upload.wikimedia.org/wikipedia/en/4/4d/Shrek_%28character%29.png\",\n  \"media_type\": \"image/png\"\n}\n```\n\n```json\n{\n  \"base64\": \"iVBORw0K....\",\n}\n```\n\n```json\n{\n  \"base64\": \"iVBORw0K....\",\n  \"media_type\": \"image/png\"\n}\n```\n</Accordion>\n\n### `audio`\n\nExample\n```rust\nfunction DescribeSound(myAudio: audio) -> string {\n  client GPT4Turbo\n  prompt #\"\n    {{ _.role(\"user\")}}\n    Describe the audio in four words:\n    {{ myAudio }}\n  \"#\n}\n```\nCalling functions that have `audio` types.\n\n<CodeBlocks>\n```python Python\nfrom baml_py import Audio\nfrom baml_client import b\n\nasync def run():\n  # from URL\n  res = await b.TestAudioInput(\n      audio=Audio.from_url(\n          \"https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg\"\n      )\n  )\n\n  # Base64\n  b64 = \"iVBORw0K....\"\n  res = await b.TestAudioInput(\n    audio=Audio.from_base64(\"audio/ogg\", b64)\n  )\n```\n\n```typescript TypeScript\nimport { b } from '../baml_client'\nimport { Audio } from \"@boundaryml/baml\"\n...\n\n  // URL\n  let res = await b.TestAudioInput(\n    Audio.fromUrl('https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg'),\n  )\n\n  // Base64\n  const audio_base64 = \"..\"\n  let res = await b.TestAudioInput(\n    Audio.fromBase64('audio/ogg', audio_base64),\n  )\n  \n```\n\n```ruby Ruby\nrequire_relative \"baml_client/client\"\n\nb = Baml.Client\nAudio = Baml::Audio\n\ndef test_audio_input\n  # from URL\n  res = b.TestAudioInput(\n      audio: Audio.from_url(\n          \"https://actions.google.com/sounds/v1/emergency/beeper_emergency_call.ogg\"\n      )\n  )\n\n  # Base64 image\n  audio_b64 = \"iVBORw0K....\"\n  res = b.TestAudioInput(\n    audio: Audio.from_base64(\"audio/mp3\", audio_b64)\n  )\nend\n```\n</CodeBlocks>\n\n## Composite/Structured Types\n\n### enum\n\n**See also:** [Enum](/docs/snippets/enum)\n\nA user-defined type consisting of a set of named constants.\nUse it when you need a model to choose from a known set of values, like in classification problems\n\n```baml\nenum Name {\n  Value1\n  Value2 @description(\"My optional description annotation\")\n}\n```\n\nIf you need to add new variants, because they need to be loaded from a file or fetched dynamically\nfrom a database, you can do this with [Dynamic Types](/guide/baml-advanced/dynamic-runtime-types).\n\n### class\n\n**See also:** [Class](/docs/snippets/class)\n\nClasses are for user-defined complex data structures.\n\nUse when you need an LLM to call another function (e.g. OpenAI's function calling), you can model the function's parameters as a class. You can also get models to return complex structured data by using a class.\n\n**Example:**\n\nNote that properties have no `:`\n```baml\nclass Car {\n  model string\n  year int @description(\"Year of manufacture\")\n}\n```\n\nIf you need to add fields to a class because some properties of your class are only\nknown at runtime, you can do this with [Dynamic Types](/docs/calling-baml/dynamic-types).\n\n### Optional (?)\n\nA type that represents a value that might or might not be present.\n\nUseful when a variable might not have a value and you want to explicitly handle its absence.\n\n**Syntax:** `Type?`\n\n**Example:** `int?` or `(MyClass | int)?`\n\n### Union (|)\n\nA type that can hold one of several specified types.\n\nThis can be helpful with **function calling**, where you want to return different types of data depending on which function should be called.\n\n**Syntax:** `Type1 | Type2`\n\n**Example:** `int | string` or `(int | string) | MyClass` or `string | MyClass | int[]`\n\n<Warning>\n  Order is important. `int | string` is not the same as `string | int`.\n\n  For example, if you have a `\"1\"` string, it will be parsed as an `int` if\n  you use `int | string`, but as a `string` if you use `string | int`.\n</Warning>\n\n### List/Array ([])\n\nA collection of elements of the same type.\n\n**Syntax:** `Type[]`\n\n**Example:** `string[]` or `(int | string)[]` or `int[][]`\n\n<Tip>\n  * Array types can be nested to create multi-dimensional arrays\n  * An array type cannot be optional\n</Tip>\n\n### Map\n\nA mapping of strings to elements of another type.\n\n**Syntax**: `map<string, ValueType>`\n\n**Example**: `map<string, string>`\n\n{/* <Info>\n  For TS users: `map<string, ValueType>` will generate a \n  `Record<string, ValueType>` type annotation, but using any other type for the\n  key will generate a `Map`, e.g. `map<int, string>` in BAML will generate a\n  `Map<number, string>` type annotation in TypeScript.\n</Info> */}\n\n### ❌ Set\n\n- Not yet supported. Use a `List` instead.\n\n### ❌ Tuple\n\n- Not yet supported. Use a `class` instead.\n\n## Examples and Equivalents\n\nHere are some examples and what their equivalents are in different languages.\n\n### Example 1\n\n<CodeBlocks>\n```baml BAML\nint? | string[] | MyClass\n````\n\n```python Python Equivalent\nUnion[Optional[int], List[str], MyClass]\n```\n\n```typescript TypeScript Equivalent\n(number | null) | string[] | MyClass\n```\n\n</CodeBlocks>\n\n### Example 2\n\n<CodeBlocks>\n```baml BAML\nstring[]\n```\n\n```python Python Equivalent\nList[str]\n```\n\n```typescript TypeScript Equivalent\nstring[]\n```\n\n</CodeBlocks>\n\n### Example 3\n\n<CodeBlocks>\n```baml BAML\n(int | float)[]\n```\n```python Python Equivalent\nList[Union[int, float]]\n```\n\n```typescript TypeScript Equivalent\nnumber[]\n```\n\n</CodeBlocks>\n\n### Example 4\n\n<CodeBlocks>\n```baml BAML\n(int? | string[] | MyClass)[]\n```\n\n```python Python Equivalent\nOptional[List[Union[Optional[int], List[str], MyClass]]]\n```\n\n```typescript TypeScript Equivalent\n((number | null) | string[] | MyClass)[]\n```\n\n</CodeBlocks>\n\n### Example 5\n\n<CodeBlocks>\n```baml BAML\n\"str\" | 1 | false\n```\n\n```python Python Equivalent\nUnion[Literal[\"str\"], Literal[1], Literal[False]]\n```\n\n```typescript TypeScript Equivalent\n\"str\" | 1 | false\n```\n\n</CodeBlocks>\n\n## ⚠️ Unsupported\n- `any/json` - Not supported. We don't want to encourage its use as it defeats the purpose of having a type system. if you really need it, for now use `string` and call `json.parse` yourself or use [dynamic types](/guide/baml-advanced/dynamic-runtime-types)\n- `datetime` - Not yet supported. Use a `string` instead.\n- `duration` - Not yet supported. We recommend using `string` and specifying that it must be an \"ISO8601 duration\" in the description, which you can parse yourself into a duration.\n- `units (currency, temperature)` - Not yet supported. Use a number (`int` or `float`) and have the unit be part of the variable name. For example, `temperature_fahrenheit` and `cost_usd` (see [@alias](/ref/baml/class))\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/types.mdx"
      },
      "03-reference/baml/function.mdx": {
        "markdown": "Functions in BAML define the contract between your application and AI models, providing type-safe interfaces for AI operations.\n\n## Overview\n\nA BAML function consists of:\n- Input parameters with explicit types\n- A return type specification\n- An [LLM client](client-llm)\n- A prompt (as a [block string](general-baml-syntax/string#block-strings))\n\n```baml\nfunction FunctionName(param: Type) -> ReturnType {\n    client ModelName\n    prompt #\"\n        Template content\n    \"#\n}\n```\n\n## Function Declaration\n\n### Syntax\n\n```baml\nfunction name(parameters) -> return_type {\n    client llm_specification\n    prompt block_string_specification\n}\n```\n\n### Parameters\n\n- `name`: The function identifier (must start with a capital letter!)\n- `parameters`: One or more typed parameters (e.g., `text: string`, `data: CustomType`)\n- `return_type`: The type that the function guarantees to return (e.g., `string | MyType`)\n- `llm_specification`: The LLM to use (e.g., `\"openai/gpt-4o-mini\"`, `GPT4Turbo`, `Claude2`)\n- `block_string_specification`: The prompt template using Jinja syntax\n\n## Type System\n\nFunctions leverage BAML's strong type system, supporting:\n\n### Built-in Types\n- `string`: Text data\n- `int`: Integer numbers\n- `float`: Decimal numbers\n- `bool`: True/false values\n- `array`: Denoted with `[]` suffix (e.g., `string[]`)\n- `map`: Key-value pairs (e.g., `map<string, int>`)\n- `literal`: Specific values (e.g., `\"red\" | \"green\" | \"blue\"`)\n- [See all](types)\n\n### Custom Types\n\nCustom types can be defined using class declarations:\n\n```baml\nclass CustomType {\n    field1 string\n    field2 int\n    nested NestedType\n}\n\nfunction ProcessCustomType(data: CustomType) -> ResultType {\n    // ...\n}\n```\n\n## Prompt Templates\n\n### Jinja Syntax\n\nBAML uses Jinja templating for dynamic prompt generation:\n\n```baml\nprompt #\"\n    Input data: {{ input_data }}\n    \n    {% if condition %}\n        Conditional content\n    {% endif %}\n    \n    {{ ctx.output_format }}\n\"#\n```\n\n### Special Variables\n\n- `ctx.output_format`: Automatically generates format instructions based on return type\n- `ctx.client`: Selected client and model name\n- `_.role`: Define the role of the message chunk\n\n## Error Handling\n\nFunctions automatically handle common AI model errors and provide type validation:\n\n- JSON parsing errors are automatically corrected\n- Type mismatches are detected and reported\n- Network and rate limit errors are propagated to the caller\n\n## Usage Examples\n\n### Basic Function\n\n```baml\nfunction ExtractEmail(text: string) -> string {\n    client GPT4Turbo\n    prompt #\"\n        Extract the email address from the following text:\n        {{ text }}\n        \n        {{ ctx.output_format }}\n    \"#\n}\n```\n\n### Complex Types\n\n```baml\nclass Person {\n    name string\n    age int\n    contacts Contact[]\n}\n\nclass Contact {\n    type \"email\" | \"phone\"\n    value string\n}\n\nfunction ParsePerson(data: string) -> Person {\n    client \"openai/gpt-4o\"\n    prompt #\"\n        {{ ctx.output_format }}\n        \n        {{ _.role('user') }}\n        {{ data }}\n    \"#\n}\n```\n\n## `baml_client` Integration\n\n<CodeBlocks>\n\n```python Python\nfrom baml_client import b\nfrom baml_client.types import Person\n\nasync def process() -> Person:\n    result = b.ParsePerson(\"John Doe, 30 years old...\")\n    print(result.name)  # Type-safe access\n    return result\n```\n\n\n```typescript TypeScript\nimport { b } from 'baml-client';\nimport { Person } from 'baml-client/types';\n\nasync function process(): Promise<Person> {\n    const result = await b.ParsePerson(\"John Doe, 30 years old...\");\n    console.log(result.name);  // Type-safe access\n    return result;\n}\n```\n\n</CodeBlocks>\n\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/function.mdx"
      },
      "03-reference/baml/test.mdx": {
        "markdown": "Tests are first-class citizens in BAML, designed to make testing AI functions straightforward and robust. BAML tests can be written anywhere in your codebase and run with minimal setup.\n\n## Overview\n\nA BAML test consists of:\n- Test name and metadata\n- Functions under test\n- Input arguments\n- Optional testing configuration\n- Optional assertions\n\n```baml\ntest TestName {\n    functions [FunctionName]\n    args {\n        paramName \"value\"\n    }\n}\n```\n\n## Test Declaration\n\n### Syntax\n\n```baml\ntest name {\n    functions [function_list]\n    args {\n        parameter_assignments\n    }\n}\n```\n\n### Components\n\n- `name`: Test identifier (unique per function)\n- `functions`: List of functions to test\n- `args`: Input parameters for the test case\n\n## Input Types\n\n### Basic Types\n\nSimple values are provided directly:\n\n```baml\ntest SimpleTest {\n    functions [ClassifyMessage]\n    args {\n        input \"Can't access my account\"\n    }\n}\n```\n\n### Complex Objects\n\nObjects are specified using nested structures:\n\n```baml\ntest ComplexTest {\n    functions [ProcessMessage]\n    args {\n        message {\n            user \"john_doe\"\n            content \"Hello world\"\n            metadata {\n                timestamp 1234567890\n                priority \"high\"\n            }\n        }\n    }\n}\n```\n\n### Arrays\n\nArrays use bracket notation:\n\n```baml\ntest ArrayTest {\n    functions [BatchProcess]\n    args {\n        messages [\n            {\n                user \"user1\"\n                content \"Message 1\"\n            }\n            {\n                user \"user2\"\n                content \"Message 2\"\n            }\n        ]\n    }\n}\n```\n\n## Media Inputs\n\n### Images\n\nImages can be specified using three methods:\n\n1. **File Reference**\n```baml {4-6}\ntest ImageFileTest {\n    functions [AnalyzeImage]\n    args {\n        param {\n            file \"../images/test.png\"\n        }\n    }\n}\n```\n\n2. **URL Reference**\n```baml {4-6}\ntest ImageUrlTest {\n    functions [AnalyzeImage]\n    args {\n        param {\n            url \"https://example.com/image.jpg\"\n        }\n    }\n}\n```\n\n3. **Base64 Data**\n```baml {4-7}\ntest ImageBase64Test {\n    functions [AnalyzeImage]\n    args {\n        param {\n            base64 \"a41f...\"\n            media_type \"image/png\"\n        }\n    }\n}\n```\n\n### Audio\n\nSimilar to images, audio can be specified in three ways:\n\n1. **File Reference**\n```baml\ntest AudioFileTest {\n    functions [TranscribeAudio]\n    args {\n        audio {\n            file \"../audio/sample.mp3\"\n        }\n    }\n}\n```\n\n2. **URL Reference**\n```baml\ntest AudioUrlTest {\n    functions [TranscribeAudio]\n    args {\n        audio {\n            url \"https://example.com/audio.mp3\"\n        }\n    }\n}\n```\n\n3. **Base64 Data**\n```baml\ntest AudioBase64Test {\n    functions [TranscribeAudio]\n    args {\n        audio {\n            base64 \"...\"\n            media_type \"audio/mp3\"\n        }\n    }\n}\n```\n\n## Multi-line Strings\n\nFor long text inputs, use the block string syntax:\n\n```baml\ntest LongTextTest {\n    functions [AnalyzeText]\n    args {\n        content #\"\n            This is a multi-line\n            text input that preserves\n            formatting and whitespace\n        \"#\n    }\n}\n```\n\n## Testing Multiple Functions\n\nThis requires each function to have teh exact same parameters:\n\n```baml\ntest EndToEndFlow {\n    functions [\n        ExtractInfo\n        ProcessInfo\n        ValidateResult\n    ]\n    args {\n        input \"test data\"\n    }\n}\n```\n\n## Integration with Development Tools\n\n### VSCode Integration\n\n- Tests can be run directly from the BAML playground\n- Real-time syntax validation\n- Test result visualization\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/test.mdx"
      },
      "03-reference/baml/template_string.mdx": {
        "markdown": "\nWriting prompts requires a lot of string manipulation. BAML has a `template_string` to let you combine different string templates together. Under-the-hood they use [jinja](/ref/prompt-syntax/what-is-jinja) to evaluate the string and its inputs.\n\nThink of template strings as functions that have variables, and return a string. They can be used to define reusable parts of a prompt, or to make the prompt more readable by breaking it into smaller parts.\n\nExample\n```baml BAML\n// Inject a list of \"system\" or \"user\" messages into the prompt.\ntemplate_string PrintMessages(messages: Message[]) #\"\n  {% for m in messages %}\n    {{ _.role(m.role) }}\n    {{ m.message }}\n  {% endfor %}\n\"#\n\nfunction ClassifyConversation(messages: Message[]) -> Category[] {\n  client GPT4Turbo\n  prompt #\"\n    Classify this conversation:\n    {{ PrintMessages(messages) }}\n\n    Use the following categories:\n    {{ ctx.output_format}}\n  \"#\n}\n```\n\nIn this example we can call the template_string `PrintMessages` to subdivide the prompt into \"user\" or \"system\" messages using `_.role()` (see [message roles](/ref/prompt-syntax/role)). This allows us to reuse the logic for printing messages in multiple prompts. \n\nYou can nest as many template strings inside each other and call them however many times you want.\n\n<Warning>\n  The BAML linter may give you a warning when you use template strings due to a static analysis limitation. You can ignore this warning. If it renders in the playground, you're good!\n</Warning>\nUse the playground preview to ensure your template string is being evaluated correctly!\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/template_string.mdx"
      },
      "03-reference/baml/client-llm.mdx": {
        "markdown": "\nClients are used to configure how LLMs are called, like so:\n\n```rust BAML\nfunction MakeHaiku(topic: string) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Write a haiku about {{ topic }}.\n  \"#\n}\n```\n\nThis is `<provider>/<model>` shorthand for:\n\n```rust BAML\nclient<llm> MyClient {\n  provider \"openai\"\n  options {\n    model \"gpt-4o\"\n    // api_key defaults to env.OPENAI_API_KEY\n  }\n}\n\nfunction MakeHaiku(topic: string) -> string {\n  client MyClient\n  prompt #\"\n    Write a haiku about {{ topic }}.\n  \"#\n}\n```\n\nConsult the [provider documentation](#fields) for a list of supported providers\nand models, and the default options.\n\nIf you want to override options like `api_key` to use a different environment\nvariable, or you want to point `base_url` to a different endpoint, you should use\nthe latter form.\n\n<Tip>\nIf you want to specify which client to use at runtime, in your Python/TS/Ruby code,\nyou can use the [client registry](/guide/baml-advanced/llm-client-registry) to do so.\n\nThis can come in handy if you're trying to, say, send 10% of your requests to a\ndifferent model.\n</Tip>\n\n## Fields\n\n<ParamField path=\"provider\" type=\"string\" required>\nThis configures which provider to use. The provider is responsible for handling the actual API calls to the LLM service. The provider is a required field.\n\nThe configuration modifies the URL request BAML runtime makes.\n\n| Provider Name    | Docs                                                                | Notes                                                      |\n| ---------------- | ------------------------------------------------------------------- | ---------------------------------------------------------- |\n| `anthropic`      | [Anthropic](/docs/snippets/clients/providers/anthropic)             |                                                            |\n| `aws-bedrock`    | [AWS Bedrock](/docs/snippets/clients/providers/aws-bedrock)         |                                                            |\n| `azure-openai`   | [Azure OpenAI](/docs/snippets/clients/providers/azure)              |                                                            |\n| `google-ai`      | [Google AI](/docs/snippets/clients/providers/gemini)                |                                                            |\n| `openai`         | [OpenAI](/docs/snippets/clients/providers/openai)                   |                                                            |\n| `openai-generic` | [OpenAI (generic)](/docs/snippets/clients/providers/openai-generic) | Any model provider that supports an OpenAI-compatible API  |\n| `vertex-ai`      | [Vertex AI](/docs/snippets/clients/providers/vertex)                |                                                            |\n\nWe also have some special providers that allow composing clients together:\n| Provider Name  | Docs                             | Notes                                                      |\n| -------------- | -------------------------------- | ---------------------------------------------------------- |\n| `fallback`     | [Fallback](/docs/snippets/clients/fallback)             | Used to chain models conditional on failures               |\n| `round-robin`  | [Round Robin](/docs/snippets/clients/round-robin)       | Used to load balance                                       |\n\n</ParamField>\n\n<ParamField path=\"options\" type=\"dict[str, Any]\" required>\nThese vary per provider. Please see provider specific documentation for more\ninformation. Generally they are pass through options to the POST request made\nto the LLM.\n</ParamField>\n\n\n\n<ParamField path=\"retry_policy\">\n  The name of the retry policy. See [Retry\n  Policy](/ref/client-strategies/retry-policy).\n</ParamField>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/client-llm.mdx"
      },
      "03-reference/baml/class.mdx": {
        "markdown": "\nClasses consist of a name, a list of properties, and their [types](class).\nIn the context of LLMs, classes describe the type of the variables you can inject into prompts and extract out from the response.\n\n<Warning>\n  Note properties have no `:`\n</Warning>\n\n<CodeBlocks>\n```baml Baml\nclass Foo {\n  property1 string\n  property2 int?\n  property3 Bar[]\n  property4 MyEnum\n}\n```\n\n```python Python Equivalent\nfrom pydantic import BaseModel\nfrom path.to.bar import Bar\nfrom path.to.my_enum import MyEnum\n\nclass Foo(BaseModel):\n  property1: str\n  property2: Optional[int]= None\n  property3: List[Bar]\n  property4: MyEnum\n```\n\n```typescript Typescript Equivalent\nimport z from \"zod\";\nimport { BarZod } from \"./path/to/bar\";\nimport { MyEnumZod } from \"./path/to/my_enum\";\n\nconst FooZod = z.object({\n  property1: z.string(),\n  property2: z.number().int().nullable().optional(),\n  property3: z.array(BarZod),\n  property4: MyEnumZod,\n});\n\ntype Foo = z.infer<typeof FooZod>;\n```\n\n</CodeBlocks>\n\n## Field Attributes\n\nWhen prompt engineering, you can also alias values and add descriptions.\n\n<ParamField\n  path=\"@alias\"\n  type=\"string\"\n>\nAliasing renames the field for the llm to potentially \"understand\" your value better, while keeping the original name in your code, so you don't need to change your downstream code everytime.\n\nThis will also be used for parsing the output of the LLM back into the original object.\n</ParamField>\n\n<ParamField\n  path=\"@description\"\n  type=\"string\"\n>\nThis adds some additional context to the field in the prompt.\n</ParamField>\n\n\n```baml BAML\nclass MyClass {\n  property1 string @alias(\"name\") @description(\"The name of the object\")\n  age int? @description(\"The age of the object\")\n}\n```\n\n## Class Attributes\n\n<ParamField\n  path=\"@@dynamic\"\n>\nIf set, will allow you to add fields to the class dynamically at runtime (in your python/ts/etc code). See [dynamic classes](/guide/baml-advanced/dynamic-runtime-types) for more information.\n</ParamField>\n\n\n```baml BAML\nclass MyClass {\n  property1 string\n  property2 int?\n\n  @@dynamic // allows me to later propert3 float[] at runtime\n}\n```\n\n## Syntax\n\nClasses may have any number of properties.\nProperty names must follow these rules:\n- Must start with a letter\n- Must contain only letters, numbers, and underscores\n- Must be unique within the class\n- classes cannot be self-referential (cannot have a property of the same type as the class itself)\n\nThe type of a property can be any [supported type](supported-types)\n\n### Default values\n\n- Not yet supported. For optional properties, the default value is `None` in python.\n\n### Dynamic classes\n\nSee [Dynamic Types](/guide/baml-advanced/dynamic-runtime-types).\n\n## Inheritance\n\nNever supported. Like rust, we take the stance that [composition is better than inheritance](https://www.digitalocean.com/community/tutorials/composition-vs-inheritance).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/class.mdx"
      },
      "03-reference/baml/enum.mdx": {
        "markdown": "Enums are useful for classification tasks. BAML has helper functions that can help you serialize an enum into your prompt in a neatly formatted list (more on that later).\n\nTo define your own custom enum in BAML:\n\n<CodeBlocks>\n```baml BAML\nenum MyEnum {\n  Value1\n  Value2\n  Value3\n}\n```\n\n```python Python Equivalent\nfrom enum import StrEnum\n\nclass MyEnum(StrEnum):\n  Value1 = \"Value1\"\n  Value2 = \"Value2\"\n  Value3 = \"Value3\"\n```\n\n```typescript Typescript Equivalent\nenum MyEnum {\n  Value1 = \"Value1\",\n  Value2 = \"Value2\",\n  Value3 = \"Value3\",\n}\n```\n\n</CodeBlocks>\n\n- You may have as many values as you'd like.\n- Values may not be duplicated or empty.\n- Values may not contain spaces or special characters and must not start with a number.\n\n## Enum Attributes\n\n<ParamField\n  path=\"@@alias\"\n  type=\"string\"\n>\nThis is the name of the enum rendered in the prompt.\n</ParamField>\n\n\n<ParamField\n  path=\"@@dynamic\"\n>\nIf set, will allow you to add/remove/modify values to the enum dynamically at runtime (in your python/ts/etc code). See [dynamic enums](/guide/baml-advanced/dynamic-runtime-types) for more information.\n</ParamField>\n\n\n```baml BAML\nenum MyEnum {\n  Value1\n  Value2\n  Value3\n\n  @@alias(\"My Custom Enum\")\n  @@dynamic // allows me to later skip Value2 at runtime\n}\n```\n\n## Value Attributes\n\nWhen prompt engineering, you can also alias values and add descriptions, or even skip them.\n\n<ParamField\n  path=\"@alias\"\n  type=\"string\"\n>\nAliasing renames the values for the llm to potentially \"understand\" your value better, while keeping the original name in your code, so you don't need to change your downstream code everytime.\n\nThis will also be used for parsing the output of the LLM back into the enum.\n</ParamField>\n\n<ParamField\n  path=\"@description\"\n  type=\"string\"\n>\nThis adds some additional context to the value in the prompt.\n</ParamField>\n\n<ParamField\n  path=\"@skip\"\n>\nSkip this value in the prompt and during parsing.\n</ParamField>\n\n\n```baml BAML\nenum MyEnum {\n  Value1 @alias(\"complete_summary\") @description(\"Answer in 2 sentences\")\n  Value2\n  Value3 @skip\n  Value4 @description(#\"\n    This is a long description that spans multiple lines.\n    It can be useful for providing more context to the value.\n  \"#)\n}\n```\n\n\nSee more in [prompt syntax docs](/ref/prompt-syntax/what-is-jinja)\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/enum.mdx"
      },
      "03-reference/generator.mdx": {
        "markdown": "\n\nEach `generator` that you define in your BAML project will tell `baml-cli\ngenerate` to generate code for a specific target language. You can define\nmultiple `generator` clauses in your BAML project, and `baml-cli generate` will\ngenerate code for each of them.\n\n<Tip>If you created your project using `baml-cli init`, then one has already been generated for you!</Tip>\n\n\n<CodeBlocks>\n\n```baml Python\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"python/pydantic\"\n    \n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    \n    // What interface you prefer to use for the generated code (sync/async)\n    // Both are generated regardless of the choice, just modifies what is exported\n    // at the top level\n    default_client_mode \"sync\"\n    \n    // Version of runtime to generate code for (should match installed baml-py version)\n    version \"0.63.0\"\n}\n```\n\n```baml TypeScript\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"typescript\"\n    \n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    \n    // What interface you prefer to use for the generated code (sync/async)\n    // Both are generated regardless of the choice, just modifies what is exported\n    // at the top level\n    default_client_mode \"async\"\n    \n    // Version of runtime to generate code for (should match the package @boundaryml/baml version)\n    version \"0.63.0\"\n}\n```\n\n```baml Ruby (beta)\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\"\n    output_type \"ruby/sorbet\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    \n    // Version of runtime to generate code for (should match installed `baml` package version)\n    version \"0.63.0\"\n}\n```\n\n```baml OpenAPI\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"rest/openapi\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n    \n    // Version of runtime to generate code for (should match installed `baml` package version)\n    version \"0.54.0\"\n\n    // 'baml-cli generate' will run this after generating openapi.yaml, to generate your OpenAPI client\n    // This command will be run from within $output_dir\n    on_generate \"npx @openapitools/openapi-generator-cli generate -i openapi.yaml -g OPENAPI_CLIENT_TYPE -o .\"\n}\n```\n\n</CodeBlocks>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/generator.mdx"
      },
      "03-reference/baml/attributes/attributes-overview.mdx": {
        "markdown": "In BAML, attributes are used to provide additional metadata or behavior to fields and types. They can be applied at different levels, such as field-level or block-level, depending on their intended use.\n\n## Field-Level Attributes\n\nField-level attributes are applied directly to individual fields within a class or enum. They modify the behavior or metadata of that specific field.\n\n### Examples of Field-Level Attributes\n\n- **`@alias`**: Renames a field for better understanding by the LLM.\n- **`@description`**: Provides additional context to a field.\n- **`@skip`**: Excludes a field from prompts or parsing.\n- **`@assert`**: Applies strict validation to a field.\n- **`@check`**: Adds non-exception-raising validation to a field.\n\n```baml BAML\nclass MyClass {\n  property1 string @alias(\"name\") @description(\"The name of the object\")\n  age int? @check(positive, {{ this > 0 }})\n}\n```\n\n## Block-Level Attributes\n\nBlock-level attributes are applied to an entire class or enum, affecting all fields or values within that block. They are used to modify the behavior or metadata of the entire block.\n\n### Examples of Block-Level Attributes\n\n- **`@@dynamic`**: Allows dynamic modification of fields or values at runtime.\n\n```baml BAML\nclass MyClass {\n  property1 string\n  property2 int?\n\n  @@dynamic // allows adding fields dynamically at runtime\n}\n```\n\n## Key Differences\n\n- **Scope**: Field-level attributes affect individual fields, while block-level attributes affect the entire class or enum.\n- **Usage**: Field-level attributes are used for specific field modifications, whereas block-level attributes are used for broader modifications affecting the whole block.\n\nUnderstanding the distinction between these types of attributes is crucial for effectively using BAML to define and manipulate data structures.\n\nFor more detailed information on each attribute, refer to the specific attribute pages in this section. \n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/attributes/attributes-overview.mdx"
      },
      "03-reference/baml/attributes/alias.mdx": {
        "markdown": "The `@alias` attribute in BAML is used to rename fields or values for better understanding by the LLM, while keeping the original name in your code. This is particularly useful for prompt engineering, as it allows you to provide a more intuitive name for the LLM without altering your existing codebase.\n\n## Prompt Impact (class)\n\n### Without `@alias`\n\n```baml BAML\nclass MyClass {\n  property1 string\n}\n```\n\n**ctx.output_format:**\n\n```\n{\n  property1: string\n}\n```\n\n### With `@alias`\n\n```baml BAML\nclass MyClass {\n  property1 string @alias(\"name\")\n}\n```\n\n**ctx.output_format:**\n\n```\n{\n  name: string\n}\n```\n\n## Prompt Impact (enum)\n\n```baml BAML\nenum MyEnum {\n  Value1 \n  // Note that @@alias is applied to the enum itself, not the value\n  @@alias(\"My Name\")\n}\n```\n\n**ctx.output_format:**\n\n```\nMy Name\n---\nValue1\n```\n\n## Prompt Impact (enum value)\n\n```baml BAML\nenum MyEnum {\n  Value1 @alias(\"Something\")\n}\n```\n\n**ctx.output_format:**\n\n```\nMyEnum\n---\nSomething\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/attributes/alias.mdx"
      },
      "03-reference/baml/attributes/description.mdx": {
        "markdown": "The `@description` attribute in BAML provides additional context to fields or values in prompts. This can help the LLM understand the intended use or meaning of a field or value.\n\n## Prompt Impact\n\n### Without `@description`\n\n```baml BAML\nclass MyClass {\n  property1 string\n}\n```\n\n**ctx.output_format:**\n\n```\n{\n  property1: string\n}\n```\n\n### With `@description`\n\n```baml BAML\nclass MyClass {\n  property1 string @description(\"The name of the object\")\n}\n```\n\n**ctx.output_format:**\n\n```\n{\n  // The name of the object\n  property1: string\n}\n```\n\n## Prompt Impact (enum - value)\n\n### Without `@description`\n\n```baml BAML\nenum MyEnum {\n  Value1\n  Value2\n}\n```\n\n**ctx.output_format:**\n\n```\nMyEnum\n---\nValue1\nValue2\n```\n\n### With `@description`\n\n```baml BAML\nenum MyEnum {\n  Value1 @description(\"The first value\")\n  Value2 @description(\"The second value\")\n}\n```\n\n**ctx.output_format:**\n\n```\nMyEnum\n---\nValue1 // The first value\nValue2 // The second value\n```\n\n## Prompt Impact (enum)\n\n```baml BAML\nenum MyEnum {\n  Value1\n  Value2\n\n  @@description(\"This enum represents status codes\")\n}\n```\n\n**ctx.output_format:**\n\n```\nMyEnum: This enum represents status codes\n---\nValue1\nValue2\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/attributes/description.mdx"
      },
      "03-reference/baml/attributes/skip.mdx": {
        "markdown": "The `@skip` attribute in BAML is used to exclude certain fields or values from being included in prompts or parsed responses. This can be useful when certain data is not relevant for the LLM's processing.\n\n## Prompt Impact\n\n### Without `@skip`\n\n```baml BAML\nenum MyEnum {\n  Value1\n  Value2\n}\n```\n\n**ctx.output_format:**\n\n```\nMyEnum\n---\nValue1\nValue2\n```\n\n### With `@skip`\n\n```baml BAML\nenum MyEnum {\n  Value1\n  Value2 @skip\n}\n```\n\n**ctx.output_format:**\n\n```\nMyEnum\n---\nValue1\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/attributes/skip.mdx"
      },
      "03-reference/baml/attributes/assert.mdx": {
        "markdown": "The `@assert` attribute in BAML is used for strict validations. If a type fails an `@assert` validation, it will not be returned in the response, and an exception will be raised if it's part of the top-level type.\n\n## Usage\n\nAsserts can be named or unnamed.\n\n### Field Assertion\n\n```baml BAML\nclass Foo {\n  // @assert will be applied to the field with the name \"bar\"\n  bar int @assert(between_0_and_10, {{ this > 0 and this < 10 }})\n}\n```\n\n```baml BAML\nclass Foo {\n  // @assert will be applied to the field with no name\n  bar int @assert({{ this > 0 and this < 10 }})\n}\n```\n\n```baml BAML\nclass MyClass {\n  // @assert will be applied to each element in the array\n  my_field (string @assert(is_valid_email, {{ this.contains(\"@\") }}))[]\n}\n```\n\n### Parameter Assertion\n\nAsserts can also be applied to parameters.\n\n```baml BAML\nfunction MyFunction(x: int @assert(between_0_and_10, {{ this > 0 and this < 10 }})) {\n  client \"openai/gpt-4o\"\n  prompt #\"Hello, world!\"#\n}\n```\n\n### Block Assertion\n\nAsserts can be used in a block definition, referencing fields within the block.\n\n```baml BAML\nclass Foo {\n  bar int\n  baz string\n  @@assert(baz_length_limit, {{ this.baz|length < this.bar }})\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/attributes/assert.mdx"
      },
      "03-reference/baml/attributes/check.mdx": {
        "markdown": "The `@check` attribute in BAML adds validations without raising exceptions if they fail. This allows the validations to be inspected at runtime.\n\n## Usage\n\n### Field Check\n\n```baml BAML\nclass Foo {\n  bar int @check(less_than_zero, {{ this < 0 }})\n}\n```\n\n### Block check\n\n```baml\nclass Bar {\n  baz int\n  quux string\n  @@check(quux_limit, {{ this.quux|length < this.baz }})\n}\n```\n\n## Benefits\n\n- **Non-Intrusive Validation**: Allows for validation checks without interrupting the flow of data processing.\n- **Runtime Inspection**: Enables inspection of validation results at runtime.\n\nSee more in [validations guide](/guide/baml-advanced/validations).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/attributes/check.mdx"
      },
      "03-reference/baml/attributes/dynamic.mdx": {
        "markdown": "The `@@dynamic` attribute in BAML allows for the dynamic modification of fields or values at runtime. This is particularly useful when you need to adapt the structure of your data models based on runtime conditions or external inputs.\n\n## Usage\n\n### Dynamic Classes\n\nThe `@@dynamic` attribute can be applied to classes, enabling the addition of fields dynamically during runtime.\n\n```baml BAML\nclass MyClass {\n  property1 string\n  property2 int?\n\n  @@dynamic // allows adding fields dynamically at runtime\n}\n```\n\n### Dynamic Enums\n\nSimilarly, the `@@dynamic` attribute can be applied to enums, allowing for the modification of enum values at runtime.\n\n```baml BAML\nenum MyEnum {\n  Value1\n  Value2\n\n  @@dynamic // allows modifying enum values dynamically at runtime\n}\n```\n\n## Using `@@dynamic` with TypeBuilder\n\nTo modify dynamic types at runtime, you can use the `TypeBuilder` from the `baml_client`. Below are examples for Python, TypeScript, and Ruby.\n\n### Python Example\n\n```python\nfrom baml_client.type_builder import TypeBuilder\nfrom baml_client import b\n\nasync def run():\n  tb = TypeBuilder()\n  tb.MyClass.add_property('email', tb.string())\n  tb.MyClass.add_property('address', tb.string()).description(\"The user's address\")\n  res = await b.DynamicUserCreator(\"some user info\", { \"tb\": tb })\n  # Now res can have email and address fields\n  print(res)\n```\n\n### TypeScript Example\n\n```typescript\nimport TypeBuilder from '../baml_client/type_builder'\nimport { b } from '../baml_client'\n\nasync function run() {\n  const tb = new TypeBuilder()\n  tb.MyClass.addProperty('email', tb.string())\n  tb.MyClass.addProperty('address', tb.string()).description(\"The user's address\")\n  const res = await b.DynamicUserCreator(\"some user info\", { tb: tb })\n  // Now res can have email and address fields\n  console.log(res)\n}\n```\n### Ruby Example\n\n```ruby\nrequire_relative 'baml_client/client'\n\ndef run\n  tb = Baml::TypeBuilder.new\n  tb.MyClass.add_property('email', tb.string)\n  tb.MyClass.add_property('address', tb.string).description(\"The user's address\")\n  \n  res = Baml::Client.dynamic_user_creator(input: \"some user info\", baml_options: {tb: tb})\n  # Now res can have email and address fields\n  puts res\nend\n```\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/attributes/dynamic.mdx"
      },
      "03-reference/baml/clients/providers/aws-bedrock.mdx": {
        "markdown": "---\ntitle: aws-bedrock\nsubtitle: AWS Bedrock provider for BAML\n---\n\n\nThe `aws-bedrock` provider supports all text-output models available via the\n[`Converse` API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).\n\nExample:\n\n```baml BAML\nclient<llm> MyClient {\n  provider aws-bedrock\n  options {\n    api_key env.MY_OPENAI_KEY\n    model \"gpt-3.5-turbo\"\n    temperature 0.1\n  }\n}\n```\n\n## Authorization\n\nWe use the AWS SDK under the hood, which will respect [all authentication\nmechanisms supported by the\nSDK](https://docs.rs/aws-config/latest/aws_config/index.html), including but not\nlimited to:\n\n  - `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` as set in your environment variables\n  - loading the specified `AWS_PROFILE` from `~/.aws/config`\n  - built-in authn for services running in EC2, ECS, Lambda, etc.\n\n\n## Playground setup\nAdd these three environment variables to your extension variables to use the AWS Bedrock provider in the playground.\n\n- `AWS_ACCESS_KEY_ID`\n- `AWS_SECRET_ACCESS_KEY`\n- `AWS_REGION` - like `us-east-1`\n<img src=\"file:2c00e872-0f76-48d3-b852-d2d6c988643a\" width=\"400px\" />\n\n## Non-forwarded options\n\n<ParamField\n  path=\"default_role\"\n  type=\"string\"\n>\n  The default role for any prompts that don't specify a role. **Default: `system`**\n\n  We don't have any checks for this field, you can pass any string you wish.\n</ParamField>\n\n<ParamField\n  path=\"allowed_role_metadata\"\n  type=\"string[]\"\n>\n  Which role metadata should we forward to the API? **Default: `[]`**\n\n  For example you can set this to `[\"foo\", \"bar\"]` to forward the cache policy to the API.\n\n  If you do not set `allowed_role_metadata`, we will not forward any role metadata to the API even if it is set in the prompt.\n\n  Then in your prompt you can use something like:\n  ```baml\n  client<llm> Foo {\n    provider openai\n    options {\n      allowed_role_metadata: [\"foo\", \"bar\"]\n    }\n  }\n\n  client<llm> FooWithout {\n    provider openai\n    options {\n    }\n  }\n  template_string Foo() #\"\n    {{ _.role('user', foo={\"type\": \"ephemeral\"}, bar=\"1\", cat=True) }}\n    This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.\n    {{ _.role('user') }}\n    This will have none of the role metadata for Foo or FooWithout.\n  \"#\n  ```\n\n  You can use the playground to see the raw curl request to see what is being sent to the API.\n</ParamField>\n\n## Forwarded options\n\n<ParamField\n   path=\"messages\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you from the prompt\n</ParamField>\n\n<ParamField\n  path=\"model_id\"\n  type=\"string\"\n>\n  The model to use.\n\n| Model           | Description                    |\n| --------------- | ------------------------------ |\n| `anthropic.claude-3-haiku-20240307-v1:0`  | Fastest + Cheapest    |\n| `anthropic.claude-3-sonnet-20240307-v1:0` | Smartest              |\n| `meta.llama3-8b-instruct-v1:0`            |                       |\n| `meta.llama3-70b-instruct-v1:0`           |                       |\n| `mistral.mistral-7b-instruct-v0:2`        |                       |\n| `mistral.mixtral-8x7b-instruct-v0:1`      |                       |\n\nRun `aws bedrock list-foundation-models | jq '.modelSummaries.[].modelId` to get\na list of available foundation models; you can also use any custom models you've\ndeployed.\n\nNote that to use any of these models you'll need to [request model access].\n\n[request model access]: https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\n\n</ParamField>\n\n<ParamField path=\"inference_configuration\" type=\"object\">\nAdditional inference configuration to send with the request; see [AWS Bedrock\ndocumentation](https://docs.rs/aws-sdk-bedrockruntime/latest/aws_sdk_bedrockruntime/types/struct.InferenceConfiguration.html).\n\nExample:\n\n```baml BAML\nclient<llm> MyClient {\n  provider aws-bedrock\n  options {\n    inference_configuration {\n      max_tokens 1000\n      temperature 1.0\n      top_p 0.8\n      stop_sequence [\"_EOF\"]\n    }\n  }\n}\n```\n\n</ParamField>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/aws-bedrock.mdx"
      },
      "03-reference/baml/clients/providers/anthropic.mdx": {
        "markdown": "---\ntitle: anthropic\n---\n\n\nThe `anthropic` provider supports all APIs that use the same interface for the `/v1/messages` endpoint.\n\nExample:\n```baml BAML\nclient<llm> MyClient {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20240620\"\n    temperature 0\n  }\n}\n```\n\nThe options are passed through directly to the API, barring a few. Here's a shorthand of the options:\n\n## Non-forwarded options\n<ParamField\n  path=\"api_key\"\n  type=\"string\"\n>\n  Will be passed as a bearer token. **Default: `env.ANTHROPIC_API_KEY`**\n  \n  `Authorization: Bearer $api_key`\n</ParamField>\n\n<ParamField\n  path=\"base_url\"\n  type=\"string\"\n>\n  The base URL for the API. **Default: `https://api.anthropic.com`**\n</ParamField>\n\n<ParamField\n  path=\"default_role\"\n  type=\"string\"\n>\n  The default role for any prompts that don't specify a role. **Default: `system`**\n\n  We don't have any checks for this field, you can pass any string you wish.\n</ParamField>\n\n<ParamField path=\"headers\" type=\"object\">\n  Additional headers to send with the request.\n\n  Unless specified with a different value, we inject in the following headers:\n  ```\n  \"anthropic-version\" \"2023-06-01\"\n  ```\n\nExample:\n```baml\nclient<llm> MyClient {\n  provider anthropic\n  options {\n    api_key env.MY_ANTHROPIC_KEY\n    model \"claude-3-5-sonnet-20240620\"\n    headers {\n      \"X-My-Header\" \"my-value\"\n    }\n  }\n}\n```\n</ParamField>\n\n<ParamField\n  path=\"allowed_role_metadata\"\n  type=\"string[]\"\n>\n  Which role metadata should we forward to the API? **Default: `[]`**\n\n  For example you can set this to `[\"cache_control\"]` to forward the cache policy to the API.\n\n  If you do not set `allowed_role_metadata`, we will not forward any role metadata to the API even if it is set in the prompt.\n\n  Then in your prompt you can use something like:\n  ```baml\n  client<llm> ClaudeWithCaching {\n    provider anthropic\n    options {\n      model claude-3-haiku-20240307\n      api_key env.ANTHROPIC_API_KEY\n      max_tokens 1000\n      allowed_role_metadata [\"cache_control\"]\n      headers {\n        \"anthropic-beta\" \"prompt-caching-2024-07-31\"\n      }\n    }\n  }\n\n  client<llm> FooWithout {\n    provider anthropic\n    options {\n    }\n  }\n\n  template_string Foo() #\"\n    {{ _.role('user', cache_control={\"type\": \"ephemeral\"}) }}\n    This will be cached for ClaudeWithCaching, but not for FooWithout!\n    {{ _.role('user') }}\n    This will not be cached for Foo or FooWithout!\n  \"#\n  ```\n\n  You can use the playground to see the raw curl request to see what is being sent to the API.\n</ParamField>\n\n## Forwarded options\n<ParamField\n   path=\"system\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you from the prompt, if necessary.\n  Only the first system message will be used, all subsequent ones will be cast to the `assistant` role.\n</ParamField>\n\n<ParamField\n   path=\"messages\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you from the prompt\n</ParamField>\n\n<ParamField\n   path=\"stream\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you based on how you call the client in your code\n</ParamField>\n\n<ParamField\n  path=\"model\"\n  type=\"string\"\n>\n  The model to use.\n\n| Model |\n| --- |\n| `claude-3-5-sonnet-20240620` |  \n| `claude-3-opus-20240229` |  \n| `claude-3-sonnet-20240229` |  \n| `claude-3-haiku-20240307` |  \n\n<img src=\"https://mintlify.s3-us-west-1.amazonaws.com/anthropic/images/3-5-sonnet-curve.png\" />\n\nSee anthropic docs for the latest list of all models. You can pass any model name you wish, we will not check if it exists.\n</ParamField>\n\n<ParamField path=\"max_tokens\" type=\"int\">\n  The maximum number of tokens to generate. **Default: `4069`**\n</ParamField>\n\n\nFor all other options, see the [official anthropic API documentation](https://docs.anthropic.com/en/api/messages).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/anthropic.mdx"
      },
      "03-reference/baml/clients/providers/google-ai.mdx": {
        "markdown": "---\ntitle: google-ai\n---\n\n\nThe `google-ai` provider supports the `https://generativelanguage.googleapis.com/v1beta/models/{model_id}/generateContent` and `https://generativelanguage.googleapis.com/v1beta/models/{model_id}/streamGenerateContent` endpoints.\n\n<Tip>\nThe use of `v1beta` rather than `v1` aligns with the endpoint conventions established in [Google's SDKs](https://github.com/google-gemini/generative-ai-python/blob/8a29017e9120f0552ee3ad6092e8545d1aa6f803/google/generativeai/client.py#L60) and offers access to both the existing `v1` models and additional models exclusive to `v1beta`.\n</Tip>\n\n<Tip>\nBAML will automatically pick `streamGenerateContent` if you call the streaming interface.\n</Tip>\n\nExample:\n```baml BAML\nclient<llm> MyClient {\n  provider google-ai\n  options {\n    model \"gemini-1.5-flash\"\n  }\n}\n```\n\nThe options are passed through directly to the API, barring a few. Here's a shorthand of the options:\n## Non-forwarded options\n<ParamField\n  path=\"api_key\"\n  type=\"string\"\n>\n  Will be passed as the `x-goog-api-key` header. **Default: `env.GOOGLE_API_KEY`**\n\n  `x-goog-api-key: $api_key`\n</ParamField>\n\n<ParamField path=\"base_url\" type=\"string\">\n  The base URL for the API. **Default: `https://generativelanguage.googleapis.com/v1beta`**\n</ParamField>\n\n<ParamField\n  path=\"default_role\"\n  type=\"string\"\n>\n  The default role for any prompts that don't specify a role. **Default: `user`**\n\n  We don't have any checks for this field, you can pass any string you wish.\n</ParamField>\n\n<ParamField\n  path=\"model\"\n  type=\"string\"\n>\n  The model to use. **Default: `gemini-1.5-flash`**\n\n  We don't have any checks for this field, you can pass any string you wish.\n\n| Model | Input(s) | Optimized for |\n| --- | ---  | --- |\n| `gemini-1.5-pro`  | Audio, images, videos, and text | Complex reasoning tasks such as code and text generation, text editing, problem solving, data extraction and generation |\n| `gemini-1.5-flash`  | Audio, images, videos, and text | Fast and versatile performance across a diverse variety of tasks |\n| `gemini-1.0-pro` | Text | Natural language tasks, multi-turn text and code chat, and code generation |\n\nSee the [Google Model Docs](https://ai.google.dev/gemini-api/docs/models/gemini) for the latest models.\n</ParamField>\n\n<ParamField path=\"headers\" type=\"object\">\n  Additional headers to send with the request.\n\nExample:\n```baml BAML\nclient<llm> MyClient {\n  provider google-ai\n  options {\n    model \"gemini-1.5-flash\"\n    headers {\n      \"X-My-Header\" \"my-value\"\n    }\n  }\n}\n```\n</ParamField>\n\n<ParamField\n  path=\"allowed_role_metadata\"\n  type=\"string[]\"\n>\n  Which role metadata should we forward to the API? **Default: `[]`**\n\n  For example you can set this to `[\"foo\", \"bar\"]` to forward the cache policy to the API.\n\n  If you do not set `allowed_role_metadata`, we will not forward any role metadata to the API even if it is set in the prompt.\n\n  Then in your prompt you can use something like:\n  ```baml\n  client<llm> Foo {\n    provider openai\n    options {\n      allowed_role_metadata: [\"foo\", \"bar\"]\n    }\n  }\n\n  client<llm> FooWithout {\n    provider openai\n    options {\n    }\n  }\n  template_string Foo() #\"\n    {{ _.role('user', foo={\"type\": \"ephemeral\"}, bar=\"1\", cat=True) }}\n    This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.\n    {{ _.role('user') }}\n    This will have none of the role metadata for Foo or FooWithout.\n  \"#\n  ```\n\n  You can use the playground to see the raw curl request to see what is being sent to the API.\n</ParamField>\n\n## Forwarded options\n<ParamField\n   path=\"contents\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you from the prompt\n</ParamField>\n\n\nFor all other options, see the [official Google Gemini API documentation](https://ai.google.dev/api/rest/v1beta/models/generateContent).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/google-ai.mdx"
      },
      "03-reference/baml/clients/providers/vertex.mdx": {
        "markdown": "---\ntitle: vertex-ai\n---\n\nThe `vertex-ai` provider is used to interact with the Google Vertex AI services, specifically the following endpoints:\n\n```\nhttps://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:generateContent\nhttps://${LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/google/models/${MODEL_ID}:streamGenerateContent\n```\n\n\n\n\nExample: \n```baml BAML\nclient<llm> MyClient {\n  provider vertex-ai\n  options {\n    model gemini-1.5-pro\n    project_id my-project-id\n    location us-central1\n  }\n}\n```\n## Authorization\nThe `vertex-ai` provider uses the Google Cloud SDK to authenticate with a temporary access token. We generate these Google Cloud Authentication Tokens using Google Cloud service account credentials. We do not store this token, and it is only used for the duration of the request.\n\n### Instructions for downloading Google Cloud credentials\n1. Go to the [Google Cloud Console](https://console.cloud.google.com/).\n2. Click on the project you want to use.\n3. Select the `IAM & Admin` section, and click on `Service Accounts`.\n5. Select an existing service account or create a new one.\n6. Click on the service account and select `Add Key`.\n7. Choose the JSON key type and click `Create`.\n9. Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of the file.\n\n\nSee the [Google Cloud Application Default Credentials Docs](https://cloud.google.com/docs/authentication/application-default-credentials) for more information.\n<Tip>\nThe `project_id` of your client object must match the `project_id` of your credentials file.\n</Tip>\n\n\nThe options are passed through directly to the API, barring a few. Here's a shorthand of the options:\n## Non-forwarded options\n<ParamField path=\"base_url\" type=\"string\">\n  The base URL for the API.\n  \n  **Default: `https://{LOCATION}-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/{LOCATION}/publishers/google/models/\n`**\n\n  Can be used in lieu of the **`project_id`** and **`location`** fields, to manually set the request URL.\n</ParamField>\n\n\n<ParamField\n  path=\"project_id\"\n  type=\"string\"\n  required\n>\n  Vertex requires a Google Cloud project ID for each request. See the [Google Cloud Project ID Docs](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects) for more information.\n\n    \n\n</ParamField>\n\n<ParamField\n  path=\"location\"\n  type=\"string\"\n  required\n>\n  Vertex requires a location for each request. Some locations may have different models avaiable.\n  \n  Common locations include:\n  - `us-central1`\n  - `us-west1`\n  - `us-east1`\n  - `us-south1`\n\n  See the [Vertex Location Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#united-states) for all locations and supported models.\n\n</ParamField>\n\n\n<ParamField\n  path=\"credentials\"\n  type=\"string | object\"\n>\n  Path to a JSON credentials file or a JSON object containing the credentials.\n  \n  **Default: `env.GOOGLE_APPLICATION_CREDENTIALS`**\n\n  <Accordion title='Example: file path'>\n  In this case, the path is resolved relative to the CWD of your process.\n\n  ```baml BAML\n  client<llm> Vertex {\n    provider vertex-ai\n    options {\n      model gemini-1.5-pro\n      project_id jane-doe-test-1\n      location us-central1\n      credentials 'path/to/credentials.json'\n    }\n  }\n  ```\n  </Accordion>\n\n  <Accordion title='Example: JSON object'>\n  ```baml BAML\n  client<llm> Vertex {\n    provider vertex-ai\n    options {\n      model gemini-1.5-pro\n      project_id jane-doe-mycompany-1\n      location us-central1\n      credentials {\n        ...\n        private_key \"-----BEGIN PRIVATE KEY-----super-duper-secret-string\\n-----END PRIVATE KEY-----\\n\"\n        client_email \"jane_doe@mycompany.com\"\n        ...\n      }\n    }\n  }\n  ```\n  </Accordion>\n  <Warning>\n   This field cannot be used in the BAML Playground. For the playground, use the **`credentials_content`** instead.\n  </Warning>\n</ParamField>\n\n<ParamField\n  path=\"credentials_content\"\n  type=\"string\"\n>\n  Overrides contents of the Google Cloud Application Credentials. **Default: `env.GOOGLE_APPLICATION_CREDENTIALS_CONTENT`**\n\n  \n  <Accordion title='Example Credentials Content'>\n```json Credentials\n    {\n      \"type\": \"service_account\",\n      \"project_id\": \"my-project-id\",\n      \"private_key_id\": \"string\",\n      \"private_key\": \"-----BEGIN PRIVATE KEY-----string\\n-----END PRIVATE KEY-----\\n\",\n      \"client_email\": \"john_doe@gmail.com\",\n      \"client_id\": \"123456\",\n      \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n      \"token_uri\": \"https://oauth2.googleapis.com/token\",\n      \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n      \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/...\",\n      \"universe_domain\": \"googleapis.com\"\n    }\n```\n\n</Accordion>\n\n    <Warning>\n    Only use this for the BAML Playground only. Use **`credentials`** for your runtime code.\n  </Warning>\n</ParamField>\n\n<ParamField\n  path=\"authorization\"\n  type=\"string\"\n>\n  Directly set Google Cloud Authentication Token in lieu of token generation via **`env.GOOGLE_APPLICATION_CREDENTIALS`** or **`env.GOOGLE_APPLICATION_CREDENTIALS_CONTENT`** fields.\n    \n</ParamField>\n\n\n\n\n<ParamField\n  path=\"default_role\"\n  type=\"string\"\n>\n  The default role for any prompts that don't specify a role. **Default: `user`**\n  \n</ParamField>\n\n<ParamField\n  path=\"model\"\n  type=\"string\"\n  required\n>\n  The Google model to use for the request.\n  \n\n| Model | Input(s) | Optimized for |\n| --- | ---  | --- |\n| `gemini-1.5-pro`  | Audio, images, videos, and text | Complex reasoning tasks such as code and text generation, text editing, problem solving, data extraction and generation |\n| `gemini-1.5-flash`  | Audio, images, videos, and text | Fast and versatile performance across a diverse variety of tasks |\n| `gemini-1.0-pro` | Text | Natural language tasks, multi-turn text and code chat, and code generation |\n\nSee the [Google Model Docs](https://ai.google.dev/gemini-api/docs/models/gemini) for the latest models.\n</ParamField>\n\n<ParamField path=\"headers\" type=\"object\">\n  Additional headers to send with the request.\n\nExample:\n```baml BAML\nclient<llm> MyClient {\n  provider vertex-ai\n  options {\n    model gemini-1.5-pro\n    project_id my-project-id\n    location us-central1\n    // Additional headers\n    headers {\n      \"X-My-Header\" \"my-value\"\n    }\n  }\n}\n```\n</ParamField>\n\n<ParamField\n  path=\"allowed_role_metadata\"\n  type=\"string[]\"\n>\n  Which role metadata should we forward to the API? **Default: `[]`**\n\n  For example you can set this to `[\"foo\", \"bar\"]` to forward the cache policy to the API.\n\n  If you do not set `allowed_role_metadata`, we will not forward any role metadata to the API even if it is set in the prompt.\n\n  Then in your prompt you can use something like:\n  ```baml\n  client<llm> Foo {\n    provider openai\n    options {\n      allowed_role_metadata: [\"foo\", \"bar\"]\n    }\n  }\n\n  client<llm> FooWithout {\n    provider openai\n    options {\n    }\n  }\n  template_string Foo() #\"\n    {{ _.role('user', foo={\"type\": \"ephemeral\"}, bar=\"1\", cat=True) }}\n    This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.\n    {{ _.role('user') }}\n    This will have none of the role metadata for Foo or FooWithout.\n  \"#\n  ```\n\n  You can use the playground to see the raw curl request to see what is being sent to the API.\n</ParamField>\n\n## Forwarded options\n<ParamField\n  path=\"safetySettings\"\n  type=\"object\"\n>\n  Safety settings to apply to the request. You can stack different safety settings with a new `safetySettings` header for each one. See the [Google Vertex API Request Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference) for more information on what safety settings can be set.\n\n```baml BAML\nclient<llm> MyClient {\n  provider vertex-ai\n  options {\n    model gemini-1.5-pro\n    project_id my-project-id\n    location us-central1\n\n    safetySettings {\n      category HARM_CATEGORY_HATE_SPEECH\n      threshold BLOCK_LOW_AND_ABOVE\n      method SEVERITY\n    }\n  }\n}\n```\n    \n\n</ParamField>\n\n<ParamField\n  path=\"generationConfig\"\n  type=\"object\"\n>\n  Generation configurations to apply to the request. See the [Google Vertex API Request Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference) for more information on what properties can be set.\n```baml BAML\nclient<llm> MyClient {\n  provider vertex-ai\n  options {\n    model gemini-1.5-pro\n    project_id my-project-id\n    location us-central1\n    \n    generationConfig {\n      maxOutputTokens 100\n      temperature 1\n    }\n  }\n}\n```\n   \n</ParamField>\n\nFor all other options, see the [official Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal).\n\n\n\n\n\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/vertex.mdx"
      },
      "03-reference/baml/clients/providers/openai.mdx": {
        "markdown": "---\ntitle: openai\n---\n\nThe `openai` provider supports the OpenAI `/chat` endpoint, setting OpenAI-specific\ndefault configuration options.\n\n<Tip>\n  For Azure, we recommend using [`azure-openai`](azure) instead.\n\n  For all other OpenAI-compatible API providers, such as Groq, HuggingFace,\n  Ollama, OpenRouter, Together AI, and others, we recommend using\n [`openai-generic`](openai-generic) instead.\n</Tip>\n\nExample:\n\n```baml BAML\nclient<llm> MyClient {\n  provider \"openai\"\n  options {\n    api_key env.MY_OPENAI_KEY\n    model \"gpt-3.5-turbo\"\n    temperature 0.1\n  }\n}\n```\n\nThe options are passed through directly to the API, barring a few. Here's a shorthand of the options:\n\n## Non-forwarded options\n\n<ParamField path=\"api_key\" type=\"string\" default=\"env.OPENAI_API_KEY\">\n  Will be used to build the `Authorization` header, like so: `Authorization: Bearer $api_key`\n\n  **Default: `env.OPENAI_API_KEY`**\n</ParamField>\n\n<ParamField path=\"base_url\" type=\"string\">\n  The base URL for the API.\n  \n  **Default: `https://api.openai.com/v1`**\n</ParamField>\n\n<ParamField path=\"default_role\" type=\"string\">\n  The default role for any prompts that don't specify a role.\n  \n  We don't do any validation of this field, so you can pass any string you wish.\n  \n  **Default: `system`**\n</ParamField>\n\n<ParamField path=\"headers\" type=\"object\">\n  Additional headers to send with the request.\n\nExample:\n\n```baml BAML\nclient<llm> MyClient {\n  provider openai\n  options {\n    api_key env.MY_OPENAI_KEY\n    model \"gpt-3.5-turbo\"\n    headers {\n      \"X-My-Header\" \"my-value\"\n    }\n  }\n}\n```\n\n</ParamField>\n\n<ParamField\n  path=\"allowed_role_metadata\"\n  type=\"string[]\"\n>\n  Which role metadata should we forward to the API? **Default: `[]`**\n\n  For example you can set this to `[\"foo\", \"bar\"]` to forward the cache policy to the API.\n\n  If you do not set `allowed_role_metadata`, we will not forward any role metadata to the API even if it is set in the prompt.\n\n  Then in your prompt you can use something like:\n  ```baml\n  client<llm> Foo {\n    provider openai\n    options {\n      allowed_role_metadata: [\"foo\", \"bar\"]\n    }\n  }\n\n  client<llm> FooWithout {\n    provider openai\n    options {\n    }\n  }\n  template_string Foo() #\"\n    {{ _.role('user', foo={\"type\": \"ephemeral\"}, bar=\"1\", cat=True) }}\n    This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.\n    {{ _.role('user') }}\n    This will have none of the role metadata for Foo or FooWithout.\n  \"#\n  ```\n\n  You can use the playground to see the raw curl request to see what is being sent to the API.\n</ParamField>\n\n## Forwarded options\n\n<ParamField\n   path=\"messages\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you from the prompt\n</ParamField>\n<ParamField\n   path=\"stream\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you based on how you call the client in your code\n</ParamField>\n<ParamField\n  path=\"model\"\n  type=\"string\"\n>\n  The model to use.\n\n| Model           | Description                    |\n| --------------- | ------------------------------ |\n| `gpt-3.5-turbo` | Fastest                        |\n| `gpt-4o`        | Fast + text + image            |\n| `gpt-4-turbo`   | Smartest + text + image + code |\n| `gpt-4o-mini`   | Cheapest + text + image        |\n\nSee openai docs for the list of openai models. You can pass any model name you wish, we will not check if it exists.\n\n</ParamField>\n\nFor all other options, see the [official OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat/create).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/openai.mdx"
      },
      "03-reference/baml/clients/providers/azure.mdx": {
        "markdown": "---\ntitle: azure-openai\n---\n\n\nFor `azure-openai`, we provide a client that can be used to interact with the OpenAI API hosted on Azure using the `/chat/completions` endpoint.\n\nExample:\n```baml BAML\nclient<llm> MyClient {\n  provider azure-openai\n  options {\n    resource_name \"my-resource-name\"\n    deployment_id \"my-deployment-id\"\n    // Alternatively, you can use the base_url field\n    // base_url \"https://my-resource-name.openai.azure.com/openai/deployments/my-deployment-id\"\n    api_version \"2024-02-01\"\n    api_key env.AZURE_OPENAI_API_KEY\n  }\n}\n```\n\n<Warning>\n  `api_version` is required. Azure will return not found if the version is not specified.\n</Warning>\n\n\nThe options are passed through directly to the API, barring a few. Here's a shorthand of the options:\n\n## Non-forwarded options\n<ParamField\n  path=\"api_key\"\n  type=\"string\"\n>\n  Will be injected via the header `API-KEY`. **Default: `env.AZURE_OPENAI_API_KEY`**\n\n  `API-KEY: $api_key`\n</ParamField>\n\n<ParamField\n  path=\"base_url\"\n  type=\"string\"\n>\n  The base URL for the API. **Default: `https://${resource_name}.openai.azure.com/openai/deployments/${deployment_id}`**\n\n  May be used instead of `resource_name` and `deployment_id`.\n</ParamField>\n\n<ParamField\n  path=\"deployment_id\"\n  type=\"string\"\n  required\n>\n  See the `base_url` field.\n</ParamField>\n\n<ParamField\n  path=\"resource_name\"\n  type=\"string\"\n  required\n>\n  See the `base_url` field.\n</ParamField>\n\n<ParamField\n  path=\"default_role\"\n  type=\"string\"\n>\n  The default role for any prompts that don't specify a role. **Default: `system`**\n  \n  We don't have any checks for this field, you can pass any string you wish.\n</ParamField>\n\n<ParamField path=\"api_version\" type=\"string\" required>\n  Will be passed via a query parameter `api-version`.\n</ParamField>\n\n<ParamField path=\"headers\" type=\"object\">\n  Additional headers to send with the request.\n\nExample:\n```baml BAML\nclient<llm> MyClient {\n  provider azure-openai\n  options {\n    resource_name \"my-resource-name\"\n    deployment_id \"my-deployment-id\"\n    api_version \"2024-02-01\"\n    api_key env.AZURE_OPENAI_API_KEY\n    headers {\n      \"X-My-Header\" \"my-value\"\n    }\n  }\n}\n```\n</ParamField>\n\n<ParamField\n  path=\"allowed_role_metadata\"\n  type=\"string[]\"\n>\n  Which role metadata should we forward to the API? **Default: `[]`**\n\n  For example you can set this to `[\"foo\", \"bar\"]` to forward the cache policy to the API.\n\n  If you do not set `allowed_role_metadata`, we will not forward any role metadata to the API even if it is set in the prompt.\n\n  Then in your prompt you can use something like:\n  ```baml\n  client<llm> Foo {\n    provider openai\n    options {\n      allowed_role_metadata: [\"foo\", \"bar\"]\n    }\n  }\n\n  client<llm> FooWithout {\n    provider openai\n    options {\n    }\n  }\n  template_string Foo() #\"\n    {{ _.role('user', foo={\"type\": \"ephemeral\"}, bar=\"1\", cat=True) }}\n    This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.\n    {{ _.role('user') }}\n    This will have none of the role metadata for Foo or FooWithout.\n  \"#\n  ```\n\n  You can use the playground to see the raw curl request to see what is being sent to the API.\n</ParamField>\n\n## Forwarded options\n<ParamField\n   path=\"messages\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you from the prompt\n</ParamField>\n<ParamField\n   path=\"stream\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you based on how you call the client in your code\n</ParamField>\n\nFor all other options, see the [official Azure API documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/reference#chat-completions).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/azure.mdx"
      },
      "03-reference/baml/clients/providers/openai-generic.mdx": {
        "markdown": "---\ntitle: openai-generic\n---\n\n\nThe `openai-generic` provider supports all APIs that use OpenAI's request and\nresponse formats, such as Groq, HuggingFace, Ollama, OpenRouter, and Together AI.\n\nExample:\n\n```baml BAML\nclient<llm> MyClient {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://api.provider.com\"\n    model \"<provider-specified-format>\"\n  }\n}\n```\n\n\n## Non-forwarded options\n\n<ParamField path=\"base_url\" type=\"string\">\n  The base URL for the API.\n  \n  **Default: `https://api.openai.com/v1`**\n</ParamField>\n\n<ParamField path=\"default_role\" type=\"string\">\n  The default role for any prompts that don't specify a role.\n  \n  We don't do any validation of this field, so you can pass any string you wish.\n  \n  **Default: `system`**\n</ParamField>\n\n<ParamField path=\"api_key\" type=\"string\" default=\"<none>\">\n  Will be used to build the `Authorization` header, like so: `Authorization: Bearer $api_key`\n  If `api_key` is not set, or is set to an empty string, the `Authorization` header will not be sent.\n\n  **Default: `<none>`**\n</ParamField>\n\n<ParamField path=\"headers\" type=\"object\">\n  Additional headers to send with the request.\n\nExample:\n\n```baml BAML\nclient<llm> MyClient {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://api.provider.com\"\n    model \"<provider-specified-format>\"\n    headers {\n      \"X-My-Header\" \"my-value\"\n    }\n  }\n}\n```\n\n</ParamField>\n\n## Forwarded options\n\n<ParamField\n   path=\"messages\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you from the prompt\n</ParamField>\n<ParamField\n   path=\"stream\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you based on how you call the client in your code\n</ParamField>\n<ParamField\n  path=\"model\"\n  type=\"string\"\n>\n  The model to use.\n\n  For OpenAI, this might be `\"gpt-4o-mini\"`; for Ollama, this might be `\"llama2\"`. The exact\n  syntax will depend on your API provider's documentation: we'll just forward it to them as-is.\n\n</ParamField>\n\nFor all other options, see the [official OpenAI API documentation](https://platform.openai.com/docs/api-reference/chat/create).\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/openai-generic.mdx"
      },
      "03-reference/baml/clients/providers/groq.mdx": {
        "markdown": "---\ntitle: groq\n---\n\n[Groq](https://groq.com) supports the OpenAI client, allowing you to use the\n[`openai-generic`](/docs/snippets/clients/providers/openai) provider with an\noverridden `base_url`.\n\nSee https://console.groq.com/docs/openai for more information.\n\n```baml BAML\nclient<llm> MyClient {\n  provider openai-generic\n  options {\n    base_url \"https://api.groq.com/openai/v1\"\n    api_key env.GROQ_API_KEY\n    model \"llama3-70b-8192\"\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/groq.mdx"
      },
      "03-reference/baml/clients/providers/huggingface.mdx": {
        "markdown": "---\ntitle: huggingface\n---\n\n[HuggingFace](https://huggingface.co/) supports the OpenAI client, allowing you to use the\n[`openai-generic`](/docs/snippets/clients/providers/openai) provider with an\noverridden `base_url`.\n\nSee https://huggingface.co/docs/inference-endpoints/index for more information on their Inference Endpoints.\n\n```baml BAML\nclient<llm> MyClient {\n  provider openai-generic\n  options {\n    base_url \"https://api-inference.huggingface.co/v1\"\n    api_key env.HUGGINGFACE_API_KEY\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/huggingface.mdx"
      },
      "03-reference/baml/clients/providers/keywordsai.mdx": {
        "markdown": "---\ntitle: Keywords AI\n---\nKeywords AI is a proxying layer that allows you to route requests to hundreds of models.\n\nFollow the [Keywords AI + BAML Installation Guide](https://docs.keywordsai.co/integration/development-frameworks/baml) to get started!\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/keywordsai.mdx"
      },
      "03-reference/baml/clients/providers/lmstudio.mdx": {
        "markdown": "---\ntitle: vLLM\n---\n\n[LMStudio](https://lmstudio.ai/docs) supports the OpenAI client, allowing you\nto use the [`openai-generic`](/docs/snippets/clients/providers/openai) provider\nwith an overridden `base_url`.\n\n\nSee https://lmstudio.ai/docs/local-server#make-an-inferencing-request-using-openais-chat-completions-format for more information.\n\n```baml BAML\nclient<llm> MyClient {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://localhost:1234/v1\"\n    model \"TheBloke/phi-2-GGUF\"\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/lmstudio.mdx"
      },
      "03-reference/baml/clients/providers/ollama.mdx": {
        "markdown": "---\ntitle: ollama\n---\n\n\n[Ollama](https://ollama.com/) supports the OpenAI client, allowing you to use the\n[`openai-generic`](/docs/snippets/clients/providers/openai) provider with an\noverridden `base_url`.\n\n<Tip>\n  Note that to call Ollama, you must use its OpenAI-compatible\n  `/v1` endpoint. See [Ollama's OpenAI compatibility\n  documentation](https://ollama.com/blog/openai-compatibility).\n</Tip>\n<Tip>You can try out BAML with Ollama at promptfiddle.com, by running `OLLAMA_ORIGINS='*' ollama serve`. Learn more in [here](https://www.boundaryml.com/blog/ollama-structured-output)</Tip>\n\n```baml BAML\nclient<llm> MyClient {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://localhost:11434/v1\"\n    model llama3\n  }\n}\n```\n\nThe options are passed through directly to the API, barring a few. Here's a shorthand of the options:\n\n## Non-forwarded options\n<ParamField\n  path=\"base_url\"\n  type=\"string\"\n>\n  The base URL for the API. **Default: `http://localhost:11434/v1`**\n  <Tip>Note the `/v1` at the end of the URL. See [Ollama's OpenAI compatability](https://ollama.com/blog/openai-compatibility)</Tip>\n</ParamField>\n\n<ParamField\n  path=\"default_role\"\n  type=\"string\"\n>\n  The default role for any prompts that don't specify a role. **Default: `system`**\n  \n  We don't have any checks for this field, you can pass any string you wish.\n</ParamField>\n\n<ParamField path=\"headers\" type=\"object\">\n  Additional headers to send with the request.\n\nExample:\n```baml BAML\nclient<llm> MyClient {\n  provider ollama\n  options {\n    model \"llama3\"\n    headers {\n      \"X-My-Header\" \"my-value\"\n    }\n  }\n}\n```\n</ParamField>\n\n<ParamField\n  path=\"allowed_role_metadata\"\n  type=\"string[]\"\n>\n  Which role metadata should we forward to the API? **Default: `[]`**\n\n  For example you can set this to `[\"foo\", \"bar\"]` to forward the cache policy to the API.\n\n  If you do not set `allowed_role_metadata`, we will not forward any role metadata to the API even if it is set in the prompt.\n\n  Then in your prompt you can use something like:\n  ```baml\n  client<llm> Foo {\n    provider openai\n    options {\n      allowed_role_metadata: [\"foo\", \"bar\"]\n    }\n  }\n\n  client<llm> FooWithout {\n    provider openai\n    options {\n    }\n  }\n  template_string Foo() #\"\n    {{ _.role('user', foo={\"type\": \"ephemeral\"}, bar=\"1\", cat=True) }}\n    This will be have foo and bar, but not cat metadata. But only for Foo, not FooWithout.\n    {{ _.role('user') }}\n    This will have none of the role metadata for Foo or FooWithout.\n  \"#\n  ```\n\n  You can use the playground to see the raw curl request to see what is being sent to the API.\n</ParamField>\n\n## Forwarded options\n<ParamField\n   path=\"messages\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you from the prompt\n</ParamField>\n<ParamField\n   path=\"stream\"\n   type=\"DO NOT USE\"\n>\n  BAML will auto construct this field for you based on how you call the client in your code\n</ParamField>\n<ParamField\n  path=\"model\"\n  type=\"string\"\n>\n  The model to use.\n\n| Model | Description |\n| --- | --- |\n| `llama3` | Meta Llama 3: The most capable openly available LLM to date |\n| `qwen2` | Qwen2 is a new series of large language models from Alibaba group |\n| `phi3` | Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft |\n| `aya` | Aya 23, released by Cohere, is a new family of state-of-the-art, multilingual models that support 23 languages. |\n| `mistral` | The 7B model released by Mistral AI, updated to version 0.3. |\n| `gemma` | Gemma is a family of lightweight, state-of-the-art open models built by Google DeepMind. Updated to version 1.1 |\n| `mixtral` | A set of Mixture of Experts (MoE) model with open weights by Mistral AI in 8x7b and 8x22b parameter sizes. |\n\nFor the most up-to-date list of models supported by Ollama, see their [Model Library](https://ollama.com/library).\n\n<Tip>To use a specific version you would do: `\"mixtral:8x22b\"`</Tip>\n</ParamField>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/ollama.mdx"
      },
      "03-reference/baml/clients/providers/openrouter.mdx": {
        "markdown": "---\ntitle: openrouter\n---\n\n[OpenRouter](https://openrouter.ai) supports the OpenAI client, allowing you to use the\n[`openai-generic`](/docs/snippets/clients/providers/openai) provider with an\noverridden `base_url`.\n\n\n\n```baml BAML\nclient<llm> MyClient {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    model \"openai/gpt-3.5-turbo\"\n    headers {\n      \"HTTP-Referer\" \"YOUR-SITE-URL\" // Optional\n      \"X-Title\" \"YOUR-TITLE\" // Optional\n    }\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/openrouter.mdx"
      },
      "03-reference/baml/clients/providers/together.mdx": {
        "markdown": "---\ntitle: Together AI\n---\n\n[Together AI](https://www.together.ai/) supports the OpenAI client, allowing you\nto use the [`openai-generic`](/docs/snippets/clients/providers/openai) provider\nwith an overridden `base_url`.\n\nSee https://docs.together.ai/docs/openai-api-compatibility for more information.\n\n```baml BAML\nclient<llm> MyClient {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://api.together.ai/v1\"\n    api_key env.TOGETHER_API_KEY\n    model \"meta-llama/Llama-3-70b-chat-hf\"\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/together.mdx"
      },
      "03-reference/baml/clients/providers/unify.mdx": {
        "markdown": "---\ntitle: Unify AI\n---\n\n[Unify AI](https://www.unify.ai/) supports the OpenAI client, allowing you\nto use the [`openai-generic`](/docs/snippets/clients/providers/openai) provider\nwith an overridden `base_url`.\n\nSee https://docs.unify.ai/universal_api/making_queries#openai-python-package for more information.\n\n```baml BAML\nclient<llm> UnifyClient {\n    provider \"openai-generic\"\n    options {\n        base_url \"https://api.unify.ai/v0\"\n        api_key env.MY_UNIFY_API_KEY\n        model \"llama-3.1-405b-chat@together-ai\"\n    }\n}\n```\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/unify.mdx"
      },
      "03-reference/baml/clients/providers/vllm.mdx": {
        "markdown": "---\ntitle: vLLM\n---\n\n[vLLM](https://docs.vllm.ai/) supports the OpenAI client, allowing you\nto use the [`openai-generic`](/docs/snippets/clients/providers/openai) provider\nwith an overridden `base_url`.\n\n\nSee https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html for more information.\n\n```baml BAML\nclient<llm> MyClient {\n  provider \"openai-generic\"\n  options {\n    base_url \"http://localhost:8000/v1\"\n    api_key \"token-abc123\"\n    model \"NousResearch/Meta-Llama-3-8B-Instruct\"\n    default_role \"user\" // Required for using VLLM\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/providers/vllm.mdx"
      },
      "03-reference/baml/clients/strategy/retry.mdx": {
        "markdown": "---\ntitle: retry_policy\n---\n\n\nA retry policy can be attached to any `client<llm>` and will attempt to retry requests that fail due to a network error.\n\n```baml BAML\nretry_policy MyPolicyName {\n  max_retries 3\n}\n```\n\nUsage:\n```baml BAML\nclient<llm> MyClient {\n  provider anthropic\n  retry_policy MyPolicyName\n  options {\n    model \"claude-3-sonnet-20240229\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n```\n\n## Fields\n<ParamField\n  path=\"max_retries\"\n  type=\"int\"\n  required\n>\n  Number of **additional** retries to attempt after the initial request fails.\n</ParamField>\n\n<ParamField\n  path=\"strategy\"\n  type=\"Strategy\"\n>\n  The strategy to use for retrying requests. Default is `constant_delay(delay_ms=200)`.\n\n| Strategy | Docs | Notes |\n| --- | --- | --- |\n| `constant_delay` | [Docs](#constant-delay) | |\n| `exponential_backoff` | [Docs](#exponential-backoff) | |\n\nExample:\n```baml BAML\nretry_policy MyPolicyName {\n  max_retries 3\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n```\n\n</ParamField>\n\n## Strategies\n\n### constant_delay\n<ParamField path=\"type\" type=\"constant_delay\" required>\n  Configures to the constant delay strategy.\n</ParamField>\n\n<ParamField path=\"delay_ms\" type=\"int\">\n  The delay in milliseconds to wait between retries. **Default: 200**\n</ParamField>\n\n\n### exponential_backoff\n<ParamField path=\"type\" type=\"exponential_backoff\" required>\n  Configures to the exponential backoff strategy.\n</ParamField>\n\n<ParamField path=\"delay_ms\" type=\"int\">\n  The initial delay in milliseconds to wait between retries. **Default: 200**\n</ParamField>\n\n<ParamField path=\"multiplier\" type=\"float\">\n  The multiplier to apply to the delay after each retry. **Default: 1.5**\n</ParamField>\n\n<ParamField path=\"max_delay_ms\" type=\"int\">\n  The maximum delay in milliseconds to wait between retries. **Default: 10000**\n</ParamField>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/strategy/retry.mdx"
      },
      "03-reference/baml/clients/strategy/fallback.mdx": {
        "markdown": "---\ntitle: fallback\n---\n\n\nYou can use the `fallback` provider to add more resilliancy to your application.\n\nA fallback will attempt to use the first client, and if it fails, it will try the second client, and so on.\n\n<Tip>You can nest fallbacks inside of other fallbacks.</Tip>\n\n```baml BAML\nclient<llm> SuperDuperClient {\n  provider fallback\n  options {\n    strategy [\n      ClientA\n      ClientB\n      ClientC\n    ]\n  }\n}\n```\n\n## Options\n\n<ParamField path=\"strategy\" type=\"List[string]\" required>\n  The list of client names to try in order. Cannot be empty.\n</ParamField>\n\n## retry_policy\n\nLike any other client, you can specify a retry policy for the fallback client. See [retry_policy](retry-policy) for more information.\n\nThe retry policy will test the fallback itself, after the entire strategy has failed.\n\n```baml BAML\nclient<llm> SuperDuperClient {\n  provider fallback\n  retry_policy MyRetryPolicy\n  options {\n    strategy [\n      ClientA\n      ClientB\n      ClientC\n    ]\n  }\n}\n```\n\n## Nesting multiple fallbacks\n\nYou can nest multiple fallbacks inside of each other. The fallbacks will just chain as you would expect.\n\n```baml BAML\nclient<llm> SuperDuperClient {\n  provider fallback\n  options {\n    strategy [\n      ClientA\n      ClientB\n      ClientC\n    ]\n  }\n}\n\nclient<llm> MegaClient {\n  provider fallback\n  options {\n    strategy [\n      SuperDuperClient\n      ClientD\n    ]\n  }\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/strategy/fallback.mdx"
      },
      "03-reference/baml/clients/strategy/round-robin.mdx": {
        "markdown": "---\ntitle: round-robin\n---\n\n\nThe `round_robin` provider allows you to distribute requests across multiple clients in a round-robin fashion. After each call, the next client in the list will be used.\n\n```baml BAML\nclient<llm> MyClient {\n  provider round-robin\n  options {\n    strategy [\n      ClientA\n      ClientB\n      ClientC\n    ]\n  }\n}\n```\n\n## Options\n\n<ParamField path=\"strategy\" type=\"List[string]\" required>\n  The list of client names to try in order. Cannot be empty.\n</ParamField>\n\n<ParamField  path=\"start\" type=\"int\">\n  The index of the client to start with.\n\n  **Default is `random(0, len(strategy))`**\n\n  In the [BAML Playground](/docs/get-started/quickstart/editors-vscode), Default is `0`.\n</ParamField>\n\n## retry_policy\n\nWhen using a retry_policy with a round-robin client, it will rotate the strategy list after each retry.\n\n```baml BAML\nclient<llm> MyClient {\n  provider round-robin\n  retry_policy MyRetryPolicy\n  options {\n    strategy [\n      ClientA\n      ClientB\n      ClientC\n    ]\n  }\n}\n```\n\n## Nesting multiple round-robin clients\n\nYou can nest multiple round-robin clients inside of each other. The round-robin as you would expect.\n\n```baml BAML\nclient<llm> MyClient {\n  provider round-robin\n  options {\n    strategy [\n      ClientA\n      ClientB\n      ClientC\n    ]\n  }\n}\n\nclient<llm> MegaClient {\n  provider round-robin\n  options {\n    strategy [\n      MyClient\n      ClientD\n      ClientE\n    ]\n  }\n}\n\n// Calling MegaClient will call:\n// MyClient(ClientA)\n// ClientD\n// ClientE\n// MyClient(ClientB)\n// etc.\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/clients/strategy/round-robin.mdx"
      },
      "03-reference/baml_client/typebuilder.mdx": {
        "markdown": "---\ntitle: TypeBuilder\n---\n\n\n`TypeBuilder` is used to create or modify output schemas at runtime. It's particularly useful when you have dynamic output structures that can't be determined at compile time - like categories from a database or user-provided schemas.\n\nHere's a simple example of using TypeBuilder to add new enum values before calling a BAML function:\n\n**BAML Code**\n```baml {4}\nenum Category {\n  RED\n  BLUE\n  @@dynamic  // Makes this enum modifiable at runtime\n}\n\nfunction Categorize(text: string) -> Category {\n  prompt #\"\n    Categorize this text:\n    {{ text }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n```\n\n**Runtime Usage**\n<CodeBlocks>\n```python Python\nfrom baml_client.type_builder import TypeBuilder\nfrom baml_client import b\n\n# Create a TypeBuilder instance\ntb = TypeBuilder()\n\n# Add new values to the Category enum\ntb.Category.add_value('GREEN') \ntb.Category.add_value('YELLOW')\n\n# Pass the typebuilder when calling the function\nresult = await b.Categorize(\"The sun is bright\", {\"tb\": tb})\n# result can now be RED, BLUE, GREEN, or YELLOW\n```\n```typescript TypeScript\nimport { TypeBuilder } from '../baml_client/type_builder'\nimport { b } from '../baml_client'\n\n// Create a TypeBuilder instance\nconst tb = new TypeBuilder()\n\n// Add new values to the Category enum\ntb.Category.addValue('GREEN')\ntb.Category.addValue('YELLOW')\n\n// Pass the typebuilder when calling the function\nconst result = await b.Categorize(\"The sun is bright\", { tb })\n// result can now be RED, BLUE, GREEN, or YELLOW\n```\n```ruby Ruby\nrequire_relative 'baml_client/client'\n\n# Create a TypeBuilder instance\ntb = Baml::TypeBuilder.new\n\n# Add new values to the Category enum\ntb.Category.add_value('GREEN')\ntb.Category.add_value('YELLOW')\n\n# Pass the typebuilder when calling the function\nresult = Baml::Client.categorize(text: \"The sun is bright\", baml_options: { tb: tb })\n# result can now be RED, BLUE, GREEN, or YELLOW\n```\n</CodeBlocks>\n\n## Dynamic Types\n\nThere are two ways to use TypeBuilder:\n1. Modifying existing BAML types marked with `@@dynamic`\n2. Creating entirely new types at runtime\n\n### Modifying Existing Types\n\nTo modify an existing BAML type, mark it with `@@dynamic`:\n\n<ParamField path=\"Classes\" type=\"example\">\n```baml\nclass User {\n  name string\n  age int\n  @@dynamic  // Allow adding more properties\n}\n```\n\n**Runtime Usage**\n<CodeBlocks>\n```python Python\ntb = TypeBuilder()\ntb.User.add_property('email', tb.string())\ntb.User.add_property('address', tb.string())\n```\n```typescript TypeScript\nconst tb = new TypeBuilder()\ntb.User.addProperty('email', tb.string())\ntb.User.addProperty('address', tb.string())\n```\n```ruby Ruby\ntb = Baml::TypeBuilder.new\ntb.User.add_property('email', tb.string)\ntb.User.add_property('address', tb.string)\n```\n</CodeBlocks>\n</ParamField>\n\n<ParamField path=\"Enums\" type=\"example\">\n```baml\nenum Category {\n  VALUE1\n  VALUE2\n  @@dynamic  // Allow adding more values\n}\n```\n\n**Runtime Usage**\n<CodeBlocks>\n```python Python\ntb = TypeBuilder()\ntb.Category.add_value('VALUE3')\ntb.Category.add_value('VALUE4')\n```\n```typescript TypeScript\nconst tb = new TypeBuilder()\ntb.Category.addValue('VALUE3')\ntb.Category.addValue('VALUE4')\n```\n```ruby Ruby\ntb = Baml::TypeBuilder.new\ntb.Category.add_value('VALUE3')\ntb.Category.add_value('VALUE4')\n```\n</CodeBlocks>\n</ParamField>\n\n### Creating New Types\n\nYou can also create entirely new types at runtime:\n\n<CodeBlocks>\n```python Python\ntb = TypeBuilder()\n\n# Create a new enum\nhobbies = tb.add_enum(\"Hobbies\")\nhobbies.add_value(\"Soccer\")\nhobbies.add_value(\"Reading\")\n\n# Create a new class\naddress = tb.add_class(\"Address\") \naddress.add_property(\"street\", tb.string())\naddress.add_property(\"city\", tb.string())\n\n# Attach new types to existing BAML type\ntb.User.add_property(\"hobbies\", hobbies.type().list())\ntb.User.add_property(\"address\", address.type())\n```\n```typescript TypeScript\nconst tb = new TypeBuilder()\n\n// Create a new enum\nconst hobbies = tb.addEnum(\"Hobbies\")\nhobbies.addValue(\"Soccer\")\nhobbies.addValue(\"Reading\")\n\n// Create a new class\nconst address = tb.addClass(\"Address\")\naddress.addProperty(\"street\", tb.string())\naddress.addProperty(\"city\", tb.string())\n\n// Attach new types to existing BAML type\ntb.User.addProperty(\"hobbies\", hobbies.type().list())\ntb.User.addProperty(\"address\", address.type())\n```\n```ruby Ruby\ntb = Baml::TypeBuilder.new\n\n# Create a new enum\nhobbies = tb.add_enum(\"Hobbies\")\nhobbies.add_value(\"Soccer\")\nhobbies.add_value(\"Reading\")\n\n# Create a new class\naddress = tb.add_class(\"Address\")\naddress.add_property(\"street\", tb.string)\naddress.add_property(\"city\", tb.string)\n\n# Attach new types to existing BAML type\ntb.User.add_property(\"hobbies\", hobbies.type.list)\ntb.User.add_property(\"address\", address.type)\n```\n</CodeBlocks>\n\n## Type Builders\n\nTypeBuilder provides methods for building different kinds of types:\n\n| Method | Description | Example |\n|--------|-------------|---------|\n| `string()` | Creates a string type | `tb.string()` |\n| `int()` | Creates an integer type | `tb.int()` |\n| `float()` | Creates a float type | `tb.float()` |\n| `bool()` | Creates a boolean type | `tb.bool()` |\n| `list()` | Makes a type into a list | `tb.string().list()` |\n| `optional()` | Makes a type optional | `tb.string().optional()` |\n\n## Adding Descriptions\n\nYou can add descriptions to properties and enum values to help guide the LLM:\n\n<CodeBlocks>\n```python Python\ntb = TypeBuilder()\n\n# Add description to a property\ntb.User.add_property(\"email\", tb.string()) \\\n   .description(\"User's primary email address\")\n\n# Add description to an enum value \ntb.Category.add_value(\"URGENT\") \\\n   .description(\"Needs immediate attention\")\n```\n```typescript TypeScript\nconst tb = new TypeBuilder()\n\n// Add description to a property\ntb.User.addProperty(\"email\", tb.string())\n   .description(\"User's primary email address\")\n\n// Add description to an enum value\ntb.Category.addValue(\"URGENT\")\n   .description(\"Needs immediate attention\")\n```\n```ruby Ruby\ntb = Baml::TypeBuilder.new\n\n# Add description to a property\ntb.User.add_property(\"email\", tb.string)\n   .description(\"User's primary email address\")\n\n# Add description to an enum value\ntb.Category.add_value(\"URGENT\")\n   .description(\"Needs immediate attention\")\n```\n</CodeBlocks>\n\n## Common Patterns\n\nHere are some common patterns when using TypeBuilder:\n\n1. **Dynamic Categories**: When categories come from a database or external source\n<CodeBlocks>\n```python Python\ncategories = fetch_categories_from_db()\ntb = TypeBuilder()\nfor category in categories:\n    tb.Category.add_value(category)\n```\n```typescript TypeScript\nconst categories = await fetchCategoriesFromDb()\nconst tb = new TypeBuilder()\ncategories.forEach(category => {\n    tb.Category.addValue(category)\n})\n```\n```ruby Ruby\ncategories = fetch_categories_from_db\ntb = Baml::TypeBuilder.new\ncategories.each do |category|\n    tb.Category.add_value(category)\nend\n```\n</CodeBlocks>\n\n2. **Form Fields**: When extracting dynamic form fields\n<CodeBlocks>\n```python Python\nfields = get_form_fields()\ntb = TypeBuilder()\nform = tb.add_class(\"Form\")\nfor field in fields:\n    form.add_property(field.name, tb.string())\n```\n```typescript TypeScript\nconst fields = getFormFields()\nconst tb = new TypeBuilder()\nconst form = tb.addClass(\"Form\")\nfields.forEach(field => {\n    form.addProperty(field.name, tb.string())\n})\n```\n```ruby Ruby\nfields = get_form_fields\ntb = Baml::TypeBuilder.new\nform = tb.add_class(\"Form\")\nfields.each do |field|\n    form.add_property(field.name, tb.string)\nend\n```\n</CodeBlocks>\n\n3. **Optional Properties**: When some fields might not be present\n<CodeBlocks>\n```python Python\ntb = TypeBuilder()\ntb.User.add_property(\"middle_name\", tb.string().optional())\n```\n```typescript TypeScript\nconst tb = new TypeBuilder()\ntb.User.addProperty(\"middle_name\", tb.string().optional())\n```\n```ruby Ruby\ntb = Baml::TypeBuilder.new\ntb.User.add_property(\"middle_name\", tb.string.optional)\n```\n</CodeBlocks>\n\n<Warning>\nAll types added through TypeBuilder must be connected to the return type of your BAML function. Standalone types that aren't referenced won't affect the output schema.\n</Warning>\n\n## Future Features\n\nWe're working on additional features for TypeBuilder:\n\n- JSON Schema support (awaiting use cases)\n- OpenAPI schema integration  \n- Pydantic model support\n\nIf you're interested in these features, please join the discussion in our GitHub issues.\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml_client/typebuilder.mdx"
      },
      "03-reference/baml/prompt-syntax/what-is-jinja.mdx": {
        "markdown": "---\ntitle: What is Jinja / Cookbook\n---\n\n\nBAML Prompt strings are essentially [Minijinja](https://docs.rs/minijinja/latest/minijinja/filters/index.html#functions) templates, which offer the ability to express logic and data manipulation within strings. Jinja is a very popular and mature templating language amongst Python developers, so Github Copilot or another LLM can already help you write most of the logic you want.\n\n## Jinja Cookbook\n\nWhen in doubt -- use the BAML VSCode Playground preview. It will show you the fully rendered prompt, even when it has complex logic.\n\n### Basic Syntax \n\n- `{% ... %}`: Use for executing statements such as for-loops or conditionals.\n- `{{ ... }}`: Use for outputting expressions or variables.\n- `{# ... #}`: Use for comments within the template, which will not be rendered.\n\n### Loops / Iterating Over Lists\n\nHere's how you can iterate over a list of items, accessing each item's attributes:\n\n```jinja Jinja\nfunction MyFunc(messages: Message[]) -> string {\n  prompt #\"\n    {% for message in messages %}\n      {{ message.user_name }}: {{ message.content }}\n    {% endfor %}\n  \"#\n}\n```\n\n### Conditional Statements\n\nUse conditional statements to control the flow and output of your templates based on conditions:\n\n```jinja Jinja\nfunction MyFunc(user: User) -> string {\n  prompt #\"\n    {% if user.is_active %}\n      Welcome back, {{ user.name }}!\n    {% else %}\n      Please activate your account.\n    {% endif %}\n  \"#\n}\n```\n\n### Setting Variables\n\nYou can define and use variables within your templates to simplify expressions or manage data:\n\n```jinja\nfunction MyFunc(items: Item[]) -> string {\n  prompt #\"\n    {% set total_price = 0 %}\n    {% for item in items %}\n      {% set total_price = total_price + item.price %}\n    {% endfor %}\n    Total price: {{ total_price }}\n  \"#\n}\n```\n\n### Including other Templates\n\nTo promote reusability, you can include other templates within a template. See [template strings](/ref/baml/template_string):\n\n```baml\ntemplate_string PrintUserInfo(arg1: string, arg2: User) #\"\n  {{ arg1 }}\n  The user's name is: {{ arg2.name }}\n\"#\n\nfunction MyFunc(arg1: string, user: User) -> string {\n  prompt #\"\n    Here is the user info:\n    {{ PrintUserInfo(arg1, user) }}\n  \"#\n}\n```\n\n### Built-in filters\nSee [jinja docs](https://jinja.palletsprojects.com/en/3.1.x/templates/#list-of-builtin-filters)\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/prompt-syntax/what-is-jinja.mdx"
      },
      "03-reference/baml/prompt-syntax/output-format.mdx": {
        "markdown": "---\ntitle: ctx.output_format\n---\n\n\n`{{ ctx.output_format }}` is used within a prompt template (or in any template_string) to print out the function's output schema into the prompt. It describes to the LLM how to generate a structure BAML can parse (usually JSON).\n\nHere's an example of a function with `{{ ctx.output_format }}`, and how it gets rendered by BAML before sending it to the LLM.\n\n**BAML Prompt**\n\n```baml\nclass Resume {\n  name string\n  education Education[]\n}\nfunction ExtractResume(resume_text: string) -> Resume {\n  prompt #\"\n    Extract this resume:\n    ---\n    {{ resume_text }}\n    ---\n\n    {{ ctx.output_format }}\n  \"#\n}\n```\n\n**Rendered prompt**\n\n```text\nExtract this resume\n---\nAaron V.\nBachelors CS, 2015\nUT Austin\n---\n\nAnswer in JSON using this schema: \n{\n  name: string\n  education: [\n    {\n      school: string\n      graduation_year: string\n    }\n  ]\n}\n```\n\n## Controlling the output_format\n\n`ctx.output_format` can also be called as a function with parameters to customize how the schema is printed, like this:\n```text\n\n{{ ctx.output_format(prefix=\"If you use this schema correctly and I'll tip $400:\\n\", always_hoist_enums=true)}}\n```\n\nHere's the parameters:\n<ParamField path=\"prefix\" type=\"string\">\nThe prefix instruction to use before printing out the schema. \n\n```text\nAnswer in this schema correctly I'll tip $400:\n{\n  ...\n}\n```\nBAML's default prefix varies based on the function's return type.\n\n| Fuction return type | Default Prefix |\n| --- | --- |\n| Primitive (String) |  |\n| Primitive (Other) | `Answer as a: ` |\n| Enum | `Answer with any of the categories:\\n` |\n| Class | `Answer in JSON using this schema:\\n` |\n| List | `Answer with a JSON Array using this schema:\\n` |\n| Union | `Answer in JSON using any of these schemas:\\n` |\n| Optional | `Answer in JSON using this schema:\\n` |\n\n</ParamField>\n\n<ParamField path=\"always_hoist_enums\" type=\"boolean\" > \nWhether to inline the enum definitions in the schema, or print them above. **Default: false**\n\n\n**Inlined**\n```\n\nAnswer in this json schema:\n{\n  categories: \"ONE\" | \"TWO\" | \"THREE\"\n}\n```\n\n**hoisted**\n```\nMyCategory\n---\nONE\nTWO\nTHREE\n\nAnswer in this json schema:\n{\n  categories: MyCategory\n}\n```\n\n<Warning>BAML will always hoist if you add a [description](/docs/snippets/enum#aliases-descriptions) to any of the enum values.</Warning>\n\n</ParamField>\n\n<ParamField path=\"or_splitter\" type=\"string\" >\n\n**Default: ` or `**\n\nIf a type is a union like `string | int` or an optional like `string?`, this indicates how it's rendered. \n\n\nBAML renders it as `property: string or null` as we have observed some LLMs have trouble identifying what `property: string | null` means (and are better with plain english).\n\nYou can always set it to ` | ` or something else for a specific model you use.\n</ParamField>\n\n## Why BAML doesn't use JSON schema format in prompts\nBAML uses \"type definitions\" or \"jsonish\" format instead of the long-winded json-schema format.\nThe tl;dr is that json schemas are\n1. 4x more inefficient than \"type definitions\".\n2. very unreadable by humans (and hence models)\n3. perform worse than type definitions (especially on deeper nested objects or smaller models)\n\nRead our [full article on json schema vs type definitions](https://www.boundaryml.com/blog/type-definition-prompting-baml)\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/prompt-syntax/output-format.mdx"
      },
      "03-reference/baml/prompt-syntax/ctx.mdx": {
        "markdown": "---\ntitle: ctx (accessing metadata)\n---\n\n\nIf you try rendering `{{ ctx }}` into the prompt (literally just write that out!), you'll see all the metadata we inject to run this prompt within the playground preview.\n\nIn the earlier tutorial we mentioned `ctx.output_format`, which contains the schema, but you can also access client information:\n\n\n## Usecase: Conditionally render based on client provider\n\nIn this example, we render the list of messages in XML tags if the provider is Anthropic (as they recommend using them as delimiters). See also  [template_string](/ref/baml/template-string) as it's used in here.\n\n```baml\ntemplate_string RenderConditionally(messages: Message[]) #\"\n  {% for message in messages %}\n    {%if ctx.client.provider == \"anthropic\" %}\n      <Message>{{ message.user_name }}: {{ message.content }}</Message>\n    {% else %}\n      {{ message.user_name }}: {{ message.content }}\n    {% endif %}\n  {% endfor %}\n\"#\n\nfunction MyFuncWithGPT4(messages: Message[]) -> string {\n  client GPT4o\n  prompt #\"\n    {{ RenderConditionally(messages)}}\n  \"#\n}\n\nfunction MyFuncWithAnthropic(messages: Message[]) -> string {\n  client Claude35\n  prompt #\"\n    {{ RenderConditionally(messages )}}\n  #\"\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/prompt-syntax/ctx.mdx"
      },
      "03-reference/baml/prompt-syntax/role.mdx": {
        "markdown": "---\ntitle: _.role\n---\n\n\nBAML prompts are compiled into a `messages` array (or equivalent) that most LLM providers use:\n\nBAML Prompt -> `[{ role: \"user\": content: \"hi there\"}, { role: \"assistant\", ...}]`\n\nBy default, BAML puts everything into a single message with the `system` role if available (or whichever one is best for the provider you have selected). \nWhen in doubt, the playground always shows you the current role for each message.\n\nTo specify a role explicitly, add the `{{ _.role(\"user\")}}` syntax to the prompt\n```rust\nprompt #\"\n  {{ _.role(\"system\") }} Everything after\n  this element will be a system prompt!\n\n  {{ _.role(\"user\")}} \n  And everything after this\n  will be a user role\n\"#\n```\nTry it out in [PromptFiddle](https://www.promptfiddle.com)\n\n<Note>\n  BAML may change the default role to `user` if using specific APIs that only support user prompts, like when using prompts with images.\n</Note>\n\nWe use `_` as the prefix of `_.role()` since we plan on adding more helpers here in the future.\n\n## Example -- Using `_.role()` in for-loops\n\nHere's how you can inject a list of user/assistant messages and mark each as a user or assistant role:\n\n```rust BAML\nclass Message {\n  role string\n  message string\n}\n\nfunction ChatWithAgent(input: Message[]) -> string {\n  client GPT4o\n  prompt #\"\n    {% for m in messages %}\n      {{ _.role(m.role) }}\n      {{ m.message }}\n    {% endfor %}\n  \"#\n}\n```\n\n```rust BAML\nfunction ChatMessages(messages: string[]) -> string {\n  client GPT4o\n  prompt #\"\n    {% for m in messages %}\n      {{ _.role(\"user\" if loop.index % 2 == 1 else \"assistant\") }}\n      {{ m }}\n    {% endfor %}\n  \"#\n}\n```\n\n## Example -- Using `_.role()` in a template string\n\n```baml BAML\ntemplate_string YouAreA(name: string, job: string) #\"\n  {{ _.role(\"system\") }} \n  You are an expert {{ name }}. {{ job }}\n\n  {{ ctx.output_format }}\n  {{ _.role(\"user\") }}\n\"#\n\nfunction CheckJobPosting(post: string) -> bool {\n  client GPT4o\n  prompt #\"\n    {{ YouAreA(\"hr admin\", \"You're role is to ensure every job posting is bias free.\") }}\n\n    {{ post }}\n  \"#\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/prompt-syntax/role.mdx"
      },
      "03-reference/baml/prompt-syntax/variables.mdx": {
        "markdown": "---\ntitle: Variables\n---\n\n\nSee [template_string](/ref/baml/template-string) to learn how to add variables in .baml files\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/prompt-syntax/variables.mdx"
      },
      "03-reference/baml/prompt-syntax/conditionals.mdx": {
        "markdown": "---\ntitle: Conditionals\n---\n\nUse conditional statements to control the flow and output of your templates based on conditions:\n\n```jinja\nfunction MyFunc(user: User) -> string {\n  prompt #\"\n    {% if user.is_active %}\n      Welcome back, {{ user.name }}!\n    {% else %}\n      Please activate your account.\n    {% endif %}\n  \"#\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/prompt-syntax/conditionals.mdx"
      },
      "03-reference/baml/prompt-syntax/loops.mdx": {
        "markdown": "---\ntitle: Loops\n---\n\nHere's how you can iterate over a list of items, accessing each item's attributes:\n\n```jinja\nfunction MyFunc(messages: Message[]) -> string {\n  prompt #\"\n    {% for message in messages %}\n      {{ message.user_name }}: {{ message.content }}\n    {% endfor %}\n  \"#\n}\n```\n\n## loop\n\nJinja provides a `loop` object that can be used to access information about the loop. Here are some of the attributes of the `loop` object:\n\n\n| Variable         | Description                                                                 |\n|------------------|-----------------------------------------------------------------------------|\n| loop.index       | The current iteration of the loop. (1 indexed)                              |\n| loop.index0      | The current iteration of the loop. (0 indexed)                              |\n| loop.revindex    | The number of iterations from the end of the loop (1 indexed)               |\n| loop.revindex0   | The number of iterations from the end of the loop (0 indexed)               |\n| loop.first       | True if first iteration.                                                    |\n| loop.last        | True if last iteration.                                                     |\n| loop.length      | The number of items in the sequence.                                        |\n| loop.cycle       | A helper function to cycle between a list of sequences. See the explanation below. |\n| loop.depth       | Indicates how deep in a recursive loop the rendering currently is. Starts at level 1 |\n| loop.depth0      | Indicates how deep in a recursive loop the rendering currently is. Starts at level 0 |\n| loop.previtem    | The item from the previous iteration of the loop. Undefined during the first iteration. |\n| loop.nextitem    | The item from the following iteration of the loop. Undefined during the last iteration. |\n| loop.changed(*val) | True if previously called with a different value (or not called at all).  |\n\n```jinja2\nprompt #\"\n  {% for item in items %}\n    {{ loop.index }}: {{ item }}\n  {% endfor %}\n\"#\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/baml/prompt-syntax/loops.mdx"
      },
      "03-reference/vscode-ext/clipath.mdx": {
        "markdown": "| Type | Value |\n| --- | --- |\n| `string \\| null` | null |\n\n\n\nIf set, all generated code will use this instead of the packaged generator shipped with the extension.\n\n<Tip>\nWe recommend this setting! This prevents mismatches between the VSCode Extension and the installed BAML package.\n</Tip>\n\n## Usage\n\nIf you use unix, you can run `where baml-cli` in your project to figure out what the path is.\n\n```json settings.json\n{\n  \"baml.cliPath\": \"/path/to/baml-cli\"\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/vscode-ext/clipath.mdx"
      },
      "03-reference/vscode-ext/generateCodeOnSave.mdx": {
        "markdown": "| Type | Default Value |\n| --- | --- |\n| `\"always\" \\| \"never\"` | \"always\" |\n\n\n- `always`: Generate code for `baml_client` on every save\n- `never`: Do not generate `baml_client` on any save\n\nIf you have a generator of type `rest/*`, `\"always\"` will not do any code generation. You will have to manually run:\n\n```\npath/to/baml-cli generate\n```\n\n## Usage\n\n```json settings.json\n{\n  \"baml.generateCodeOnSave\": \"never\",\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/vscode-ext/generateCodeOnSave.mdx"
      },
      "03-reference/vscode-ext/restartTSServerOnSave.mdx": {
        "markdown": "| Type | Default Value |\n| --- | --- |\n| `boolean` | `true` |\n\n- `true`: Automatically restarts the TypeScript Language Server in VSCode when the BAML extension generates the TypeScript `baml_client` files. This is a workaround for VSCode's issues with recognizing newly added directories and files in the TypeScript Language Server. No-op if not generating TypeScript files.\n- `false`: Does not automatically restart the TypeScript Server. You may need to manually reload the TS server to ensure it recognizes the new types.\n\n## Usage\n\n```json\n{\n  \"baml.restartTSServerOnSave\": true\n}\n```\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/vscode-ext/restartTSServerOnSave.mdx"
      },
      "03-reference/extract/summary.mdx": {
        "markdown": "---\ntitle: Overview\n---\n\nWe leveraged our expertise in structured data extraction to create a general purpose extraction API that is independent of BAML.\n\nIf you are interested in converting PDF documents, invoices, images into readable structured data, this API is for you.\n\n\nTo try it out visit our [Dashboard v2](https://dashboard.boundaryml.com). Note that this is a different website from the current tracing/observability dashboard (app.boundaryml.com). We are working on unifying the two.\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/extract/summary.mdx"
      },
      "03-reference/extract/examples.mdx": {
        "markdown": "---\ntitle: Examples\n---\n\n### Upload a File (PDF, images)\n\n\n<CodeBlocks>\n\n```python title=\"Python\"\nimport requests\nfrom typing import List, Dict, Any\n\ndef extract_data(api_key: str, file_paths: List[str], prompt: str) -> Dict[str, Any]:\n    url = \"https://api2.boundaryml.com/v3/extract\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n    files = [('files', open(file_path, 'rb')) for file_path in file_paths]\n    data = {\n        'prompt': prompt\n    }\n    response = requests.post(url, headers=headers, files=files, data=data)\n    response.raise_for_status()\n    return response.json()\n\n# Usage example\napi_key = 'your_api_key_here'\nfile_paths = ['path/to/file1.pdf', 'path/to/file2.png']\nprompt = 'Please extract the text content.'\n\nresult = extract_data(api_key, file_paths, prompt)\nprint(result)\n```\n\n```typescript title=\"TypeScript\"\nimport axios, { AxiosResponse } from 'axios';\nimport * as FormData from 'form-data';\nimport * as fs from 'fs';\n\ninterface ExtractResponse {\n  extractions: Extraction[];\n  usage: Usage;\n  request_id: string;\n}\n\ninterface Extraction {\n  source: Source;\n  output: any;\n}\n\ninterface Source {\n  type: string;\n  name?: string;\n  page?: number;\n}\n\ninterface Usage {\n  consumed_chars: number;\n  produced_chars: number;\n  consumed_megapixels: number;\n}\n\nasync function extractData(apiKey: string, filePaths: string[], prompt: string): Promise<ExtractResponse> {\n  const url = 'https://api2.boundaryml.com/v3/extract';\n  const formData = new FormData();\n\n  filePaths.forEach(filePath => {\n    formData.append('files', fs.createReadStream(filePath));\n  });\n  formData.append('prompt', prompt);\n\n  const headers = {\n    ...formData.getHeaders(),\n    'Authorization': `Bearer ${apiKey}`,\n  };\n\n  const response: AxiosResponse<ExtractResponse> = await axios.post(url, formData, { headers });\n  return response.data;\n}\n\n// Usage example\nconst apiKey = 'your_api_key_here';\nconst filePaths = ['path/to/file1.pdf', 'path/to/file2.png'];\nconst prompt = 'Please extract the text content.';\n\nextractData(apiKey, filePaths, prompt)\n  .then(result => console.log(result))\n  .catch(error => console.error(error));\n```\n\n```ruby title=\"Ruby\"\nrequire 'net/http'\nrequire 'uri'\nrequire 'json'\n\ndef extract_data(api_key, file_paths, prompt)\n  uri = URI.parse('https://api2.boundaryml.com/v3/extract')\n  request = Net::HTTP::Post.new(uri)\n  request['Authorization'] = \"Bearer #{api_key}\"\n\n  form_data = [['prompt', prompt]]\n  file_paths.each do |file_path|\n    form_data << ['files', File.open(file_path)]\n  end\n\n  request.set_form(form_data, 'multipart/form-data')\n\n  req_options = {\n    use_ssl: uri.scheme == 'https',\n  }\n\n  response = Net::HTTP.start(uri.hostname, uri.port, req_options) do |http|\n    http.request(request)\n  end\n\n  if response.is_a?(Net::HTTPSuccess)\n    JSON.parse(response.body)\n  else\n    raise \"Request failed: #{response.code} #{response.message}\"\n  end\nend\n\n# Usage example\napi_key = 'your_api_key_here'\nfile_paths = ['path/to/file1.pdf', 'path/to/file2.png']\nprompt = 'Please extract the text content.'\n\nresult = extract_data(api_key, file_paths, prompt)\nputs result\n```\n\n```go title=\"Go\"\npackage main\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io\"\n\t\"mime/multipart\"\n\t\"net/http\"\n\t\"os\"\n)\n\ntype ExtractResponse struct {\n\tExtractions []Extraction `json:\"extractions\"`\n\tUsage       Usage        `json:\"usage\"`\n\tRequestID   string       `json:\"request_id\"`\n}\n\ntype Extraction struct {\n\tSource Source      `json:\"source\"`\n\tOutput interface{} `json:\"output\"`\n}\n\ntype Source struct {\n\tType string `json:\"type\"`\n\tName string `json:\"name,omitempty\"`\n\tPage int    `json:\"page,omitempty\"`\n}\n\ntype Usage struct {\n\tConsumedChars      int     `json:\"consumed_chars\"`\n\tProducedChars      int     `json:\"produced_chars\"`\n\tConsumedMegapixels float64 `json:\"consumed_megapixels\"`\n}\n\nfunc extractData(apiKey string, filePaths []string, prompt string) (ExtractResponse, error) {\n\turl := \"https://api2.boundaryml.com/v3/extract\"\n\tbody := &bytes.Buffer{}\n\twriter := multipart.NewWriter(body)\n\n\tfor _, filePath := range filePaths {\n\t\tfile, err := os.Open(filePath)\n\t\tif err != nil {\n\t\t\treturn ExtractResponse{}, err\n\t\t}\n\t\tdefer file.Close()\n\n\t\tpart, err := writer.CreateFormFile(\"files\", file.Name())\n\t\tif err != nil {\n\t\t\treturn ExtractResponse{}, err\n\t\t}\n\t\t_, err = io.Copy(part, file)\n\t\tif err != nil {\n\t\t\treturn ExtractResponse{}, err\n\t\t}\n\t}\n\n\t_ = writer.WriteField(\"prompt\", prompt)\n\terr := writer.Close()\n\tif err != nil {\n\t\treturn ExtractResponse{}, err\n\t}\n\n\treq, err := http.NewRequest(\"POST\", url, body)\n\tif err != nil {\n\t\treturn ExtractResponse{}, err\n\t}\n\treq.Header.Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", apiKey))\n\treq.Header.Set(\"Content-Type\", writer.FormDataContentType())\n\n\tclient := &http.Client{}\n\tresp, err := client.Do(req)\n\tif err != nil {\n\t\treturn ExtractResponse{}, err\n\t}\n\tdefer resp.Body.Close()\n\n\tif resp.StatusCode != http.StatusOK {\n\t\treturn ExtractResponse{}, fmt.Errorf(\"Request failed with status %s\", resp.Status)\n\t}\n\n\tvar extractResponse ExtractResponse\n\terr = json.NewDecoder(resp.Body).Decode(&extractResponse)\n\tif err != nil {\n\t\treturn ExtractResponse{}, err\n\t}\n\n\treturn extractResponse, nil\n}\n\nfunc main() {\n\tapiKey := \"your_api_key_here\"\n\tfilePaths := []string{\"path/to/file1.pdf\", \"path/to/file2.png\"}\n\tprompt := \"Please extract the text content.\"\n\n\tresult, err := extractData(apiKey, filePaths, prompt)\n\tif err != nil {\n\t\tfmt.Println(\"Error:\", err)\n\t\treturn\n\t}\n\n\tfmt.Printf(\"Result: %+v\\n\", result)\n}\n\n```\n\n</CodeBlocks>\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/extract/examples.mdx"
      },
      "03-reference/cloud/limits.mdx": {
        "markdown": "---\nslug: ref/cloud/limits\n---\n\n## Boundary Functions\n\n`baml_src`, per-project:\n\n- You may deploy up to 100MiB of `baml_src`.\n\nEnvironment variables, per-project:\n\n- You may have up to 1000 environment variables.\n- You may have up to 64KiB of data across all environment variables combined.\n\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/03-reference/cloud/limits.mdx"
      },
      "pages/changelog.mdx": {
        "markdown": "---\ntitle: Changelog\n---\n\nAll notable changes to this project will be documented in this file. See [conventional commits](https://www.conventionalcommits.org/) for commit guidelines.\n\n\n\n## [0.65.0](https://github.com/boundaryml/baml/compare/0.64.0..0.65.0) - 2024-10-31\n\n### Documentation\n\n- **New Documentation Structure**: Introduced version 3 of the documentation, enhancing clarity and organization. ([#1118](https://github.com/boundaryml/baml/commit/bab2767414172dd632437a57631c4cee04910518))\n\n\n### Bug Fixes\n\n- **Python Type Handling**: Moved Python Checked and Check types into `baml_client` for better type management. ([#1122](https://github.com/boundaryml/baml/commit/0ccf473fd821d25d431bbf4341c4e837967104bf))\n- **Literal Input Type Checking**: Fixed an issue where literal inputs were not being type-checked correctly. ([#1121](https://github.com/boundaryml/baml/commit/aa5dc85026a175216b5caae6320d09a1fcd35752))\n\n\n## [0.64.0](https://github.com/boundaryml/baml/compare/0.63.0..0.64.0) - 2024-10-29\n\n### Bug Fixes\n- **Playground Stability:** Prevented crashes in the playground due to malformed vertex credentials ([#1107](https://github.com/boundaryml/baml/commit/e665346fbc84a9b969a979cfdf1c70d530201e93)) - _Samuel Lijin_\n- **Union Handling:** Addressed an issue with union types in the schema ([#1096](https://github.com/boundaryml/baml/commit/cb5ce7623d3e95464fb5e5152c4d2339458caa26)) - _Greg Hale_\n- **WASM Function Signatures:** Resolved stack overflow when computing WASM function signatures ([#1100](https://github.com/boundaryml/baml/commit/aa736ed2d7386cae78421c22d5669c73d8921085)) - _aaronvg_\n- **VSCode Extension:** Fixed crashes in the VSCode extension that caused the output panel to open unexpectedly ([#1103](https://github.com/boundaryml/baml/commit/cb5a266bc68f15483f3ec3fa0f4edbc8d176287a)) - _hellovai_\n- **Static Analysis Improvements:** Enhanced static analysis on Jinja expressions and `regex_match` functions ([#1102](https://github.com/boundaryml/baml/commit/7ca8136ffbc690877091627415941674f6f14b2f), [#1104](https://github.com/boundaryml/baml/commit/83ddb1cfe81c9b5f6ae620c331c4eefe512c78bd)) - _hellovai_\n- **Codegen Enhancements:** Fixed code generation for Python boolean literals and updated integration tests ([#1099](https://github.com/boundaryml/baml/commit/635976238fd9246bfb8764875358a36b4ec6a7f5)) - _Antonio Sarosi_\n- **Enum Handling:** Improved substring alias handling for enums ([#1098](https://github.com/boundaryml/baml/commit/0c5cbd4ae03d2bc836ee4b61a7df638855bb72ca)) - _Miguel Cárdenas_\n- **Syntax Highlighting:** Refined span calculations for Jinja expressions and improved VSCode syntax highlighting with Lezer ([#1110](https://github.com/boundaryml/baml/commit/a53072f5fe9fe83a0accb36e43a06550602a3c65)) - _hellovai_\n- **Ruby Support:** Fixed literal boolean tests for Ruby ([#1109](https://github.com/boundaryml/baml/commit/23e590b0b2fdb51f80e7eced769baabd12b3be22)) - _Antonio Sarosi_\n\n### Features\n- **Constraint Support:** Added the ability to define constraints using Jinja expressions ([#1006](https://github.com/boundaryml/baml/commit/d794f28b4f8830b1a40cd08043ecdc562938d36e)) - _Greg Hale_\n- **VSCode & Fiddle UI:** Introduced a new \"Intro to Checks\" UI for easier onboarding ([#1106](https://github.com/boundaryml/baml/commit/11efa5e97f8e9b8f385b7fb0e823f5ff2bc4c314)) - _Samuel Lijin_\n- **Dev Container Configurations:** Added Dev Container configurations for streamlined development environments ([#1112](https://github.com/boundaryml/baml/commit/5790393d7ad320e9e257c09e461c9bc39310a834)) - _Antonio Sarosi_\n\n### Documentation\n- **Constraints Documentation:** Published new documentation for defining constraints in BAML ([#1113](https://github.com/boundaryml/baml/commit/6332021a59661d3931934adc2afbf4f99f6f4bee)) - _Greg Hale_\n- **Dynamic Types Linking:** Added cross-links to dynamic types documentation for easier navigation ([#1116](https://github.com/boundaryml/baml/commit/8ce0a539d74d05438e8047e4e02022ddd7121e21)) - _Greg Hale_\n\n### Miscellaneous\n- **Code Quality:** Improved style and fixed typos in the codebase ([#1115](https://github.com/boundaryml/baml/commit/4c3970a6e6ce998a784e682f4c218ba2a69cf86a)) - _Greg Hale_\n- **Parsing Stability:** Added logic to prevent assertions from parsing errors and ensured checks no longer affect parsing ([#1101](https://github.com/boundaryml/baml/commit/5ec89c92ab14622afddc3ce348c5b981b4840492)) - _hellovai_\n- **Version Bump:** Bumped version to 0.64.0 ([#1114](https://github.com/boundaryml/baml/commit/90d3c17ba67bc1467ee5973ff6cf257069e265b9), [#ff7e152](https://github.com/boundaryml/baml/commit/ff7e152510395bab1d38afa60211226070d12cc2)) - _Vaibhav Gupta_\n\n\n## [0.63.0](https://github.com/boundaryml/baml/compare/0.62.0..0.63.0) - 2024-10-23\n\n### Bug Fixes\n- Fix dynamic enums which already are defined in BAML (#1080) - ([22d0f1c](https://github.com/boundaryml/baml/commit/22d0f1cff3428c2cd58ea78c50c4fc7ea39c8d0c)) - hellovai\n\n### Features\n- Updated clients.baml to use the latest sonnet model (#1081) - ([71df0b7](https://github.com/boundaryml/baml/commit/71df0b7b627ba218d581d2c21be01fea4e4993c1)) - aaronvg\n- Improved clients.baml generated via baml init (#1089) - ([682dd66](https://github.com/boundaryml/baml/commit/682dd66f4adab8c4fad13bfe32a3fc0268d8b511)) - hellovai\n\n<hr/>\n## [0.62.0](https://github.com/boundaryml/baml/compare/0.61.1..0.62.0) - 2024-10-21\n\n### Features\n\n- Support serializing/deserializing `baml_py.Image`, `baml_py.Audio` for pydantic (#1062) - ([11cb699](https://github.com/boundaryml/baml/commit/11cb69903dce1ae348c68f88a82b4731da3977a7)) - Samuel Lijin\n- Support rendering input classes with aliases (#1045) - ([3824cda](https://github.com/boundaryml/baml/commit/3824cda75524105f3401e5c7e4c21e604d639f76)) - aaronvg\n- Add unstable_internal_repr on FunctionResult in python (#1068) - ([00082e8](https://github.com/boundaryml/baml/commit/00082e8b941d3648ec499215d2c38091f36db944)) - hellovai\n- Add literal support for type_builder (#1069) - ([c0085d9](https://github.com/boundaryml/baml/commit/c0085d908cbf8696623fd70f49de5ca8325de06c)) - hellovai\n\n### Bug Fixes\n\n- Surface errors in fallbacks containing only erroneous clients (#1061) - ([b69ef79](https://github.com/boundaryml/baml/commit/b69ef79542ec818b8779f9710dad65d33166c862)) - Greg Hale\n- Fix parser so that we are able to correctly detect sequences of empty strings. (#1048) - ([977e277](https://github.com/boundaryml/baml/commit/977e2776119a6f1e79f29cbe596b1c31697becb5)) - hellovai\n- Make substring match algorithm case insensitive (#1056) - ([fa2c477](https://github.com/boundaryml/baml/commit/fa2c4770791297a7a37a3f0c837ede4bb709f0ef)) - Antonio Sarosi\n- Fix vertex-ai citation data being optional (#1058) - ([5eae0a7](https://github.com/boundaryml/baml/commit/5eae0a73be6cc8286ce045185537aeed0b9feb7d)) - aaronvg\n- Fix bug to correctly cast to pydantic types in ambiguous scenarios where BAML knows better (#1059) - ([830b0cb](https://github.com/boundaryml/baml/commit/830b0cb194b99fa6f019928e7466dcf3e3992596)) - hellovai\n- Parser: Prefer case sensitive match over case insensitive (#1063) - ([cd6b141](https://github.com/boundaryml/baml/commit/cd6b141020ec8dfd2514c82ffffaebc5678a025b)) - Antonio Sarosi\n- Only popup the vscode env var dialog once (#1066) - ([1951474](https://github.com/boundaryml/baml/commit/19514745cfc8efeb8bda0be655e0fa2f216e4b29)) - aaronvg\n\n### Documentation\n\n- Docs for literal types (#1030) - ([55e5964](https://github.com/boundaryml/baml/commit/55e596419055c8da52b841b9ecbf16e328bc1033)) - Antonio Sarosi\n- Contribution guide (#1055) - ([f09d943](https://github.com/boundaryml/baml/commit/f09d9432d95c876f5e63f3abdb47a40417c5c45a)) - aaronvg\n\n### Misc\n\n- Fix VSCode metrics (#1044) - ([a131336](https://github.com/boundaryml/baml/commit/a13133656e1610cac9a92aa4b4459c78340c7304)) - hellovai\n- Add more test cases for unquoted strings in objects (#1054) - ([2d1b700](https://github.com/boundaryml/baml/commit/2d1b700e82604e444d904cfeb67f46ced97153a5)) - hellovai\n<hr/>\n\n## [0.61.1](https://github.com/boundaryml/baml/compare/0.61.0..0.61.1) - 2024-10-15\n\n### Bug Fixes\n\n- add musl to the ts release artifacts (#1042) - ([e74f3e9](https://github.com/boundaryml/baml/commit/e74f3e90489a403e38b39cc694d30d038ad38b8d)) - Samuel Lijin\n\n<hr/>\n\n## [0.61.0](https://github.com/boundaryml/baml/compare/0.60.0..0.61.0) - 2024-10-14\n\n### Features\n\n- Implement literal types (#978) - ([9e7431f](https://github.com/boundaryml/baml/commit/9e7431f43b74d4428e6a20b9aa3a1e93768ff905)) - Antonio Sarosi\n- allow installing the TS library on node-alpine (#1029) - ([1c37a0d](https://github.com/boundaryml/baml/commit/1c37a0d71d921d13f05340ff6727255ba6074152)) - Samuel Lijin\n- Add WYSIWYG UI (Swagger UI) to baml-cli dev (#1019) - ([0c73cab](https://github.com/boundaryml/baml/commit/0c73cab3d6ac3bbb04cc898ac102900ca9b17f86)) - Greg Hale\n- Suppress streaming for Numbers (#1032) - ([3f4621b](https://github.com/boundaryml/baml/commit/3f4621b36555062312aabd9ba8435b965ba8fd92)) - Greg Hale\n\n### Bug Fixes\n\n- Add limit on connection pool to prevent stalling issues in pyo3 and other ffi boundaries (#1027) - ([eb90e62](https://github.com/boundaryml/baml/commit/eb90e62ffe21109e0da1bd74439d36bb37246ec3)) - hellovai\n- Update docs (#1025) - ([2dd1bb6](https://github.com/boundaryml/baml/commit/2dd1bb6cf743c20af53d7147db8a4573de9cdbe0)) - Farookh Zaheer Siddiqui\n- Fix parsing for streaming of objects more stable (#1031) - ([8aa9c00](https://github.com/boundaryml/baml/commit/8aa9c00b8f26a8c30178ff25aecc1c3b47b6696e)) - hellovai\n- Fix python BamlValidationError type (#1036) - ([59a9510](https://github.com/boundaryml/baml/commit/59a9510c9d2c1216df01b0701cc23afb02e3f700)) - aaronvg\n\n### Miscellaneous\n\n- Popup settings dialog when no env vars set (#1033) - ([b9fa52a](https://github.com/boundaryml/baml/commit/b9fa52aea8686f8095878e7f210c2d937b533c63)) - aaronvg\n- Bump version to 0.61.0 - ([ca2242b](https://github.com/boundaryml/baml/commit/ca2242b26214699268fda9e9ac07338c6491026d)) - Aaron Villalpando\n\n## [0.60.0](https://github.com/boundaryml/baml/compare/0.59.0..0.60.0) - 2024-10-09\n\n### Miscellaneous Chores\n\n- update Dockerfile (#1017) - ([51539b7](https://github.com/boundaryml/baml/commit/51539b7b5778d6a3e6619698d2033d4f66f15d27)) - Ikko Eltociear Ashimine\n- Revert \"feat: add a WYSIWYG UI (Swagger UI) to `baml-cli dev` (#1011)\" (#1018) - ([f235050](https://github.com/boundaryml/baml/commit/f235050a57916116aff8359236b819ac69011a21)) - Greg Hale\n\n### Bug fixes\n\n- Fix python types for BamlValidationError (#1020) - ([520a09c](https://github.com/boundaryml/baml/commit/520a09c478ea8c5eb811447ce9b36689692aa01d)) - aaronvg\n- coerce floats and ints with commas and other special cases (#1023) - ([904492e](https://github.com/boundaryml/baml/commit/904492ee298727085e00a391beb628c8d999083e)) - aaronvg\n\n### Docs\n\n- Add Docs for Jupyter notebook usage (#1008) - ([c51d918](https://github.com/boundaryml/baml/commit/c51d918f76f63ce55b353661459ba3b27b9a0ea7)) - aaronvg\n\n## [0.59.0](https://github.com/boundaryml/baml/compare/0.58.0..0.59.0) - 2024-10-04\n\n### Features\n\n- **(vertex)** allow specifying creds as JSON object (#1009) - ([98868da](https://github.com/boundaryml/baml/commit/98868da4e75dde3a00178cbf60afebc501d37b0c)) - Samuel Lijin\n- Add prompt, raw_output and error message to BamlValidationError in TS and Python (#1005) - ([447dbf4](https://github.com/boundaryml/baml/commit/447dbf4e0d0cf0744307ef50f89050752334d982)) - aaronvg\n- Add BamlValidationError to `baml-cli serve` (#1007) - ([3b8cf16](https://github.com/boundaryml/baml/commit/3b8cf1636594c1a7245a733556efa690da40e139)) - aaronvg\n- Include a WYSIWYG UI (Swagger UI) to `baml-cli dev` (#1011) - ([fe9dde4](https://github.com/BoundaryML/baml/commit/fe9dde4f3a7ff0503fd13087da50e4da9d97c3a0)) - imalsogreg\n\n## [0.58.0](https://github.com/boundaryml/baml/compare/0.57.1..0.58.0) - 2024-10-02\n\n### Features\n\n- Add client registry support for BAML over Rest (OpenAPI) (#1000) - ([abe70bf](https://github.com/boundaryml/baml/commit/abe70bf368c9361a3ab32643735f68e0fafd8425)) - Lorenz Ohly\n\n### Bug Fixes\n\n- Improve performance of parsing escaped characters in strings during streaming. (#1002) - ([b35ae2c](https://github.com/boundaryml/baml/commit/b35ae2c4777572206a79af5c2943f5bdd6ada081)) - hellovai\n\n### Documentation\n\n- Add Docs for Document Extraction API (#996) - ([da1a5e8](https://github.com/boundaryml/baml/commit/da1a5e876368074235f4474673a1ebfe632e11ed)) - aaronvg\n\n## [0.57.1](https://github.com/boundaryml/baml/compare/0.57.0..0.57.1) - 2024-09-29\n\n### Bug Fixes\n\n- [BUGFIX] Parser should require a space between class keyword and class name (#990) - ([7528247](https://github.com/boundaryml/baml/commit/752824723404a4ed4c4b1e31c43d140e9346dca2)) - Greg Hale\n- Remove dynamic string attributes (#991) - ([0960ab2](https://github.com/boundaryml/baml/commit/0960ab2e0d16c50fef58772336b91297ddac6919)) - Greg Hale\n- ts fixes (#992) - ([36af43f](https://github.com/boundaryml/baml/commit/36af43f4f773e1565527916eff7d7837d9f8a983)) - aaronvg\n- Bump version to 0.57.1 - ([0aa71dd](https://github.com/boundaryml/baml/commit/0aa71dd4d3aa7082db6a19f0a3a976ff55789d83)) - Aaron Villalpando\n\n## [0.57.0](https://github.com/boundaryml/baml/compare/0.56.1..0.57.0) - 2024-09-27\n\n### Documentation\n\n- Fix Python dynamic types example (#979) - ([eade116](https://github.com/boundaryml/baml/commit/eade116de14bcc15d738fec911d8653685c13706)) - lorenzoh\n\n### Features\n\n- teach vscode/fiddle to explain when we drop information (#897) - ([93e2b9b](https://github.com/boundaryml/baml/commit/93e2b9b8d54a4ced0853ce72596d0b0a9896a0da)) - Samuel Lijin\n- Add ability for users to reset env vars to their desire. (#984) - ([69e6c29](https://github.com/boundaryml/baml/commit/69e6c29c82ccc06f8939b9ece75dd7797c8f6b98)) - hellovai\n\n### Bug Fixes\n\n- Fixed panic during logging for splitting on UTF-8 strings. (#987) - ([c27a64f](https://github.com/boundaryml/baml/commit/c27a64f6320515cd5ab6385ab93013d3d7ba88b8)) - hellovai\n- Improve SAP for triple quoted strings along with unions (#977) - ([44202ab](https://github.com/boundaryml/baml/commit/44202ab63aa3d2881485b9b32fa744797c908e33)) - hellovai\n- Add more unit tests for parsing logic inspired by user (#980) - ([48dd09f](https://github.com/boundaryml/baml/commit/48dd09f89b6447cbc1a539ecade66ab4da87b8dc)) - hellovai\n- Improve syntax errors e.g. class / enum parsing and also update pestmodel to handle traling comments (#981) - ([adbb6ae](https://github.com/boundaryml/baml/commit/adbb6ae38833d700bfe0123ac712cd90d7e4d970)) - hellovai\n- Updating docs for env vars (#985) - ([305d6b3](https://github.com/boundaryml/baml/commit/305d6b3e5a57513adc43c8ab9068c523dfc2e69c)) - hellovai\n- When using openai-generic, use a string as the content type in the api request if theres no media (#988) - ([e8fa739](https://github.com/boundaryml/baml/commit/e8fa739838cc124a8eed49103871b1b971063821)) - aaronvg\n\n## [0.56.1](https://github.com/boundaryml/baml/compare/0.56.0..0.56.1) - 2024-09-21\n\n### Bug Fixes\n\n- Improved parser for unions (#975) - ([b390521](https://github.com/boundaryml/baml/commit/b39052111529f217762b3271846006bec4a604de)) - hellovai\n- [syntax] Allow lists to contain trailing comma (#974) - ([9e3dc6c](https://github.com/boundaryml/baml/commit/9e3dc6c90954905a96b599ef28c40094fe48a43e)) - Greg Hale\n\n## [0.56.0](https://github.com/boundaryml/baml/compare/0.55.3..0.56.0) - 2024-09-20\n\nShout outs to Nico for fixing some internal Rust dependencies, and to Lorenz for correcting our documentation! We really appreciate it :)\n\n### Features\n\n- use better default for openapi/rust client (#958) - ([b74ef15](https://github.com/boundaryml/baml/commit/b74ef15fd4dc09ecc7d1ac8284e7f22cd6d5864c)) - Samuel Lijin\n\n### Bug Fixes\n\n- push optional-list and optional-map validation to post-parse (#959) - ([c0480d5](https://github.com/boundaryml/baml/commit/c0480d5cfd46ce979e957223dc7b5fa744778552)) - Samuel Lijin\n- improve OpenAPI instructions for windows/java (#962) - ([6010efb](https://github.com/boundaryml/baml/commit/6010efbb7990fda966640c3af267de41362d3fa4)) - Samuel Lijin\n- assorted fixes: unquoted strings, openai-generic add api_key for bearer auth, support escape characters in quoted strings (#965) - ([847f3a9](https://github.com/boundaryml/baml/commit/847f3a9bb0f00303eae7e410663efc63e54c38b6)) - hellovai\n- serde-serialize can cause a package dependency cycle (#967) - ([109ae09](https://github.com/boundaryml/baml/commit/109ae0914852f2ee4a771d27103e4e46ad672647)) - Nico\n- make anthropic work in fiddle/vscode (#970) - ([32eccae](https://github.com/boundaryml/baml/commit/32eccae44b27c3fec5fbc3270b6657819d75a426)) - Samuel Lijin\n- make dynamic enums work as outputs in Ruby (#972) - ([7530402](https://github.com/boundaryml/baml/commit/7530402f0dc063f10f57cf7aa7f06790574de705)) - Samuel Lijin\n\n### Documentation\n\n- suggest correct python init command in vscode readme (#954) - ([e99c5dd](https://github.com/boundaryml/baml/commit/e99c5dd1903078d08aef451e4addc6110d7ca279)) - Samuel Lijin\n- add more vscode debugging instructions (#955) - ([342b657](https://github.com/boundaryml/baml/commit/342b657da69441306fa7711d7d14893cf8036f84)) - Samuel Lijin\n- NextJS hook needs to be bound to the correct context (#957) - ([ee80451](https://github.com/boundaryml/baml/commit/ee80451de85063b37e658ba58571c791e8514273)) - aaronvg\n- update nextjs hooks and docs (#952) - ([01cf855](https://github.com/boundaryml/baml/commit/01cf855500159066fdcd162dc2e2087768d5ba28)) - aaronvg\n- Fix some documentation typos (#966) - ([5193cd7](https://github.com/boundaryml/baml/commit/5193cd70686173c863af5ce40fd6bb3792406951)) - Greg Hale\n- Keywords AI router (#953) - ([1c6f975](https://github.com/boundaryml/baml/commit/1c6f975d8cc793841745da0db82ee1e2f1908e56)) - aaronvg\n- Fix `post_generate` comment (#968) - ([919c79f](https://github.com/boundaryml/baml/commit/919c79fa8cd85a96e6559055b2bb436d925dcb2a)) - lorenzoh\n\n### Bug Fixes\n\n- show actionable errors for string[]? and map\\<...\\>? type validation (#946) - ([48879c0](https://github.com/boundaryml/baml/commit/48879c0744f79b482ef0d2b0624464053558ada4)) - Samuel Lijin\n\n### Documentation\n\n- add reference docs about env vars (#945) - ([dd43bc5](https://github.com/boundaryml/baml/commit/dd43bc59087e809e09ca7d3caf628e179a28fc3e)) - Samuel Lijin\n\n## [0.55.2](https://github.com/boundaryml/baml/compare/0.55.1..0.55.2) - 2024-09-11\n\n### Bug Fixes\n\n- use correct locking strategy inside baml-cli serve (#943) - ([fcb694d](https://github.com/boundaryml/baml/commit/fcb694d033317d8538cc7b2c61aaa94f772778db)) - Samuel Lijin\n\n### Features\n\n- allow using DANGER_ACCEPT_INVALID_CERTS to disable https verification (#901) - ([8873fe7](https://github.com/boundaryml/baml/commit/8873fe7577bc879cf0d550063252c4532dcdfced)) - Samuel Lijin\n\n## [0.55.1](https://github.com/boundaryml/baml/compare/0.55.0..0.55.1) - 2024-09-10\n\n### Bug Fixes\n\n- in generated TS code, put eslint-disable before ts-nocheck - ([16d04c6](https://github.com/BoundaryML/baml/commit/16d04c6e360eefca10b4e0d008b03c34de279491)) - Sam Lijin\n- baml-cli in python works again - ([b57ca0f](https://github.com/boundaryml/baml/commit/b57ca0f529c80f59b79b19132a8f1339a6b7bfe2)) - Sam Lijin\n\n### Documentation\n\n- update java install instructions (#933) - ([b497003](https://github.com/boundaryml/baml/commit/b49700356f2f69c4acbdc953a66a95224656ffaf)) - Samuel Lijin\n\n### Miscellaneous Chores\n\n- add version headers to the openapi docs (#931) - ([21545f2](https://github.com/boundaryml/baml/commit/21545f2a4d9b3987134d98ac720705dde2045290)) - Samuel Lijin\n\n## [0.55.0](https://github.com/boundaryml/baml/compare/0.54.2..0.55.0) - 2024-09-09\n\nWith this release, we're announcing support for BAML in all languages: we now\nallow you to call your functions over an HTTP interface, and will generate an\nOpenAPI specification for your BAML functions, so you can now generate a client\nin any language of your choice, be it Golang, Java, PHP, Ruby, Rust, or any of\nthe other languages which OpenAPI supports.\n\nStart here to learn more: https://docs.boundaryml.com/docs/get-started/quickstart/openapi\n\n### Features\n\n- implement BAML-over-HTTP (#908) - ([484fa93](https://github.com/boundaryml/baml/commit/484fa93a5a4b4677f531e6ef03bb88d144925c12)) - Samuel Lijin\n- Add anonymous telemetry about playground actions (#925) - ([6f58c9e](https://github.com/boundaryml/baml/commit/6f58c9e3e464a8e774771706c2b0d76adb9e6cda)) - hellovai\n\n## [0.54.2](https://github.com/boundaryml/baml/compare/0.54.1..0.54.2) - 2024-09-05\n\n### Features\n\n- Add a setting to disable restarting TS server in VSCode (#920) - ([628f236](https://github.com/boundaryml/baml/commit/628f2360c415fa8a7b0cd90d7249733ff06acaa9)) - aaronvg\n- Add prompt prefix for map types in ctx.output_format and add more type validation for map params (#919) - ([4d304c5](https://github.com/boundaryml/baml/commit/4d304c583b9188c1963a34e2a153baaf003e36ac)) - hellovai\n\n### Bug fixes\n\n- Fix glibC issues for python linux-x86_64 (#922) - ([9161bec](https://github.com/boundaryml/baml/commit/9161becccf626f8d13a15626481720f29e0f992c)) - Samuel Lijin\n\n### Documentation\n\n- Add nextjs hooks (#921) - ([fe14f5a](https://github.com/boundaryml/baml/commit/fe14f5a4ef95c9ccda916ff80ce852d3855554a3)) - aaronvg\n\n## [0.54.1](https://github.com/boundaryml/baml/compare/0.54.0..0.54.1) - 2024-09-03\n\n### BREAKING CHANGE\n\n- Fix escape characters in quoted strings (#905) - ([9ba6eb8](https://github.com/boundaryml/baml/commit/9ba6eb834e0145f4c57e582b63730d3d0ac9b2e9)) - hellovai\n\nPrior `\"\\n\"` was interpreted as `\"\\\\n\"` in quoted strings. This has been fixed to interpret `\"\\n\"` as newline characters and true for other escape characters.\n\n### Documentation\n\n- updated dead vs-code-extension link (#914) - ([b12f164](https://github.com/boundaryml/baml/commit/b12f1649cf5bfd0d457c5d6d117fd3a21ba5dc6b)) - Christian Warmuth\n- Update docs for setting env vars (#904) - ([ec1ca94](https://github.com/boundaryml/baml/commit/ec1ca94c91af2a51b4190a0bad0e0bc1c052f2a3)) - hellovai\n- Add docs for LMStudio (#906) - ([ea4c187](https://github.com/boundaryml/baml/commit/ea4c18782de1f713e8d69d473f9e1818c97024c6)) - hellovai\n- Fix docs for anthropic (#910) - ([aba2764](https://github.com/boundaryml/baml/commit/aba2764e5b04820d00b08bf52bda603ee27631f1)) - hellovai\n- Update discord links on docs (#911) - ([927357d](https://github.com/boundaryml/baml/commit/927357dd64b36c25513352ed4968ebc62dad6132)) - hellovai\n\n### Features\n\n- BAML_LOG will truncate messages to 1000 characters (modify using env var BOUNDARY_MAX_LOG_CHUNK_SIZE) (#907) - ([d266e5c](https://github.com/boundaryml/baml/commit/d266e5c4157f3b28d2f6454a7ea265dda7296bb2)) - hellovai\n\n### Bug Fixes\n\n- Improve parsing parsing when there are initial closing `]` or `}` (#903) - ([46b0cde](https://github.com/boundaryml/baml/commit/46b0cdeffb15bbab20a43728f52ad2a05623e6f7)) - hellovai\n- Update build script for ruby to build all platforms (#915) - ([df2f51e](https://github.com/boundaryml/baml/commit/df2f51e52615451b3643cc124e7262f11965f3ef)) - hellovai\n- Add unit-test for openai-generic provider and ensure it compiles (#916) - ([fde7c50](https://github.com/boundaryml/baml/commit/fde7c50c939c505906417596d16c7c4607173339)) - hellovai\n\n## [0.54.0](https://github.com/boundaryml/baml/compare/0.53.1..0.54.0) - 2024-08-27\n\n### BREAKING CHANGE\n\n- Update Default Gemini Base URL to v1beta (#891) - ([a5d8c58](https://github.com/boundaryml/baml/commit/a5d8c588e0fd0b7e186d7c71f1f6171334250629)) - gleed\n\nThe default base URL for the Gemini provider has been updated to v1beta. This change is should have no impact on existing users as v1beta is the default version for the Gemini python library, we are mirroring this change in BAML.\n\n### Bug Fixes\n\n- Allow promptfiddle to talk to localhost ollama (#886) - ([5f02b2a](https://github.com/boundaryml/baml/commit/5f02b2ac688ceeb5a34e848a8ff87fd43a6b093a)) - Samuel Lijin\n- Update Parser for unions so they handle nested objects better (#900) - ([c5b9a75](https://github.com/boundaryml/baml/commit/c5b9a75ea6da7c45da1999032e2b256bec97d922)) - hellovai\n\n### Documentation\n\n- Add ollama to default prompt fiddle example (#888) - ([49146c0](https://github.com/boundaryml/baml/commit/49146c0e50c88615e4cc97adb595849c23bad8ae)) - Samuel Lijin\n- Adding improved docs + unit tests for caching (#895) - ([ff7be44](https://github.com/boundaryml/baml/commit/ff7be4478b706da049085d432b2ec98627b5da1f)) - hellovai\n\n### Features\n\n- Allow local filepaths to be used in tests in BAML files (image and audio) (#871) - ([fa6dc03](https://github.com/boundaryml/baml/commit/fa6dc03fcdd3255dd83e25d0bfb3b0e740991408)) - Samuel Lijin\n- Add support for absolute file paths in the file specifier (#881) - ([fcd189e](https://github.com/boundaryml/baml/commit/fcd189ed7eb81712bf3b641eb3dde158fc6a62af)) - hellovai\n- Implement shorthand clients (You can now use \"openai/gpt-4o\" as short for creating a complete client.) (#879) - ([ddd15c9](https://github.com/boundaryml/baml/commit/ddd15c92c3e8d81c24cb7305c9fcbb36b819900f)) - Samuel Lijin\n- Add support for arbritrary metadata (e.g. cache_policy for anthropic) (#893) - ([0d63a70](https://github.com/boundaryml/baml/commit/0d63a70332477761a97783e203c98fd0bf67f151)) - hellovai\n- Expose Exceptions to user code: BamlError, BamlInvalidArgumentError, BamlClientError, BamlClientHttpError, BamlValidationError (#770) - ([7da14c4](https://github.com/boundaryml/baml/commit/7da14c480506e9791b3f4ce52ac73836a042d38a)) - hellovai\n\n### Internal\n\n- AST Restructuring (#857) - ([75b51cb](https://github.com/boundaryml/baml/commit/75b51cbf80a0c8ba19ae05b021ef3c94dacb4e30)) - Anish Palakurthi\n\n## [0.53.1](https://github.com/boundaryml/baml/compare/0.53.0..0.53.1) - 2024-08-11\n\n### Bug Fixes\n\n- fix github release not passing params to napi script causing issues in x86_64 (#872)\n\n- ([06b962b](https://github.com/boundaryml/baml/commit/06b962b945f958bf0637d13fec22bd2d59c64c5f)) - aaronvg\n\n### Features\n\n- Add Client orchestration graph in playground (#801) - ([24b5895](https://github.com/boundaryml/baml/commit/24b5895a1f45ac04cba0f19e6da727b5ee766186)) - Anish Palakurthi\n- increase range of python FFI support (#870) - ([ec9b66c](https://github.com/boundaryml/baml/commit/ec9b66c31faf97a58c81c264c7fa1b32e0e9f0ae)) - Samuel Lijin\n\n### Misc\n\n- Bump version to 0.53.1 - ([e4301e3](https://github.com/boundaryml/baml/commit/e4301e37835483f51edf1cad6478e46ff67508fc)) - Aaron Villalpando\n\n## [0.53.0](https://github.com/boundaryml/baml/compare/0.52.1..0.53.0) - 2024-08-05\n\n### Bug Fixes\n\n- make image[] render correctly in prompts (#855) - ([4a17dce](https://github.com/boundaryml/baml/commit/4a17dce43c05efd5f4ea304f2609fe140de1dd8c)) - Samuel Lijin\n\n### Features\n\n- **(ruby)** implement dynamic types, dynamic clients, images, and audio (#842) - ([4a21eed](https://github.com/boundaryml/baml/commit/4a21eed668f32b042fba61f24c9efb8b3794a420)) - Samuel Lijin\n- Codelenses for test cases (#812) - ([7cd8794](https://github.com/boundaryml/baml/commit/7cd87942bf50a72de0ad46154f164fb2c174f25b)) - Anish Palakurthi\n\n### Issue\n\n- removed vertex auth token printing (#846) - ([b839316](https://github.com/boundaryml/baml/commit/b83931665a2c3b840eb6c6d31cf3d01c7926e52e)) - Anish Palakurthi\n- Fix google type deserialization issue - ([a55b9a1](https://github.com/boundaryml/baml/commit/a55b9a106176ed1ce34bb63397610c2640b37f16)) - Aaron Villalpando\n\n### Miscellaneous Chores\n\n- clean up release stuff (#836) - ([eed41b7](https://github.com/boundaryml/baml/commit/eed41b7474417d2e65b2c5d742234cc20fc5644e)) - Samuel Lijin\n- Add bfcl results to readme, fix links icons (#856) - ([5ef7f3d](https://github.com/boundaryml/baml/commit/5ef7f3db99d8d23ff97f1e8372ee71ab7aa127aa)) - aaronvg\n- Fix prompt fiddle and playground styles, add more logging, and add stop-reason to playground (#858) - ([38e3153](https://github.com/boundaryml/baml/commit/38e3153843a17ae1e87ae9879ab4374b083d77d0)) - aaronvg\n- Bump version to 0.53.0 - ([fd16839](https://github.com/boundaryml/baml/commit/fd16839a2c0b9d92bd5bdcb57f950e22d0a29959)) - Aaron Villalpando\n\n## [0.52.1](https://github.com/boundaryml/baml/compare/0.52.0..0.52.1) - 2024-07-24\n\n### Bug Fixes\n\n- build python x86_64-linux with an older glibc (#834) - ([db12540](https://github.com/boundaryml/baml/commit/db12540a92abf055e286c60864299f53c246b62a)) - Samuel Lijin\n\n## [0.52.0](https://github.com/boundaryml/baml/compare/0.51.3..0.52.0) - 2024-07-24\n\n### Features\n\n- Add official support for ruby (#823) - ([e81cc79](https://github.com/boundaryml/baml/commit/e81cc79498809a79f427864704b140967a41277a)) - Samuel Lijin\n\n### Bug Fixes\n\n- Fix ClientRegistry for Typescript code-gen (#828) - ([b69921f](https://github.com/boundaryml/baml/commit/b69921f45df0182072b09ab28fe6231ccfaa5767)) - hellovai\n\n## [0.51.2](https://github.com/boundaryml/baml/compare/0.51.1..0.51.2) - 2024-07-24\n\n### Features\n\n- Add support for unions / maps / null in TypeBuilder. (#820) - ([8d9e92d](https://github.com/boundaryml/baml/commit/8d9e92d3050a67edbec5ee6056397becbcdb754b)) - hellovai\n\n### Bug Fixes\n\n- [Playground] Add a feedback button (#818) - ([f749f2b](https://github.com/boundaryml/baml/commit/f749f2b19b247de2f050beccd1fe8e50b7625757)) - Samuel Lijin\n\n### Documentation\n\n- Improvements across docs (#807) - ([bc0c176](https://github.com/boundaryml/baml/commit/bc0c1761699ee2485a0a8ee61cf4fda6b579f974)) - Anish Palakurthi\n\n## [0.51.1](https://github.com/boundaryml/baml/compare/0.51.0..0.51.1) - 2024-07-21\n\n### Features\n\n- Add a feedback button to VSCode Extension (#811) - ([f371912](https://github.com/boundaryml/baml/commit/f3719127174d8f998579747f14fae8675dafba4c)) - Samuel Lijin\n\n### Bug\n\n- Allow default_client_mode in the generator #813 (#815) - ([6df7fca](https://github.com/boundaryml/baml/commit/6df7fcabc1eb55b08a50741f2346440f631abd63)) - hellovai\n\n## [0.51.0](https://github.com/boundaryml/baml/compare/0.50.0..0.51.0) - 2024-07-19\n\n### Bug Fixes\n\n- Improve BAML Parser for numbers and single-key objects (#785) - ([c5af7b0](https://github.com/boundaryml/baml/commit/c5af7b0d0e881c3046171ca17f317d820e8882e3)) - hellovai\n- Add docs for VLLM (#792) - ([79e8773](https://github.com/boundaryml/baml/commit/79e8773e38da524795dda606b9fae09a274118e1)) - hellovai\n- LLVM install and rebuild script (#794) - ([9ee66ed](https://github.com/boundaryml/baml/commit/9ee66ed2dd14bc0ee12a788f41eae64377e7f2b0)) - Anish Palakurthi\n- Prevent version mismatches when generating baml_client (#791) - ([d793603](https://github.com/boundaryml/baml/commit/d7936036e6afa4a0e738242cfb3feaa9e15b3657)) - aaronvg\n- fiddle build fix (#800) - ([d304203](https://github.com/boundaryml/baml/commit/d304203241726ac0ba8781db7ac5693339189eb4)) - aaronvg\n- Dont drop extra fields in dynamic classes when passing them as inputs to a function (#802) - ([4264c9b](https://github.com/boundaryml/baml/commit/4264c9b143edda0239af197d110357b1969bf12c)) - aaronvg\n- Adding support for a sync client for Python + Typescript (#803) - ([62085e7](https://github.com/boundaryml/baml/commit/62085e79d4d86f580ce189bc60f36bd1414893c4)) - hellovai\n- Fix WASM-related issues introduced in #803 (#804) - ([0a950e0](https://github.com/boundaryml/baml/commit/0a950e084748837ee2e269504d22dba66f339ca4)) - hellovai\n- Adding various fixes (#806) - ([e8c1a61](https://github.com/boundaryml/baml/commit/e8c1a61a96051160566b6458dac5c89d5ddfb86e)) - hellovai\n\n### Features\n\n- implement maps in BAML (#797) - ([97d7e62](https://github.com/boundaryml/baml/commit/97d7e6223c68e9c338fe7110554f1f26b966f7e3)) - Samuel Lijin\n- Support Vertex AI (Google Cloud SDK) (#790) - ([d98ee81](https://github.com/boundaryml/baml/commit/d98ee81a9440de0aaa6de05b33b8d3f709003a00)) - Anish Palakurthi\n- Add copy buttons to test results in playground (#799) - ([b5eee3d](https://github.com/boundaryml/baml/commit/b5eee3d15a1be4373e25cc8ef1cf6e70d5dd39c9)) - aaronvg\n\n### Miscellaneous Chores\n\n- in fern config, defer to installed version (#789) - ([479f1b2](https://github.com/boundaryml/baml/commit/479f1b2b0b52faf47bc529e4c06c533a9467269a)) - fern\n- publish docs on every push to the default branch (#796) - ([180824a](https://github.com/boundaryml/baml/commit/180824a3857a32eae679e4df5704abba3aa6246c)) - Samuel Lijin\n- 🌿 introducing fern docs (#779) - ([46f06a9](https://github.com/boundaryml/baml/commit/46f06a95a1e262e62476768b812b372b696da1be)) - fern\n- Add test for dynamic list input (#798) - ([7528d6a](https://github.com/boundaryml/baml/commit/7528d6ae10427c1304e356cf5b3c664e4fb2b1b1)) - aaronvg\n\n## [0.50.0](https://github.com/boundaryml/baml/compare/0.49.0..0.50.0) - 2024-07-11\n\n### Bug Fixes\n\n- [Playground] Environment variable button is now visible on all themes (#762) - ([adc4da1](https://github.com/boundaryml/baml/commit/adc4da1fa36cc9c30ea36e25de1a6cefcce0bc97)) - aaronvg\n- [Playground] Fix to cURL rendering and mime_type overriding (#763) - ([67f9c6a](https://github.com/boundaryml/baml/commit/67f9c6add5ea8bbbd5ee82c28476fe0ebbefe344)) - Anish Palakurthi\n\n### Features\n\n- [Runtime] Add support for clients that change at runtime using ClientRegistry (#683) - ([c0fb454](https://github.com/boundaryml/baml/commit/c0fb4540d9193194fcafd7fcef71468442d9e6fa)) - hellovai\n  https://docs.boundaryml.com/docs/calling-baml/client-registry\n\n### Documentation\n\n- Add more documentation for TypeBuilder (#767) - ([85dc8ab](https://github.com/boundaryml/baml/commit/85dc8ab41e0df3267249a1efc4a95f010e52cc73)) - Samuel Lijin\n\n## [0.49.0](https://github.com/boundaryml/baml/compare/0.46.0..0.49.0) - 2024-07-08\n\n### Bug Fixes\n\n- Fixed Azure / Ollama clients. Removing stream_options from azure and ollama clients (#760) - ([30bf88f](https://github.com/boundaryml/baml/commit/30bf88f65c8583ab02db6a7b7db40c1e9f3b05b6)) - hellovai\n\n### Features\n\n- Add support for arm64-linux (#751) - ([adb8ee3](https://github.com/boundaryml/baml/commit/adb8ee3097fd386370f75b3ba179d18b952e9678)) - Samuel Lijin\n\n## [0.48.0](https://github.com/boundaryml/baml/compare/0.47.0..0.48.0) - 2024-07-04\n\n### Bug Fixes\n\n- Fix env variables dialoge on VSCode (#750)\n- Playground selects correct function after loading (#757) - ([09963a0](https://github.com/boundaryml/baml/commit/09963a02e581da9eb8f7bafd3ba812058c97f672)) - aaronvg\n\n### Miscellaneous Chores\n\n- Better error messages on logging failures to Boundary Studio (#754) - ([49c768f](https://github.com/boundaryml/baml/commit/49c768fbe8eb8023cba28b8dc68c2553d8b2318a)) - aaronvg\n\n## [0.47.0](https://github.com/boundaryml/baml/compare/0.46.0..0.47.0) - 2024-07-03\n\n### Bug Fixes\n\n- make settings dialog work in vscode again (#750) ([c94e355](https://github.com/boundaryml/baml/commit/c94e35551872f65404136b60f800fb1688902c11)) - aaronvg\n- restore releases on arm64-linux (#751) - ([adb8ee3](https://github.com/boundaryml/baml/commit/adb8ee3097fd386370f75b3ba179d18b952e9678)) - Samuel Lijin\n\n## [0.46.0](https://github.com/boundaryml/baml/compare/0.45.0..0.46.0) - 2024-07-03\n\n### Bug Fixes\n\n- Fixed tracing issues for Boundary Studio (#740) - ([77a4db7](https://github.com/boundaryml/baml/commit/77a4db7ef4b939636472ad4975d74e9d1a577cbf)) - Samuel Lijin\n- Fixed flush() to be more reliable (#744) - ([9dd5fda](https://github.com/boundaryml/baml/commit/9dd5fdad5c2897b49a5a536df2e9ef775857a39d)) - Samuel Lijin\n- Remove error when user passes in extra fields in a class (#746) - ([2755b43](https://github.com/boundaryml/baml/commit/2755b43257f9405ae66a30982d9711fc3f2c0854)) - aaronvg\n\n### Features\n\n- Add support for base_url for the google-ai provider (#747) - ([005b1d9](https://github.com/boundaryml/baml/commit/005b1d93b7f7d2aa12a1487911766cccd9c25e98)) - hellovai\n- Playground UX improvements (#742) - ([5cb56fd](https://github.com/boundaryml/baml/commit/5cb56fdc39496f0aedacd79766c0e93cb0e401b8)) - hellovai\n- Prompt Fiddle now auto-switches functions when to change files (#745)\n\n### Documentation\n\n- Added a large example project on promptfiddle.com (#741) - ([f80da1e](https://github.com/boundaryml/baml/commit/f80da1e1dd11f0457b5789bc9ce6923a8ed88b51)) - aaronvg\n- Mark ruby as in beta (#743) - ([901109d](https://github.com/boundaryml/baml/commit/901109dbb327e6e3e1b65fda37100fcd45f97e07)) - Samuel Lijin\n\n## [0.45.0](https://github.com/boundaryml/baml/compare/0.44.0..0.45.0) - 2024-06-29\n\n### Bug Fixes\n\n- Fixed streaming in Python Client which didn't show result until later (#726) - ([e4f2daa](https://github.com/boundaryml/baml/commit/e4f2daa9e85bb1711d112fb0c87c0d769be0bb2d)) - Anish Palakurthi\n- Improve playground stability on first load (#732) - ([2ac7b32](https://github.com/boundaryml/baml/commit/2ac7b328e89400cba0d9eb4f6d09c6a03feb71a5)) - Anish Palakurthi\n- Add improved static analysis for jinja (#734) - ([423faa1](https://github.com/boundaryml/baml/commit/423faa1af5a594b7f78f7bb5620e3146a8989da5)) - hellovai\n\n### Documentation\n\n- Docs for Dynamic Types (#722) [https://docs.boundaryml.com/docs/calling-baml/dynamic-types](https://docs.boundaryml.com/docs/calling-baml/dynamic-types)\n\n### Features\n\n- Show raw cURL request in Playground (#723) - ([57928e1](https://github.com/boundaryml/baml/commit/57928e178549cb3e5118ce374aab5d0fbad7038b)) - Anish Palakurthi\n- Support bedrock as a provider (#725) - ([c64c665](https://github.com/boundaryml/baml/commit/c64c66522a1d496493a30f593103209acd201364)) - Samuel Lijin\n\n## [0.44.0](https://github.com/boundaryml/baml/compare/0.43.0..0.44.0) - 2024-06-26\n\n### Bug Fixes\n\n- Fix typebuilder for random enums (#721)\n\n## [0.43.0](https://github.com/boundaryml/baml/compare/0.42.0..0.43.0) - 2024-06-26\n\n### Bug Fixes\n\n- fix pnpm lockfile issue (#720)\n\n## [0.42.0](https://github.com/boundaryml/baml/compare/0.41.0..0.42.0) - 2024-06-26\n\n### Bug Fixes\n\n- correctly propagate LICENSE to baml-py (#695) - ([3fda880](https://github.com/boundaryml/baml/commit/3fda880bf39b32191b425ae75e8b491d10884cf6)) - Samuel Lijin\n\n### Miscellaneous Chores\n\n- update jsonish readme (#685) - ([b19f04a](https://github.com/boundaryml/baml/commit/b19f04a059ba18d54544cb278b6990b95170d3f3)) - Samuel Lijin\n\n### Vscode\n\n- add link to tracing, show token counts (#703) - ([64aa18a](https://github.com/boundaryml/baml/commit/64aa18a9cc34071655141c8f6e2ad04ac90e7be1)) - Samuel Lijin\n\n## [0.41.0] - 2024-06-20\n\n### Bug Fixes\n\n- rollback git lfs, images broken in docs rn (#534) - ([6945506](https://github.com/boundaryml/baml/commit/694550664fa45b5f76987e2663c9d7e7a9a6a2d2)) - Samuel Lijin\n- search for markdown blocks correctly (#641) - ([6b8abf1](https://github.com/boundaryml/baml/commit/6b8abf1ccf55bbe7c3bc1046c78081126e01f134)) - Samuel Lijin\n- restore one-workspace-per-folder (#656) - ([a464bde](https://github.com/boundaryml/baml/commit/a464bde566199ace45285a78a7f542cd7217fb65)) - Samuel Lijin\n- ruby generator should be ruby/sorbet (#661) - ([0019f39](https://github.com/boundaryml/baml/commit/0019f3951b8fe2b49e62eb11d869516b8088e9cb)) - Samuel Lijin\n- ruby compile error snuck in (#663) - ([0cb2583](https://github.com/boundaryml/baml/commit/0cb25831788eb8b3eb0a38383917f6d1ffb5633a)) - Samuel Lijin\n\n### Documentation\n\n- add typescript examples (#477) - ([532481c](https://github.com/boundaryml/baml/commit/532481c3df4063b37a8834a5fe2bbce3bb37d2f5)) - Samuel Lijin\n- add titles to code blocks for all CodeGroup elems (#483) - ([76c6b68](https://github.com/boundaryml/baml/commit/76c6b68b27ee37972fa226be0b4dfe31f7b4b5ec)) - Samuel Lijin\n- add docs for round-robin clients (#500) - ([221f902](https://github.com/boundaryml/baml/commit/221f9020d850e6d24fe2fd8a684081726a0659af)) - Samuel Lijin\n- add ruby example (#689) - ([16e187f](https://github.com/boundaryml/baml/commit/16e187f6698a1cc86a37eedf2447648d810370ad)) - Samuel Lijin\n\n### Features\n\n- implement `baml version --check --output json` (#444) - ([5f076ac](https://github.com/boundaryml/baml/commit/5f076ace1f92dc2141b231c9e62f4dc23f7fef18)) - Samuel Lijin\n- show update prompts in vscode (#451) - ([b66da3e](https://github.com/boundaryml/baml/commit/b66da3ee355fcd6a8677d834ecb05af44cbf8f20)) - Samuel Lijin\n- add tests to check that baml version --check works (#454) - ([be1499d](https://github.com/boundaryml/baml/commit/be1499dfa82ff8ab923a16d45290758120d95015)) - Samuel Lijin\n- parse typescript versions in version --check (#473) - ([b4b2250](https://github.com/boundaryml/baml/commit/b4b2250c37b900db899256159bbfc3aa2ec819cb)) - Samuel Lijin\n- implement round robin client strategies (#494) - ([599fcdd](https://github.com/boundaryml/baml/commit/599fcdd2a45c5b1e935f36769784ca944566b88c)) - Samuel Lijin\n- add integ-tests support to build (#542) - ([f59cf2e](https://github.com/boundaryml/baml/commit/f59cf2e1a9ec7edbe174f4bc7ff9391f2cff3208)) - Samuel Lijin\n- make ruby work again (#650) - ([6472bec](https://github.com/boundaryml/baml/commit/6472bec231b581076ee7edefaab2e7979b2bf336)) - Samuel Lijin\n- Add RB2B tracking script (#682) - ([54547a3](https://github.com/boundaryml/baml/commit/54547a34d40cd40a43767919dbc9faa68a82faea)) - hellovai\n\n### Miscellaneous Chores\n\n- add nodemon config to typescript/ (#435) - ([231b396](https://github.com/boundaryml/baml/commit/231b3967bc947c4651156bc55fd66552782824c9)) - Samuel Lijin\n- finish gloo to BoundaryML renames (#452) - ([88a7fda](https://github.com/boundaryml/baml/commit/88a7fdacc826e78ef21c6b24745ee469d9d02e6a)) - Samuel Lijin\n- set up lfs (#511) - ([3a43143](https://github.com/boundaryml/baml/commit/3a431431e8e38dfc68763f15ccdcd1d131f23984)) - Samuel Lijin\n- add internal build tooling for sam (#512) - ([9ebacca](https://github.com/boundaryml/baml/commit/9ebaccaa542760cb96382ae2a91d780f1ade613b)) - Samuel Lijin\n- delete clients dir, this is now dead code (#652) - ([ec2627f](https://github.com/boundaryml/baml/commit/ec2627f59c7fe9edfff46fcdb65f9b9f0e2e072c)) - Samuel Lijin\n- consolidate vscode workspace, bump a bunch of deps (#654) - ([82bf6ab](https://github.com/boundaryml/baml/commit/82bf6ab1ad839f84782a7ef0441f21124c368757)) - Samuel Lijin\n- Add RB2B tracking script to propmt fiddle (#681) - ([4cf806b](https://github.com/boundaryml/baml/commit/4cf806bba26563fd8b6ddbd68296ab8bdfac21c4)) - hellovai\n- Adding better release script (#688) - ([5bec282](https://github.com/boundaryml/baml/commit/5bec282d39d2250b39ef4aba5d6bba9830a35988)) - hellovai\n\n### [AUTO\n\n- patch] Version bump for nightly release [NIGHTLY:cli] [NIGHTLY:vscode_ext] [NIGHTLY:client-python] - ([d05a22c](https://github.com/boundaryml/baml/commit/d05a22ca4135887738adbce638193d71abca42ec)) - GitHub Action\n\n### Build\n\n- fix baml-core-ffi script (#521) - ([b1b7f4a](https://github.com/boundaryml/baml/commit/b1b7f4af0991ef6453f888f27930f3faaae337f5)) - Samuel Lijin\n- fix engine/ (#522) - ([154f646](https://github.com/boundaryml/baml/commit/154f6468ec0aa6de1b033ee1cbc76e60acc363ea)) - Samuel Lijin\n\n### Integ-tests\n\n- add ruby test - ([c0bc101](https://github.com/boundaryml/baml/commit/c0bc10126ea32d099f1398f2c5faa08b111554ba)) - Sam Lijin\n\n### Readme\n\n- add function calling, collapse the table (#505) - ([2f9024c](https://github.com/boundaryml/baml/commit/2f9024c28ba438267de37ac43c6570a2f0398b5a)) - Samuel Lijin\n\n### Release\n\n- bump versions for everything (#662) - ([c0254ae](https://github.com/boundaryml/baml/commit/c0254ae680365854c51c7a4e58ea68d1901ea033)) - Samuel Lijin\n\n### Vscode\n\n- check for updates on the hour (#434) - ([c70a3b3](https://github.com/boundaryml/baml/commit/c70a3b373cb2346a0df9a1eba0ebacb74d59b53e)) - Samuel Lijin\n",
        "editThisPageUrl": "https://github.com/BoundaryML/baml/blob/canary/fern/pages/changelog.mdx"
      }
    },
    "search": {
      "type": "singleAlgoliaIndex",
      "value": {
        "type": "unversioned",
        "indexSegment": {
          "id": "seg_boundary.docs.buildwithfern.com_2237c142-299b-41ae-8913-bd307e22266d",
          "searchApiKey": "N2UwY2E5YjMyZDU0ZDZhMWI2YTMwYzk1OTAyNmI2YWI1NTI3M2I2Njg1ZWE2ZGY4YjQwNjlhNzQ0YmQ0ZGFmNmZpbHRlcnM9aW5kZXhTZWdtZW50SWQlM0FzZWdfYm91bmRhcnkuZG9jcy5idWlsZHdpdGhmZXJuLmNvbV8yMjM3YzE0Mi0yOTliLTQxYWUtODkxMy1iZDMwN2UyMjI2NmQmdmFsaWRVbnRpbD0xNzMwNzg5NTQ2"
        }
      }
    },
    "id": "docs_definition_5f3a522d-7cec-416f-ae5f-0a3cc47cb188"
  },
  "lightModeEnabled": true
}
